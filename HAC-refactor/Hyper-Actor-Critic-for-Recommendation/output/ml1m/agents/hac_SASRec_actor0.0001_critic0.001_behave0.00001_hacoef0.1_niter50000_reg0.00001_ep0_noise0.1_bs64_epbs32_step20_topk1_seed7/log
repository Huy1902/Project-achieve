Namespace(env_class='ML1MEnvironment_GPU', policy_class='SASRec', critic_class='GeneralCritic', agent_class='HAC', facade_class='OneStageFacade_HyperAction')
Loading environment
Environment arguments: 
Namespace(seed=19, batch_size=128, lr=0.001, epoch=2, model_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.model', loss='bce', l2_coef=0.0003, feature_dim=16, attn_n_head=2, hidden_dims=[256], dropout_rate=0.2, train_file='dataset/ml1m/ml1m_b_train.csv', val_file='dataset/ml1m/ml1m_b_test.csv', test_file='', n_worker=0, data_separator='@', user_meta_file='dataset/ml1m/user_info.npy', item_meta_file='dataset/ml1m/item_info.npy', max_seq_len=50, meta_data_separator=' ')
Loading raw data
init ml1m reader
Loading data filesLoad item meta data
Loading user response model
{'length': 5078, 'n_item': 1682, 'item_vec_size': 19, 'user_portrait_len': 24, 'max_seq_len': 50}
Load (checkpoint) from output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.model.checkpoint
Setup policy:
SASRec(
  (item_map): Linear(in_features=19, out_features=32, bias=True)
  (pos_emb): Embedding(50, 32)
  (emb_dropout): Dropout(p=0.1, inplace=False)
  (emb_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  (transformer): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
Setup critic:
GeneralCritic(
  (net): DNN(
    (layers): Sequential(
      (0): Linear(in_features=64, out_features=256, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.2, inplace=False)
      (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (4): Linear(in_features=256, out_features=64, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.2, inplace=False)
      (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (8): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Setup agent with data-specific facade
Namespace(seed=7, cuda=-1, env_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.env', reward_func='mean_with_cost', max_step_per_episode=20, initial_temper=20, urm_log_path='output/ml1m/env/log/ml1m_user_env_lr0.001_reg0.0003.model.log', temper_sweet_point=0.9, temper_prob_lag=100, sasrec_n_layer=2, sasrec_d_model=32, sasrec_d_forward=64, sasrec_n_head=4, sasrec_dropout=0.1, critic_hidden_dims=[256, 64], critic_dropout_rate=0.2, gamma=0.9, n_iter=[50000], train_every_n_step=1, initial_greedy_epsilon=0.0, final_greedy_epsilon=0.0, elbow_greedy=0.1, check_episode=10, with_eval=False, save_path='output/ml1m/agents/hac_SASRec_actor0.0001_critic0.001_behave0.00001_hacoef0.1_niter50000_reg0.00001_ep0_noise0.1_bs64_epbs32_step20_topk1_seed7/model', episode_batch_size=32, batch_size=64, actor_lr=0.0001, critic_lr=0.001, actor_decay=1e-05, critic_decay=1e-05, target_mitigate_coef=0.01, behavior_lr=1e-05, behavior_decay=1e-05, hyper_actor_coef=0.1, slate_size=9, buffer_size=100000, start_timestamp=2000, noise_var=0.1, q_laplace_smoothness=0.5, topk_rate=1.0, empty_start_rate=0.0, device='cpu')
Run procedures before training
Total 63 prepare steps
Training:
Episode step 0, time diff 0.24844789505004883, total time dif 0.0)
step: 0 @ episode report: {'average_total_reward': np.float64(0.0), 'reward_variance': np.float64(0.0), 'max_total_reward': np.float64(0.0), 'min_total_reward': np.float64(0.0), 'average_n_step': np.float64(0.0), 'max_n_step': np.float64(0.0), 'min_n_step': np.float64(0.0), 'buffer_size': 2048} @ step loss: {'critic_loss': np.float64(0.2494354546070099), 'actor_loss': np.float64(0.09359893202781677), 'hyper_actor_loss': np.float64(0.0891341045498848), 'behavior_loss': np.float64(1.052399754524231)}

Episode step 10, time diff 0.949378252029419, total time dif 0.24844789505004883)
step: 10 @ episode report: {'average_total_reward': np.float32(0.27999997), 'reward_variance': np.float32(0.07224198), 'max_total_reward': np.float32(0.6777778), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.7), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2368} @ step loss: {'critic_loss': np.float64(0.16893137395381927), 'actor_loss': np.float64(-0.04940007347613573), 'hyper_actor_loss': np.float64(0.08924866989254951), 'behavior_loss': np.float64(1.1810205936431886)}

Episode step 20, time diff 0.8167891502380371, total time dif 1.1978261470794678)
step: 20 @ episode report: {'average_total_reward': np.float32(0.4777778), 'reward_variance': np.float32(0.13039507), 'max_total_reward': np.float32(1.1666667), 'min_total_reward': np.float32(-0.077777795), 'average_n_step': np.float32(2.8), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2688} @ step loss: {'critic_loss': np.float64(0.11232584789395332), 'actor_loss': np.float64(-0.0643725786358118), 'hyper_actor_loss': np.float64(0.08951787650585175), 'behavior_loss': np.float64(1.2304974317550659)}

Episode step 30, time diff 1.1844215393066406, total time dif 2.014615297317505)
step: 30 @ episode report: {'average_total_reward': np.float32(0.6166667), 'reward_variance': np.float32(0.048549395), 'max_total_reward': np.float32(1.0444446), 'min_total_reward': np.float32(0.31111112), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(3.0), 'buffer_size': 3008} @ step loss: {'critic_loss': np.float64(0.07615532241761684), 'actor_loss': np.float64(-0.13623821809887887), 'hyper_actor_loss': np.float64(0.08950287625193595), 'behavior_loss': np.float64(1.2225734353065492)}

Episode step 40, time diff 1.1103589534759521, total time dif 3.1990368366241455)
step: 40 @ episode report: {'average_total_reward': np.float32(0.96111107), 'reward_variance': np.float32(0.24756172), 'max_total_reward': np.float32(1.8), 'min_total_reward': np.float32(0.044444427), 'average_n_step': np.float32(3.1), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(2.0), 'buffer_size': 3328} @ step loss: {'critic_loss': np.float64(0.06856036446988582), 'actor_loss': np.float64(-0.2286664843559265), 'hyper_actor_loss': np.float64(0.08914055079221725), 'behavior_loss': np.float64(1.2156113266944886)}

Episode step 50, time diff 1.0303876399993896, total time dif 4.309395790100098)
step: 50 @ episode report: {'average_total_reward': np.float32(1.448889), 'reward_variance': np.float32(0.15042469), 'max_total_reward': np.float32(2.0444446), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 3648} @ step loss: {'critic_loss': np.float64(0.05844113975763321), 'actor_loss': np.float64(-0.40875521004199983), 'hyper_actor_loss': np.float64(0.08805120065808296), 'behavior_loss': np.float64(1.1782491683959961)}

Episode step 60, time diff 0.9547936916351318, total time dif 5.339783430099487)
step: 60 @ episode report: {'average_total_reward': np.float32(1.2633334), 'reward_variance': np.float32(0.33516175), 'max_total_reward': np.float32(2.0444446), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.5), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 3968} @ step loss: {'critic_loss': np.float64(0.056975200027227405), 'actor_loss': np.float64(-0.5697870045900345), 'hyper_actor_loss': np.float64(0.08615740910172462), 'behavior_loss': np.float64(1.2477060794830321)}

Episode step 70, time diff 0.9255564212799072, total time dif 6.294577121734619)
step: 70 @ episode report: {'average_total_reward': np.float32(1.0244443), 'reward_variance': np.float32(0.22784694), 'max_total_reward': np.float32(1.9222223), 'min_total_reward': np.float32(0.31111103), 'average_n_step': np.float32(3.2), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 4288} @ step loss: {'critic_loss': np.float64(0.05242590103298426), 'actor_loss': np.float64(-0.6073082387447357), 'hyper_actor_loss': np.float64(0.0843026615679264), 'behavior_loss': np.float64(1.3097288250923156)}

Episode step 80, time diff 0.8587174415588379, total time dif 7.220133543014526)
step: 80 @ episode report: {'average_total_reward': np.float32(1.2244445), 'reward_variance': np.float32(0.31952593), 'max_total_reward': np.float32(1.9222224), 'min_total_reward': np.float32(0.18888885), 'average_n_step': np.float32(3.4), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 4608} @ step loss: {'critic_loss': np.float64(0.050489587150514124), 'actor_loss': np.float64(-0.7057380020618439), 'hyper_actor_loss': np.float64(0.08304633870720864), 'behavior_loss': np.float64(1.3997803926467896)}

Episode step 90, time diff 0.7272045612335205, total time dif 8.078850984573364)
step: 90 @ episode report: {'average_total_reward': np.float32(1.3855556), 'reward_variance': np.float32(0.32266793), 'max_total_reward': np.float32(2.0444446), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.5), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 4928} @ step loss: {'critic_loss': np.float64(0.044739565998315814), 'actor_loss': np.float64(-0.8827426254749298), 'hyper_actor_loss': np.float64(0.08241790756583214), 'behavior_loss': np.float64(1.3610913038253785)}

Episode step 100, time diff 0.9236118793487549, total time dif 8.806055545806885)
step: 100 @ episode report: {'average_total_reward': np.float32(1.7077777), 'reward_variance': np.float32(0.7581742), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(0.67777777), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 5248} @ step loss: {'critic_loss': np.float64(0.04503850024193525), 'actor_loss': np.float64(-1.0494397580623627), 'hyper_actor_loss': np.float64(0.08243591189384461), 'behavior_loss': np.float64(1.3453640937805176)}

Episode step 110, time diff 0.8039085865020752, total time dif 9.72966742515564)
step: 110 @ episode report: {'average_total_reward': np.float32(3.478889), 'reward_variance': np.float32(1.183542), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(1.8000002), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 5568} @ step loss: {'critic_loss': np.float64(0.05777755565941334), 'actor_loss': np.float64(-1.2744593381881715), 'hyper_actor_loss': np.float64(0.08263331577181816), 'behavior_loss': np.float64(1.3049477219581604)}

Episode step 120, time diff 0.8219218254089355, total time dif 10.533576011657715)
step: 120 @ episode report: {'average_total_reward': np.float32(4.286667), 'reward_variance': np.float32(0.5513285), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 5888} @ step loss: {'critic_loss': np.float64(0.06713835299015045), 'actor_loss': np.float64(-1.3361833214759826), 'hyper_actor_loss': np.float64(0.08346449509263039), 'behavior_loss': np.float64(1.1638303697109222)}

Episode step 130, time diff 0.75441575050354, total time dif 11.35549783706665)
step: 130 @ episode report: {'average_total_reward': np.float32(5.2355556), 'reward_variance': np.float32(2.0848842), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(2.2888892), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 6208} @ step loss: {'critic_loss': np.float64(0.07176913246512413), 'actor_loss': np.float64(-1.3045130729675294), 'hyper_actor_loss': np.float64(0.08395245224237442), 'behavior_loss': np.float64(1.1030292570590974)}

Episode step 140, time diff 0.7240850925445557, total time dif 12.10991358757019)
step: 140 @ episode report: {'average_total_reward': np.float32(5.208889), 'reward_variance': np.float32(0.53189623), 'max_total_reward': np.float32(6.6555552), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 6528} @ step loss: {'critic_loss': np.float64(0.08948954194784164), 'actor_loss': np.float64(-1.3237006187438964), 'hyper_actor_loss': np.float64(0.08390326425433159), 'behavior_loss': np.float64(1.0257850289344788)}

Episode step 150, time diff 0.7324891090393066, total time dif 12.833998680114746)
step: 150 @ episode report: {'average_total_reward': np.float32(6.6700006), 'reward_variance': np.float32(0.45185286), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 6848} @ step loss: {'critic_loss': np.float64(0.09928115010261536), 'actor_loss': np.float64(-1.2144667625427246), 'hyper_actor_loss': np.float64(0.083849685639143), 'behavior_loss': np.float64(0.9110943794250488)}

Episode step 160, time diff 0.7538647651672363, total time dif 13.566487789154053)
step: 160 @ episode report: {'average_total_reward': np.float32(6.245556), 'reward_variance': np.float32(1.6550725), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 7168} @ step loss: {'critic_loss': np.float64(0.12959343269467355), 'actor_loss': np.float64(-1.225445318222046), 'hyper_actor_loss': np.float64(0.08342109024524688), 'behavior_loss': np.float64(0.9363728344440461)}

Episode step 170, time diff 0.6717140674591064, total time dif 14.320352554321289)
step: 170 @ episode report: {'average_total_reward': np.float32(6.6577783), 'reward_variance': np.float32(2.7134776), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 7488} @ step loss: {'critic_loss': np.float64(0.1484835922718048), 'actor_loss': np.float64(-1.237583339214325), 'hyper_actor_loss': np.float64(0.08290838450193405), 'behavior_loss': np.float64(0.8767217695713043)}

Episode step 180, time diff 0.8247144222259521, total time dif 14.992066621780396)
step: 180 @ episode report: {'average_total_reward': np.float32(7.0700006), 'reward_variance': np.float32(2.5032852), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 7808} @ step loss: {'critic_loss': np.float64(0.15185772478580475), 'actor_loss': np.float64(-1.3013682246208191), 'hyper_actor_loss': np.float64(0.08255099654197692), 'behavior_loss': np.float64(0.9130083978176117)}

Episode step 190, time diff 0.7196805477142334, total time dif 15.816781044006348)
step: 190 @ episode report: {'average_total_reward': np.float32(7.38), 'reward_variance': np.float32(2.4983895), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 8128} @ step loss: {'critic_loss': np.float64(0.17429763525724412), 'actor_loss': np.float64(-1.3305085897445679), 'hyper_actor_loss': np.float64(0.0829025849699974), 'behavior_loss': np.float64(0.8597514152526855)}

Episode step 200, time diff 0.6952419281005859, total time dif 16.53646159172058)
step: 200 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(1.2291719), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 8448} @ step loss: {'critic_loss': np.float64(0.194579017162323), 'actor_loss': np.float64(-1.3252508521080018), 'hyper_actor_loss': np.float64(0.08300609514117241), 'behavior_loss': np.float64(0.8132534325122833)}

Episode step 210, time diff 0.8884928226470947, total time dif 17.231703519821167)
step: 210 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(2.2879262), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 8768} @ step loss: {'critic_loss': np.float64(0.20100993439555168), 'actor_loss': np.float64(-1.2676724076271058), 'hyper_actor_loss': np.float64(0.08334338590502739), 'behavior_loss': np.float64(0.8008822858333587)}

Episode step 220, time diff 0.8416783809661865, total time dif 18.12019634246826)
step: 220 @ episode report: {'average_total_reward': np.float32(6.418889), 'reward_variance': np.float32(2.032298), 'max_total_reward': np.float32(7.777779), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 9088} @ step loss: {'critic_loss': np.float64(0.21152124106884002), 'actor_loss': np.float64(-1.3049031496047974), 'hyper_actor_loss': np.float64(0.08329992294311524), 'behavior_loss': np.float64(0.7634059190750122)}

Episode step 230, time diff 0.7275667190551758, total time dif 18.96187472343445)
step: 230 @ episode report: {'average_total_reward': np.float32(7.267778), 'reward_variance': np.float32(0.9793199), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 9408} @ step loss: {'critic_loss': np.float64(0.2150678351521492), 'actor_loss': np.float64(-1.3712555527687074), 'hyper_actor_loss': np.float64(0.08332634344696999), 'behavior_loss': np.float64(0.7358091771602631)}

Episode step 240, time diff 0.8826346397399902, total time dif 19.689441442489624)
step: 240 @ episode report: {'average_total_reward': np.float32(7.0922227), 'reward_variance': np.float32(1.8946431), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 9728} @ step loss: {'critic_loss': np.float64(0.23339018523693084), 'actor_loss': np.float64(-1.41249760389328), 'hyper_actor_loss': np.float64(0.08380725979804993), 'behavior_loss': np.float64(0.7442789375782013)}

Episode step 250, time diff 1.0216403007507324, total time dif 20.572076082229614)
step: 250 @ episode report: {'average_total_reward': np.float32(7.5555563), 'reward_variance': np.float32(1.3553585), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 10048} @ step loss: {'critic_loss': np.float64(0.23550590127706528), 'actor_loss': np.float64(-1.3673874735832214), 'hyper_actor_loss': np.float64(0.08405027016997338), 'behavior_loss': np.float64(0.6810296773910522)}

Episode step 260, time diff 1.2794277667999268, total time dif 21.593716382980347)
step: 260 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(1.6071466), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 10368} @ step loss: {'critic_loss': np.float64(0.23058131486177444), 'actor_loss': np.float64(-1.2940270543098449), 'hyper_actor_loss': np.float64(0.08379296362400054), 'behavior_loss': np.float64(0.712055367231369)}

Episode step 270, time diff 1.014986276626587, total time dif 22.873144149780273)
step: 270 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(1.1472703), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 10688} @ step loss: {'critic_loss': np.float64(0.24817729294300078), 'actor_loss': np.float64(-1.3512688636779786), 'hyper_actor_loss': np.float64(0.08371772766113281), 'behavior_loss': np.float64(0.7049904227256775)}

Episode step 280, time diff 1.1485512256622314, total time dif 23.88813042640686)
step: 280 @ episode report: {'average_total_reward': np.float32(7.667778), 'reward_variance': np.float32(3.2417893), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 11008} @ step loss: {'critic_loss': np.float64(0.23100051134824753), 'actor_loss': np.float64(-1.2939896941184998), 'hyper_actor_loss': np.float64(0.08390239924192429), 'behavior_loss': np.float64(0.6821023285388946)}

Episode step 290, time diff 0.8995859622955322, total time dif 25.036681652069092)
step: 290 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(3.3648753), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 11328} @ step loss: {'critic_loss': np.float64(0.2113395720720291), 'actor_loss': np.float64(-1.3865808486938476), 'hyper_actor_loss': np.float64(0.08471741750836373), 'behavior_loss': np.float64(0.6806216061115264)}

Episode step 300, time diff 0.8283555507659912, total time dif 25.936267614364624)
step: 300 @ episode report: {'average_total_reward': np.float32(6.172223), 'reward_variance': np.float32(6.587686), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(2.166667), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(4.0), 'buffer_size': 11648} @ step loss: {'critic_loss': np.float64(0.2379418581724167), 'actor_loss': np.float64(-1.3779961347579956), 'hyper_actor_loss': np.float64(0.0856569156050682), 'behavior_loss': np.float64(0.6819262862205505)}

Episode step 310, time diff 0.8947198390960693, total time dif 26.764623165130615)
step: 310 @ episode report: {'average_total_reward': np.float32(3.2544446), 'reward_variance': np.float32(2.1311717), 'max_total_reward': np.float32(6.1666675), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 11968} @ step loss: {'critic_loss': np.float64(0.2156720533967018), 'actor_loss': np.float64(-1.2540178656578065), 'hyper_actor_loss': np.float64(0.08613978773355484), 'behavior_loss': np.float64(0.7134166836738587)}

Episode step 320, time diff 0.7565770149230957, total time dif 27.659343004226685)
step: 320 @ episode report: {'average_total_reward': np.float32(2.7788892), 'reward_variance': np.float32(0.46307287), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 12288} @ step loss: {'critic_loss': np.float64(0.21694644540548325), 'actor_loss': np.float64(-1.2393389821052552), 'hyper_actor_loss': np.float64(0.08555449619889259), 'behavior_loss': np.float64(0.7260985314846039)}

Episode step 330, time diff 1.1305077075958252, total time dif 28.41592001914978)
step: 330 @ episode report: {'average_total_reward': np.float32(3.154445), 'reward_variance': np.float32(1.0631716), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 12608} @ step loss: {'critic_loss': np.float64(0.23539384976029396), 'actor_loss': np.float64(-1.1656975030899048), 'hyper_actor_loss': np.float64(0.08527376353740693), 'behavior_loss': np.float64(0.7519384801387787)}

Episode step 340, time diff 0.8749585151672363, total time dif 29.546427726745605)
step: 340 @ episode report: {'average_total_reward': np.float32(4.747778), 'reward_variance': np.float32(1.9557297), 'max_total_reward': np.float32(7.288889), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 12928} @ step loss: {'critic_loss': np.float64(0.24427498131990433), 'actor_loss': np.float64(-1.2219285488128662), 'hyper_actor_loss': np.float64(0.08427044302225113), 'behavior_loss': np.float64(0.7674116909503936)}

Episode step 350, time diff 0.9130926132202148, total time dif 30.421386241912842)
step: 350 @ episode report: {'average_total_reward': np.float32(6.9455557), 'reward_variance': np.float32(2.3038878), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 13248} @ step loss: {'critic_loss': np.float64(0.2173641949892044), 'actor_loss': np.float64(-1.1774759769439698), 'hyper_actor_loss': np.float64(0.08523248061537743), 'behavior_loss': np.float64(0.7608910858631134)}

Episode step 360, time diff 0.8223845958709717, total time dif 31.334478855133057)
step: 360 @ episode report: {'average_total_reward': np.float32(4.786667), 'reward_variance': np.float32(0.56814307), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 13568} @ step loss: {'critic_loss': np.float64(0.21639735549688338), 'actor_loss': np.float64(-1.2127796411514282), 'hyper_actor_loss': np.float64(0.08727142065763474), 'behavior_loss': np.float64(0.6881153464317322)}

Episode step 370, time diff 0.9441719055175781, total time dif 32.15686345100403)
step: 370 @ episode report: {'average_total_reward': np.float32(3.691111), 'reward_variance': np.float32(1.116662), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 13888} @ step loss: {'critic_loss': np.float64(0.24333009123802185), 'actor_loss': np.float64(-1.4057806491851808), 'hyper_actor_loss': np.float64(0.09015911668539048), 'behavior_loss': np.float64(0.6551831185817718)}

Episode step 380, time diff 0.8513028621673584, total time dif 33.101035356521606)
step: 380 @ episode report: {'average_total_reward': np.float32(1.3755556), 'reward_variance': np.float32(0.681131), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(0.3111111), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 14208} @ step loss: {'critic_loss': np.float64(0.21572522222995758), 'actor_loss': np.float64(-1.4843965411186217), 'hyper_actor_loss': np.float64(0.0913826934993267), 'behavior_loss': np.float64(0.6272674381732941)}

Episode step 390, time diff 0.8749558925628662, total time dif 33.952338218688965)
step: 390 @ episode report: {'average_total_reward': np.float32(0.9877777), 'reward_variance': np.float32(0.113270365), 'max_total_reward': np.float32(1.5555556), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.2), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 14528} @ step loss: {'critic_loss': np.float64(0.21419054716825486), 'actor_loss': np.float64(-1.4210358738899231), 'hyper_actor_loss': np.float64(0.09117745906114579), 'behavior_loss': np.float64(0.6428479254245758)}

Episode step 400, time diff 0.8502242565155029, total time dif 34.82729411125183)
step: 400 @ episode report: {'average_total_reward': np.float32(1.1611111), 'reward_variance': np.float32(0.15452468), 'max_total_reward': np.float32(2.0444443), 'min_total_reward': np.float32(0.6777777), 'average_n_step': np.float32(3.3), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 14848} @ step loss: {'critic_loss': np.float64(0.21100203394889833), 'actor_loss': np.float64(-1.3160995721817017), 'hyper_actor_loss': np.float64(0.09085547477006913), 'behavior_loss': np.float64(0.6790434837341308)}

Episode step 410, time diff 0.8852274417877197, total time dif 35.677518367767334)
step: 410 @ episode report: {'average_total_reward': np.float32(1.697778), 'reward_variance': np.float32(0.15940246), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 15168} @ step loss: {'critic_loss': np.float64(0.23850127011537553), 'actor_loss': np.float64(-1.278928017616272), 'hyper_actor_loss': np.float64(0.09074963927268982), 'behavior_loss': np.float64(0.6569061934947967)}

Episode step 420, time diff 1.018108606338501, total time dif 36.562745809555054)
step: 420 @ episode report: {'average_total_reward': np.float32(1.6611111), 'reward_variance': np.float32(0.92625314), 'max_total_reward': np.float32(3.0444448), 'min_total_reward': np.float32(0.044444427), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(2.0), 'buffer_size': 15488} @ step loss: {'critic_loss': np.float64(0.25090172439813613), 'actor_loss': np.float64(-1.2187878370285035), 'hyper_actor_loss': np.float64(0.09062324464321136), 'behavior_loss': np.float64(0.6808764398097992)}

Episode step 430, time diff 0.8610615730285645, total time dif 37.580854415893555)
step: 430 @ episode report: {'average_total_reward': np.float32(2.5322223), 'reward_variance': np.float32(0.809147), 'max_total_reward': np.float32(4.0444446), 'min_total_reward': np.float32(1.1666666), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 15808} @ step loss: {'critic_loss': np.float64(0.23801193982362748), 'actor_loss': np.float64(-1.0782204866409302), 'hyper_actor_loss': np.float64(0.09020079374313354), 'behavior_loss': np.float64(0.6768935322761536)}

Episode step 440, time diff 0.8692333698272705, total time dif 38.44191598892212)
step: 440 @ episode report: {'average_total_reward': np.float32(2.7544446), 'reward_variance': np.float32(1.5482584), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(3.0), 'buffer_size': 16128} @ step loss: {'critic_loss': np.float64(0.2086434245109558), 'actor_loss': np.float64(-1.096431803703308), 'hyper_actor_loss': np.float64(0.09037454277276993), 'behavior_loss': np.float64(0.6504742980003357)}

Episode step 450, time diff 0.9014673233032227, total time dif 39.31114935874939)
step: 450 @ episode report: {'average_total_reward': np.float32(3.4644444), 'reward_variance': np.float32(1.3043408), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(1.9222221), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 16448} @ step loss: {'critic_loss': np.float64(0.2024049624800682), 'actor_loss': np.float64(-1.122527801990509), 'hyper_actor_loss': np.float64(0.09091987386345864), 'behavior_loss': np.float64(0.6702564239501954)}

Episode step 460, time diff 0.859684944152832, total time dif 40.21261668205261)
step: 460 @ episode report: {'average_total_reward': np.float32(4.772222), 'reward_variance': np.float32(2.7262533), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 16768} @ step loss: {'critic_loss': np.float64(0.21515318602323533), 'actor_loss': np.float64(-1.1923758268356324), 'hyper_actor_loss': np.float64(0.09056587666273117), 'behavior_loss': np.float64(0.6654137969017029)}

Episode step 470, time diff 0.8553917407989502, total time dif 41.072301626205444)
step: 470 @ episode report: {'average_total_reward': np.float32(5.347778), 'reward_variance': np.float32(2.2291121), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 17088} @ step loss: {'critic_loss': np.float64(0.20093441605567933), 'actor_loss': np.float64(-1.2687384724617004), 'hyper_actor_loss': np.float64(0.08959057331085205), 'behavior_loss': np.float64(0.6644936740398407)}

Episode step 480, time diff 0.8474719524383545, total time dif 41.927693367004395)
step: 480 @ episode report: {'average_total_reward': np.float32(6.145556), 'reward_variance': np.float32(2.5389247), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 17408} @ step loss: {'critic_loss': np.float64(0.21424129009246826), 'actor_loss': np.float64(-1.3819300651550293), 'hyper_actor_loss': np.float64(0.09011470749974251), 'behavior_loss': np.float64(0.6677825808525085)}

Episode step 490, time diff 0.917311429977417, total time dif 42.77516531944275)
step: 490 @ episode report: {'average_total_reward': np.float32(5.608889), 'reward_variance': np.float32(0.982563), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 17728} @ step loss: {'critic_loss': np.float64(0.1901116594672203), 'actor_loss': np.float64(-1.457580578327179), 'hyper_actor_loss': np.float64(0.08941571116447448), 'behavior_loss': np.float64(0.6321690022945404)}

Episode step 500, time diff 0.8690574169158936, total time dif 43.692476749420166)
step: 500 @ episode report: {'average_total_reward': np.float32(7.853334), 'reward_variance': np.float32(2.0085382), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 18048} @ step loss: {'critic_loss': np.float64(0.19849577993154527), 'actor_loss': np.float64(-1.5086021065711974), 'hyper_actor_loss': np.float64(0.08954779505729675), 'behavior_loss': np.float64(0.592994076013565)}

Episode step 510, time diff 0.9038224220275879, total time dif 44.56153416633606)
step: 510 @ episode report: {'average_total_reward': np.float32(6.4700003), 'reward_variance': np.float32(2.2206936), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 18368} @ step loss: {'critic_loss': np.float64(0.2141862466931343), 'actor_loss': np.float64(-1.4436532974243164), 'hyper_actor_loss': np.float64(0.08985347747802734), 'behavior_loss': np.float64(0.6380758941173553)}

Episode step 520, time diff 0.9626078605651855, total time dif 45.46535658836365)
step: 520 @ episode report: {'average_total_reward': np.float32(7.2188888), 'reward_variance': np.float32(3.8562489), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 18688} @ step loss: {'critic_loss': np.float64(0.21576564610004426), 'actor_loss': np.float64(-1.4586330890655517), 'hyper_actor_loss': np.float64(0.09037360548973083), 'behavior_loss': np.float64(0.6088385581970215)}

Episode step 530, time diff 0.861677885055542, total time dif 46.42796444892883)
step: 530 @ episode report: {'average_total_reward': np.float32(6.0211115), 'reward_variance': np.float32(2.7698386), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 19008} @ step loss: {'critic_loss': np.float64(0.2308237835764885), 'actor_loss': np.float64(-1.5339673638343811), 'hyper_actor_loss': np.float64(0.09023459330201149), 'behavior_loss': np.float64(0.6179191112518311)}

Episode step 540, time diff 0.8692762851715088, total time dif 47.289642333984375)
step: 540 @ episode report: {'average_total_reward': np.float32(6.4944444), 'reward_variance': np.float32(1.2431175), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 19328} @ step loss: {'critic_loss': np.float64(0.19556057304143906), 'actor_loss': np.float64(-1.375489854812622), 'hyper_actor_loss': np.float64(0.09036673158407212), 'behavior_loss': np.float64(0.6151082813739777)}

Episode step 550, time diff 0.8661515712738037, total time dif 48.158918619155884)
step: 550 @ episode report: {'average_total_reward': np.float32(4.423334), 'reward_variance': np.float32(0.7963568), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 19648} @ step loss: {'critic_loss': np.float64(0.21020080298185348), 'actor_loss': np.float64(-1.3044018507003785), 'hyper_actor_loss': np.float64(0.08972214683890342), 'behavior_loss': np.float64(0.639858889579773)}

Episode step 560, time diff 0.8666446208953857, total time dif 49.02507019042969)
step: 560 @ episode report: {'average_total_reward': np.float32(2.5955555), 'reward_variance': np.float32(1.5256099), 'max_total_reward': np.float32(4.922222), 'min_total_reward': np.float32(0.8000001), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(3.0), 'buffer_size': 19968} @ step loss: {'critic_loss': np.float64(0.22386176139116287), 'actor_loss': np.float64(-1.3157557606697083), 'hyper_actor_loss': np.float64(0.08948599100112915), 'behavior_loss': np.float64(0.6743368685245514)}

Episode step 570, time diff 0.8492610454559326, total time dif 49.89171481132507)
step: 570 @ episode report: {'average_total_reward': np.float32(2.232222), 'reward_variance': np.float32(0.62299883), 'max_total_reward': np.float32(3.288889), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 20288} @ step loss: {'critic_loss': np.float64(0.22190186083316804), 'actor_loss': np.float64(-1.3747975707054139), 'hyper_actor_loss': np.float64(0.08903575390577316), 'behavior_loss': np.float64(0.6834980428218842)}

Episode step 580, time diff 1.0746963024139404, total time dif 50.740975856781006)
step: 580 @ episode report: {'average_total_reward': np.float32(2.0077777), 'reward_variance': np.float32(0.87592715), 'max_total_reward': np.float32(3.288889), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 20608} @ step loss: {'critic_loss': np.float64(0.23160126358270644), 'actor_loss': np.float64(-1.3758429408073425), 'hyper_actor_loss': np.float64(0.088214660435915), 'behavior_loss': np.float64(0.697436398267746)}

Episode step 590, time diff 0.8502306938171387, total time dif 51.815672159194946)
step: 590 @ episode report: {'average_total_reward': np.float32(1.5222223), 'reward_variance': np.float32(0.24190125), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.67777777), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 20928} @ step loss: {'critic_loss': np.float64(0.2351089522242546), 'actor_loss': np.float64(-1.3210700273513794), 'hyper_actor_loss': np.float64(0.08774556517601013), 'behavior_loss': np.float64(0.7229820787906647)}

Episode step 600, time diff 0.8256163597106934, total time dif 52.665902853012085)
step: 600 @ episode report: {'average_total_reward': np.float32(1.307778), 'reward_variance': np.float32(0.176742), 'max_total_reward': np.float32(2.0444446), 'min_total_reward': np.float32(0.8000001), 'average_n_step': np.float32(3.3), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 21248} @ step loss: {'critic_loss': np.float64(0.2403948724269867), 'actor_loss': np.float64(-1.3014143586158753), 'hyper_actor_loss': np.float64(0.08715080171823501), 'behavior_loss': np.float64(0.7490226447582244)}

Episode step 610, time diff 0.855353593826294, total time dif 53.49151921272278)
step: 610 @ episode report: {'average_total_reward': np.float32(1.5955557), 'reward_variance': np.float32(0.24630126), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 21568} @ step loss: {'critic_loss': np.float64(0.2060711622238159), 'actor_loss': np.float64(-1.3496487855911254), 'hyper_actor_loss': np.float64(0.08654336109757424), 'behavior_loss': np.float64(0.7704312622547149)}

Episode step 620, time diff 0.809628963470459, total time dif 54.34687280654907)
step: 620 @ episode report: {'average_total_reward': np.float32(1.3344446), 'reward_variance': np.float32(0.26443088), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(3.4), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 21888} @ step loss: {'critic_loss': np.float64(0.23489209413528442), 'actor_loss': np.float64(-1.375463855266571), 'hyper_actor_loss': np.float64(0.08592816814780235), 'behavior_loss': np.float64(0.8205505311489105)}

Episode step 630, time diff 0.8207077980041504, total time dif 55.15650177001953)
step: 630 @ episode report: {'average_total_reward': np.float32(1.4833335), 'reward_variance': np.float32(0.17361109), 'max_total_reward': np.float32(2.1666665), 'min_total_reward': np.float32(0.9222224), 'average_n_step': np.float32(3.5), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 22208} @ step loss: {'critic_loss': np.float64(0.2411483883857727), 'actor_loss': np.float64(-1.398985540866852), 'hyper_actor_loss': np.float64(0.08551945090293885), 'behavior_loss': np.float64(0.7863253295421601)}

Episode step 640, time diff 0.8391478061676025, total time dif 55.97720956802368)
step: 640 @ episode report: {'average_total_reward': np.float32(1.5855558), 'reward_variance': np.float32(0.35763094), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.43333337), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 22528} @ step loss: {'critic_loss': np.float64(0.2442786365747452), 'actor_loss': np.float64(-1.3854611396789551), 'hyper_actor_loss': np.float64(0.08485584259033203), 'behavior_loss': np.float64(0.8541436433792114)}

Episode step 650, time diff 0.8180027008056641, total time dif 56.816357374191284)
step: 650 @ episode report: {'average_total_reward': np.float32(2.0711112), 'reward_variance': np.float32(0.41146174), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(0.67777777), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 22848} @ step loss: {'critic_loss': np.float64(0.23600105196237564), 'actor_loss': np.float64(-1.318431031703949), 'hyper_actor_loss': np.float64(0.0843698613345623), 'behavior_loss': np.float64(0.8712394058704376)}

Episode step 660, time diff 0.8447039127349854, total time dif 57.63436007499695)
step: 660 @ episode report: {'average_total_reward': np.float32(1.6322224), 'reward_variance': np.float32(0.30136916), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.8000001), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 23168} @ step loss: {'critic_loss': np.float64(0.2508716359734535), 'actor_loss': np.float64(-1.330858314037323), 'hyper_actor_loss': np.float64(0.0841468669474125), 'behavior_loss': np.float64(0.8823404431343078)}

Episode step 670, time diff 0.8401486873626709, total time dif 58.479063987731934)
step: 670 @ episode report: {'average_total_reward': np.float32(1.9344447), 'reward_variance': np.float32(0.8681593), 'max_total_reward': np.float32(4.0444446), 'min_total_reward': np.float32(0.67777777), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 23488} @ step loss: {'critic_loss': np.float64(0.24839155673980712), 'actor_loss': np.float64(-1.4610501408576966), 'hyper_actor_loss': np.float64(0.08463992550969124), 'behavior_loss': np.float64(0.8313158094882965)}

Episode step 680, time diff 0.8544456958770752, total time dif 59.319212675094604)
step: 680 @ episode report: {'average_total_reward': np.float32(1.8711112), 'reward_variance': np.float32(0.30227655), 'max_total_reward': np.float32(2.9222224), 'min_total_reward': np.float32(0.9222223), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 23808} @ step loss: {'critic_loss': np.float64(0.2589428052306175), 'actor_loss': np.float64(-1.492961049079895), 'hyper_actor_loss': np.float64(0.08458655104041099), 'behavior_loss': np.float64(0.8990075647830963)}

Episode step 690, time diff 0.8397996425628662, total time dif 60.17365837097168)
step: 690 @ episode report: {'average_total_reward': np.float32(2.866667), 'reward_variance': np.float32(1.0962718), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 24128} @ step loss: {'critic_loss': np.float64(0.28479048758745196), 'actor_loss': np.float64(-1.4904771447181702), 'hyper_actor_loss': np.float64(0.08402921855449677), 'behavior_loss': np.float64(0.9127048909664154)}

Episode step 700, time diff 0.805656909942627, total time dif 61.013458013534546)
step: 700 @ episode report: {'average_total_reward': np.float32(2.7644446), 'reward_variance': np.float32(0.7346865), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 24448} @ step loss: {'critic_loss': np.float64(0.29208657145500183), 'actor_loss': np.float64(-1.412292242050171), 'hyper_actor_loss': np.float64(0.08339296504855156), 'behavior_loss': np.float64(0.9250135004520417)}

Episode step 710, time diff 0.8396506309509277, total time dif 61.81911492347717)
step: 710 @ episode report: {'average_total_reward': np.float32(4.2377777), 'reward_variance': np.float32(1.3701781), 'max_total_reward': np.float32(6.5333343), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 24768} @ step loss: {'critic_loss': np.float64(0.28560504168272016), 'actor_loss': np.float64(-1.4263062596321106), 'hyper_actor_loss': np.float64(0.0828729122877121), 'behavior_loss': np.float64(0.9994125008583069)}

Episode step 720, time diff 1.0146732330322266, total time dif 62.6587655544281)
step: 720 @ episode report: {'average_total_reward': np.float32(3.7644448), 'reward_variance': np.float32(0.7251803), 'max_total_reward': np.float32(5.2888894), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 25088} @ step loss: {'critic_loss': np.float64(0.2633097156882286), 'actor_loss': np.float64(-1.3919548869132996), 'hyper_actor_loss': np.float64(0.0825638271868229), 'behavior_loss': np.float64(0.9863818287849426)}

Episode step 730, time diff 0.887603759765625, total time dif 63.67343878746033)
step: 730 @ episode report: {'average_total_reward': np.float32(4.2011113), 'reward_variance': np.float32(0.58504826), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 25408} @ step loss: {'critic_loss': np.float64(0.2808897405862808), 'actor_loss': np.float64(-1.4127074003219604), 'hyper_actor_loss': np.float64(0.08218844011425971), 'behavior_loss': np.float64(0.9864829480648041)}

Episode step 740, time diff 0.9402029514312744, total time dif 64.56104254722595)
step: 740 @ episode report: {'average_total_reward': np.float32(4.7744446), 'reward_variance': np.float32(0.80012476), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 25728} @ step loss: {'critic_loss': np.float64(0.3029620170593262), 'actor_loss': np.float64(-1.4334131002426147), 'hyper_actor_loss': np.float64(0.08114418238401414), 'behavior_loss': np.float64(1.012954980134964)}

Episode step 750, time diff 0.8004190921783447, total time dif 65.50124549865723)
step: 750 @ episode report: {'average_total_reward': np.float32(4.8355556), 'reward_variance': np.float32(0.99340236), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 26048} @ step loss: {'critic_loss': np.float64(0.2748985767364502), 'actor_loss': np.float64(-1.37128643989563), 'hyper_actor_loss': np.float64(0.08034287467598915), 'behavior_loss': np.float64(1.008607769012451)}

Episode step 760, time diff 0.8848631381988525, total time dif 66.30166459083557)
step: 760 @ episode report: {'average_total_reward': np.float32(4.6622224), 'reward_variance': np.float32(0.7795357), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.2888894), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 26368} @ step loss: {'critic_loss': np.float64(0.2791884675621986), 'actor_loss': np.float64(-1.4322988986968994), 'hyper_actor_loss': np.float64(0.0788856327533722), 'behavior_loss': np.float64(0.9934552788734436)}

Episode step 770, time diff 1.0206351280212402, total time dif 67.18652772903442)
step: 770 @ episode report: {'average_total_reward': np.float32(5.1844444), 'reward_variance': np.float32(2.2162032), 'max_total_reward': np.float32(7.411112), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 26688} @ step loss: {'critic_loss': np.float64(0.28336364328861235), 'actor_loss': np.float64(-1.3907130002975463), 'hyper_actor_loss': np.float64(0.07745768800377846), 'behavior_loss': np.float64(0.9945900619029999)}

Episode step 780, time diff 0.9729607105255127, total time dif 68.20716285705566)
step: 780 @ episode report: {'average_total_reward': np.float32(4.5622225), 'reward_variance': np.float32(0.90481985), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 27008} @ step loss: {'critic_loss': np.float64(0.28373668491840365), 'actor_loss': np.float64(-1.3838728427886964), 'hyper_actor_loss': np.float64(0.07639360576868057), 'behavior_loss': np.float64(1.0107287883758544)}

Episode step 790, time diff 0.9221899509429932, total time dif 69.18012356758118)
step: 790 @ episode report: {'average_total_reward': np.float32(5.221111), 'reward_variance': np.float32(0.6892953), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 27328} @ step loss: {'critic_loss': np.float64(0.25618006438016894), 'actor_loss': np.float64(-1.4664801836013794), 'hyper_actor_loss': np.float64(0.07547250986099244), 'behavior_loss': np.float64(1.03059424161911)}

Episode step 800, time diff 0.9160134792327881, total time dif 70.10231351852417)
step: 800 @ episode report: {'average_total_reward': np.float32(4.7211113), 'reward_variance': np.float32(1.1218629), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 27648} @ step loss: {'critic_loss': np.float64(0.2828988015651703), 'actor_loss': np.float64(-1.4345525860786439), 'hyper_actor_loss': np.float64(0.07454597875475884), 'behavior_loss': np.float64(1.0134474813938141)}

Episode step 810, time diff 0.8558743000030518, total time dif 71.01832699775696)
step: 810 @ episode report: {'average_total_reward': np.float32(4.15), 'reward_variance': np.float32(0.89790756), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 27968} @ step loss: {'critic_loss': np.float64(0.2639752119779587), 'actor_loss': np.float64(-1.460268795490265), 'hyper_actor_loss': np.float64(0.07394924536347389), 'behavior_loss': np.float64(1.0108021557331086)}

Episode step 820, time diff 0.7883319854736328, total time dif 71.87420129776001)
step: 820 @ episode report: {'average_total_reward': np.float32(4.45), 'reward_variance': np.float32(0.29645061), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 28288} @ step loss: {'critic_loss': np.float64(0.3017728358507156), 'actor_loss': np.float64(-1.4730283737182617), 'hyper_actor_loss': np.float64(0.07341004237532615), 'behavior_loss': np.float64(1.0832592248916626)}

Episode step 830, time diff 0.7613182067871094, total time dif 72.66253328323364)
step: 830 @ episode report: {'average_total_reward': np.float32(4.374444), 'reward_variance': np.float32(0.39064327), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 28608} @ step loss: {'critic_loss': np.float64(0.28425985127687453), 'actor_loss': np.float64(-1.4388948917388915), 'hyper_actor_loss': np.float64(0.07278542518615723), 'behavior_loss': np.float64(1.0586007595062257)}

Episode step 840, time diff 0.7537505626678467, total time dif 73.42385149002075)
step: 840 @ episode report: {'average_total_reward': np.float32(4.8622227), 'reward_variance': np.float32(0.9478323), 'max_total_reward': np.float32(6.2888894), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 28928} @ step loss: {'critic_loss': np.float64(0.2708112999796867), 'actor_loss': np.float64(-1.4110317468643188), 'hyper_actor_loss': np.float64(0.07189693078398704), 'behavior_loss': np.float64(1.0636033117771149)}

Episode step 850, time diff 0.9482877254486084, total time dif 74.1776020526886)
step: 850 @ episode report: {'average_total_reward': np.float32(4.223333), 'reward_variance': np.float32(0.61811), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 29248} @ step loss: {'critic_loss': np.float64(0.2628029651939869), 'actor_loss': np.float64(-1.3083117961883546), 'hyper_actor_loss': np.float64(0.07136405482888222), 'behavior_loss': np.float64(1.0513768076896668)}

Episode step 860, time diff 0.9157655239105225, total time dif 75.1258897781372)
step: 860 @ episode report: {'average_total_reward': np.float32(3.8766665), 'reward_variance': np.float32(0.2649741), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 29568} @ step loss: {'critic_loss': np.float64(0.2522968232631683), 'actor_loss': np.float64(-1.268574845790863), 'hyper_actor_loss': np.float64(0.07108602076768875), 'behavior_loss': np.float64(1.0580645084381104)}

Episode step 870, time diff 0.8716356754302979, total time dif 76.04165530204773)
step: 870 @ episode report: {'average_total_reward': np.float32(4.298889), 'reward_variance': np.float32(0.6564803), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 29888} @ step loss: {'critic_loss': np.float64(0.2871165782213211), 'actor_loss': np.float64(-1.4411540031433105), 'hyper_actor_loss': np.float64(0.0708754263818264), 'behavior_loss': np.float64(1.1075618982315063)}

Episode step 880, time diff 0.930943489074707, total time dif 76.91329097747803)
step: 880 @ episode report: {'average_total_reward': np.float32(3.7766666), 'reward_variance': np.float32(0.94233227), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.0444443), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 30208} @ step loss: {'critic_loss': np.float64(0.2934668481349945), 'actor_loss': np.float64(-1.4072302103042602), 'hyper_actor_loss': np.float64(0.07111579105257988), 'behavior_loss': np.float64(1.146776795387268)}

Episode step 890, time diff 0.8793432712554932, total time dif 77.84423446655273)
step: 890 @ episode report: {'average_total_reward': np.float32(4.037778), 'reward_variance': np.float32(1.3506473), 'max_total_reward': np.float32(6.6555567), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 30528} @ step loss: {'critic_loss': np.float64(0.27171346098184584), 'actor_loss': np.float64(-1.413872230052948), 'hyper_actor_loss': np.float64(0.07046127840876579), 'behavior_loss': np.float64(1.142907691001892)}

Episode step 900, time diff 1.0969398021697998, total time dif 78.72357773780823)
step: 900 @ episode report: {'average_total_reward': np.float32(4.186667), 'reward_variance': np.float32(0.6927606), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 30848} @ step loss: {'critic_loss': np.float64(0.268994602560997), 'actor_loss': np.float64(-1.4389726638793945), 'hyper_actor_loss': np.float64(0.06965088769793511), 'behavior_loss': np.float64(1.0871660947799682)}

Episode step 910, time diff 0.7694761753082275, total time dif 79.82051753997803)
step: 910 @ episode report: {'average_total_reward': np.float32(3.9522223), 'reward_variance': np.float32(0.6001001), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 31168} @ step loss: {'critic_loss': np.float64(0.2546416163444519), 'actor_loss': np.float64(-1.425717532634735), 'hyper_actor_loss': np.float64(0.06914722546935081), 'behavior_loss': np.float64(1.130844873189926)}

Episode step 920, time diff 0.8479833602905273, total time dif 80.58999371528625)
step: 920 @ episode report: {'average_total_reward': np.float32(3.876667), 'reward_variance': np.float32(1.5762334), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 31488} @ step loss: {'critic_loss': np.float64(0.27204621583223343), 'actor_loss': np.float64(-1.4205093026161193), 'hyper_actor_loss': np.float64(0.06878916546702385), 'behavior_loss': np.float64(1.1575475931167603)}

Episode step 930, time diff 1.0193114280700684, total time dif 81.43797707557678)
step: 930 @ episode report: {'average_total_reward': np.float32(4.137778), 'reward_variance': np.float32(0.7078075), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 31808} @ step loss: {'critic_loss': np.float64(0.25798811614513395), 'actor_loss': np.float64(-1.423772895336151), 'hyper_actor_loss': np.float64(0.06783995479345321), 'behavior_loss': np.float64(1.1933996319770812)}

Episode step 940, time diff 0.9437670707702637, total time dif 82.45728850364685)
step: 940 @ episode report: {'average_total_reward': np.float32(4.1011114), 'reward_variance': np.float32(1.0909002), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 32128} @ step loss: {'critic_loss': np.float64(0.24121879190206527), 'actor_loss': np.float64(-1.373693871498108), 'hyper_actor_loss': np.float64(0.06719662398099899), 'behavior_loss': np.float64(1.1983871817588807)}

Episode step 950, time diff 0.8259057998657227, total time dif 83.40105557441711)
step: 950 @ episode report: {'average_total_reward': np.float32(4.1255555), 'reward_variance': np.float32(1.0810137), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 32448} @ step loss: {'critic_loss': np.float64(0.3070122703909874), 'actor_loss': np.float64(-1.4241244554519654), 'hyper_actor_loss': np.float64(0.06676306277513504), 'behavior_loss': np.float64(1.208012545108795)}

Episode step 960, time diff 0.8563854694366455, total time dif 84.22696137428284)
step: 960 @ episode report: {'average_total_reward': np.float32(4.113333), 'reward_variance': np.float32(1.0694025), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 32768} @ step loss: {'critic_loss': np.float64(0.2649839401245117), 'actor_loss': np.float64(-1.3923160314559937), 'hyper_actor_loss': np.float64(0.06661201864480973), 'behavior_loss': np.float64(1.17714262008667)}

Episode step 970, time diff 0.868018388748169, total time dif 85.08334684371948)
step: 970 @ episode report: {'average_total_reward': np.float32(3.5544448), 'reward_variance': np.float32(0.76628274), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 33088} @ step loss: {'critic_loss': np.float64(0.2342709094285965), 'actor_loss': np.float64(-1.3524147272109985), 'hyper_actor_loss': np.float64(0.066707281768322), 'behavior_loss': np.float64(1.194549036026001)}

Episode step 980, time diff 0.8125686645507812, total time dif 85.95136523246765)
step: 980 @ episode report: {'average_total_reward': np.float32(4.0255556), 'reward_variance': np.float32(0.23758161), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 33408} @ step loss: {'critic_loss': np.float64(0.2558374270796776), 'actor_loss': np.float64(-1.3254530549049377), 'hyper_actor_loss': np.float64(0.06635023429989814), 'behavior_loss': np.float64(1.2044105768203734)}

Episode step 990, time diff 0.7880458831787109, total time dif 86.76393389701843)
step: 990 @ episode report: {'average_total_reward': np.float32(4.1255555), 'reward_variance': np.float32(0.47503823), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 33728} @ step loss: {'critic_loss': np.float64(0.23052679151296615), 'actor_loss': np.float64(-1.26901695728302), 'hyper_actor_loss': np.float64(0.06603015884757042), 'behavior_loss': np.float64(1.210806679725647)}

Episode step 1000, time diff 0.789165735244751, total time dif 87.55197978019714)
step: 1000 @ episode report: {'average_total_reward': np.float32(3.776667), 'reward_variance': np.float32(0.96677655), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 34048} @ step loss: {'critic_loss': np.float64(0.24722741097211837), 'actor_loss': np.float64(-1.2658369541168213), 'hyper_actor_loss': np.float64(0.06623187139630318), 'behavior_loss': np.float64(1.2649154663085938)}

Episode step 1010, time diff 1.0215494632720947, total time dif 88.3411455154419)
step: 1010 @ episode report: {'average_total_reward': np.float32(3.952222), 'reward_variance': np.float32(0.41856918), 'max_total_reward': np.float32(5.2888894), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 34368} @ step loss: {'critic_loss': np.float64(0.27021132707595824), 'actor_loss': np.float64(-1.2687161922454835), 'hyper_actor_loss': np.float64(0.06648707613348961), 'behavior_loss': np.float64(1.2381425380706788)}

Episode step 1020, time diff 0.9961137771606445, total time dif 89.36269497871399)
step: 1020 @ episode report: {'average_total_reward': np.float32(4.4866667), 'reward_variance': np.float32(0.9227112), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 34688} @ step loss: {'critic_loss': np.float64(0.2766727447509766), 'actor_loss': np.float64(-1.367883026599884), 'hyper_actor_loss': np.float64(0.06723309829831123), 'behavior_loss': np.float64(1.2475776314735412)}

Episode step 1030, time diff 0.8469221591949463, total time dif 90.35880875587463)
step: 1030 @ episode report: {'average_total_reward': np.float32(4.076667), 'reward_variance': np.float32(1.6182833), 'max_total_reward': np.float32(7.5333343), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35008} @ step loss: {'critic_loss': np.float64(0.258431912958622), 'actor_loss': np.float64(-1.2849687218666077), 'hyper_actor_loss': np.float64(0.0677307404577732), 'behavior_loss': np.float64(1.2378803968429566)}

Episode step 1040, time diff 0.839855432510376, total time dif 91.20573091506958)
step: 1040 @ episode report: {'average_total_reward': np.float32(4.586667), 'reward_variance': np.float32(0.8035754), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.9222224), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35328} @ step loss: {'critic_loss': np.float64(0.19818059355020523), 'actor_loss': np.float64(-1.269745147228241), 'hyper_actor_loss': np.float64(0.06816087812185287), 'behavior_loss': np.float64(1.2284478187561034)}

Episode step 1050, time diff 0.8529143333435059, total time dif 92.04558634757996)
step: 1050 @ episode report: {'average_total_reward': np.float32(4.0255556), 'reward_variance': np.float32(0.5717544), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35648} @ step loss: {'critic_loss': np.float64(0.22437775433063506), 'actor_loss': np.float64(-1.2500660181045533), 'hyper_actor_loss': np.float64(0.06772715300321579), 'behavior_loss': np.float64(1.2747225046157837)}

Episode step 1060, time diff 1.0621411800384521, total time dif 92.89850068092346)
step: 1060 @ episode report: {'average_total_reward': np.float32(3.9500008), 'reward_variance': np.float32(0.8113149), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.1666665), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35968} @ step loss: {'critic_loss': np.float64(0.20707164257764815), 'actor_loss': np.float64(-1.3128533959388733), 'hyper_actor_loss': np.float64(0.067948317527771), 'behavior_loss': np.float64(1.2257298707962037)}

Episode step 1070, time diff 0.7973427772521973, total time dif 93.96064186096191)
step: 1070 @ episode report: {'average_total_reward': np.float32(4.0255556), 'reward_variance': np.float32(0.92639637), 'max_total_reward': np.float32(6.5333333), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 36288} @ step loss: {'critic_loss': np.float64(0.2621344789862633), 'actor_loss': np.float64(-1.3839929938316344), 'hyper_actor_loss': np.float64(0.06866006329655647), 'behavior_loss': np.float64(1.2455185770988464)}

Episode step 1080, time diff 0.8190488815307617, total time dif 94.75798463821411)
step: 1080 @ episode report: {'average_total_reward': np.float32(4.1255555), 'reward_variance': np.float32(0.46308765), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 36608} @ step loss: {'critic_loss': np.float64(0.22055220305919648), 'actor_loss': np.float64(-1.2287355542182923), 'hyper_actor_loss': np.float64(0.06940914541482926), 'behavior_loss': np.float64(1.2659910559654235)}

Episode step 1090, time diff 0.9650835990905762, total time dif 95.57703351974487)
step: 1090 @ episode report: {'average_total_reward': np.float32(4.15), 'reward_variance': np.float32(0.646031), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 36928} @ step loss: {'critic_loss': np.float64(0.21430328488349915), 'actor_loss': np.float64(-1.1602348208427429), 'hyper_actor_loss': np.float64(0.0704489916563034), 'behavior_loss': np.float64(1.1994625926017761)}

Episode step 1100, time diff 0.7880599498748779, total time dif 96.54211711883545)
step: 1100 @ episode report: {'average_total_reward': np.float32(4.125556), 'reward_variance': np.float32(1.3972607), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 37248} @ step loss: {'critic_loss': np.float64(0.2207990199327469), 'actor_loss': np.float64(-1.2329264163970948), 'hyper_actor_loss': np.float64(0.07188790440559387), 'behavior_loss': np.float64(1.1641940474510193)}

Episode step 1110, time diff 0.8465123176574707, total time dif 97.33017706871033)
step: 1110 @ episode report: {'average_total_reward': np.float32(4.162223), 'reward_variance': np.float32(0.43230113), 'max_total_reward': np.float32(5.2888885), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 37568} @ step loss: {'critic_loss': np.float64(0.215465684235096), 'actor_loss': np.float64(-1.286117720603943), 'hyper_actor_loss': np.float64(0.07300463393330574), 'behavior_loss': np.float64(1.1307626247406006)}

Episode step 1120, time diff 0.7773103713989258, total time dif 98.1766893863678)
step: 1120 @ episode report: {'average_total_reward': np.float32(4.0255556), 'reward_variance': np.float32(1.2870138), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 37888} @ step loss: {'critic_loss': np.float64(0.19842618107795715), 'actor_loss': np.float64(-1.1952974438667296), 'hyper_actor_loss': np.float64(0.07410825267434121), 'behavior_loss': np.float64(1.1640831708908081)}

Episode step 1130, time diff 0.9609677791595459, total time dif 98.95399975776672)
step: 1130 @ episode report: {'average_total_reward': np.float32(4.5133333), 'reward_variance': np.float32(0.67130387), 'max_total_reward': np.float32(6.411112), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 38208} @ step loss: {'critic_loss': np.float64(0.20049514174461364), 'actor_loss': np.float64(-1.2362434267997742), 'hyper_actor_loss': np.float64(0.07627212703227997), 'behavior_loss': np.float64(1.1175929963588715)}

Episode step 1140, time diff 0.8755035400390625, total time dif 99.91496753692627)
step: 1140 @ episode report: {'average_total_reward': np.float32(5.247778), 'reward_variance': np.float32(0.7897544), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 38528} @ step loss: {'critic_loss': np.float64(0.2051194727420807), 'actor_loss': np.float64(-1.2170907735824585), 'hyper_actor_loss': np.float64(0.07747355923056602), 'behavior_loss': np.float64(1.0683771669864655)}

Episode step 1150, time diff 0.9581537246704102, total time dif 100.79047107696533)
step: 1150 @ episode report: {'average_total_reward': np.float32(4.7477775), 'reward_variance': np.float32(2.268989), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 38848} @ step loss: {'critic_loss': np.float64(0.19066613167524338), 'actor_loss': np.float64(-1.2909749150276184), 'hyper_actor_loss': np.float64(0.07903332188725472), 'behavior_loss': np.float64(1.0616907596588134)}

Episode step 1160, time diff 0.8216726779937744, total time dif 101.74862480163574)
step: 1160 @ episode report: {'average_total_reward': np.float32(5.286667), 'reward_variance': np.float32(1.3284147), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 39168} @ step loss: {'critic_loss': np.float64(0.19143788516521454), 'actor_loss': np.float64(-1.3190520524978637), 'hyper_actor_loss': np.float64(0.07936451062560082), 'behavior_loss': np.float64(1.0228525221347808)}

Episode step 1170, time diff 0.8488039970397949, total time dif 102.57029747962952)
step: 1170 @ episode report: {'average_total_reward': np.float32(4.896667), 'reward_variance': np.float32(1.3762232), 'max_total_reward': np.float32(7.5333333), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 39488} @ step loss: {'critic_loss': np.float64(0.1839679554104805), 'actor_loss': np.float64(-1.2746289253234864), 'hyper_actor_loss': np.float64(0.08026051074266434), 'behavior_loss': np.float64(0.962588369846344)}

Episode step 1180, time diff 0.8496506214141846, total time dif 103.41910147666931)
step: 1180 @ episode report: {'average_total_reward': np.float32(4.835555), 'reward_variance': np.float32(0.48964962), 'max_total_reward': np.float32(6.411112), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 39808} @ step loss: {'critic_loss': np.float64(0.19803139120340346), 'actor_loss': np.float64(-1.2697283267974853), 'hyper_actor_loss': np.float64(0.08073205575346946), 'behavior_loss': np.float64(0.940025269985199)}

Episode step 1190, time diff 0.8880057334899902, total time dif 104.2687520980835)
step: 1190 @ episode report: {'average_total_reward': np.float32(5.0233335), 'reward_variance': np.float32(1.0795424), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.4111116), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 40128} @ step loss: {'critic_loss': np.float64(0.1997222952544689), 'actor_loss': np.float64(-1.2765913605690002), 'hyper_actor_loss': np.float64(0.08146915510296822), 'behavior_loss': np.float64(0.9349593162536621)}

Episode step 1200, time diff 0.9024808406829834, total time dif 105.15675783157349)
step: 1200 @ episode report: {'average_total_reward': np.float32(5.7844443), 'reward_variance': np.float32(2.010499), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 40448} @ step loss: {'critic_loss': np.float64(0.20368448793888091), 'actor_loss': np.float64(-1.2861229300498962), 'hyper_actor_loss': np.float64(0.08196259588003159), 'behavior_loss': np.float64(0.8717397809028625)}

Episode step 1210, time diff 0.8353302478790283, total time dif 106.05923867225647)
step: 1210 @ episode report: {'average_total_reward': np.float32(4.274445), 'reward_variance': np.float32(0.73768026), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.2888892), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 40768} @ step loss: {'critic_loss': np.float64(0.20450511947274208), 'actor_loss': np.float64(-1.2815827131271362), 'hyper_actor_loss': np.float64(0.0817651979625225), 'behavior_loss': np.float64(0.8363547027111053)}

Episode step 1220, time diff 0.9833271503448486, total time dif 106.8945689201355)
step: 1220 @ episode report: {'average_total_reward': np.float32(4.46), 'reward_variance': np.float32(0.48588157), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 41088} @ step loss: {'critic_loss': np.float64(0.21191979199647903), 'actor_loss': np.float64(-1.1327611923217773), 'hyper_actor_loss': np.float64(0.08316175267100334), 'behavior_loss': np.float64(0.7583310663700104)}

Episode step 1230, time diff 0.8859968185424805, total time dif 107.87789607048035)
step: 1230 @ episode report: {'average_total_reward': np.float32(4.074445), 'reward_variance': np.float32(0.97419864), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 41408} @ step loss: {'critic_loss': np.float64(0.19173220172524452), 'actor_loss': np.float64(-1.151466977596283), 'hyper_actor_loss': np.float64(0.08393743485212327), 'behavior_loss': np.float64(0.7391290068626404)}

Episode step 1240, time diff 0.9462604522705078, total time dif 108.76389288902283)
step: 1240 @ episode report: {'average_total_reward': np.float32(4.3133335), 'reward_variance': np.float32(0.95747674), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 41728} @ step loss: {'critic_loss': np.float64(0.17458880841732025), 'actor_loss': np.float64(-1.1027681589126588), 'hyper_actor_loss': np.float64(0.08324806019663811), 'behavior_loss': np.float64(0.7845952332019805)}

Episode step 1250, time diff 0.8610324859619141, total time dif 109.71015334129333)
step: 1250 @ episode report: {'average_total_reward': np.float32(4.8622227), 'reward_variance': np.float32(0.63610363), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 42048} @ step loss: {'critic_loss': np.float64(0.1801509365439415), 'actor_loss': np.float64(-1.1411921501159668), 'hyper_actor_loss': np.float64(0.0821010410785675), 'behavior_loss': np.float64(0.7879822790622711)}

Episode step 1260, time diff 0.8185577392578125, total time dif 110.57118582725525)
step: 1260 @ episode report: {'average_total_reward': np.float32(4.9355555), 'reward_variance': np.float32(1.7421926), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 42368} @ step loss: {'critic_loss': np.float64(0.19722171127796173), 'actor_loss': np.float64(-1.1981088876724244), 'hyper_actor_loss': np.float64(0.08113137409090995), 'behavior_loss': np.float64(0.8029058814048767)}

Episode step 1270, time diff 0.9069459438323975, total time dif 111.38974356651306)
step: 1270 @ episode report: {'average_total_reward': np.float32(4.6988883), 'reward_variance': np.float32(1.8381348), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 42688} @ step loss: {'critic_loss': np.float64(0.19311962574720382), 'actor_loss': np.float64(-1.1495683431625365), 'hyper_actor_loss': np.float64(0.08186430335044861), 'behavior_loss': np.float64(0.8325634241104126)}

Episode step 1280, time diff 0.8895933628082275, total time dif 112.29668951034546)
step: 1280 @ episode report: {'average_total_reward': np.float32(4.7377777), 'reward_variance': np.float32(0.8837827), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 43008} @ step loss: {'critic_loss': np.float64(0.17095421478152276), 'actor_loss': np.float64(-1.1609143972396851), 'hyper_actor_loss': np.float64(0.08081680834293366), 'behavior_loss': np.float64(0.7846004724502563)}

Episode step 1290, time diff 0.8501548767089844, total time dif 113.18628287315369)
step: 1290 @ episode report: {'average_total_reward': np.float32(4.55), 'reward_variance': np.float32(1.5587472), 'max_total_reward': np.float32(7.411112), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 43328} @ step loss: {'critic_loss': np.float64(0.18700848072767257), 'actor_loss': np.float64(-1.1417814493179321), 'hyper_actor_loss': np.float64(0.07992713302373886), 'behavior_loss': np.float64(0.7954204857349396)}

Episode step 1300, time diff 0.9444830417633057, total time dif 114.03643774986267)
step: 1300 @ episode report: {'average_total_reward': np.float32(4.886667), 'reward_variance': np.float32(0.70372343), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 43648} @ step loss: {'critic_loss': np.float64(0.15839389264583587), 'actor_loss': np.float64(-1.1569371581077577), 'hyper_actor_loss': np.float64(0.07787660881876945), 'behavior_loss': np.float64(0.8415459096431732)}

Episode step 1310, time diff 0.8292286396026611, total time dif 114.98092079162598)
step: 1310 @ episode report: {'average_total_reward': np.float32(4.6355557), 'reward_variance': np.float32(0.64387167), 'max_total_reward': np.float32(5.6555552), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 43968} @ step loss: {'critic_loss': np.float64(0.16428592652082444), 'actor_loss': np.float64(-1.138317835330963), 'hyper_actor_loss': np.float64(0.07800238654017448), 'behavior_loss': np.float64(0.8071264863014221)}

Episode step 1320, time diff 0.8550970554351807, total time dif 115.81014943122864)
step: 1320 @ episode report: {'average_total_reward': np.float32(4.8744445), 'reward_variance': np.float32(1.1047418), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 44288} @ step loss: {'critic_loss': np.float64(0.16660727486014365), 'actor_loss': np.float64(-1.1790091156959535), 'hyper_actor_loss': np.float64(0.07719874083995819), 'behavior_loss': np.float64(0.8302156686782837)}

Episode step 1330, time diff 0.969130277633667, total time dif 116.66524648666382)
step: 1330 @ episode report: {'average_total_reward': np.float32(4.8988886), 'reward_variance': np.float32(0.84509784), 'max_total_reward': np.float32(7.411112), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 44608} @ step loss: {'critic_loss': np.float64(0.18160430565476418), 'actor_loss': np.float64(-1.1257766008377075), 'hyper_actor_loss': np.float64(0.07629681453108787), 'behavior_loss': np.float64(0.8459791243076324)}

Episode step 1340, time diff 0.853992223739624, total time dif 117.63437676429749)
step: 1340 @ episode report: {'average_total_reward': np.float32(3.937778), 'reward_variance': np.float32(1.1265975), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 44928} @ step loss: {'critic_loss': np.float64(0.2033621147274971), 'actor_loss': np.float64(-1.1339894652366638), 'hyper_actor_loss': np.float64(0.07572239339351654), 'behavior_loss': np.float64(0.8606654107570648)}

Episode step 1350, time diff 0.8563776016235352, total time dif 118.48836898803711)
step: 1350 @ episode report: {'average_total_reward': np.float32(4.35), 'reward_variance': np.float32(0.9438826), 'max_total_reward': np.float32(6.411111), 'min_total_reward': np.float32(3.0444443), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 45248} @ step loss: {'critic_loss': np.float64(0.1723473884165287), 'actor_loss': np.float64(-1.1237635374069215), 'hyper_actor_loss': np.float64(0.0758187159895897), 'behavior_loss': np.float64(0.8162995278835297)}

Episode step 1360, time diff 0.8128776550292969, total time dif 119.34474658966064)
step: 1360 @ episode report: {'average_total_reward': np.float32(4.186667), 'reward_variance': np.float32(0.6349087), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 45568} @ step loss: {'critic_loss': np.float64(0.15145312920212745), 'actor_loss': np.float64(-1.0512728214263916), 'hyper_actor_loss': np.float64(0.07679885029792785), 'behavior_loss': np.float64(0.819199514389038)}

Episode step 1370, time diff 1.273043155670166, total time dif 120.15762424468994)
step: 1370 @ episode report: {'average_total_reward': np.float32(4.1788893), 'reward_variance': np.float32(0.5928259), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.677778), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 45888} @ step loss: {'critic_loss': np.float64(0.194199800491333), 'actor_loss': np.float64(-1.1978116512298584), 'hyper_actor_loss': np.float64(0.07791238129138947), 'behavior_loss': np.float64(0.7698199212551117)}

Episode step 1380, time diff 1.177757740020752, total time dif 121.43066740036011)
step: 1380 @ episode report: {'average_total_reward': np.float32(4.723333), 'reward_variance': np.float32(1.1900606), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 46208} @ step loss: {'critic_loss': np.float64(0.18739860951900483), 'actor_loss': np.float64(-1.1372562646865845), 'hyper_actor_loss': np.float64(0.07734387740492821), 'behavior_loss': np.float64(0.778845876455307)}

Episode step 1390, time diff 0.7879552841186523, total time dif 122.60842514038086)
step: 1390 @ episode report: {'average_total_reward': np.float32(3.788889), 'reward_variance': np.float32(0.7227901), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 46528} @ step loss: {'critic_loss': np.float64(0.20104188174009324), 'actor_loss': np.float64(-1.1043847799301147), 'hyper_actor_loss': np.float64(0.07682694494724274), 'behavior_loss': np.float64(0.7679519057273865)}

Episode step 1400, time diff 0.7881259918212891, total time dif 123.39638042449951)
step: 1400 @ episode report: {'average_total_reward': np.float32(3.7400002), 'reward_variance': np.float32(0.32936296), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 46848} @ step loss: {'critic_loss': np.float64(0.18247987926006318), 'actor_loss': np.float64(-1.0665838241577148), 'hyper_actor_loss': np.float64(0.07598180025815963), 'behavior_loss': np.float64(0.7896438896656036)}

Episode step 1410, time diff 0.7589499950408936, total time dif 124.1845064163208)
step: 1410 @ episode report: {'average_total_reward': np.float32(3.8888886), 'reward_variance': np.float32(0.3187655), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.9222224), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 47168} @ step loss: {'critic_loss': np.float64(0.1777049943804741), 'actor_loss': np.float64(-0.9921643555164337), 'hyper_actor_loss': np.float64(0.07555868104100227), 'behavior_loss': np.float64(0.7964547753334046)}

Episode step 1420, time diff 0.7934386730194092, total time dif 124.9434564113617)
step: 1420 @ episode report: {'average_total_reward': np.float32(3.9644446), 'reward_variance': np.float32(0.52836525), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(2.9222226), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 47488} @ step loss: {'critic_loss': np.float64(0.16377561539411545), 'actor_loss': np.float64(-0.9770248174667359), 'hyper_actor_loss': np.float64(0.0748997338116169), 'behavior_loss': np.float64(0.8096924483776092)}

Episode step 1430, time diff 0.8484461307525635, total time dif 125.7368950843811)
step: 1430 @ episode report: {'average_total_reward': np.float32(4.113333), 'reward_variance': np.float32(0.435995), 'max_total_reward': np.float32(5.411111), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 47808} @ step loss: {'critic_loss': np.float64(0.1498930387198925), 'actor_loss': np.float64(-0.9986032664775848), 'hyper_actor_loss': np.float64(0.07404507547616959), 'behavior_loss': np.float64(0.7812219500541687)}

Episode step 1440, time diff 0.8677003383636475, total time dif 126.58534121513367)
step: 1440 @ episode report: {'average_total_reward': np.float32(4.076667), 'reward_variance': np.float32(0.6900853), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 48128} @ step loss: {'critic_loss': np.float64(0.17001927644014359), 'actor_loss': np.float64(-1.0355841398239136), 'hyper_actor_loss': np.float64(0.07375500574707985), 'behavior_loss': np.float64(0.7554966449737549)}

Episode step 1450, time diff 0.9939351081848145, total time dif 127.45304155349731)
step: 1450 @ episode report: {'average_total_reward': np.float32(3.5133336), 'reward_variance': np.float32(1.4728346), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 48448} @ step loss: {'critic_loss': np.float64(0.16301662102341652), 'actor_loss': np.float64(-1.1006060600280763), 'hyper_actor_loss': np.float64(0.07360115274786949), 'behavior_loss': np.float64(0.7778743267059326)}

Episode step 1460, time diff 1.01666259765625, total time dif 128.44697666168213)
step: 1460 @ episode report: {'average_total_reward': np.float32(3.3277779), 'reward_variance': np.float32(1.3528458), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 48768} @ step loss: {'critic_loss': np.float64(0.158207868039608), 'actor_loss': np.float64(-1.062838888168335), 'hyper_actor_loss': np.float64(0.0735851526260376), 'behavior_loss': np.float64(0.7301403939723968)}

Episode step 1470, time diff 1.1798889636993408, total time dif 129.46363925933838)
step: 1470 @ episode report: {'average_total_reward': np.float32(3.3400002), 'reward_variance': np.float32(0.37316552), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 49088} @ step loss: {'critic_loss': np.float64(0.14564668238162995), 'actor_loss': np.float64(-1.0767432928085328), 'hyper_actor_loss': np.float64(0.0739600658416748), 'behavior_loss': np.float64(0.7181000530719757)}

Episode step 1480, time diff 0.9283592700958252, total time dif 130.64352822303772)
step: 1480 @ episode report: {'average_total_reward': np.float32(3.5011108), 'reward_variance': np.float32(0.38986307), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 49408} @ step loss: {'critic_loss': np.float64(0.14363605603575708), 'actor_loss': np.float64(-1.0281074285507201), 'hyper_actor_loss': np.float64(0.07424496114253998), 'behavior_loss': np.float64(0.7032316982746124)}

Episode step 1490, time diff 0.9149124622344971, total time dif 131.57188749313354)
step: 1490 @ episode report: {'average_total_reward': np.float32(3.0544448), 'reward_variance': np.float32(0.26689997), 'max_total_reward': np.float32(3.9222224), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 49728} @ step loss: {'critic_loss': np.float64(0.15046589374542235), 'actor_loss': np.float64(-1.0586535930633545), 'hyper_actor_loss': np.float64(0.07378239184617996), 'behavior_loss': np.float64(0.7048013150691986)}

Episode step 1500, time diff 0.8581430912017822, total time dif 132.48679995536804)
step: 1500 @ episode report: {'average_total_reward': np.float32(3.0277781), 'reward_variance': np.float32(0.520821), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.166667), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 50048} @ step loss: {'critic_loss': np.float64(0.1429615281522274), 'actor_loss': np.float64(-1.0168226480484008), 'hyper_actor_loss': np.float64(0.07284364774823189), 'behavior_loss': np.float64(0.7122219920158386)}

Episode step 1510, time diff 0.8891422748565674, total time dif 133.34494304656982)
step: 1510 @ episode report: {'average_total_reward': np.float32(3.6644447), 'reward_variance': np.float32(1.2090567), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 50368} @ step loss: {'critic_loss': np.float64(0.1429864451289177), 'actor_loss': np.float64(-1.0124155461788178), 'hyper_actor_loss': np.float64(0.07185514196753502), 'behavior_loss': np.float64(0.7158338069915772)}

Episode step 1520, time diff 0.8073084354400635, total time dif 134.2340853214264)
step: 1520 @ episode report: {'average_total_reward': np.float32(3.1155555), 'reward_variance': np.float32(1.0310173), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(1.1666666), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 50688} @ step loss: {'critic_loss': np.float64(0.13445981368422508), 'actor_loss': np.float64(-1.0429152488708495), 'hyper_actor_loss': np.float64(0.07098375707864761), 'behavior_loss': np.float64(0.733691519498825)}

Episode step 1530, time diff 0.823798418045044, total time dif 135.04139375686646)
step: 1530 @ episode report: {'average_total_reward': np.float32(3.1277778), 'reward_variance': np.float32(0.63361126), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 51008} @ step loss: {'critic_loss': np.float64(0.13169334903359414), 'actor_loss': np.float64(-1.0325069546699523), 'hyper_actor_loss': np.float64(0.07034217938780785), 'behavior_loss': np.float64(0.7527162671089173)}

Episode step 1540, time diff 0.9758796691894531, total time dif 135.8651921749115)
step: 1540 @ episode report: {'average_total_reward': np.float32(3.637778), 'reward_variance': np.float32(0.5214125), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 51328} @ step loss: {'critic_loss': np.float64(0.14403281807899476), 'actor_loss': np.float64(-1.0764463305473329), 'hyper_actor_loss': np.float64(0.0696004718542099), 'behavior_loss': np.float64(0.7311861157417298)}

Episode step 1550, time diff 0.7656636238098145, total time dif 136.84107184410095)
step: 1550 @ episode report: {'average_total_reward': np.float32(3.288889), 'reward_variance': np.float32(0.52167916), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 51648} @ step loss: {'critic_loss': np.float64(0.1382335126399994), 'actor_loss': np.float64(-1.0794941306114196), 'hyper_actor_loss': np.float64(0.06855606511235238), 'behavior_loss': np.float64(0.7750239908695221)}

Episode step 1560, time diff 0.8307244777679443, total time dif 137.60673546791077)
step: 1560 @ episode report: {'average_total_reward': np.float32(3.3644447), 'reward_variance': np.float32(1.2354271), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 51968} @ step loss: {'critic_loss': np.float64(0.1278611607849598), 'actor_loss': np.float64(-1.0072100341320038), 'hyper_actor_loss': np.float64(0.06771918907761573), 'behavior_loss': np.float64(0.8199450075626373)}

Episode step 1570, time diff 0.8614838123321533, total time dif 138.4374599456787)
step: 1570 @ episode report: {'average_total_reward': np.float32(3.5155556), 'reward_variance': np.float32(0.38370872), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 52288} @ step loss: {'critic_loss': np.float64(0.12495428919792176), 'actor_loss': np.float64(-1.030988347530365), 'hyper_actor_loss': np.float64(0.06701075285673141), 'behavior_loss': np.float64(0.8296540021896363)}

Episode step 1580, time diff 1.542689323425293, total time dif 139.29894375801086)
step: 1580 @ episode report: {'average_total_reward': np.float32(3.1277778), 'reward_variance': np.float32(0.31790745), 'max_total_reward': np.float32(4.166667), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 52608} @ step loss: {'critic_loss': np.float64(0.1338854745030403), 'actor_loss': np.float64(-0.9927438497543335), 'hyper_actor_loss': np.float64(0.06665771156549453), 'behavior_loss': np.float64(0.8094274461269378)}

Episode step 1590, time diff 2.3145575523376465, total time dif 140.84163308143616)
step: 1590 @ episode report: {'average_total_reward': np.float32(3.191111), 'reward_variance': np.float32(0.2417728), 'max_total_reward': np.float32(4.1666665), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 52928} @ step loss: {'critic_loss': np.float64(0.1571672037243843), 'actor_loss': np.float64(-1.0413887619972229), 'hyper_actor_loss': np.float64(0.06615992933511734), 'behavior_loss': np.float64(0.8442388236522674)}

Episode step 1600, time diff 1.9955523014068604, total time dif 143.1561906337738)
step: 1600 @ episode report: {'average_total_reward': np.float32(3.0644445), 'reward_variance': np.float32(0.18248895), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 53248} @ step loss: {'critic_loss': np.float64(0.13033307567238808), 'actor_loss': np.float64(-0.931851613521576), 'hyper_actor_loss': np.float64(0.06610325425863266), 'behavior_loss': np.float64(0.871362429857254)}

Episode step 1610, time diff 2.039132833480835, total time dif 145.15174293518066)
step: 1610 @ episode report: {'average_total_reward': np.float32(3.3522224), 'reward_variance': np.float32(0.61019886), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 53568} @ step loss: {'critic_loss': np.float64(0.1419228047132492), 'actor_loss': np.float64(-0.9407122373580933), 'hyper_actor_loss': np.float64(0.06580718159675598), 'behavior_loss': np.float64(0.887171596288681)}

Episode step 1620, time diff 1.6428744792938232, total time dif 147.1908757686615)
step: 1620 @ episode report: {'average_total_reward': np.float32(3.2522225), 'reward_variance': np.float32(1.1313349), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.0444443), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 53888} @ step loss: {'critic_loss': np.float64(0.14928072690963745), 'actor_loss': np.float64(-0.920966112613678), 'hyper_actor_loss': np.float64(0.06554752886295319), 'behavior_loss': np.float64(0.8768945634365082)}

Episode step 1630, time diff 1.492896556854248, total time dif 148.83375024795532)
step: 1630 @ episode report: {'average_total_reward': np.float32(3.4155555), 'reward_variance': np.float32(0.69015324), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 54208} @ step loss: {'critic_loss': np.float64(0.14788179248571395), 'actor_loss': np.float64(-1.0048039197921752), 'hyper_actor_loss': np.float64(0.0652521938085556), 'behavior_loss': np.float64(0.8436655223369598)}

Episode step 1640, time diff 1.7002894878387451, total time dif 150.32664680480957)
step: 1640 @ episode report: {'average_total_reward': np.float32(3.417778), 'reward_variance': np.float32(1.1401532), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 54528} @ step loss: {'critic_loss': np.float64(0.13100543096661568), 'actor_loss': np.float64(-0.9168185293674469), 'hyper_actor_loss': np.float64(0.0646838515996933), 'behavior_loss': np.float64(0.8767375409603119)}

Episode step 1650, time diff 1.5585293769836426, total time dif 152.02693629264832)
step: 1650 @ episode report: {'average_total_reward': np.float32(3.2888894), 'reward_variance': np.float32(0.19049387), 'max_total_reward': np.float32(4.166667), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 54848} @ step loss: {'critic_loss': np.float64(0.12798314541578293), 'actor_loss': np.float64(-0.9308613896369934), 'hyper_actor_loss': np.float64(0.06368694454431534), 'behavior_loss': np.float64(0.9120724081993103)}

Episode step 1660, time diff 1.8263466358184814, total time dif 153.58546566963196)
step: 1660 @ episode report: {'average_total_reward': np.float32(3.3033333), 'reward_variance': np.float32(1.2144456), 'max_total_reward': np.float32(5.411111), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 55168} @ step loss: {'critic_loss': np.float64(0.1225882314145565), 'actor_loss': np.float64(-0.95435711145401), 'hyper_actor_loss': np.float64(0.06309513002634048), 'behavior_loss': np.float64(0.9238826036453247)}

Episode step 1670, time diff 1.7954940795898438, total time dif 155.41181230545044)
step: 1670 @ episode report: {'average_total_reward': np.float32(3.44), 'reward_variance': np.float32(0.7746223), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(1.6777778), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 55488} @ step loss: {'critic_loss': np.float64(0.12481576427817345), 'actor_loss': np.float64(-0.9667093873023986), 'hyper_actor_loss': np.float64(0.06283417940139771), 'behavior_loss': np.float64(0.9236033916473388)}

Episode step 1680, time diff 1.3422205448150635, total time dif 157.20730638504028)
step: 1680 @ episode report: {'average_total_reward': np.float32(3.266667), 'reward_variance': np.float32(0.5837038), 'max_total_reward': np.float32(4.166667), 'min_total_reward': np.float32(1.6777779), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 55808} @ step loss: {'critic_loss': np.float64(0.1261641077697277), 'actor_loss': np.float64(-1.035962426662445), 'hyper_actor_loss': np.float64(0.06262880116701126), 'behavior_loss': np.float64(0.9043260037899017)}

Episode step 1690, time diff 1.077958583831787, total time dif 158.54952692985535)
step: 1690 @ episode report: {'average_total_reward': np.float32(2.5277781), 'reward_variance': np.float32(0.20449999), 'max_total_reward': np.float32(3.2888887), 'min_total_reward': np.float32(2.0444443), 'average_n_step': np.float32(4.3), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 56128} @ step loss: {'critic_loss': np.float64(0.13024672567844392), 'actor_loss': np.float64(-0.9640595614910126), 'hyper_actor_loss': np.float64(0.06237973496317863), 'behavior_loss': np.float64(0.9283713817596435)}

Episode step 1700, time diff 1.4453213214874268, total time dif 159.62748551368713)
step: 1700 @ episode report: {'average_total_reward': np.float32(2.22), 'reward_variance': np.float32(0.2982914), 'max_total_reward': np.float32(3.0444448), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 56448} @ step loss: {'critic_loss': np.float64(0.1322885550558567), 'actor_loss': np.float64(-1.0413134217262268), 'hyper_actor_loss': np.float64(0.06196601837873459), 'behavior_loss': np.float64(0.9200897336006164)}

Episode step 1710, time diff 1.4620211124420166, total time dif 161.07280683517456)
step: 1710 @ episode report: {'average_total_reward': np.float32(2.4322221), 'reward_variance': np.float32(0.3557889), 'max_total_reward': np.float32(3.8), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 56768} @ step loss: {'critic_loss': np.float64(0.1266123943030834), 'actor_loss': np.float64(-0.9726266682147979), 'hyper_actor_loss': np.float64(0.061768009513616565), 'behavior_loss': np.float64(0.9205081224441528)}

Episode step 1720, time diff 1.305654525756836, total time dif 162.53482794761658)
step: 1720 @ episode report: {'average_total_reward': np.float32(1.9588888), 'reward_variance': np.float32(0.25447038), 'max_total_reward': np.float32(3.1666667), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 57088} @ step loss: {'critic_loss': np.float64(0.11538924425840377), 'actor_loss': np.float64(-1.0404803156852722), 'hyper_actor_loss': np.float64(0.0621916513890028), 'behavior_loss': np.float64(1.0198444306850434)}

Episode step 1730, time diff 1.259366750717163, total time dif 163.8404824733734)
step: 1730 @ episode report: {'average_total_reward': np.float32(1.72), 'reward_variance': np.float32(0.51940256), 'max_total_reward': np.float32(3.166667), 'min_total_reward': np.float32(0.79999995), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 57408} @ step loss: {'critic_loss': np.float64(0.11968760788440705), 'actor_loss': np.float64(-1.1183519601821899), 'hyper_actor_loss': np.float64(0.06205957680940628), 'behavior_loss': np.float64(0.982478803396225)}

Episode step 1740, time diff 1.2376067638397217, total time dif 165.09984922409058)
step: 1740 @ episode report: {'average_total_reward': np.float32(1.0955555), 'reward_variance': np.float32(0.066424705), 'max_total_reward': np.float32(1.8000001), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(3.1), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 57728} @ step loss: {'critic_loss': np.float64(0.12477253898978233), 'actor_loss': np.float64(-1.1208660304546356), 'hyper_actor_loss': np.float64(0.06183453015983105), 'behavior_loss': np.float64(1.0547059535980225)}

Episode step 1750, time diff 1.2271912097930908, total time dif 166.3374559879303)
step: 1750 @ episode report: {'average_total_reward': np.float32(1.5488889), 'reward_variance': np.float32(0.2692642), 'max_total_reward': np.float32(2.1666667), 'min_total_reward': np.float32(0.6777778), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 58048} @ step loss: {'critic_loss': np.float64(0.1301190122961998), 'actor_loss': np.float64(-0.9159998297691345), 'hyper_actor_loss': np.float64(0.061774886399507525), 'behavior_loss': np.float64(1.0907983481884003)}

Episode step 1760, time diff 1.2597341537475586, total time dif 167.5646471977234)
step: 1760 @ episode report: {'average_total_reward': np.float32(2.02), 'reward_variance': np.float32(0.12552597), 'max_total_reward': np.float32(2.677778), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 58368} @ step loss: {'critic_loss': np.float64(0.1272615075111389), 'actor_loss': np.float64(-0.9208194673061371), 'hyper_actor_loss': np.float64(0.06115924678742886), 'behavior_loss': np.float64(1.0556856155395509)}

Episode step 1770, time diff 1.268427848815918, total time dif 168.82438135147095)
step: 1770 @ episode report: {'average_total_reward': np.float32(1.8733333), 'reward_variance': np.float32(0.036449395), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(1.5555556), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(4.0), 'buffer_size': 58688} @ step loss: {'critic_loss': np.float64(0.12143982946872711), 'actor_loss': np.float64(-0.928116774559021), 'hyper_actor_loss': np.float64(0.060728348791599274), 'behavior_loss': np.float64(1.144535207748413)}

Episode step 1780, time diff 1.691321849822998, total time dif 170.09280920028687)
step: 1780 @ episode report: {'average_total_reward': np.float32(2.181111), 'reward_variance': np.float32(0.48526055), 'max_total_reward': np.float32(3.166667), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 59008} @ step loss: {'critic_loss': np.float64(0.12336539775133133), 'actor_loss': np.float64(-0.927590012550354), 'hyper_actor_loss': np.float64(0.06028868295252323), 'behavior_loss': np.float64(1.1170696020126343)}

Episode step 1790, time diff 1.284710168838501, total time dif 171.78413105010986)
step: 1790 @ episode report: {'average_total_reward': np.float32(1.8344444), 'reward_variance': np.float32(0.08672719), 'max_total_reward': np.float32(2.166667), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 59328} @ step loss: {'critic_loss': np.float64(0.11044640839099884), 'actor_loss': np.float64(-0.8934359312057495), 'hyper_actor_loss': np.float64(0.05964840054512024), 'behavior_loss': np.float64(1.114350813627243)}

Episode step 1800, time diff 1.3402760028839111, total time dif 173.06884121894836)
step: 1800 @ episode report: {'average_total_reward': np.float32(2.02), 'reward_variance': np.float32(0.32552594), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(1.1666666), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 59648} @ step loss: {'critic_loss': np.float64(0.13243651688098906), 'actor_loss': np.float64(-0.9423709273338318), 'hyper_actor_loss': np.float64(0.05873299725353718), 'behavior_loss': np.float64(1.2162353456020356)}

Episode step 1810, time diff 2.1097376346588135, total time dif 174.40911722183228)
step: 1810 @ episode report: {'average_total_reward': np.float32(1.8222221), 'reward_variance': np.float32(0.3736543), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 59968} @ step loss: {'critic_loss': np.float64(0.10685045272111893), 'actor_loss': np.float64(-0.8959798097610474), 'hyper_actor_loss': np.float64(0.057635863125324246), 'behavior_loss': np.float64(1.1861202120780945)}

Episode step 1820, time diff 1.3659296035766602, total time dif 176.5188548564911)
step: 1820 @ episode report: {'average_total_reward': np.float32(2.0444446), 'reward_variance': np.float32(0.035851866), 'max_total_reward': np.float32(2.2888892), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(4.0), 'buffer_size': 60288} @ step loss: {'critic_loss': np.float64(0.12023888751864434), 'actor_loss': np.float64(-0.9075864315032959), 'hyper_actor_loss': np.float64(0.05701662115752697), 'behavior_loss': np.float64(1.318400502204895)}

Episode step 1830, time diff 1.4169540405273438, total time dif 177.88478446006775)
step: 1830 @ episode report: {'average_total_reward': np.float32(1.82), 'reward_variance': np.float32(0.17950127), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.9222223), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 60608} @ step loss: {'critic_loss': np.float64(0.10444430932402611), 'actor_loss': np.float64(-0.8924096167087555), 'hyper_actor_loss': np.float64(0.05690288245677948), 'behavior_loss': np.float64(1.243919551372528)}

Episode step 1840, time diff 1.5535919666290283, total time dif 179.3017385005951)
step: 1840 @ episode report: {'average_total_reward': np.float32(1.9200001), 'reward_variance': np.float32(0.37967414), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 60928} @ step loss: {'critic_loss': np.float64(0.11005515083670617), 'actor_loss': np.float64(-0.8944991409778595), 'hyper_actor_loss': np.float64(0.05692809410393238), 'behavior_loss': np.float64(1.3387646436691285)}

Episode step 1850, time diff 1.4012722969055176, total time dif 180.85533046722412)
step: 1850 @ episode report: {'average_total_reward': np.float32(2.3833334), 'reward_variance': np.float32(0.40082103), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 61248} @ step loss: {'critic_loss': np.float64(0.11186228469014167), 'actor_loss': np.float64(-0.8884937584400177), 'hyper_actor_loss': np.float64(0.056905566900968554), 'behavior_loss': np.float64(1.354737138748169)}

Episode step 1860, time diff 1.2023162841796875, total time dif 182.25660276412964)
step: 1860 @ episode report: {'average_total_reward': np.float32(1.9444447), 'reward_variance': np.float32(0.41612354), 'max_total_reward': np.float32(3.0444448), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 61568} @ step loss: {'critic_loss': np.float64(0.11416937336325646), 'actor_loss': np.float64(-0.9052496492862702), 'hyper_actor_loss': np.float64(0.056768960878252984), 'behavior_loss': np.float64(1.2743561744689942)}

Episode step 1870, time diff 0.8454217910766602, total time dif 183.45891904830933)
step: 1870 @ episode report: {'average_total_reward': np.float32(2.1444447), 'reward_variance': np.float32(0.3014074), 'max_total_reward': np.float32(3.166667), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 61888} @ step loss: {'critic_loss': np.float64(0.11371674165129661), 'actor_loss': np.float64(-0.9234718322753906), 'hyper_actor_loss': np.float64(0.057558974251151085), 'behavior_loss': np.float64(1.242739701271057)}

Episode step 1880, time diff 0.9056098461151123, total time dif 184.304340839386)
step: 1880 @ episode report: {'average_total_reward': np.float32(2.6177778), 'reward_variance': np.float32(0.2989432), 'max_total_reward': np.float32(3.288889), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 62208} @ step loss: {'critic_loss': np.float64(0.12962031960487366), 'actor_loss': np.float64(-0.9300172388553619), 'hyper_actor_loss': np.float64(0.05835817828774452), 'behavior_loss': np.float64(1.2741232514381409)}

Episode step 1890, time diff 0.7735886573791504, total time dif 185.2099506855011)
step: 1890 @ episode report: {'average_total_reward': np.float32(3.268889), 'reward_variance': np.float32(0.9864643), 'max_total_reward': np.float32(5.166667), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 62528} @ step loss: {'critic_loss': np.float64(0.11624840423464775), 'actor_loss': np.float64(-0.9407837986946106), 'hyper_actor_loss': np.float64(0.059309064596891406), 'behavior_loss': np.float64(1.219559872150421)}

Episode step 1900, time diff 0.924720048904419, total time dif 185.98353934288025)
step: 1900 @ episode report: {'average_total_reward': np.float32(4.2011113), 'reward_variance': np.float32(1.051863), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 62848} @ step loss: {'critic_loss': np.float64(0.10716530755162239), 'actor_loss': np.float64(-0.8980162501335144), 'hyper_actor_loss': np.float64(0.05968076512217522), 'behavior_loss': np.float64(1.160148823261261)}

Episode step 1910, time diff 0.8715434074401855, total time dif 186.90825939178467)
step: 1910 @ episode report: {'average_total_reward': np.float32(3.7888894), 'reward_variance': np.float32(1.0509877), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 63168} @ step loss: {'critic_loss': np.float64(0.11425493136048318), 'actor_loss': np.float64(-0.9379426598548889), 'hyper_actor_loss': np.float64(0.05919554978609085), 'behavior_loss': np.float64(1.141563594341278)}

Episode step 1920, time diff 0.7791814804077148, total time dif 187.77980279922485)
step: 1920 @ episode report: {'average_total_reward': np.float32(4.511111), 'reward_variance': np.float32(0.8745183), 'max_total_reward': np.float32(6.411111), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 63488} @ step loss: {'critic_loss': np.float64(0.11448942646384239), 'actor_loss': np.float64(-0.8980252146720886), 'hyper_actor_loss': np.float64(0.05860581956803799), 'behavior_loss': np.float64(1.131355309486389)}

Episode step 1930, time diff 0.7201790809631348, total time dif 188.55898427963257)
step: 1930 @ episode report: {'average_total_reward': np.float32(4.037778), 'reward_variance': np.float32(0.74161), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 63808} @ step loss: {'critic_loss': np.float64(0.10905285403132439), 'actor_loss': np.float64(-0.9389427959918976), 'hyper_actor_loss': np.float64(0.05825975015759468), 'behavior_loss': np.float64(1.0618271470069884)}

Episode step 1940, time diff 0.7504632472991943, total time dif 189.2791633605957)
step: 1940 @ episode report: {'average_total_reward': np.float32(4.498889), 'reward_variance': np.float32(1.340678), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 64128} @ step loss: {'critic_loss': np.float64(0.12008479684591293), 'actor_loss': np.float64(-0.9477187335491181), 'hyper_actor_loss': np.float64(0.057966163754463194), 'behavior_loss': np.float64(1.0836361706256867)}

Episode step 1950, time diff 0.7777976989746094, total time dif 190.0296266078949)
step: 1950 @ episode report: {'average_total_reward': np.float32(4.5600004), 'reward_variance': np.float32(1.5431902), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 64448} @ step loss: {'critic_loss': np.float64(0.10724797695875168), 'actor_loss': np.float64(-0.9269739866256714), 'hyper_actor_loss': np.float64(0.0581328172236681), 'behavior_loss': np.float64(0.9537643611431121)}

Episode step 1960, time diff 0.7622349262237549, total time dif 190.8074243068695)
step: 1960 @ episode report: {'average_total_reward': np.float32(4.8477783), 'reward_variance': np.float32(1.8652111), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 64768} @ step loss: {'critic_loss': np.float64(0.10600480511784553), 'actor_loss': np.float64(-0.9384749293327331), 'hyper_actor_loss': np.float64(0.057999420538544656), 'behavior_loss': np.float64(0.9629403591156006)}

Episode step 1970, time diff 0.7412619590759277, total time dif 191.56965923309326)
step: 1970 @ episode report: {'average_total_reward': np.float32(4.9722223), 'reward_variance': np.float32(0.7238088), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 65088} @ step loss: {'critic_loss': np.float64(0.11722481474280358), 'actor_loss': np.float64(-0.9269642353057861), 'hyper_actor_loss': np.float64(0.058370545506477356), 'behavior_loss': np.float64(0.9805149793624878)}

Episode step 1980, time diff 0.8090429306030273, total time dif 192.3109211921692)
step: 1980 @ episode report: {'average_total_reward': np.float32(5.223334), 'reward_variance': np.float32(0.9851468), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(4.044445), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 65408} @ step loss: {'critic_loss': np.float64(0.10902479737997055), 'actor_loss': np.float64(-0.969264668226242), 'hyper_actor_loss': np.float64(0.058670812845230104), 'behavior_loss': np.float64(0.9728838682174683)}

Episode step 1990, time diff 1.3418054580688477, total time dif 193.11996412277222)
step: 1990 @ episode report: {'average_total_reward': np.float32(5.347778), 'reward_variance': np.float32(1.2859769), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 65728} @ step loss: {'critic_loss': np.float64(0.1273512452840805), 'actor_loss': np.float64(-0.9987354457378388), 'hyper_actor_loss': np.float64(0.059180884063243865), 'behavior_loss': np.float64(0.9668432354927063)}

Episode step 2000, time diff 1.1465880870819092, total time dif 194.46176958084106)
step: 2000 @ episode report: {'average_total_reward': np.float32(5.123334), 'reward_variance': np.float32(1.2411222), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 66048} @ step loss: {'critic_loss': np.float64(0.11760059520602226), 'actor_loss': np.float64(-1.00610870718956), 'hyper_actor_loss': np.float64(0.05994746685028076), 'behavior_loss': np.float64(0.9325786352157592)}

Episode step 2010, time diff 1.2866194248199463, total time dif 195.60835766792297)
step: 2010 @ episode report: {'average_total_reward': np.float32(6.3700004), 'reward_variance': np.float32(3.4751124), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 66368} @ step loss: {'critic_loss': np.float64(0.12171093299984932), 'actor_loss': np.float64(-1.0001587390899658), 'hyper_actor_loss': np.float64(0.06085318960249424), 'behavior_loss': np.float64(0.949582040309906)}

Episode step 2020, time diff 1.0743296146392822, total time dif 196.89497709274292)
step: 2020 @ episode report: {'average_total_reward': np.float32(5.2333336), 'reward_variance': np.float32(0.864864), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 66688} @ step loss: {'critic_loss': np.float64(0.12049582526087761), 'actor_loss': np.float64(-1.0222251176834107), 'hyper_actor_loss': np.float64(0.06141192838549614), 'behavior_loss': np.float64(0.8817253589630127)}

Episode step 2030, time diff 0.8536376953125, total time dif 197.9693067073822)
step: 2030 @ episode report: {'average_total_reward': np.float32(6.0577774), 'reward_variance': np.float32(2.3332295), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 67008} @ step loss: {'critic_loss': np.float64(0.11828462630510331), 'actor_loss': np.float64(-1.0648486733436584), 'hyper_actor_loss': np.float64(0.06302323751151562), 'behavior_loss': np.float64(0.8848832368850708)}

Episode step 2040, time diff 0.8025236129760742, total time dif 198.8229444026947)
step: 2040 @ episode report: {'average_total_reward': np.float32(6.2822227), 'reward_variance': np.float32(4.0380054), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 67328} @ step loss: {'critic_loss': np.float64(0.12917297706007957), 'actor_loss': np.float64(-1.066592514514923), 'hyper_actor_loss': np.float64(0.06398903653025627), 'behavior_loss': np.float64(0.8534786999225616)}

Episode step 2050, time diff 0.8374719619750977, total time dif 199.62546801567078)
step: 2050 @ episode report: {'average_total_reward': np.float32(6.1944447), 'reward_variance': np.float32(1.8178327), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 67648} @ step loss: {'critic_loss': np.float64(0.1343573309481144), 'actor_loss': np.float64(-1.0408676147460938), 'hyper_actor_loss': np.float64(0.0643252708017826), 'behavior_loss': np.float64(0.8555146515369415)}

Episode step 2060, time diff 0.8099164962768555, total time dif 200.46293997764587)
step: 2060 @ episode report: {'average_total_reward': np.float32(6.494445), 'reward_variance': np.float32(3.2202048), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 67968} @ step loss: {'critic_loss': np.float64(0.13180167973041534), 'actor_loss': np.float64(-1.083421540260315), 'hyper_actor_loss': np.float64(0.0648369163274765), 'behavior_loss': np.float64(0.8414643347263336)}

Episode step 2070, time diff 1.1141841411590576, total time dif 201.27285647392273)
step: 2070 @ episode report: {'average_total_reward': np.float32(6.1966667), 'reward_variance': np.float32(1.410594), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 68288} @ step loss: {'critic_loss': np.float64(0.14145674481987952), 'actor_loss': np.float64(-1.0990034937858582), 'hyper_actor_loss': np.float64(0.06434719637036324), 'behavior_loss': np.float64(0.815870600938797)}

Episode step 2080, time diff 2.0319578647613525, total time dif 202.3870406150818)
step: 2080 @ episode report: {'average_total_reward': np.float32(5.784445), 'reward_variance': np.float32(2.4070425), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 68608} @ step loss: {'critic_loss': np.float64(0.12708347514271737), 'actor_loss': np.float64(-1.0292720437049865), 'hyper_actor_loss': np.float64(0.06342497393488884), 'behavior_loss': np.float64(0.8460204005241394)}

Episode step 2090, time diff 2.0208303928375244, total time dif 204.41899847984314)
step: 2090 @ episode report: {'average_total_reward': np.float32(6.457778), 'reward_variance': np.float32(1.147156), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 68928} @ step loss: {'critic_loss': np.float64(0.15392922461032868), 'actor_loss': np.float64(-1.0403966903686523), 'hyper_actor_loss': np.float64(0.06291311159729958), 'behavior_loss': np.float64(0.8563884437084198)}

Episode step 2100, time diff 1.961122989654541, total time dif 206.43982887268066)
step: 2100 @ episode report: {'average_total_reward': np.float32(6.1333337), 'reward_variance': np.float32(1.5970862), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 69248} @ step loss: {'critic_loss': np.float64(0.13010629042983055), 'actor_loss': np.float64(-1.0095984637737274), 'hyper_actor_loss': np.float64(0.061840837076306346), 'behavior_loss': np.float64(0.8139242768287659)}

Episode step 2110, time diff 1.6308553218841553, total time dif 208.4009518623352)
step: 2110 @ episode report: {'average_total_reward': np.float32(6.331111), 'reward_variance': np.float32(0.92293346), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 69568} @ step loss: {'critic_loss': np.float64(0.12446799725294114), 'actor_loss': np.float64(-0.9493712842464447), 'hyper_actor_loss': np.float64(0.061361901462078094), 'behavior_loss': np.float64(0.8036917686462403)}

Episode step 2120, time diff 1.7008018493652344, total time dif 210.03180718421936)
step: 2120 @ episode report: {'average_total_reward': np.float32(6.1333337), 'reward_variance': np.float32(2.3083456), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 69888} @ step loss: {'critic_loss': np.float64(0.14407659322023392), 'actor_loss': np.float64(-1.0132602512836457), 'hyper_actor_loss': np.float64(0.060644886642694476), 'behavior_loss': np.float64(0.8440067768096924)}

Episode step 2130, time diff 1.5048303604125977, total time dif 211.7326090335846)
step: 2130 @ episode report: {'average_total_reward': np.float32(5.296667), 'reward_variance': np.float32(0.85521114), 'max_total_reward': np.float32(6.6555567), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 70208} @ step loss: {'critic_loss': np.float64(0.1416337512433529), 'actor_loss': np.float64(-1.0077206492424011), 'hyper_actor_loss': np.float64(0.060396114364266396), 'behavior_loss': np.float64(0.8116600632667541)}

Episode step 2140, time diff 1.448173999786377, total time dif 213.2374393939972)
step: 2140 @ episode report: {'average_total_reward': np.float32(6.0088897), 'reward_variance': np.float32(1.2444153), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 70528} @ step loss: {'critic_loss': np.float64(0.12582507058978082), 'actor_loss': np.float64(-0.9803492665290833), 'hyper_actor_loss': np.float64(0.060813446342945096), 'behavior_loss': np.float64(0.8311022520065308)}

Episode step 2150, time diff 1.6519765853881836, total time dif 214.68561339378357)
step: 2150 @ episode report: {'average_total_reward': np.float32(5.1333337), 'reward_variance': np.float32(1.5053583), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 70848} @ step loss: {'critic_loss': np.float64(0.12525070011615752), 'actor_loss': np.float64(-1.0319798648357392), 'hyper_actor_loss': np.float64(0.06090528033673763), 'behavior_loss': np.float64(0.8287118315696717)}

Episode step 2160, time diff 1.4729368686676025, total time dif 216.33758997917175)
step: 2160 @ episode report: {'average_total_reward': np.float32(4.137778), 'reward_variance': np.float32(0.66489387), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.0444448), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 71168} @ step loss: {'critic_loss': np.float64(0.1160230040550232), 'actor_loss': np.float64(-1.0148234128952027), 'hyper_actor_loss': np.float64(0.05980498716235161), 'behavior_loss': np.float64(0.8327938258647919)}

Episode step 2170, time diff 1.7329661846160889, total time dif 217.81052684783936)
step: 2170 @ episode report: {'average_total_reward': np.float32(4.0622225), 'reward_variance': np.float32(0.91244954), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 71488} @ step loss: {'critic_loss': np.float64(0.1176795057952404), 'actor_loss': np.float64(-0.9524244129657745), 'hyper_actor_loss': np.float64(0.058618148788809776), 'behavior_loss': np.float64(0.9316366195678711)}

Episode step 2180, time diff 1.1776249408721924, total time dif 219.54349303245544)
step: 2180 @ episode report: {'average_total_reward': np.float32(4.3988886), 'reward_variance': np.float32(0.9355172), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 71808} @ step loss: {'critic_loss': np.float64(0.1363238587975502), 'actor_loss': np.float64(-1.0355576813220977), 'hyper_actor_loss': np.float64(0.05750029534101486), 'behavior_loss': np.float64(0.8747193574905395)}

Episode step 2190, time diff 1.26912522315979, total time dif 220.72111797332764)
step: 2190 @ episode report: {'average_total_reward': np.float32(4.1011114), 'reward_variance': np.float32(0.32630745), 'max_total_reward': np.float32(5.166667), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 72128} @ step loss: {'critic_loss': np.float64(0.1323771946132183), 'actor_loss': np.float64(-1.051649558544159), 'hyper_actor_loss': np.float64(0.0562645822763443), 'behavior_loss': np.float64(0.8647021770477294)}

Episode step 2200, time diff 1.4888570308685303, total time dif 221.99024319648743)
step: 2200 @ episode report: {'average_total_reward': np.float32(4.0644445), 'reward_variance': np.float32(1.2144396), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(2.8000002), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 72448} @ step loss: {'critic_loss': np.float64(0.13066384196281433), 'actor_loss': np.float64(-0.9732765853404999), 'hyper_actor_loss': np.float64(0.05524692460894585), 'behavior_loss': np.float64(0.9218967258930206)}

Episode step 2210, time diff 1.4320614337921143, total time dif 223.47910022735596)
step: 2210 @ episode report: {'average_total_reward': np.float32(3.4011111), 'reward_variance': np.float32(0.8016162), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 72768} @ step loss: {'critic_loss': np.float64(0.1341255523264408), 'actor_loss': np.float64(-0.993244856595993), 'hyper_actor_loss': np.float64(0.053814608976244925), 'behavior_loss': np.float64(0.9300445675849914)}

Episode step 2220, time diff 1.2907023429870605, total time dif 224.91116166114807)
step: 2220 @ episode report: {'average_total_reward': np.float32(3.0400002), 'reward_variance': np.float32(0.5175854), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 73088} @ step loss: {'critic_loss': np.float64(0.1322969302535057), 'actor_loss': np.float64(-0.9806553304195404), 'hyper_actor_loss': np.float64(0.052496602013707164), 'behavior_loss': np.float64(0.976815277338028)}

Episode step 2230, time diff 1.3923523426055908, total time dif 226.20186400413513)
step: 2230 @ episode report: {'average_total_reward': np.float32(2.817778), 'reward_variance': np.float32(0.40760994), 'max_total_reward': np.float32(3.9222224), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 73408} @ step loss: {'critic_loss': np.float64(0.13093168139457703), 'actor_loss': np.float64(-0.916029816865921), 'hyper_actor_loss': np.float64(0.05132207423448563), 'behavior_loss': np.float64(1.0082427620887757)}

Episode step 2240, time diff 1.4441964626312256, total time dif 227.59421634674072)
step: 2240 @ episode report: {'average_total_reward': np.float32(2.6688888), 'reward_variance': np.float32(0.24592102), 'max_total_reward': np.float32(3.2888892), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 73728} @ step loss: {'critic_loss': np.float64(0.12287167906761169), 'actor_loss': np.float64(-0.9493444383144378), 'hyper_actor_loss': np.float64(0.05045277215540409), 'behavior_loss': np.float64(1.0177005529403687)}

Episode step 2250, time diff 1.359968900680542, total time dif 229.03841280937195)
step: 2250 @ episode report: {'average_total_reward': np.float32(2.4955556), 'reward_variance': np.float32(0.24054818), 'max_total_reward': np.float32(3.288889), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 74048} @ step loss: {'critic_loss': np.float64(0.13847400844097138), 'actor_loss': np.float64(-0.9199648976325989), 'hyper_actor_loss': np.float64(0.049433665722608565), 'behavior_loss': np.float64(0.9860843062400818)}

Episode step 2260, time diff 1.376654863357544, total time dif 230.3983817100525)
step: 2260 @ episode report: {'average_total_reward': np.float32(2.1688893), 'reward_variance': np.float32(0.52987164), 'max_total_reward': np.float32(3.166667), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 74368} @ step loss: {'critic_loss': np.float64(0.11879692673683166), 'actor_loss': np.float64(-0.9347039878368377), 'hyper_actor_loss': np.float64(0.04868319034576416), 'behavior_loss': np.float64(1.0692884743213653)}

Episode step 2270, time diff 1.4565393924713135, total time dif 231.77503657341003)
step: 2270 @ episode report: {'average_total_reward': np.float32(2.4933333), 'reward_variance': np.float32(0.2677581), 'max_total_reward': np.float32(3.4111116), 'min_total_reward': np.float32(1.8000002), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 74688} @ step loss: {'critic_loss': np.float64(0.11632300838828087), 'actor_loss': np.float64(-0.9092784225940704), 'hyper_actor_loss': np.float64(0.04783574342727661), 'behavior_loss': np.float64(1.0175266027450562)}

Episode step 2280, time diff 1.3880548477172852, total time dif 233.23157596588135)
step: 2280 @ episode report: {'average_total_reward': np.float32(2.3544447), 'reward_variance': np.float32(0.4330482), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 75008} @ step loss: {'critic_loss': np.float64(0.13026246950030326), 'actor_loss': np.float64(-0.9493135511875153), 'hyper_actor_loss': np.float64(0.047048314660787585), 'behavior_loss': np.float64(1.0604610085487365)}

Episode step 2290, time diff 1.3901481628417969, total time dif 234.61963081359863)
step: 2290 @ episode report: {'average_total_reward': np.float32(2.2322223), 'reward_variance': np.float32(0.19610992), 'max_total_reward': np.float32(3.1666667), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 75328} @ step loss: {'critic_loss': np.float64(0.1272871471941471), 'actor_loss': np.float64(-0.9176594018936157), 'hyper_actor_loss': np.float64(0.046165190264582635), 'behavior_loss': np.float64(1.079918909072876)}

Episode step 2300, time diff 1.303938388824463, total time dif 236.00977897644043)
step: 2300 @ episode report: {'average_total_reward': np.float32(3.0155559), 'reward_variance': np.float32(0.48037535), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 75648} @ step loss: {'critic_loss': np.float64(0.12070812210440636), 'actor_loss': np.float64(-0.9310201823711395), 'hyper_actor_loss': np.float64(0.0447179552167654), 'behavior_loss': np.float64(1.060957008600235)}

Episode step 2310, time diff 1.6132831573486328, total time dif 237.3137173652649)
step: 2310 @ episode report: {'average_total_reward': np.float32(3.5400002), 'reward_variance': np.float32(0.78551096), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(1.8000002), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 75968} @ step loss: {'critic_loss': np.float64(0.11301153376698495), 'actor_loss': np.float64(-0.9069272398948669), 'hyper_actor_loss': np.float64(0.042932097986340526), 'behavior_loss': np.float64(1.1080190420150757)}

Episode step 2320, time diff 1.2886028289794922, total time dif 238.92700052261353)
step: 2320 @ episode report: {'average_total_reward': np.float32(3.225556), 'reward_variance': np.float32(0.60721105), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(2.1666665), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 76288} @ step loss: {'critic_loss': np.float64(0.11920368745923042), 'actor_loss': np.float64(-0.9052174925804138), 'hyper_actor_loss': np.float64(0.0413416426628828), 'behavior_loss': np.float64(1.1647603511810303)}

Episode step 2330, time diff 1.4010794162750244, total time dif 240.21560335159302)
step: 2330 @ episode report: {'average_total_reward': np.float32(3.8400002), 'reward_variance': np.float32(0.5565728), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 76608} @ step loss: {'critic_loss': np.float64(0.12135111466050148), 'actor_loss': np.float64(-0.9053200721740723), 'hyper_actor_loss': np.float64(0.03995014950633049), 'behavior_loss': np.float64(1.2185524582862854)}

Episode step 2340, time diff 1.2329649925231934, total time dif 241.61668276786804)
step: 2340 @ episode report: {'average_total_reward': np.float32(4.423333), 'reward_variance': np.float32(0.68364084), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 76928} @ step loss: {'critic_loss': np.float64(0.11423730328679085), 'actor_loss': np.float64(-0.918761545419693), 'hyper_actor_loss': np.float64(0.03878768607974052), 'behavior_loss': np.float64(1.2055898189544678)}

Episode step 2350, time diff 1.3022236824035645, total time dif 242.84964776039124)
step: 2350 @ episode report: {'average_total_reward': np.float32(4.2500005), 'reward_variance': np.float32(0.5149198), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 77248} @ step loss: {'critic_loss': np.float64(0.11265119686722755), 'actor_loss': np.float64(-0.9502141356468201), 'hyper_actor_loss': np.float64(0.03778894655406475), 'behavior_loss': np.float64(1.2463275909423828)}

Episode step 2360, time diff 1.1843433380126953, total time dif 244.1518714427948)
step: 2360 @ episode report: {'average_total_reward': np.float32(4.5622225), 'reward_variance': np.float32(0.9901037), 'max_total_reward': np.float32(6.5333333), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 77568} @ step loss: {'critic_loss': np.float64(0.12235894724726677), 'actor_loss': np.float64(-0.9161453127861023), 'hyper_actor_loss': np.float64(0.03691379576921463), 'behavior_loss': np.float64(1.323500967025757)}

Episode step 2370, time diff 1.1414380073547363, total time dif 245.3362147808075)
step: 2370 @ episode report: {'average_total_reward': np.float32(4.7599998), 'reward_variance': np.float32(0.5398816), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 77888} @ step loss: {'critic_loss': np.float64(0.112334256619215), 'actor_loss': np.float64(-0.928941136598587), 'hyper_actor_loss': np.float64(0.03610042184591293), 'behavior_loss': np.float64(1.3493748664855958)}

Episode step 2380, time diff 1.0941059589385986, total time dif 246.47765278816223)
step: 2380 @ episode report: {'average_total_reward': np.float32(4.9111114), 'reward_variance': np.float32(0.2524445), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(6.0), 'buffer_size': 78208} @ step loss: {'critic_loss': np.float64(0.1223772868514061), 'actor_loss': np.float64(-0.9603433907032013), 'hyper_actor_loss': np.float64(0.03552234284579754), 'behavior_loss': np.float64(1.2645027875900268)}

Episode step 2390, time diff 1.3417387008666992, total time dif 247.57175874710083)
step: 2390 @ episode report: {'average_total_reward': np.float32(4.535556), 'reward_variance': np.float32(0.2690321), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 78528} @ step loss: {'critic_loss': np.float64(0.11124405264854431), 'actor_loss': np.float64(-0.9299152374267579), 'hyper_actor_loss': np.float64(0.03476792573928833), 'behavior_loss': np.float64(1.3034056544303894)}

Episode step 2400, time diff 1.1094856262207031, total time dif 248.91349744796753)
step: 2400 @ episode report: {'average_total_reward': np.float32(5.533334), 'reward_variance': np.float32(2.3386924), 'max_total_reward': np.float32(8.77778), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 78848} @ step loss: {'critic_loss': np.float64(0.11056759431958199), 'actor_loss': np.float64(-0.9085335969924927), 'hyper_actor_loss': np.float64(0.034237489476799964), 'behavior_loss': np.float64(1.366631543636322)}

Episode step 2410, time diff 1.2154221534729004, total time dif 250.02298307418823)
step: 2410 @ episode report: {'average_total_reward': np.float32(4.5622225), 'reward_variance': np.float32(0.54321486), 'max_total_reward': np.float32(6.2888894), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 79168} @ step loss: {'critic_loss': np.float64(0.11673926562070847), 'actor_loss': np.float64(-0.9236276328563691), 'hyper_actor_loss': np.float64(0.033793560788035394), 'behavior_loss': np.float64(1.4357552647590637)}

Episode step 2420, time diff 1.1092643737792969, total time dif 251.23840522766113)
step: 2420 @ episode report: {'average_total_reward': np.float32(4.7233334), 'reward_variance': np.float32(0.18554191), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(6.0), 'buffer_size': 79488} @ step loss: {'critic_loss': np.float64(0.11830135732889176), 'actor_loss': np.float64(-0.9314057767391205), 'hyper_actor_loss': np.float64(0.03343815691769123), 'behavior_loss': np.float64(1.3364117980003356)}

Episode step 2430, time diff 1.132864236831665, total time dif 252.34766960144043)
step: 2430 @ episode report: {'average_total_reward': np.float32(4.9111114), 'reward_variance': np.float32(0.5532099), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 79808} @ step loss: {'critic_loss': np.float64(0.11408650279045104), 'actor_loss': np.float64(-0.9047096610069275), 'hyper_actor_loss': np.float64(0.03294864855706692), 'behavior_loss': np.float64(1.3852940320968627)}

Episode step 2440, time diff 1.1970593929290771, total time dif 253.4805338382721)
step: 2440 @ episode report: {'average_total_reward': np.float32(5.0233335), 'reward_variance': np.float32(1.1852953), 'max_total_reward': np.float32(7.5333343), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 80128} @ step loss: {'critic_loss': np.float64(0.11551918089389801), 'actor_loss': np.float64(-0.9359049141407013), 'hyper_actor_loss': np.float64(0.03245339021086693), 'behavior_loss': np.float64(1.3045710444450378)}

Episode step 2450, time diff 1.1257271766662598, total time dif 254.67759323120117)
step: 2450 @ episode report: {'average_total_reward': np.float32(4.6477776), 'reward_variance': np.float32(0.60034704), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.4111109), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 80448} @ step loss: {'critic_loss': np.float64(0.11373234540224075), 'actor_loss': np.float64(-0.9703447580337524), 'hyper_actor_loss': np.float64(0.03227594904601574), 'behavior_loss': np.float64(1.3693935751914978)}

Episode step 2460, time diff 1.1584179401397705, total time dif 255.80332040786743)
step: 2460 @ episode report: {'average_total_reward': np.float32(5.3333335), 'reward_variance': np.float32(0.39041975), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 80768} @ step loss: {'critic_loss': np.float64(0.12306777015328407), 'actor_loss': np.float64(-0.9408521592617035), 'hyper_actor_loss': np.float64(0.03207617923617363), 'behavior_loss': np.float64(1.3114898800849915)}

Episode step 2470, time diff 1.212343692779541, total time dif 256.9617383480072)
step: 2470 @ episode report: {'average_total_reward': np.float32(5.660001), 'reward_variance': np.float32(1.9316099), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.1666665), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 81088} @ step loss: {'critic_loss': np.float64(0.10636549070477486), 'actor_loss': np.float64(-0.940278023481369), 'hyper_actor_loss': np.float64(0.032031891494989397), 'behavior_loss': np.float64(1.3745633125305177)}

Episode step 2480, time diff 1.2336969375610352, total time dif 258.17408204078674)
step: 2480 @ episode report: {'average_total_reward': np.float32(5.5477777), 'reward_variance': np.float32(0.5192112), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 81408} @ step loss: {'critic_loss': np.float64(0.11123853176832199), 'actor_loss': np.float64(-0.9315622508525848), 'hyper_actor_loss': np.float64(0.03161003086715937), 'behavior_loss': np.float64(1.3722504258155823)}

Episode step 2490, time diff 1.4196085929870605, total time dif 259.4077789783478)
step: 2490 @ episode report: {'average_total_reward': np.float32(5.4722223), 'reward_variance': np.float32(0.98526555), 'max_total_reward': np.float32(7.2888894), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 81728} @ step loss: {'critic_loss': np.float64(0.11353496089577675), 'actor_loss': np.float64(-0.9247730851173401), 'hyper_actor_loss': np.float64(0.03129623811691999), 'behavior_loss': np.float64(1.4252323031425476)}

Episode step 2500, time diff 1.5722103118896484, total time dif 260.82738757133484)
step: 2500 @ episode report: {'average_total_reward': np.float32(5.4455557), 'reward_variance': np.float32(0.62497413), 'max_total_reward': np.float32(6.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 82048} @ step loss: {'critic_loss': np.float64(0.13065300658345222), 'actor_loss': np.float64(-0.9585657119750977), 'hyper_actor_loss': np.float64(0.03057099673897028), 'behavior_loss': np.float64(1.4582764387130738)}

Episode step 2510, time diff 1.4065206050872803, total time dif 262.3995978832245)
step: 2510 @ episode report: {'average_total_reward': np.float32(5.633333), 'reward_variance': np.float32(1.7004942), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 82368} @ step loss: {'critic_loss': np.float64(0.11797091364860535), 'actor_loss': np.float64(-0.9395618736743927), 'hyper_actor_loss': np.float64(0.030077259801328183), 'behavior_loss': np.float64(1.3980273604393005)}

Episode step 2520, time diff 1.7907309532165527, total time dif 263.80611848831177)
step: 2520 @ episode report: {'average_total_reward': np.float32(5.272222), 'reward_variance': np.float32(2.7602787), 'max_total_reward': np.float32(7.5333343), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 82688} @ step loss: {'critic_loss': np.float64(0.12356618791818619), 'actor_loss': np.float64(-0.953196668624878), 'hyper_actor_loss': np.float64(0.02970536779612303), 'behavior_loss': np.float64(1.499699354171753)}

Episode step 2530, time diff 1.2312638759613037, total time dif 265.5968494415283)
step: 2530 @ episode report: {'average_total_reward': np.float32(6.096667), 'reward_variance': np.float32(2.013335), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 83008} @ step loss: {'critic_loss': np.float64(0.12044441625475884), 'actor_loss': np.float64(-0.9617627084255218), 'hyper_actor_loss': np.float64(0.029637000150978567), 'behavior_loss': np.float64(1.3769710063934326)}

Episode step 2540, time diff 0.9798030853271484, total time dif 266.8281133174896)
step: 2540 @ episode report: {'average_total_reward': np.float32(5.845556), 'reward_variance': np.float32(3.13174), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 83328} @ step loss: {'critic_loss': np.float64(0.12716367095708847), 'actor_loss': np.float64(-0.9769594192504882), 'hyper_actor_loss': np.float64(0.029437537863850595), 'behavior_loss': np.float64(1.5360989928245545)}

Episode step 2550, time diff 0.9392378330230713, total time dif 267.8079164028168)
step: 2550 @ episode report: {'average_total_reward': np.float32(6.0944448), 'reward_variance': np.float32(1.0490186), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 83648} @ step loss: {'critic_loss': np.float64(0.11857822239398956), 'actor_loss': np.float64(-0.8898955285549164), 'hyper_actor_loss': np.float64(0.029263451881706714), 'behavior_loss': np.float64(1.423262083530426)}

Episode step 2560, time diff 1.1849606037139893, total time dif 268.74715423583984)
step: 2560 @ episode report: {'average_total_reward': np.float32(6.3455553), 'reward_variance': np.float32(2.24853), 'max_total_reward': np.float32(8.411111), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 83968} @ step loss: {'critic_loss': np.float64(0.10986970216035843), 'actor_loss': np.float64(-0.9629064440727234), 'hyper_actor_loss': np.float64(0.029190383851528168), 'behavior_loss': np.float64(1.4256958365440369)}

Episode step 2570, time diff 0.8408641815185547, total time dif 269.93211483955383)
step: 2570 @ episode report: {'average_total_reward': np.float32(7.055556), 'reward_variance': np.float32(2.751259), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 84288} @ step loss: {'critic_loss': np.float64(0.10622457787394524), 'actor_loss': np.float64(-0.9273974418640136), 'hyper_actor_loss': np.float64(0.02952987216413021), 'behavior_loss': np.float64(1.4182985186576844)}

Episode step 2580, time diff 0.7889690399169922, total time dif 270.7729790210724)
step: 2580 @ episode report: {'average_total_reward': np.float32(7.967778), 'reward_variance': np.float32(0.82048017), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 84608} @ step loss: {'critic_loss': np.float64(0.11310441866517067), 'actor_loss': np.float64(-0.9251865983009339), 'hyper_actor_loss': np.float64(0.029331801831722258), 'behavior_loss': np.float64(1.436233353614807)}

Episode step 2590, time diff 0.7680580615997314, total time dif 271.5619480609894)
step: 2590 @ episode report: {'average_total_reward': np.float32(6.594445), 'reward_variance': np.float32(2.1913154), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 84928} @ step loss: {'critic_loss': np.float64(0.10397378355264664), 'actor_loss': np.float64(-0.9350311756134033), 'hyper_actor_loss': np.float64(0.029115261882543562), 'behavior_loss': np.float64(1.43721684217453)}

Episode step 2600, time diff 0.74904465675354, total time dif 272.3300061225891)
step: 2600 @ episode report: {'average_total_reward': np.float32(8.32889), 'reward_variance': np.float32(2.920252), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 85248} @ step loss: {'critic_loss': np.float64(0.11847943365573883), 'actor_loss': np.float64(-0.9325350046157836), 'hyper_actor_loss': np.float64(0.029184651933610438), 'behavior_loss': np.float64(1.4092382192611694)}

Episode step 2610, time diff 0.7511112689971924, total time dif 273.07905077934265)
step: 2610 @ episode report: {'average_total_reward': np.float32(7.2188888), 'reward_variance': np.float32(1.9006189), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 85568} @ step loss: {'critic_loss': np.float64(0.1052391104400158), 'actor_loss': np.float64(-0.9687100052833557), 'hyper_actor_loss': np.float64(0.028961307555437087), 'behavior_loss': np.float64(1.3899147987365723)}

Episode step 2620, time diff 0.9353623390197754, total time dif 273.83016204833984)
step: 2620 @ episode report: {'average_total_reward': np.float32(7.0922227), 'reward_variance': np.float32(3.9919515), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 85888} @ step loss: {'critic_loss': np.float64(0.10607737675309181), 'actor_loss': np.float64(-0.937759953737259), 'hyper_actor_loss': np.float64(0.02898626569658518), 'behavior_loss': np.float64(1.3835393071174622)}

Episode step 2630, time diff 0.84415602684021, total time dif 274.7655243873596)
step: 2630 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(3.9982827), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 86208} @ step loss: {'critic_loss': np.float64(0.11578012332320213), 'actor_loss': np.float64(-0.9716809988021851), 'hyper_actor_loss': np.float64(0.02918267883360386), 'behavior_loss': np.float64(1.4341760277748108)}

Episode step 2640, time diff 0.7474706172943115, total time dif 275.60968041419983)
step: 2640 @ episode report: {'average_total_reward': np.float32(8.553334), 'reward_variance': np.float32(4.1701436), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 86528} @ step loss: {'critic_loss': np.float64(0.12375179454684257), 'actor_loss': np.float64(-0.961487090587616), 'hyper_actor_loss': np.float64(0.02917307708412409), 'behavior_loss': np.float64(1.4333772659301758)}

Episode step 2650, time diff 0.8921012878417969, total time dif 276.35715103149414)
step: 2650 @ episode report: {'average_total_reward': np.float32(8.128889), 'reward_variance': np.float32(4.7705727), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 86848} @ step loss: {'critic_loss': np.float64(0.12860831543803214), 'actor_loss': np.float64(-1.005249136686325), 'hyper_actor_loss': np.float64(0.0295422799885273), 'behavior_loss': np.float64(1.4418240308761596)}

Episode step 2660, time diff 0.7691335678100586, total time dif 277.24925231933594)
step: 2660 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(3.3557098), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 87168} @ step loss: {'critic_loss': np.float64(0.11337782591581344), 'actor_loss': np.float64(-0.9716742932796478), 'hyper_actor_loss': np.float64(0.030169720016419887), 'behavior_loss': np.float64(1.3686438202857971)}

Episode step 2670, time diff 0.8041317462921143, total time dif 278.018385887146)
step: 2670 @ episode report: {'average_total_reward': np.float32(7.828889), 'reward_variance': np.float32(1.7078817), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 87488} @ step loss: {'critic_loss': np.float64(0.1019580140709877), 'actor_loss': np.float64(-0.9680467844009399), 'hyper_actor_loss': np.float64(0.03069437127560377), 'behavior_loss': np.float64(1.3555695772171021)}

Episode step 2680, time diff 0.7618429660797119, total time dif 278.8225176334381)
step: 2680 @ episode report: {'average_total_reward': np.float32(7.5311112), 'reward_variance': np.float32(2.1498718), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 87808} @ step loss: {'critic_loss': np.float64(0.11711247339844703), 'actor_loss': np.float64(-0.9859416484832764), 'hyper_actor_loss': np.float64(0.03121491950005293), 'behavior_loss': np.float64(1.4070697546005249)}

Episode step 2690, time diff 0.750673770904541, total time dif 279.5843605995178)
step: 2690 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(2.7527704), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 88128} @ step loss: {'critic_loss': np.float64(0.1034113883972168), 'actor_loss': np.float64(-0.9560985863208771), 'hyper_actor_loss': np.float64(0.03171659708023071), 'behavior_loss': np.float64(1.3123900532722472)}

Episode step 2700, time diff 0.7603073120117188, total time dif 280.33503437042236)
step: 2700 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(1.4067224), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 88448} @ step loss: {'critic_loss': np.float64(0.1146602489054203), 'actor_loss': np.float64(-0.9330815970897675), 'hyper_actor_loss': np.float64(0.03197403475642204), 'behavior_loss': np.float64(1.3765299916267395)}

Episode step 2710, time diff 0.7829437255859375, total time dif 281.0953416824341)
step: 2710 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(0.20561597), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(9.0), 'buffer_size': 88768} @ step loss: {'critic_loss': np.float64(0.12949065193533899), 'actor_loss': np.float64(-0.9937438011169434), 'hyper_actor_loss': np.float64(0.03195982277393341), 'behavior_loss': np.float64(1.3981531381607055)}

Episode step 2720, time diff 0.7614250183105469, total time dif 281.87828540802)
step: 2720 @ episode report: {'average_total_reward': np.float32(8.641111), 'reward_variance': np.float32(1.5955323), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 89088} @ step loss: {'critic_loss': np.float64(0.11983596608042717), 'actor_loss': np.float64(-0.9611847639083863), 'hyper_actor_loss': np.float64(0.03170827887952328), 'behavior_loss': np.float64(1.313206946849823)}

Episode step 2730, time diff 0.7413203716278076, total time dif 282.63971042633057)
step: 2730 @ episode report: {'average_total_reward': np.float32(7.3166666), 'reward_variance': np.float32(3.3839574), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 89408} @ step loss: {'critic_loss': np.float64(0.11214619874954224), 'actor_loss': np.float64(-0.9838933944702148), 'hyper_actor_loss': np.float64(0.03176141530275345), 'behavior_loss': np.float64(1.275551199913025)}

Episode step 2740, time diff 0.7472426891326904, total time dif 283.3810307979584)
step: 2740 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(4.888475), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 89728} @ step loss: {'critic_loss': np.float64(0.10716423280537128), 'actor_loss': np.float64(-0.9448747754096984), 'hyper_actor_loss': np.float64(0.03132018595933914), 'behavior_loss': np.float64(1.3355792880058288)}

Episode step 2750, time diff 0.7449648380279541, total time dif 284.12827348709106)
step: 2750 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(1.2041594), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 90048} @ step loss: {'critic_loss': np.float64(0.1327056899666786), 'actor_loss': np.float64(-1.0070292234420777), 'hyper_actor_loss': np.float64(0.030859277956187724), 'behavior_loss': np.float64(1.3157789826393127)}

Episode step 2760, time diff 0.7440352439880371, total time dif 284.873238325119)
step: 2760 @ episode report: {'average_total_reward': np.float32(7.267778), 'reward_variance': np.float32(2.8661344), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 90368} @ step loss: {'critic_loss': np.float64(0.1207636259496212), 'actor_loss': np.float64(-0.9444805681705475), 'hyper_actor_loss': np.float64(0.03010421209037304), 'behavior_loss': np.float64(1.3865292429924012)}

Episode step 2770, time diff 0.7593402862548828, total time dif 285.61727356910706)
step: 2770 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(3.2339954), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 90688} @ step loss: {'critic_loss': np.float64(0.10935204029083252), 'actor_loss': np.float64(-0.95847247838974), 'hyper_actor_loss': np.float64(0.02930881567299366), 'behavior_loss': np.float64(1.3576523423194886)}

Episode step 2780, time diff 0.7703707218170166, total time dif 286.37661385536194)
step: 2780 @ episode report: {'average_total_reward': np.float32(7.055556), 'reward_variance': np.float32(2.0749378), 'max_total_reward': np.float32(9.655555), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 91008} @ step loss: {'critic_loss': np.float64(0.11584864482283593), 'actor_loss': np.float64(-0.9199716567993164), 'hyper_actor_loss': np.float64(0.028489244543015955), 'behavior_loss': np.float64(1.408287727832794)}

Episode step 2790, time diff 0.7659590244293213, total time dif 287.14698457717896)
step: 2790 @ episode report: {'average_total_reward': np.float32(6.418889), 'reward_variance': np.float32(1.3285449), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 91328} @ step loss: {'critic_loss': np.float64(0.11348721906542777), 'actor_loss': np.float64(-0.9652386486530304), 'hyper_actor_loss': np.float64(0.028250923939049245), 'behavior_loss': np.float64(1.3997246980667115)}

Episode step 2800, time diff 0.7992331981658936, total time dif 287.9129436016083)
step: 2800 @ episode report: {'average_total_reward': np.float32(7.8555555), 'reward_variance': np.float32(1.2572347), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 91648} @ step loss: {'critic_loss': np.float64(0.11804479360580444), 'actor_loss': np.float64(-0.9285771071910858), 'hyper_actor_loss': np.float64(0.027864081785082817), 'behavior_loss': np.float64(1.4532278180122375)}

Episode step 2810, time diff 0.9136147499084473, total time dif 288.71217679977417)
step: 2810 @ episode report: {'average_total_reward': np.float32(7.28), 'reward_variance': np.float32(4.2430816), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 91968} @ step loss: {'critic_loss': np.float64(0.11749935150146484), 'actor_loss': np.float64(-1.010441243648529), 'hyper_actor_loss': np.float64(0.027977925166487692), 'behavior_loss': np.float64(1.411731445789337)}

Episode step 2820, time diff 0.7453839778900146, total time dif 289.6257915496826)
step: 2820 @ episode report: {'average_total_reward': np.float32(7.167778), 'reward_variance': np.float32(2.2052464), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 92288} @ step loss: {'critic_loss': np.float64(0.11767651587724685), 'actor_loss': np.float64(-0.9749851644039154), 'hyper_actor_loss': np.float64(0.02791748456656933), 'behavior_loss': np.float64(1.2809831142425536)}

Episode step 2830, time diff 0.7887380123138428, total time dif 290.37117552757263)
step: 2830 @ episode report: {'average_total_reward': np.float32(7.6800003), 'reward_variance': np.float32(3.0418472), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 92608} @ step loss: {'critic_loss': np.float64(0.10643796995282173), 'actor_loss': np.float64(-0.9806787550449372), 'hyper_actor_loss': np.float64(0.028440847247838973), 'behavior_loss': np.float64(1.3237024068832397)}

Episode step 2840, time diff 0.7505147457122803, total time dif 291.1599135398865)
step: 2840 @ episode report: {'average_total_reward': np.float32(7.928889), 'reward_variance': np.float32(1.7662017), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 92928} @ step loss: {'critic_loss': np.float64(0.10940276682376862), 'actor_loss': np.float64(-0.9408160150051117), 'hyper_actor_loss': np.float64(0.028887290693819522), 'behavior_loss': np.float64(1.3473971366882325)}

Episode step 2850, time diff 0.7660102844238281, total time dif 291.91042828559875)
step: 2850 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(2.516496), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 93248} @ step loss: {'critic_loss': np.float64(0.11992989555001259), 'actor_loss': np.float64(-0.9971895456314087), 'hyper_actor_loss': np.float64(0.028968650847673416), 'behavior_loss': np.float64(1.3942396879196166)}

Episode step 2860, time diff 0.771526575088501, total time dif 292.6764385700226)
step: 2860 @ episode report: {'average_total_reward': np.float32(6.9800005), 'reward_variance': np.float32(3.9594033), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 93568} @ step loss: {'critic_loss': np.float64(0.12379366457462311), 'actor_loss': np.float64(-0.9908772587776185), 'hyper_actor_loss': np.float64(0.028424414806067944), 'behavior_loss': np.float64(1.3027055382728576)}

Episode step 2870, time diff 0.7493481636047363, total time dif 293.4479651451111)
step: 2870 @ episode report: {'average_total_reward': np.float32(7.3433332), 'reward_variance': np.float32(1.8968014), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 93888} @ step loss: {'critic_loss': np.float64(0.12147376388311386), 'actor_loss': np.float64(-0.9305514097213745), 'hyper_actor_loss': np.float64(0.02776126340031624), 'behavior_loss': np.float64(1.357430100440979)}

Episode step 2880, time diff 0.772498369216919, total time dif 294.1973133087158)
step: 2880 @ episode report: {'average_total_reward': np.float32(6.4822226), 'reward_variance': np.float32(2.6235855), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 94208} @ step loss: {'critic_loss': np.float64(0.12557579204440117), 'actor_loss': np.float64(-0.9940993010997772), 'hyper_actor_loss': np.float64(0.027218043431639673), 'behavior_loss': np.float64(1.3768962979316712)}

Episode step 2890, time diff 0.7502858638763428, total time dif 294.96981167793274)
step: 2890 @ episode report: {'average_total_reward': np.float32(6.306667), 'reward_variance': np.float32(1.9748195), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 94528} @ step loss: {'critic_loss': np.float64(0.1255062997341156), 'actor_loss': np.float64(-0.949719887971878), 'hyper_actor_loss': np.float64(0.026932599395513533), 'behavior_loss': np.float64(1.3198248386383056)}

Episode step 2900, time diff 0.7323391437530518, total time dif 295.7200975418091)
step: 2900 @ episode report: {'average_total_reward': np.float32(5.845556), 'reward_variance': np.float32(2.7611718), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 94848} @ step loss: {'critic_loss': np.float64(0.11209127977490425), 'actor_loss': np.float64(-0.994501394033432), 'hyper_actor_loss': np.float64(0.026589673943817616), 'behavior_loss': np.float64(1.359328842163086)}

Episode step 2910, time diff 0.7416012287139893, total time dif 296.45243668556213)
step: 2910 @ episode report: {'average_total_reward': np.float32(7.604445), 'reward_variance': np.float32(2.0420547), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 95168} @ step loss: {'critic_loss': np.float64(0.11458728983998298), 'actor_loss': np.float64(-0.9628147840499878), 'hyper_actor_loss': np.float64(0.02611854877322912), 'behavior_loss': np.float64(1.285640501976013)}

Episode step 2920, time diff 0.8286893367767334, total time dif 297.1940379142761)
step: 2920 @ episode report: {'average_total_reward': np.float32(6.582223), 'reward_variance': np.float32(1.5347704), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 95488} @ step loss: {'critic_loss': np.float64(0.11372323855757713), 'actor_loss': np.float64(-0.9786547660827637), 'hyper_actor_loss': np.float64(0.025573867745697497), 'behavior_loss': np.float64(1.300009310245514)}

Episode step 2930, time diff 0.8148746490478516, total time dif 298.02272725105286)
step: 2930 @ episode report: {'average_total_reward': np.float32(6.082223), 'reward_variance': np.float32(2.0366478), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 95808} @ step loss: {'critic_loss': np.float64(0.11085422486066818), 'actor_loss': np.float64(-0.9552857220172882), 'hyper_actor_loss': np.float64(0.025177172757685183), 'behavior_loss': np.float64(1.4807110667228698)}

Episode step 2940, time diff 0.7582874298095703, total time dif 298.8376019001007)
step: 2940 @ episode report: {'average_total_reward': np.float32(6.0577784), 'reward_variance': np.float32(2.3392053), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 96128} @ step loss: {'critic_loss': np.float64(0.11938479766249657), 'actor_loss': np.float64(-0.978114926815033), 'hyper_actor_loss': np.float64(0.024746598862111568), 'behavior_loss': np.float64(1.4814425468444825)}

Episode step 2950, time diff 0.7719461917877197, total time dif 299.5958893299103)
step: 2950 @ episode report: {'average_total_reward': np.float32(5.9333334), 'reward_variance': np.float32(1.5741732), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 96448} @ step loss: {'critic_loss': np.float64(0.11691146567463875), 'actor_loss': np.float64(-0.9763832092285156), 'hyper_actor_loss': np.float64(0.02463793959468603), 'behavior_loss': np.float64(1.3791382551193236)}

Episode step 2960, time diff 0.7410967350006104, total time dif 300.367835521698)
step: 2960 @ episode report: {'average_total_reward': np.float32(5.8700004), 'reward_variance': np.float32(2.6505194), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 96768} @ step loss: {'critic_loss': np.float64(0.12211677432060242), 'actor_loss': np.float64(-1.0036380469799042), 'hyper_actor_loss': np.float64(0.024769046157598496), 'behavior_loss': np.float64(1.364120888710022)}

Episode step 2970, time diff 0.8836278915405273, total time dif 301.1089322566986)
step: 2970 @ episode report: {'average_total_reward': np.float32(6.3700004), 'reward_variance': np.float32(0.60822374), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 97088} @ step loss: {'critic_loss': np.float64(0.1211940474808216), 'actor_loss': np.float64(-0.9712122678756714), 'hyper_actor_loss': np.float64(0.02532186731696129), 'behavior_loss': np.float64(1.4163204431533813)}

Episode step 2980, time diff 0.8830041885375977, total time dif 301.99256014823914)
step: 2980 @ episode report: {'average_total_reward': np.float32(6.2188888), 'reward_variance': np.float32(2.422841), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 97408} @ step loss: {'critic_loss': np.float64(0.11131098121404648), 'actor_loss': np.float64(-0.9884893238544464), 'hyper_actor_loss': np.float64(0.025914025865495204), 'behavior_loss': np.float64(1.4533331155776978)}

Episode step 2990, time diff 0.9544765949249268, total time dif 302.87556433677673)
step: 2990 @ episode report: {'average_total_reward': np.float32(6.494445), 'reward_variance': np.float32(3.0336852), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 97728} @ step loss: {'critic_loss': np.float64(0.11093606129288673), 'actor_loss': np.float64(-0.9811088979244232), 'hyper_actor_loss': np.float64(0.02678609937429428), 'behavior_loss': np.float64(1.3427056908607482)}

Episode step 3000, time diff 0.7803323268890381, total time dif 303.83004093170166)
step: 3000 @ episode report: {'average_total_reward': np.float32(5.596667), 'reward_variance': np.float32(1.4865196), 'max_total_reward': np.float32(7.6555552), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 98048} @ step loss: {'critic_loss': np.float64(0.12074626311659813), 'actor_loss': np.float64(-0.9908698499202728), 'hyper_actor_loss': np.float64(0.027134263701736928), 'behavior_loss': np.float64(1.444931721687317)}

Episode step 3010, time diff 0.7473564147949219, total time dif 304.6103732585907)
step: 3010 @ episode report: {'average_total_reward': np.float32(5.3966665), 'reward_variance': np.float32(0.87032205), 'max_total_reward': np.float32(7.4111114), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 98368} @ step loss: {'critic_loss': np.float64(0.12163991332054139), 'actor_loss': np.float64(-0.9906300663948059), 'hyper_actor_loss': np.float64(0.02692617829889059), 'behavior_loss': np.float64(1.4554650902748107)}

Episode step 3020, time diff 0.7772121429443359, total time dif 305.3577296733856)
step: 3020 @ episode report: {'average_total_reward': np.float32(4.9477777), 'reward_variance': np.float32(0.86634696), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.4111116), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 98688} @ step loss: {'critic_loss': np.float64(0.11436471790075302), 'actor_loss': np.float64(-0.9522143840789795), 'hyper_actor_loss': np.float64(0.026217076927423477), 'behavior_loss': np.float64(1.4011476755142211)}

Episode step 3030, time diff 0.7540249824523926, total time dif 306.13494181632996)
step: 3030 @ episode report: {'average_total_reward': np.float32(5.408889), 'reward_variance': np.float32(2.2720938), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 99008} @ step loss: {'critic_loss': np.float64(0.1259591296315193), 'actor_loss': np.float64(-0.9879396438598633), 'hyper_actor_loss': np.float64(0.025040063448250292), 'behavior_loss': np.float64(1.408583974838257)}

Episode step 3040, time diff 0.7788553237915039, total time dif 306.88896679878235)
step: 3040 @ episode report: {'average_total_reward': np.float32(4.8477774), 'reward_variance': np.float32(0.31755686), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(6.0), 'buffer_size': 99328} @ step loss: {'critic_loss': np.float64(0.11790560558438301), 'actor_loss': np.float64(-0.962435919046402), 'hyper_actor_loss': np.float64(0.023713708110153674), 'behavior_loss': np.float64(1.4023601770401002)}

Episode step 3050, time diff 0.7483649253845215, total time dif 307.66782212257385)
step: 3050 @ episode report: {'average_total_reward': np.float32(5.6700006), 'reward_variance': np.float32(2.8429651), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 99648} @ step loss: {'critic_loss': np.float64(0.12225362733006477), 'actor_loss': np.float64(-0.9454716742038727), 'hyper_actor_loss': np.float64(0.02282653860747814), 'behavior_loss': np.float64(1.467245304584503)}

Episode step 3060, time diff 0.7344534397125244, total time dif 308.4161870479584)
step: 3060 @ episode report: {'average_total_reward': np.float32(6.045556), 'reward_variance': np.float32(2.4326785), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 99968} @ step loss: {'critic_loss': np.float64(0.11925323829054832), 'actor_loss': np.float64(-0.9553357481956481), 'hyper_actor_loss': np.float64(0.022084563598036767), 'behavior_loss': np.float64(1.4645419001579285)}

Episode step 3070, time diff 0.7682058811187744, total time dif 309.1506404876709)
step: 3070 @ episode report: {'average_total_reward': np.float32(6.7066665), 'reward_variance': np.float32(2.0864255), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11865822449326516), 'actor_loss': np.float64(-0.9606812775135041), 'hyper_actor_loss': np.float64(0.021559022925794126), 'behavior_loss': np.float64(1.4947909474372865)}

Episode step 3080, time diff 0.7441351413726807, total time dif 309.9188463687897)
step: 3080 @ episode report: {'average_total_reward': np.float32(5.845556), 'reward_variance': np.float32(0.99151754), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.12580062225461006), 'actor_loss': np.float64(-0.993611854314804), 'hyper_actor_loss': np.float64(0.021074405498802663), 'behavior_loss': np.float64(1.3904338955879212)}

Episode step 3090, time diff 0.747748613357544, total time dif 310.66298151016235)
step: 3090 @ episode report: {'average_total_reward': np.float32(6.5066667), 'reward_variance': np.float32(2.4254873), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.12011706158518791), 'actor_loss': np.float64(-0.9490566670894622), 'hyper_actor_loss': np.float64(0.020639857836067678), 'behavior_loss': np.float64(1.5110806465148925)}

Episode step 3100, time diff 0.7562024593353271, total time dif 311.4107301235199)
step: 3100 @ episode report: {'average_total_reward': np.float32(6.208889), 'reward_variance': np.float32(2.2905145), 'max_total_reward': np.float32(8.655557), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10889115780591965), 'actor_loss': np.float64(-0.9494619965553284), 'hyper_actor_loss': np.float64(0.020225037634372712), 'behavior_loss': np.float64(1.4793744206428527)}

Episode step 3110, time diff 0.7486588954925537, total time dif 312.1669325828552)
step: 3110 @ episode report: {'average_total_reward': np.float32(5.508889), 'reward_variance': np.float32(1.4921932), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11360745653510093), 'actor_loss': np.float64(-0.9663132190704345), 'hyper_actor_loss': np.float64(0.019846809469163416), 'behavior_loss': np.float64(1.4466369986534118)}

Episode step 3120, time diff 0.716210126876831, total time dif 312.9155914783478)
step: 3120 @ episode report: {'average_total_reward': np.float32(6.457778), 'reward_variance': np.float32(1.9322169), 'max_total_reward': np.float32(8.655555), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10748732239007949), 'actor_loss': np.float64(-0.9495836019515991), 'hyper_actor_loss': np.float64(0.019385290518403053), 'behavior_loss': np.float64(1.4252403855323792)}

Episode step 3130, time diff 0.9222221374511719, total time dif 313.6318016052246)
step: 3130 @ episode report: {'average_total_reward': np.float32(6.357778), 'reward_variance': np.float32(2.5108597), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1302650071680546), 'actor_loss': np.float64(-0.9669942319393158), 'hyper_actor_loss': np.float64(0.0191716343164444), 'behavior_loss': np.float64(1.4599551439285279)}

Episode step 3140, time diff 0.7513055801391602, total time dif 314.5540237426758)
step: 3140 @ episode report: {'average_total_reward': np.float32(5.6211114), 'reward_variance': np.float32(1.0997641), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.12180460169911385), 'actor_loss': np.float64(-0.9542173683643341), 'hyper_actor_loss': np.float64(0.018559109047055246), 'behavior_loss': np.float64(1.5260980367660522)}

Episode step 3150, time diff 0.780247688293457, total time dif 315.30532932281494)
step: 3150 @ episode report: {'average_total_reward': np.float32(6.2333336), 'reward_variance': np.float32(3.409062), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11197188720107079), 'actor_loss': np.float64(-0.9563564658164978), 'hyper_actor_loss': np.float64(0.017800178937613965), 'behavior_loss': np.float64(1.481863272190094)}

Episode step 3160, time diff 0.7668464183807373, total time dif 316.0855770111084)
step: 3160 @ episode report: {'average_total_reward': np.float32(6.5311112), 'reward_variance': np.float32(2.6446614), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11134387776255608), 'actor_loss': np.float64(-0.9551308214664459), 'hyper_actor_loss': np.float64(0.01733121518045664), 'behavior_loss': np.float64(1.4888096928596497)}

Episode step 3170, time diff 0.7619013786315918, total time dif 316.85242342948914)
step: 3170 @ episode report: {'average_total_reward': np.float32(6.8066664), 'reward_variance': np.float32(1.5662034), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.12550045996904374), 'actor_loss': np.float64(-0.9937607765197753), 'hyper_actor_loss': np.float64(0.01701852735131979), 'behavior_loss': np.float64(1.4909001469612122)}

Episode step 3180, time diff 0.7654144763946533, total time dif 317.6143248081207)
step: 3180 @ episode report: {'average_total_reward': np.float32(6.5188894), 'reward_variance': np.float32(4.2474823), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11855214685201645), 'actor_loss': np.float64(-0.9557173609733581), 'hyper_actor_loss': np.float64(0.01676121409982443), 'behavior_loss': np.float64(1.5415449619293213)}

Episode step 3190, time diff 0.7513251304626465, total time dif 318.3797392845154)
step: 3190 @ episode report: {'average_total_reward': np.float32(5.957778), 'reward_variance': np.float32(5.0802917), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10241163596510887), 'actor_loss': np.float64(-0.9513785541057587), 'hyper_actor_loss': np.float64(0.016680551506578923), 'behavior_loss': np.float64(1.4862898349761964)}

Episode step 3200, time diff 0.7672574520111084, total time dif 319.131064414978)
step: 3200 @ episode report: {'average_total_reward': np.float32(6.955556), 'reward_variance': np.float32(2.3516796), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11007621064782143), 'actor_loss': np.float64(-0.9621604442596435), 'hyper_actor_loss': np.float64(0.01661008931696415), 'behavior_loss': np.float64(1.461902904510498)}

Episode step 3210, time diff 0.7660820484161377, total time dif 319.89832186698914)
step: 3210 @ episode report: {'average_total_reward': np.float32(6.8433332), 'reward_variance': np.float32(3.4849994), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(3.4111116), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11217959076166154), 'actor_loss': np.float64(-0.9489650785923004), 'hyper_actor_loss': np.float64(0.015963877830654383), 'behavior_loss': np.float64(1.5590610027313232)}

Episode step 3220, time diff 0.7532169818878174, total time dif 320.6644039154053)
step: 3220 @ episode report: {'average_total_reward': np.float32(6.321111), 'reward_variance': np.float32(1.6751473), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10566566810011864), 'actor_loss': np.float64(-0.9747955977916718), 'hyper_actor_loss': np.float64(0.015632953122258188), 'behavior_loss': np.float64(1.4183656692504882)}

Episode step 3230, time diff 0.7426979541778564, total time dif 321.4176208972931)
step: 3230 @ episode report: {'average_total_reward': np.float32(6.2822227), 'reward_variance': np.float32(1.6259804), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1042795829474926), 'actor_loss': np.float64(-0.9369294106960296), 'hyper_actor_loss': np.float64(0.015148654859513045), 'behavior_loss': np.float64(1.6202530145645142)}

Episode step 3240, time diff 0.7393133640289307, total time dif 322.16031885147095)
step: 3240 @ episode report: {'average_total_reward': np.float32(6.1211114), 'reward_variance': np.float32(1.739295), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09956090599298477), 'actor_loss': np.float64(-0.9165685832500458), 'hyper_actor_loss': np.float64(0.01448963638395071), 'behavior_loss': np.float64(1.5103099346160889)}

Episode step 3250, time diff 0.7322428226470947, total time dif 322.8996322154999)
step: 3250 @ episode report: {'average_total_reward': np.float32(6.7333326), 'reward_variance': np.float32(2.9211352), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10189294032752513), 'actor_loss': np.float64(-0.9399263918399811), 'hyper_actor_loss': np.float64(0.014257857296615838), 'behavior_loss': np.float64(1.7127798080444336)}

Episode step 3260, time diff 0.7840497493743896, total time dif 323.631875038147)
step: 3260 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(1.9124066), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09488463178277015), 'actor_loss': np.float64(-0.9106863260269165), 'hyper_actor_loss': np.float64(0.01381978215649724), 'behavior_loss': np.float64(1.6113911986351013)}

Episode step 3270, time diff 0.7239608764648438, total time dif 324.41592478752136)
step: 3270 @ episode report: {'average_total_reward': np.float32(6.8700004), 'reward_variance': np.float32(1.629063), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11323291659355164), 'actor_loss': np.float64(-0.9308052659034729), 'hyper_actor_loss': np.float64(0.013729412667453289), 'behavior_loss': np.float64(1.715693199634552)}

Episode step 3280, time diff 0.7464497089385986, total time dif 325.1398856639862)
step: 3280 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(1.2598964), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10701677799224854), 'actor_loss': np.float64(-0.9501792788505554), 'hyper_actor_loss': np.float64(0.013136651273816824), 'behavior_loss': np.float64(1.531034529209137)}

Episode step 3290, time diff 0.9383502006530762, total time dif 325.8863353729248)
step: 3290 @ episode report: {'average_total_reward': np.float32(6.1433334), 'reward_variance': np.float32(0.93988764), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1028354026377201), 'actor_loss': np.float64(-0.9101447343826294), 'hyper_actor_loss': np.float64(0.012927928008139133), 'behavior_loss': np.float64(1.6122668862342835)}

Episode step 3300, time diff 0.7233872413635254, total time dif 326.8246855735779)
step: 3300 @ episode report: {'average_total_reward': np.float32(7.6433344), 'reward_variance': np.float32(1.4452457), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10703677535057068), 'actor_loss': np.float64(-0.9211052060127258), 'hyper_actor_loss': np.float64(0.012692197598516941), 'behavior_loss': np.float64(1.6465009093284606)}

Episode step 3310, time diff 0.7534000873565674, total time dif 327.5480728149414)
step: 3310 @ episode report: {'average_total_reward': np.float32(6.5311117), 'reward_variance': np.float32(2.1409092), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10108672231435775), 'actor_loss': np.float64(-0.9499699115753174), 'hyper_actor_loss': np.float64(0.01258218865841627), 'behavior_loss': np.float64(1.6246408462524413)}

Episode step 3320, time diff 0.7737436294555664, total time dif 328.301472902298)
step: 3320 @ episode report: {'average_total_reward': np.float32(7.1433334), 'reward_variance': np.float32(1.3867768), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10634309127926826), 'actor_loss': np.float64(-0.945727926492691), 'hyper_actor_loss': np.float64(0.012716557458043098), 'behavior_loss': np.float64(1.5596556901931762)}

Episode step 3330, time diff 0.7988789081573486, total time dif 329.07521653175354)
step: 3330 @ episode report: {'average_total_reward': np.float32(7.267778), 'reward_variance': np.float32(0.7364062), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09326599091291428), 'actor_loss': np.float64(-0.9612459063529968), 'hyper_actor_loss': np.float64(0.012787662539631128), 'behavior_loss': np.float64(1.6506685376167298)}

Episode step 3340, time diff 0.7435307502746582, total time dif 329.8740954399109)
step: 3340 @ episode report: {'average_total_reward': np.float32(6.4455557), 'reward_variance': np.float32(1.6933196), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09656369648873805), 'actor_loss': np.float64(-0.9080411911010742), 'hyper_actor_loss': np.float64(0.012559275794774293), 'behavior_loss': np.float64(1.6485820412635803)}

Episode step 3350, time diff 0.7297072410583496, total time dif 330.61762619018555)
step: 3350 @ episode report: {'average_total_reward': np.float32(6.682223), 'reward_variance': np.float32(1.7950672), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09888423308730125), 'actor_loss': np.float64(-0.9102816760540009), 'hyper_actor_loss': np.float64(0.01206801738590002), 'behavior_loss': np.float64(1.6374873280525208)}

Episode step 3360, time diff 0.7402818202972412, total time dif 331.3473334312439)
step: 3360 @ episode report: {'average_total_reward': np.float32(6.145556), 'reward_variance': np.float32(0.51494944), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09720731973648071), 'actor_loss': np.float64(-0.918950480222702), 'hyper_actor_loss': np.float64(0.011267486680299043), 'behavior_loss': np.float64(1.643638801574707)}

Episode step 3370, time diff 0.7354893684387207, total time dif 332.08761525154114)
step: 3370 @ episode report: {'average_total_reward': np.float32(6.2944446), 'reward_variance': np.float32(2.3127723), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10489671006798744), 'actor_loss': np.float64(-0.9346432983875275), 'hyper_actor_loss': np.float64(0.01113807549700141), 'behavior_loss': np.float64(1.8397605895996094)}

Episode step 3380, time diff 0.7601284980773926, total time dif 332.82310461997986)
step: 3380 @ episode report: {'average_total_reward': np.float32(6.2822223), 'reward_variance': np.float32(1.6733382), 'max_total_reward': np.float32(8.533333), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10174028277397155), 'actor_loss': np.float64(-0.9360570907592773), 'hyper_actor_loss': np.float64(0.010691321082413196), 'behavior_loss': np.float64(1.8103615283966064)}

Episode step 3390, time diff 0.7446362972259521, total time dif 333.58323311805725)
step: 3390 @ episode report: {'average_total_reward': np.float32(6.321111), 'reward_variance': np.float32(1.6691717), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10939762517809867), 'actor_loss': np.float64(-0.9621552348136901), 'hyper_actor_loss': np.float64(0.01087065041065216), 'behavior_loss': np.float64(1.8839013218879699)}

Episode step 3400, time diff 0.7616128921508789, total time dif 334.3278694152832)
step: 3400 @ episode report: {'average_total_reward': np.float32(7.0944443), 'reward_variance': np.float32(2.5418086), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.2888885), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09685591012239456), 'actor_loss': np.float64(-0.9530457258224487), 'hyper_actor_loss': np.float64(0.011069837119430303), 'behavior_loss': np.float64(1.7386348962783813)}

Episode step 3410, time diff 0.7575905323028564, total time dif 335.0894823074341)
step: 3410 @ episode report: {'average_total_reward': np.float32(6.3066664), 'reward_variance': np.float32(2.735437), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08288772255182267), 'actor_loss': np.float64(-0.9228837311267852), 'hyper_actor_loss': np.float64(0.01145491497591138), 'behavior_loss': np.float64(1.7659491181373597)}

Episode step 3420, time diff 0.7571172714233398, total time dif 335.84707283973694)
step: 3420 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(3.537847), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1081812970340252), 'actor_loss': np.float64(-0.9527904450893402), 'hyper_actor_loss': np.float64(0.011591962818056346), 'behavior_loss': np.float64(1.6773830771446228)}

Episode step 3430, time diff 0.7437937259674072, total time dif 336.6041901111603)
step: 3430 @ episode report: {'average_total_reward': np.float32(6.6700006), 'reward_variance': np.float32(2.142199), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09779067188501359), 'actor_loss': np.float64(-0.9263197183609009), 'hyper_actor_loss': np.float64(0.011660492047667503), 'behavior_loss': np.float64(1.8647297143936157)}

Episode step 3440, time diff 0.7140929698944092, total time dif 337.3479838371277)
step: 3440 @ episode report: {'average_total_reward': np.float32(6.794445), 'reward_variance': np.float32(1.9557594), 'max_total_reward': np.float32(9.655557), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08997727185487747), 'actor_loss': np.float64(-0.9412630319595336), 'hyper_actor_loss': np.float64(0.011730899568647146), 'behavior_loss': np.float64(1.683675730228424)}

Episode step 3450, time diff 0.9705784320831299, total time dif 338.0620768070221)
step: 3450 @ episode report: {'average_total_reward': np.float32(7.055556), 'reward_variance': np.float32(2.3542473), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08755319751799107), 'actor_loss': np.float64(-0.9527000665664673), 'hyper_actor_loss': np.float64(0.012041037995368242), 'behavior_loss': np.float64(1.569700050354004)}

Episode step 3460, time diff 0.7365531921386719, total time dif 339.0326552391052)
step: 3460 @ episode report: {'average_total_reward': np.float32(7.667779), 'reward_variance': np.float32(3.224851), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10009824559092521), 'actor_loss': np.float64(-0.9661704182624817), 'hyper_actor_loss': np.float64(0.012284009717404842), 'behavior_loss': np.float64(1.7246853351593017)}

Episode step 3470, time diff 0.7437565326690674, total time dif 339.7692084312439)
step: 3470 @ episode report: {'average_total_reward': np.float32(7.316667), 'reward_variance': np.float32(1.2756851), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10767882689833641), 'actor_loss': np.float64(-0.9541949987411499), 'hyper_actor_loss': np.float64(0.01269629243761301), 'behavior_loss': np.float64(1.7121493816375732)}

Episode step 3480, time diff 0.7391419410705566, total time dif 340.51296496391296)
step: 3480 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(0.9561434), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1016418308019638), 'actor_loss': np.float64(-0.9462600469589233), 'hyper_actor_loss': np.float64(0.012959011737257243), 'behavior_loss': np.float64(1.6778053402900697)}

Episode step 3490, time diff 0.7448079586029053, total time dif 341.2521069049835)
step: 3490 @ episode report: {'average_total_reward': np.float32(6.3944445), 'reward_variance': np.float32(1.2261049), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10911628156900406), 'actor_loss': np.float64(-0.9167653620243073), 'hyper_actor_loss': np.float64(0.013151604402810334), 'behavior_loss': np.float64(1.8070833921432494)}

Episode step 3500, time diff 0.734246015548706, total time dif 341.9969148635864)
step: 3500 @ episode report: {'average_total_reward': np.float32(6.7188897), 'reward_variance': np.float32(3.0117302), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10469367280602455), 'actor_loss': np.float64(-0.9557181835174561), 'hyper_actor_loss': np.float64(0.013330927304923534), 'behavior_loss': np.float64(1.8383395314216613)}

Episode step 3510, time diff 0.7496531009674072, total time dif 342.73116087913513)
step: 3510 @ episode report: {'average_total_reward': np.float32(6.467778), 'reward_variance': np.float32(2.0256162), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10957811549305915), 'actor_loss': np.float64(-0.9388185799121856), 'hyper_actor_loss': np.float64(0.012993668671697378), 'behavior_loss': np.float64(1.814522886276245)}

Episode step 3520, time diff 0.7740910053253174, total time dif 343.48081398010254)
step: 3520 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(1.9871557), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09958667233586312), 'actor_loss': np.float64(-0.9466268122196198), 'hyper_actor_loss': np.float64(0.01254446543753147), 'behavior_loss': np.float64(1.7835349798202516)}

Episode step 3530, time diff 0.7542445659637451, total time dif 344.25490498542786)
step: 3530 @ episode report: {'average_total_reward': np.float32(6.518889), 'reward_variance': np.float32(2.5362232), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10616127923130989), 'actor_loss': np.float64(-0.9715279638767242), 'hyper_actor_loss': np.float64(0.01204102486371994), 'behavior_loss': np.float64(1.762698245048523)}

Episode step 3540, time diff 0.7627360820770264, total time dif 345.0091495513916)
step: 3540 @ episode report: {'average_total_reward': np.float32(6.6066666), 'reward_variance': np.float32(1.1896347), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1124176800251007), 'actor_loss': np.float64(-0.9702205181121826), 'hyper_actor_loss': np.float64(0.011866080854088069), 'behavior_loss': np.float64(1.6712029576301575)}

Episode step 3550, time diff 0.7645628452301025, total time dif 345.7718856334686)
step: 3550 @ episode report: {'average_total_reward': np.float32(7.055556), 'reward_variance': np.float32(1.574173), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09886038154363633), 'actor_loss': np.float64(-0.9810581505298615), 'hyper_actor_loss': np.float64(0.011620242055505514), 'behavior_loss': np.float64(1.735885000228882)}

Episode step 3560, time diff 0.7292659282684326, total time dif 346.53644847869873)
step: 3560 @ episode report: {'average_total_reward': np.float32(6.4433336), 'reward_variance': np.float32(1.7854191), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09209963604807854), 'actor_loss': np.float64(-0.9034643530845642), 'hyper_actor_loss': np.float64(0.012045438773930073), 'behavior_loss': np.float64(1.8214439511299134)}

Episode step 3570, time diff 0.7464771270751953, total time dif 347.26571440696716)
step: 3570 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(1.5364206), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09565875753760338), 'actor_loss': np.float64(-0.906687980890274), 'hyper_actor_loss': np.float64(0.011877212207764387), 'behavior_loss': np.float64(1.8256433606147766)}

Episode step 3580, time diff 0.7521669864654541, total time dif 348.01219153404236)
step: 3580 @ episode report: {'average_total_reward': np.float32(6.6066666), 'reward_variance': np.float32(1.1806719), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10094833150506019), 'actor_loss': np.float64(-0.9376501619815827), 'hyper_actor_loss': np.float64(0.011575668770819902), 'behavior_loss': np.float64(1.9142463088035584)}

Episode step 3590, time diff 0.7292451858520508, total time dif 348.7643585205078)
step: 3590 @ episode report: {'average_total_reward': np.float32(5.857778), 'reward_variance': np.float32(1.063082), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10210254862904548), 'actor_loss': np.float64(-0.8944116473197937), 'hyper_actor_loss': np.float64(0.011198758520185948), 'behavior_loss': np.float64(1.7428508758544923)}

Episode step 3600, time diff 0.7525334358215332, total time dif 349.49360370635986)
step: 3600 @ episode report: {'average_total_reward': np.float32(6.27), 'reward_variance': np.float32(2.425705), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10207372009754181), 'actor_loss': np.float64(-0.9372596800327301), 'hyper_actor_loss': np.float64(0.010744328796863555), 'behavior_loss': np.float64(1.8853995680809021)}

Episode step 3610, time diff 0.914766788482666, total time dif 350.2461371421814)
step: 3610 @ episode report: {'average_total_reward': np.float32(6.1211114), 'reward_variance': np.float32(3.7443557), 'max_total_reward': np.float32(9.777777), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10991686582565308), 'actor_loss': np.float64(-0.9275790810585022), 'hyper_actor_loss': np.float64(0.010356789175420999), 'behavior_loss': np.float64(1.9055718421936034)}

Episode step 3620, time diff 0.7789998054504395, total time dif 351.16090393066406)
step: 3620 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(0.9472294), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10291747003793716), 'actor_loss': np.float64(-0.9781077384948731), 'hyper_actor_loss': np.float64(0.010207338444888591), 'behavior_loss': np.float64(1.7248695969581604)}

Episode step 3630, time diff 0.7561376094818115, total time dif 351.9399037361145)
step: 3630 @ episode report: {'average_total_reward': np.float32(5.9822226), 'reward_variance': np.float32(1.3296843), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10687037110328675), 'actor_loss': np.float64(-0.9130025625228881), 'hyper_actor_loss': np.float64(0.010559391789138317), 'behavior_loss': np.float64(1.7933650493621827)}

Episode step 3640, time diff 0.747753381729126, total time dif 352.6960413455963)
step: 3640 @ episode report: {'average_total_reward': np.float32(6.7066665), 'reward_variance': np.float32(1.5108693), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09928165897727012), 'actor_loss': np.float64(-0.9651824474334717), 'hyper_actor_loss': np.float64(0.011129019036889076), 'behavior_loss': np.float64(1.8900224447250367)}

Episode step 3650, time diff 0.7322032451629639, total time dif 353.44379472732544)
step: 3650 @ episode report: {'average_total_reward': np.float32(6.1577783), 'reward_variance': np.float32(0.49834076), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09176231622695923), 'actor_loss': np.float64(-0.9064579427242279), 'hyper_actor_loss': np.float64(0.011805720906704665), 'behavior_loss': np.float64(1.8761938095092774)}

Episode step 3660, time diff 0.7531599998474121, total time dif 354.1759979724884)
step: 3660 @ episode report: {'average_total_reward': np.float32(7.194444), 'reward_variance': np.float32(2.8128464), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07950348369777202), 'actor_loss': np.float64(-0.9296350598335266), 'hyper_actor_loss': np.float64(0.012236387748271228), 'behavior_loss': np.float64(1.8801252841949463)}

Episode step 3670, time diff 0.7547726631164551, total time dif 354.9291579723358)
step: 3670 @ episode report: {'average_total_reward': np.float32(7.0700006), 'reward_variance': np.float32(2.7107925), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09768772348761559), 'actor_loss': np.float64(-0.9447811722755433), 'hyper_actor_loss': np.float64(0.012772602122277021), 'behavior_loss': np.float64(1.842496132850647)}

Episode step 3680, time diff 0.7653398513793945, total time dif 355.68393063545227)
step: 3680 @ episode report: {'average_total_reward': np.float32(7.443334), 'reward_variance': np.float32(1.818826), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11123507395386696), 'actor_loss': np.float64(-0.9879153072834015), 'hyper_actor_loss': np.float64(0.01348033370450139), 'behavior_loss': np.float64(1.960041606426239)}

Episode step 3690, time diff 0.7515463829040527, total time dif 356.44927048683167)
step: 3690 @ episode report: {'average_total_reward': np.float32(7.1433334), 'reward_variance': np.float32(2.12095), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09783496633172035), 'actor_loss': np.float64(-0.9540544927120209), 'hyper_actor_loss': np.float64(0.014421585761010648), 'behavior_loss': np.float64(1.7118605852127076)}

Episode step 3700, time diff 0.7706551551818848, total time dif 357.2008168697357)
step: 3700 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(2.1327903), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08818217664957047), 'actor_loss': np.float64(-0.9499906659126282), 'hyper_actor_loss': np.float64(0.014968611020594835), 'behavior_loss': np.float64(1.8439070582389832)}

Episode step 3710, time diff 0.7720937728881836, total time dif 357.9714720249176)
step: 3710 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(2.0914083), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.2888885), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10303376093506814), 'actor_loss': np.float64(-0.9546579480171203), 'hyper_actor_loss': np.float64(0.015062970481812954), 'behavior_loss': np.float64(1.8334047079086304)}

Episode step 3720, time diff 0.7884409427642822, total time dif 358.7435657978058)
step: 3720 @ episode report: {'average_total_reward': np.float32(6.918889), 'reward_variance': np.float32(1.2486184), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1098415769636631), 'actor_loss': np.float64(-1.0051311135292054), 'hyper_actor_loss': np.float64(0.014940571039915085), 'behavior_loss': np.float64(1.768101978302002)}

Episode step 3730, time diff 0.7776224613189697, total time dif 359.53200674057007)
step: 3730 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(3.6142726), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1077342040836811), 'actor_loss': np.float64(-0.9518525183200837), 'hyper_actor_loss': np.float64(0.014280109480023385), 'behavior_loss': np.float64(1.8274929404258728)}

Episode step 3740, time diff 0.7896459102630615, total time dif 360.30962920188904)
step: 3740 @ episode report: {'average_total_reward': np.float32(6.7211113), 'reward_variance': np.float32(1.6240852), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09710652977228165), 'actor_loss': np.float64(-0.9307234883308411), 'hyper_actor_loss': np.float64(0.013385837618261575), 'behavior_loss': np.float64(1.9669955611228942)}

Episode step 3750, time diff 0.787259578704834, total time dif 361.0992751121521)
step: 3750 @ episode report: {'average_total_reward': np.float32(7.1922226), 'reward_variance': np.float32(1.6424452), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09654879495501519), 'actor_loss': np.float64(-0.9316401541233063), 'hyper_actor_loss': np.float64(0.012808927334845066), 'behavior_loss': np.float64(1.90042964220047)}

Episode step 3760, time diff 0.7760441303253174, total time dif 361.88653469085693)
step: 3760 @ episode report: {'average_total_reward': np.float32(7.78), 'reward_variance': np.float32(2.4291804), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10026921406388282), 'actor_loss': np.float64(-0.9602168321609497), 'hyper_actor_loss': np.float64(0.012686486076563596), 'behavior_loss': np.float64(2.149006116390228)}

Episode step 3770, time diff 0.9073441028594971, total time dif 362.66257882118225)
step: 3770 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(2.18597), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10151400417089462), 'actor_loss': np.float64(-0.9088052392005921), 'hyper_actor_loss': np.float64(0.01258367756381631), 'behavior_loss': np.float64(2.050432229042053)}

Episode step 3780, time diff 0.7675361633300781, total time dif 363.56992292404175)
step: 3780 @ episode report: {'average_total_reward': np.float32(6.2822223), 'reward_variance': np.float32(1.580079), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10633886605501175), 'actor_loss': np.float64(-0.9644688189029693), 'hyper_actor_loss': np.float64(0.012985457945615054), 'behavior_loss': np.float64(2.201034462451935)}

Episode step 3790, time diff 0.8313946723937988, total time dif 364.3374590873718)
step: 3790 @ episode report: {'average_total_reward': np.float32(7.1066675), 'reward_variance': np.float32(2.6241531), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09753255620598793), 'actor_loss': np.float64(-0.9453819811344146), 'hyper_actor_loss': np.float64(0.012693012040108443), 'behavior_loss': np.float64(1.9284406900405884)}

Episode step 3800, time diff 0.8631486892700195, total time dif 365.1688537597656)
step: 3800 @ episode report: {'average_total_reward': np.float32(7.4044447), 'reward_variance': np.float32(4.0928683), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10322174429893494), 'actor_loss': np.float64(-0.9708306908607482), 'hyper_actor_loss': np.float64(0.012828746624290944), 'behavior_loss': np.float64(1.8841200947761536)}

Episode step 3810, time diff 0.7536766529083252, total time dif 366.03200244903564)
step: 3810 @ episode report: {'average_total_reward': np.float32(7.8044443), 'reward_variance': np.float32(1.346178), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08935541659593582), 'actor_loss': np.float64(-0.9195713937282562), 'hyper_actor_loss': np.float64(0.01275159902870655), 'behavior_loss': np.float64(2.093666434288025)}

Episode step 3820, time diff 0.7248396873474121, total time dif 366.78567910194397)
step: 3820 @ episode report: {'average_total_reward': np.float32(6.4944444), 'reward_variance': np.float32(1.9578333), 'max_total_reward': np.float32(8.533333), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10550491139292717), 'actor_loss': np.float64(-0.9519269526004791), 'hyper_actor_loss': np.float64(0.012782469484955072), 'behavior_loss': np.float64(1.973112905025482)}

Episode step 3830, time diff 0.7724337577819824, total time dif 367.5105187892914)
step: 3830 @ episode report: {'average_total_reward': np.float32(6.257778), 'reward_variance': np.float32(2.1872053), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09835689961910248), 'actor_loss': np.float64(-0.9271777391433715), 'hyper_actor_loss': np.float64(0.01256299577653408), 'behavior_loss': np.float64(2.1051852345466613)}

Episode step 3840, time diff 0.766209602355957, total time dif 368.28295254707336)
step: 3840 @ episode report: {'average_total_reward': np.float32(6.406667), 'reward_variance': np.float32(1.7241039), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10631690695881843), 'actor_loss': np.float64(-0.9835103929042817), 'hyper_actor_loss': np.float64(0.012697675079107285), 'behavior_loss': np.float64(1.9754111170768738)}

Episode step 3850, time diff 0.7677907943725586, total time dif 369.0491621494293)
step: 3850 @ episode report: {'average_total_reward': np.float32(6.8944445), 'reward_variance': np.float32(1.2074878), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09679218642413616), 'actor_loss': np.float64(-0.9614261507987976), 'hyper_actor_loss': np.float64(0.012751005031168461), 'behavior_loss': np.float64(2.0031798124313354)}

Episode step 3860, time diff 0.7432544231414795, total time dif 369.8169529438019)
step: 3860 @ episode report: {'average_total_reward': np.float32(6.8433332), 'reward_variance': np.float32(2.6450734), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10761672034859657), 'actor_loss': np.float64(-1.003825694322586), 'hyper_actor_loss': np.float64(0.013079501129686833), 'behavior_loss': np.float64(1.9599796772003173)}

Episode step 3870, time diff 0.7576131820678711, total time dif 370.56020736694336)
step: 3870 @ episode report: {'average_total_reward': np.float32(6.406667), 'reward_variance': np.float32(1.1285483), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09601248651742936), 'actor_loss': np.float64(-0.9463086545467376), 'hyper_actor_loss': np.float64(0.012742174882441759), 'behavior_loss': np.float64(2.0068376302719115)}

Episode step 3880, time diff 0.7337758541107178, total time dif 371.31782054901123)
step: 3880 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(1.8420607), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10066491365432739), 'actor_loss': np.float64(-0.9221215665340423), 'hyper_actor_loss': np.float64(0.013012906443327666), 'behavior_loss': np.float64(1.9580352663993836)}

Episode step 3890, time diff 0.7382152080535889, total time dif 372.05159640312195)
step: 3890 @ episode report: {'average_total_reward': np.float32(7.455556), 'reward_variance': np.float32(2.611408), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10587713345885277), 'actor_loss': np.float64(-0.9747801303863526), 'hyper_actor_loss': np.float64(0.012447815481573343), 'behavior_loss': np.float64(2.11003041267395)}

Episode step 3900, time diff 0.7373878955841064, total time dif 372.78981161117554)
step: 3900 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(3.291542), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08678979948163032), 'actor_loss': np.float64(-0.940639442205429), 'hyper_actor_loss': np.float64(0.012056405842304229), 'behavior_loss': np.float64(1.9471355199813842)}

Episode step 3910, time diff 0.7299675941467285, total time dif 373.52719950675964)
step: 3910 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(1.8864149), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09759869500994682), 'actor_loss': np.float64(-0.969790643453598), 'hyper_actor_loss': np.float64(0.012108115572482347), 'behavior_loss': np.float64(2.1053018450736998)}

Episode step 3920, time diff 0.7373809814453125, total time dif 374.2571671009064)
step: 3920 @ episode report: {'average_total_reward': np.float32(7.0311117), 'reward_variance': np.float32(3.0964153), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09929735362529754), 'actor_loss': np.float64(-0.9575856626033783), 'hyper_actor_loss': np.float64(0.012316887080669404), 'behavior_loss': np.float64(2.043703520298004)}

Episode step 3930, time diff 0.9214935302734375, total time dif 374.9945480823517)
step: 3930 @ episode report: {'average_total_reward': np.float32(7.3433332), 'reward_variance': np.float32(2.0604055), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10278294160962105), 'actor_loss': np.float64(-0.9770667910575866), 'hyper_actor_loss': np.float64(0.012361844722181558), 'behavior_loss': np.float64(2.051603841781616)}

Episode step 3940, time diff 0.7557885646820068, total time dif 375.9160416126251)
step: 3940 @ episode report: {'average_total_reward': np.float32(6.231111), 'reward_variance': np.float32(1.3206124), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10093010514974594), 'actor_loss': np.float64(-0.9504400968551636), 'hyper_actor_loss': np.float64(0.012130493763834238), 'behavior_loss': np.float64(2.2507097482681275)}

Episode step 3950, time diff 0.767920970916748, total time dif 376.67183017730713)
step: 3950 @ episode report: {'average_total_reward': np.float32(6.967778), 'reward_variance': np.float32(1.8085301), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09393704049289227), 'actor_loss': np.float64(-0.9371017873287201), 'hyper_actor_loss': np.float64(0.012209896743297578), 'behavior_loss': np.float64(2.1989560604095457)}

Episode step 3960, time diff 0.7354741096496582, total time dif 377.4397511482239)
step: 3960 @ episode report: {'average_total_reward': np.float32(6.6922226), 'reward_variance': np.float32(3.7648158), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09003214538097382), 'actor_loss': np.float64(-0.9383653342723847), 'hyper_actor_loss': np.float64(0.012299723085016012), 'behavior_loss': np.float64(2.1407353401184084)}

Episode step 3970, time diff 0.7667710781097412, total time dif 378.17522525787354)
step: 3970 @ episode report: {'average_total_reward': np.float32(7.455556), 'reward_variance': np.float32(2.2957041), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10502601936459541), 'actor_loss': np.float64(-0.9542508780956268), 'hyper_actor_loss': np.float64(0.0122163200750947), 'behavior_loss': np.float64(2.1458826065063477)}

Episode step 3980, time diff 0.7627012729644775, total time dif 378.9419963359833)
step: 3980 @ episode report: {'average_total_reward': np.float32(6.7822227), 'reward_variance': np.float32(0.73751146), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09131437242031097), 'actor_loss': np.float64(-0.9233269691467285), 'hyper_actor_loss': np.float64(0.012932796496897936), 'behavior_loss': np.float64(2.233921205997467)}

Episode step 3990, time diff 0.7384665012359619, total time dif 379.70469760894775)
step: 3990 @ episode report: {'average_total_reward': np.float32(6.8188896), 'reward_variance': np.float32(3.1484466), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10462269335985183), 'actor_loss': np.float64(-0.9831256926059723), 'hyper_actor_loss': np.float64(0.01345239169895649), 'behavior_loss': np.float64(2.237104618549347)}

Episode step 4000, time diff 0.7391979694366455, total time dif 380.4431641101837)
step: 4000 @ episode report: {'average_total_reward': np.float32(6.545556), 'reward_variance': np.float32(1.9903822), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09734714403748512), 'actor_loss': np.float64(-0.9076876640319824), 'hyper_actor_loss': np.float64(0.0128969794139266), 'behavior_loss': np.float64(2.263696014881134)}

Episode step 4010, time diff 0.730485200881958, total time dif 381.18236207962036)
step: 4010 @ episode report: {'average_total_reward': np.float32(6.8822227), 'reward_variance': np.float32(3.2725976), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10584346912801265), 'actor_loss': np.float64(-0.9879575550556183), 'hyper_actor_loss': np.float64(0.012422244343906642), 'behavior_loss': np.float64(2.243857181072235)}

Episode step 4020, time diff 0.738253116607666, total time dif 381.9128472805023)
step: 4020 @ episode report: {'average_total_reward': np.float32(6.9944444), 'reward_variance': np.float32(1.906105), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09022896103560925), 'actor_loss': np.float64(-0.903047513961792), 'hyper_actor_loss': np.float64(0.011327620130032301), 'behavior_loss': np.float64(2.201269268989563)}

Episode step 4030, time diff 0.7462713718414307, total time dif 382.65110039711)
step: 4030 @ episode report: {'average_total_reward': np.float32(6.37), 'reward_variance': np.float32(1.0386436), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09542313292622566), 'actor_loss': np.float64(-0.9744322240352631), 'hyper_actor_loss': np.float64(0.010496775712817907), 'behavior_loss': np.float64(2.123712694644928)}

Episode step 4040, time diff 0.7634766101837158, total time dif 383.3973717689514)
step: 4040 @ episode report: {'average_total_reward': np.float32(6.594445), 'reward_variance': np.float32(1.5593641), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08882136158645153), 'actor_loss': np.float64(-0.9505168735980988), 'hyper_actor_loss': np.float64(0.00917108291760087), 'behavior_loss': np.float64(2.2492119431495667)}

Episode step 4050, time diff 0.7355341911315918, total time dif 384.16084837913513)
step: 4050 @ episode report: {'average_total_reward': np.float32(6.745556), 'reward_variance': np.float32(2.786406), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10101454220712185), 'actor_loss': np.float64(-0.9388625383377075), 'hyper_actor_loss': np.float64(0.009006747417151928), 'behavior_loss': np.float64(2.2659266352653504)}

Episode step 4060, time diff 0.756817102432251, total time dif 384.8963825702667)
step: 4060 @ episode report: {'average_total_reward': np.float32(6.8188887), 'reward_variance': np.float32(6.655261), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10192016288638114), 'actor_loss': np.float64(-0.9723938584327698), 'hyper_actor_loss': np.float64(0.008701580297201871), 'behavior_loss': np.float64(2.2056081295013428)}

Episode step 4070, time diff 0.7791774272918701, total time dif 385.653199672699)
step: 4070 @ episode report: {'average_total_reward': np.float32(7.3922224), 'reward_variance': np.float32(2.5434089), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0997992530465126), 'actor_loss': np.float64(-0.9466830968856812), 'hyper_actor_loss': np.float64(0.00866908933967352), 'behavior_loss': np.float64(2.21013799905777)}

Episode step 4080, time diff 0.7176494598388672, total time dif 386.43237709999084)
step: 4080 @ episode report: {'average_total_reward': np.float32(6.8333335), 'reward_variance': np.float32(2.7880745), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09281901195645333), 'actor_loss': np.float64(-0.9671257376670838), 'hyper_actor_loss': np.float64(0.009311403427273036), 'behavior_loss': np.float64(2.3383276224136353)}

Episode step 4090, time diff 0.9334602355957031, total time dif 387.1500265598297)
step: 4090 @ episode report: {'average_total_reward': np.float32(6.8677773), 'reward_variance': np.float32(2.3440356), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10229086354374886), 'actor_loss': np.float64(-0.9593328952789306), 'hyper_actor_loss': np.float64(0.010137540102005006), 'behavior_loss': np.float64(2.2992671251297)}

Episode step 4100, time diff 0.7655577659606934, total time dif 388.0834867954254)
step: 4100 @ episode report: {'average_total_reward': np.float32(8.202223), 'reward_variance': np.float32(3.8009834), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09376278482377529), 'actor_loss': np.float64(-0.9978220403194428), 'hyper_actor_loss': np.float64(0.01135683637112379), 'behavior_loss': np.float64(2.2787046790122987)}

Episode step 4110, time diff 0.7358403205871582, total time dif 388.8490445613861)
step: 4110 @ episode report: {'average_total_reward': np.float32(7.9677787), 'reward_variance': np.float32(4.1008515), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09442805051803589), 'actor_loss': np.float64(-0.9430685758590698), 'hyper_actor_loss': np.float64(0.01280400026589632), 'behavior_loss': np.float64(2.285397481918335)}

Episode step 4120, time diff 0.7670776844024658, total time dif 389.58488488197327)
step: 4120 @ episode report: {'average_total_reward': np.float32(7.38), 'reward_variance': np.float32(2.2864408), 'max_total_reward': np.float32(11.022224), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10017073675990104), 'actor_loss': np.float64(-0.9958623766899108), 'hyper_actor_loss': np.float64(0.014002441056072712), 'behavior_loss': np.float64(2.232887363433838)}

Episode step 4130, time diff 0.7603092193603516, total time dif 390.35196256637573)
step: 4130 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(2.4163568), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10023289993405342), 'actor_loss': np.float64(-0.9721092164516449), 'hyper_actor_loss': np.float64(0.014717854745686054), 'behavior_loss': np.float64(2.269693684577942)}

Episode step 4140, time diff 0.731536865234375, total time dif 391.1122717857361)
step: 4140 @ episode report: {'average_total_reward': np.float32(6.6066666), 'reward_variance': np.float32(1.0065728), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10323721319437026), 'actor_loss': np.float64(-0.9605162560939788), 'hyper_actor_loss': np.float64(0.015868019964545965), 'behavior_loss': np.float64(2.0891300201416017)}

Episode step 4150, time diff 0.7420334815979004, total time dif 391.84380865097046)
step: 4150 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(1.6075798), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10381081104278564), 'actor_loss': np.float64(-1.0083229780197143), 'hyper_actor_loss': np.float64(0.017655358463525773), 'behavior_loss': np.float64(2.402464199066162)}

Episode step 4160, time diff 0.7370421886444092, total time dif 392.58584213256836)
step: 4160 @ episode report: {'average_total_reward': np.float32(5.335555), 'reward_variance': np.float32(1.1715999), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10612455829977989), 'actor_loss': np.float64(-1.080329704284668), 'hyper_actor_loss': np.float64(0.02211693823337555), 'behavior_loss': np.float64(2.1065141558647156)}

Episode step 4170, time diff 0.7380187511444092, total time dif 393.32288432121277)
step: 4170 @ episode report: {'average_total_reward': np.float32(3.4500003), 'reward_variance': np.float32(0.30894446), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10236390456557273), 'actor_loss': np.float64(-1.3671956419944764), 'hyper_actor_loss': np.float64(0.03033082578331232), 'behavior_loss': np.float64(1.6649402141571046)}

Episode step 4180, time diff 0.745457649230957, total time dif 394.0609030723572)
step: 4180 @ episode report: {'average_total_reward': np.float32(4.1744447), 'reward_variance': np.float32(0.65521127), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10233928523957729), 'actor_loss': np.float64(-1.5386653184890746), 'hyper_actor_loss': np.float64(0.04067598059773445), 'behavior_loss': np.float64(1.5404892683029174)}

Episode step 4190, time diff 0.7369139194488525, total time dif 394.80636072158813)
step: 4190 @ episode report: {'average_total_reward': np.float32(3.8622222), 'reward_variance': np.float32(1.0271409), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10116417109966278), 'actor_loss': np.float64(-1.4476410031318665), 'hyper_actor_loss': np.float64(0.049252279475331305), 'behavior_loss': np.float64(1.476413869857788)}

Episode step 4200, time diff 0.7728185653686523, total time dif 395.543274641037)
step: 4200 @ episode report: {'average_total_reward': np.float32(3.7155557), 'reward_variance': np.float32(0.3427705), 'max_total_reward': np.float32(5.166667), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1087893195450306), 'actor_loss': np.float64(-1.3752469301223755), 'hyper_actor_loss': np.float64(0.05728829316794872), 'behavior_loss': np.float64(1.211335289478302)}

Episode step 4210, time diff 0.740882158279419, total time dif 396.31609320640564)
step: 4210 @ episode report: {'average_total_reward': np.float32(2.5544443), 'reward_variance': np.float32(0.15578893), 'max_total_reward': np.float32(3.288889), 'min_total_reward': np.float32(2.1666665), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10716996043920517), 'actor_loss': np.float64(-1.467644500732422), 'hyper_actor_loss': np.float64(0.06407138966023922), 'behavior_loss': np.float64(0.9764006018638611)}

Episode step 4220, time diff 0.7241494655609131, total time dif 397.05697536468506)
step: 4220 @ episode report: {'average_total_reward': np.float32(1.5344445), 'reward_variance': np.float32(0.1677395), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08658047467470169), 'actor_loss': np.float64(-1.5279937148094178), 'hyper_actor_loss': np.float64(0.065849918872118), 'behavior_loss': np.float64(0.8482878863811493)}

Episode step 4230, time diff 0.7538337707519531, total time dif 397.781124830246)
step: 4230 @ episode report: {'average_total_reward': np.float32(0.87333333), 'reward_variance': np.float32(0.03644939), 'max_total_reward': np.float32(1.1666667), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10627427399158478), 'actor_loss': np.float64(-1.4414923191070557), 'hyper_actor_loss': np.float64(0.06480872705578804), 'behavior_loss': np.float64(0.7361684024333954)}

Episode step 4240, time diff 0.76766037940979, total time dif 398.5349586009979)
step: 4240 @ episode report: {'average_total_reward': np.float32(0.16555554), 'reward_variance': np.float32(0.043838274), 'max_total_reward': np.float32(0.5555556), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.5), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.13052943274378775), 'actor_loss': np.float64(-1.2573691964149476), 'hyper_actor_loss': np.float64(0.06343553811311722), 'behavior_loss': np.float64(0.6790271997451782)}

Episode step 4250, time diff 0.9308919906616211, total time dif 399.3026189804077)
step: 4250 @ episode report: {'average_total_reward': np.float32(0.23888886), 'reward_variance': np.float32(0.08370988), 'max_total_reward': np.float32(0.79999995), 'min_total_reward': np.float32(-0.07777779), 'average_n_step': np.float32(2.5), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.12716061547398566), 'actor_loss': np.float64(-0.5936624974012374), 'hyper_actor_loss': np.float64(0.0631713043898344), 'behavior_loss': np.float64(0.6112317442893982)}

Episode step 4260, time diff 0.7682077884674072, total time dif 400.23351097106934)
step: 4260 @ episode report: {'average_total_reward': np.float32(0.64111114), 'reward_variance': np.float32(0.08380372), 'max_total_reward': np.float32(1.0444446), 'min_total_reward': np.float32(0.3111111), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09874292947351933), 'actor_loss': np.float64(-0.8043553471565247), 'hyper_actor_loss': np.float64(0.060813768208026885), 'behavior_loss': np.float64(0.6321784853935242)}

Episode step 4270, time diff 0.7782604694366455, total time dif 401.00171875953674)
step: 4270 @ episode report: {'average_total_reward': np.float32(1.2466667), 'reward_variance': np.float32(0.23955064), 'max_total_reward': np.float32(2.2888892), 'min_total_reward': np.float32(0.6777779), 'average_n_step': np.float32(3.3), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0897217832505703), 'actor_loss': np.float64(-0.8921907365322113), 'hyper_actor_loss': np.float64(0.058610791340470314), 'behavior_loss': np.float64(0.6216698408126831)}

Episode step 4280, time diff 0.8052120208740234, total time dif 401.7799792289734)
step: 4280 @ episode report: {'average_total_reward': np.float32(1.2488891), 'reward_variance': np.float32(0.1949679), 'max_total_reward': np.float32(1.9222223), 'min_total_reward': np.float32(0.79999995), 'average_n_step': np.float32(3.4), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08967634290456772), 'actor_loss': np.float64(-0.827280443906784), 'hyper_actor_loss': np.float64(0.05662606842815876), 'behavior_loss': np.float64(0.6292793214321136)}

Episode step 4290, time diff 0.7879974842071533, total time dif 402.5851912498474)
step: 4290 @ episode report: {'average_total_reward': np.float32(1.958889), 'reward_variance': np.float32(0.5916308), 'max_total_reward': np.float32(3.4111109), 'min_total_reward': np.float32(0.6777778), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08870884478092193), 'actor_loss': np.float64(-0.8496049106121063), 'hyper_actor_loss': np.float64(0.05453013144433498), 'behavior_loss': np.float64(0.6794405043125152)}

Episode step 4300, time diff 0.7974681854248047, total time dif 403.37318873405457)
step: 4300 @ episode report: {'average_total_reward': np.float32(2.766667), 'reward_variance': np.float32(0.48834568), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08764345347881317), 'actor_loss': np.float64(-0.9223357796669006), 'hyper_actor_loss': np.float64(0.05266254059970379), 'behavior_loss': np.float64(0.7058583319187164)}

Episode step 4310, time diff 0.7687647342681885, total time dif 404.17065691947937)
step: 4310 @ episode report: {'average_total_reward': np.float32(2.3422225), 'reward_variance': np.float32(0.17760003), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09814826026558876), 'actor_loss': np.float64(-0.9535131573677063), 'hyper_actor_loss': np.float64(0.05111182704567909), 'behavior_loss': np.float64(0.7977591693401337)}

Episode step 4320, time diff 0.7755956649780273, total time dif 404.93942165374756)
step: 4320 @ episode report: {'average_total_reward': np.float32(2.5688891), 'reward_variance': np.float32(0.8832544), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07828943431377411), 'actor_loss': np.float64(-0.9106605648994446), 'hyper_actor_loss': np.float64(0.05055606067180633), 'behavior_loss': np.float64(0.8617164969444275)}

Episode step 4330, time diff 0.7848269939422607, total time dif 405.7150173187256)
step: 4330 @ episode report: {'average_total_reward': np.float32(2.5811114), 'reward_variance': np.float32(0.4618284), 'max_total_reward': np.float32(4.0444446), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09658264294266701), 'actor_loss': np.float64(-0.9445956885814667), 'hyper_actor_loss': np.float64(0.050174951925873755), 'behavior_loss': np.float64(0.9264850735664367)}

Episode step 4340, time diff 0.7610418796539307, total time dif 406.49984431266785)
step: 4340 @ episode report: {'average_total_reward': np.float32(3.676667), 'reward_variance': np.float32(0.3054433), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08621414825320244), 'actor_loss': np.float64(-0.8980744659900666), 'hyper_actor_loss': np.float64(0.05018735937774181), 'behavior_loss': np.float64(0.994108647108078)}

Episode step 4350, time diff 0.7637910842895508, total time dif 407.2608861923218)
step: 4350 @ episode report: {'average_total_reward': np.float32(4.186667), 'reward_variance': np.float32(0.39199513), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08671423345804215), 'actor_loss': np.float64(-0.9483416855335236), 'hyper_actor_loss': np.float64(0.051302208378911016), 'behavior_loss': np.float64(1.018638050556183)}

Episode step 4360, time diff 0.7549736499786377, total time dif 408.0246772766113)
step: 4360 @ episode report: {'average_total_reward': np.float32(5.9577775), 'reward_variance': np.float32(1.5370817), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09432843849062919), 'actor_loss': np.float64(-0.9195187389850616), 'hyper_actor_loss': np.float64(0.05111732594668865), 'behavior_loss': np.float64(1.149388349056244)}

Episode step 4370, time diff 0.7569506168365479, total time dif 408.77965092658997)
step: 4370 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(3.5713692), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09663445949554443), 'actor_loss': np.float64(-0.9329168736934662), 'hyper_actor_loss': np.float64(0.048585842922329904), 'behavior_loss': np.float64(1.2268148183822631)}

Episode step 4380, time diff 0.7547461986541748, total time dif 409.5366015434265)
step: 4380 @ episode report: {'average_total_reward': np.float32(8.887778), 'reward_variance': np.float32(3.2512956), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09528054445981979), 'actor_loss': np.float64(-0.9239796936511994), 'hyper_actor_loss': np.float64(0.04449036121368408), 'behavior_loss': np.float64(1.3819907784461976)}

Episode step 4390, time diff 0.8053061962127686, total time dif 410.2913477420807)
step: 4390 @ episode report: {'average_total_reward': np.float32(8.204445), 'reward_variance': np.float32(1.1189435), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08815336301922798), 'actor_loss': np.float64(-0.9078457355499268), 'hyper_actor_loss': np.float64(0.0400348786264658), 'behavior_loss': np.float64(1.3975378751754761)}

Episode step 4400, time diff 0.762556791305542, total time dif 411.09665393829346)
step: 4400 @ episode report: {'average_total_reward': np.float32(8.1044445), 'reward_variance': np.float32(3.3676095), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08455256298184395), 'actor_loss': np.float64(-0.933534836769104), 'hyper_actor_loss': np.float64(0.0352848544716835), 'behavior_loss': np.float64(1.452060091495514)}

Episode step 4410, time diff 0.9446096420288086, total time dif 411.859210729599)
step: 4410 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(2.1058476), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08774136342108249), 'actor_loss': np.float64(-0.9293901860713959), 'hyper_actor_loss': np.float64(0.03090268075466156), 'behavior_loss': np.float64(1.6527408361434937)}

Episode step 4420, time diff 0.7522425651550293, total time dif 412.8038203716278)
step: 4420 @ episode report: {'average_total_reward': np.float32(9.451112), 'reward_variance': np.float32(4.4070673), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.095327477902174), 'actor_loss': np.float64(-0.9368855237960816), 'hyper_actor_loss': np.float64(0.027881213277578355), 'behavior_loss': np.float64(1.6623038291931151)}

Episode step 4430, time diff 0.7592558860778809, total time dif 413.55606293678284)
step: 4430 @ episode report: {'average_total_reward': np.float32(8.204445), 'reward_variance': np.float32(0.93289393), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10228797867894172), 'actor_loss': np.float64(-0.9454318165779114), 'hyper_actor_loss': np.float64(0.025805096514523028), 'behavior_loss': np.float64(1.703144097328186)}

Episode step 4440, time diff 0.7894937992095947, total time dif 414.3153188228607)
step: 4440 @ episode report: {'average_total_reward': np.float32(8.777779), 'reward_variance': np.float32(2.840445), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09407705441117287), 'actor_loss': np.float64(-0.9634635865688324), 'hyper_actor_loss': np.float64(0.023888576962053775), 'behavior_loss': np.float64(1.7225586533546449)}

Episode step 4450, time diff 0.804466962814331, total time dif 415.1048126220703)
step: 4450 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(2.0391622), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0819141186773777), 'actor_loss': np.float64(-0.8944907486438751), 'hyper_actor_loss': np.float64(0.022445685975253583), 'behavior_loss': np.float64(1.8331543684005738)}

Episode step 4460, time diff 0.788109302520752, total time dif 415.90927958488464)
step: 4460 @ episode report: {'average_total_reward': np.float32(8.875555), 'reward_variance': np.float32(2.4662914), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0972585417330265), 'actor_loss': np.float64(-0.9679140448570251), 'hyper_actor_loss': np.float64(0.02118565794080496), 'behavior_loss': np.float64(1.853458285331726)}

Episode step 4470, time diff 0.7623906135559082, total time dif 416.6973888874054)
step: 4470 @ episode report: {'average_total_reward': np.float32(8.016667), 'reward_variance': np.float32(2.4195125), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08453030623495579), 'actor_loss': np.float64(-0.9265939354896545), 'hyper_actor_loss': np.float64(0.019787408411502838), 'behavior_loss': np.float64(1.8127821326255797)}

Episode step 4480, time diff 0.7472012042999268, total time dif 417.4597795009613)
step: 4480 @ episode report: {'average_total_reward': np.float32(7.928889), 'reward_variance': np.float32(1.1113379), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09110616520047188), 'actor_loss': np.float64(-0.9216897130012512), 'hyper_actor_loss': np.float64(0.017838801629841326), 'behavior_loss': np.float64(1.8395151138305663)}

Episode step 4490, time diff 0.8305928707122803, total time dif 418.20698070526123)
step: 4490 @ episode report: {'average_total_reward': np.float32(8.4388895), 'reward_variance': np.float32(3.8877113), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10076583251357078), 'actor_loss': np.float64(-0.9811138808727264), 'hyper_actor_loss': np.float64(0.016052223462611436), 'behavior_loss': np.float64(1.893499529361725)}

Episode step 4500, time diff 0.7430610656738281, total time dif 419.0375735759735)
step: 4500 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(1.7469151), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08863478973507881), 'actor_loss': np.float64(-0.9246405303478241), 'hyper_actor_loss': np.float64(0.014497632067650557), 'behavior_loss': np.float64(1.9188524961471558)}

Episode step 4510, time diff 0.7566685676574707, total time dif 419.78063464164734)
step: 4510 @ episode report: {'average_total_reward': np.float32(8.626667), 'reward_variance': np.float32(1.5033628), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09375023394823075), 'actor_loss': np.float64(-0.9714014232158661), 'hyper_actor_loss': np.float64(0.013347030431032181), 'behavior_loss': np.float64(1.9657287001609802)}

Episode step 4520, time diff 0.7718737125396729, total time dif 420.5373032093048)
step: 4520 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(0.98357505), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1033881239593029), 'actor_loss': np.float64(-0.9291890442371369), 'hyper_actor_loss': np.float64(0.012166780047118663), 'behavior_loss': np.float64(2.0339904189109803)}

Episode step 4530, time diff 0.7515685558319092, total time dif 421.3091769218445)
step: 4530 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(1.4839004), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0959940031170845), 'actor_loss': np.float64(-0.9469612538814545), 'hyper_actor_loss': np.float64(0.011709072068333626), 'behavior_loss': np.float64(2.04833208322525)}

Episode step 4540, time diff 0.8133902549743652, total time dif 422.0607454776764)
step: 4540 @ episode report: {'average_total_reward': np.float32(7.9800005), 'reward_variance': np.float32(2.2461433), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09076762571930885), 'actor_loss': np.float64(-0.9077489256858826), 'hyper_actor_loss': np.float64(0.01092075351625681), 'behavior_loss': np.float64(2.1432384252548218)}

Episode step 4550, time diff 0.7583260536193848, total time dif 422.87413573265076)
step: 4550 @ episode report: {'average_total_reward': np.float32(7.4288893), 'reward_variance': np.float32(4.7534375), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0808191180229187), 'actor_loss': np.float64(-0.9646200001239776), 'hyper_actor_loss': np.float64(0.01072116643190384), 'behavior_loss': np.float64(2.0076420068740846)}

Episode step 4560, time diff 0.7684845924377441, total time dif 423.63246178627014)
step: 4560 @ episode report: {'average_total_reward': np.float32(7.8288894), 'reward_variance': np.float32(3.2525487), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09270388633012772), 'actor_loss': np.float64(-0.9537958264350891), 'hyper_actor_loss': np.float64(0.010268368013203143), 'behavior_loss': np.float64(2.057032859325409)}

Episode step 4570, time diff 0.947035551071167, total time dif 424.4009463787079)
step: 4570 @ episode report: {'average_total_reward': np.float32(7.0166674), 'reward_variance': np.float32(4.9921556), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10425925105810166), 'actor_loss': np.float64(-0.958416473865509), 'hyper_actor_loss': np.float64(0.009984381031244993), 'behavior_loss': np.float64(2.447680687904358)}

Episode step 4580, time diff 0.7363967895507812, total time dif 425.34798192977905)
step: 4580 @ episode report: {'average_total_reward': np.float32(8.1044445), 'reward_variance': np.float32(2.1157322), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09266477525234222), 'actor_loss': np.float64(-0.9425197005271911), 'hyper_actor_loss': np.float64(0.009971248731017113), 'behavior_loss': np.float64(2.2548773527145385)}

Episode step 4590, time diff 0.7865796089172363, total time dif 426.08437871932983)
step: 4590 @ episode report: {'average_total_reward': np.float32(7.5166674), 'reward_variance': np.float32(5.409241), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08536890670657157), 'actor_loss': np.float64(-0.9511505603790283), 'hyper_actor_loss': np.float64(0.009883857984095811), 'behavior_loss': np.float64(2.184362518787384)}

Episode step 4600, time diff 0.7656493186950684, total time dif 426.87095832824707)
step: 4600 @ episode report: {'average_total_reward': np.float32(7.6922235), 'reward_variance': np.float32(1.3019025), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.2888894), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0993525356054306), 'actor_loss': np.float64(-0.9902603507041932), 'hyper_actor_loss': np.float64(0.009839963167905807), 'behavior_loss': np.float64(2.21732382774353)}

Episode step 4610, time diff 0.790355920791626, total time dif 427.63660764694214)
step: 4610 @ episode report: {'average_total_reward': np.float32(8.341112), 'reward_variance': np.float32(3.208891), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11105536818504333), 'actor_loss': np.float64(-0.9660284459590912), 'hyper_actor_loss': np.float64(0.009500935953110457), 'behavior_loss': np.float64(2.17423529624939)}

Episode step 4620, time diff 0.7761378288269043, total time dif 428.42696356773376)
step: 4620 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(1.3050973), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08932631462812424), 'actor_loss': np.float64(-0.927755069732666), 'hyper_actor_loss': np.float64(0.009482892230153084), 'behavior_loss': np.float64(2.3247575044631956)}

Episode step 4630, time diff 0.7659704685211182, total time dif 429.20310139656067)
step: 4630 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(1.5816902), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08741684630513191), 'actor_loss': np.float64(-0.934154611825943), 'hyper_actor_loss': np.float64(0.009409698750823736), 'behavior_loss': np.float64(2.359821248054504)}

Episode step 4640, time diff 0.7784631252288818, total time dif 429.9690718650818)
step: 4640 @ episode report: {'average_total_reward': np.float32(6.967778), 'reward_variance': np.float32(4.8036165), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09531694054603576), 'actor_loss': np.float64(-0.9539050757884979), 'hyper_actor_loss': np.float64(0.009257206693291664), 'behavior_loss': np.float64(2.3504693269729615)}

Episode step 4650, time diff 0.7804036140441895, total time dif 430.74753499031067)
step: 4650 @ episode report: {'average_total_reward': np.float32(7.6800013), 'reward_variance': np.float32(3.400465), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0791063578799367), 'actor_loss': np.float64(-0.9157944977283478), 'hyper_actor_loss': np.float64(0.009576385095715522), 'behavior_loss': np.float64(2.3041127800941466)}

Episode step 4660, time diff 0.7418861389160156, total time dif 431.52793860435486)
step: 4660 @ episode report: {'average_total_reward': np.float32(8.177778), 'reward_variance': np.float32(2.7068896), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10395648404955864), 'actor_loss': np.float64(-0.9922063946723938), 'hyper_actor_loss': np.float64(0.009721554908901453), 'behavior_loss': np.float64(2.150192129611969)}

Episode step 4670, time diff 0.7528188228607178, total time dif 432.2698247432709)
step: 4670 @ episode report: {'average_total_reward': np.float32(7.9655557), 'reward_variance': np.float32(2.6989484), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08528936542570591), 'actor_loss': np.float64(-0.9793677270412445), 'hyper_actor_loss': np.float64(0.00961155453696847), 'behavior_loss': np.float64(2.185708236694336)}

Episode step 4680, time diff 0.7810697555541992, total time dif 433.0226435661316)
step: 4680 @ episode report: {'average_total_reward': np.float32(7.304445), 'reward_variance': np.float32(1.6413136), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10556772202253342), 'actor_loss': np.float64(-0.9865764498710632), 'hyper_actor_loss': np.float64(0.009604998771101237), 'behavior_loss': np.float64(2.2633816957473756)}

Episode step 4690, time diff 0.813162088394165, total time dif 433.8037133216858)
step: 4690 @ episode report: {'average_total_reward': np.float32(7.204444), 'reward_variance': np.float32(2.8286717), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09158833883702755), 'actor_loss': np.float64(-0.9399330615997314), 'hyper_actor_loss': np.float64(0.009603017941117287), 'behavior_loss': np.float64(2.369928503036499)}

Episode step 4700, time diff 0.7450323104858398, total time dif 434.61687541007996)
step: 4700 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(2.870988), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1052088513970375), 'actor_loss': np.float64(-0.9626002013683319), 'hyper_actor_loss': np.float64(0.009869437478482724), 'behavior_loss': np.float64(2.4101229190826414)}

Episode step 4710, time diff 0.7728288173675537, total time dif 435.3619077205658)
step: 4710 @ episode report: {'average_total_reward': np.float32(7.5800004), 'reward_variance': np.float32(3.1486871), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08836750090122222), 'actor_loss': np.float64(-0.9403839707374573), 'hyper_actor_loss': np.float64(0.009850827697664499), 'behavior_loss': np.float64(2.267162036895752)}

Episode step 4720, time diff 0.7508735656738281, total time dif 436.13473653793335)
step: 4720 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(1.199921), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07963314652442932), 'actor_loss': np.float64(-0.9156887292861938), 'hyper_actor_loss': np.float64(0.009609358571469783), 'behavior_loss': np.float64(2.2618597030639647)}

Episode step 4730, time diff 0.7396097183227539, total time dif 436.8856101036072)
step: 4730 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(0.79011875), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1041012778878212), 'actor_loss': np.float64(-0.9749060094356536), 'hyper_actor_loss': np.float64(0.009436630923300982), 'behavior_loss': np.float64(2.1327061891555785)}

Episode step 4740, time diff 0.9196102619171143, total time dif 437.62521982192993)
step: 4740 @ episode report: {'average_total_reward': np.float32(7.967778), 'reward_variance': np.float32(2.8798623), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08923860006034375), 'actor_loss': np.float64(-0.9363182604312896), 'hyper_actor_loss': np.float64(0.009161569830030202), 'behavior_loss': np.float64(2.2236571431159975)}

Episode step 4750, time diff 0.766909122467041, total time dif 438.54483008384705)
step: 4750 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(0.8585197), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09499431774020195), 'actor_loss': np.float64(-0.9619360864162445), 'hyper_actor_loss': np.float64(0.009036448784172535), 'behavior_loss': np.float64(2.4109049081802367)}

Episode step 4760, time diff 0.7695789337158203, total time dif 439.3117392063141)
step: 4760 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(2.0450568), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08231346830725669), 'actor_loss': np.float64(-0.9556121170520783), 'hyper_actor_loss': np.float64(0.008920655120164156), 'behavior_loss': np.float64(2.310381555557251)}

Episode step 4770, time diff 0.7677047252655029, total time dif 440.0813181400299)
step: 4770 @ episode report: {'average_total_reward': np.float32(7.0922227), 'reward_variance': np.float32(1.415335), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08662466369569302), 'actor_loss': np.float64(-0.9508047580718995), 'hyper_actor_loss': np.float64(0.008834791462868452), 'behavior_loss': np.float64(2.182893443107605)}

Episode step 4780, time diff 0.7659745216369629, total time dif 440.8490228652954)
step: 4780 @ episode report: {'average_total_reward': np.float32(6.3700004), 'reward_variance': np.float32(0.6630878), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09786761105060578), 'actor_loss': np.float64(-0.9871228039264679), 'hyper_actor_loss': np.float64(0.008933070860803128), 'behavior_loss': np.float64(2.294252407550812)}

Episode step 4790, time diff 0.8089020252227783, total time dif 441.6149973869324)
step: 4790 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(2.7683713), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08176215924322605), 'actor_loss': np.float64(-0.9748595476150512), 'hyper_actor_loss': np.float64(0.009020519908517599), 'behavior_loss': np.float64(2.1543057203292846)}

Episode step 4800, time diff 0.7522678375244141, total time dif 442.42389941215515)
step: 4800 @ episode report: {'average_total_reward': np.float32(7.504445), 'reward_variance': np.float32(4.1782274), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09240080267190934), 'actor_loss': np.float64(-0.9320030331611633), 'hyper_actor_loss': np.float64(0.009210899006575346), 'behavior_loss': np.float64(2.3082958102226256)}

Episode step 4810, time diff 0.736860990524292, total time dif 443.17616724967957)
step: 4810 @ episode report: {'average_total_reward': np.float32(6.4066668), 'reward_variance': np.float32(1.7545239), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08924246951937675), 'actor_loss': np.float64(-0.9782218337059021), 'hyper_actor_loss': np.float64(0.009481625352054834), 'behavior_loss': np.float64(2.2529278993606567)}

Episode step 4820, time diff 0.7566680908203125, total time dif 443.91302824020386)
step: 4820 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(3.4206417), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09340493381023407), 'actor_loss': np.float64(-0.9512404561042785), 'hyper_actor_loss': np.float64(0.009425495378673076), 'behavior_loss': np.float64(2.200134539604187)}

Episode step 4830, time diff 0.736781120300293, total time dif 444.66969633102417)
step: 4830 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(1.948272), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09216374009847642), 'actor_loss': np.float64(-0.9861165404319763), 'hyper_actor_loss': np.float64(0.009357935842126608), 'behavior_loss': np.float64(2.3046958208084107)}

Episode step 4840, time diff 0.7742557525634766, total time dif 445.40647745132446)
step: 4840 @ episode report: {'average_total_reward': np.float32(6.9944444), 'reward_variance': np.float32(1.7424997), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08782911747694015), 'actor_loss': np.float64(-0.9361362516880035), 'hyper_actor_loss': np.float64(0.009430419933050872), 'behavior_loss': np.float64(2.1908345222473145)}

Episode step 4850, time diff 0.773996114730835, total time dif 446.18073320388794)
step: 4850 @ episode report: {'average_total_reward': np.float32(7.182223), 'reward_variance': np.float32(1.6777089), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10135288909077644), 'actor_loss': np.float64(-0.9923248887062073), 'hyper_actor_loss': np.float64(0.009412035346031189), 'behavior_loss': np.float64(2.190815496444702)}

Episode step 4860, time diff 0.7461168766021729, total time dif 446.9547293186188)
step: 4860 @ episode report: {'average_total_reward': np.float32(6.4333334), 'reward_variance': np.float32(0.7812592), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09784755222499371), 'actor_loss': np.float64(-0.9531318008899688), 'hyper_actor_loss': np.float64(0.009069743007421494), 'behavior_loss': np.float64(2.215849959850311)}

Episode step 4870, time diff 0.7492344379425049, total time dif 447.70084619522095)
step: 4870 @ episode report: {'average_total_reward': np.float32(7.204445), 'reward_variance': np.float32(1.9618574), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08958167061209679), 'actor_loss': np.float64(-0.9307655394077301), 'hyper_actor_loss': np.float64(0.009292559698224068), 'behavior_loss': np.float64(2.2850651502609254)}

Episode step 4880, time diff 0.7275807857513428, total time dif 448.45008063316345)
step: 4880 @ episode report: {'average_total_reward': np.float32(7.38), 'reward_variance': np.float32(1.1123412), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09083211272954941), 'actor_loss': np.float64(-0.9847256004810333), 'hyper_actor_loss': np.float64(0.009342050086706877), 'behavior_loss': np.float64(2.2254660725593567)}

Episode step 4890, time diff 0.7197825908660889, total time dif 449.1776614189148)
step: 4890 @ episode report: {'average_total_reward': np.float32(6.8822227), 'reward_variance': np.float32(2.498499), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10247440859675408), 'actor_loss': np.float64(-0.9538584768772125), 'hyper_actor_loss': np.float64(0.009483123011887074), 'behavior_loss': np.float64(2.37701735496521)}

Episode step 4900, time diff 0.9855220317840576, total time dif 449.8974440097809)
step: 4900 @ episode report: {'average_total_reward': np.float32(7.0555563), 'reward_variance': np.float32(1.3771609), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08749728687107564), 'actor_loss': np.float64(-0.9693285584449768), 'hyper_actor_loss': np.float64(0.009532917104661465), 'behavior_loss': np.float64(2.1343676686286925)}

Episode step 4910, time diff 0.7586302757263184, total time dif 450.88296604156494)
step: 4910 @ episode report: {'average_total_reward': np.float32(7.216667), 'reward_variance': np.float32(4.106945), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10366158038377762), 'actor_loss': np.float64(-0.9644304513931274), 'hyper_actor_loss': np.float64(0.009578893892467022), 'behavior_loss': np.float64(2.2579703450202944)}

Episode step 4920, time diff 0.8003594875335693, total time dif 451.64159631729126)
step: 4920 @ episode report: {'average_total_reward': np.float32(6.482222), 'reward_variance': np.float32(3.7104015), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08971863649785519), 'actor_loss': np.float64(-0.9576635897159577), 'hyper_actor_loss': np.float64(0.009320820774883031), 'behavior_loss': np.float64(2.321039307117462)}

Episode step 4930, time diff 0.7742533683776855, total time dif 452.44195580482483)
step: 4930 @ episode report: {'average_total_reward': np.float32(6.6211114), 'reward_variance': np.float32(1.3820604), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09257758036255836), 'actor_loss': np.float64(-0.9338232755661011), 'hyper_actor_loss': np.float64(0.00890228347852826), 'behavior_loss': np.float64(2.0069863557815553)}

Episode step 4940, time diff 0.7872881889343262, total time dif 453.2162091732025)
step: 4940 @ episode report: {'average_total_reward': np.float32(7.767778), 'reward_variance': np.float32(0.8230729), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09041729792952538), 'actor_loss': np.float64(-0.9655650734901429), 'hyper_actor_loss': np.float64(0.008491757418960333), 'behavior_loss': np.float64(2.2436669945716856)}

Episode step 4950, time diff 0.7858593463897705, total time dif 454.00349736213684)
step: 4950 @ episode report: {'average_total_reward': np.float32(6.494445), 'reward_variance': np.float32(2.7847962), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08310501500964165), 'actor_loss': np.float64(-0.9438883900642395), 'hyper_actor_loss': np.float64(0.008353145606815816), 'behavior_loss': np.float64(2.1218374609947204)}

Episode step 4960, time diff 0.7707920074462891, total time dif 454.7893567085266)
step: 4960 @ episode report: {'average_total_reward': np.float32(8.031111), 'reward_variance': np.float32(0.9686861), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08198372125625611), 'actor_loss': np.float64(-0.944393390417099), 'hyper_actor_loss': np.float64(0.008192900568246841), 'behavior_loss': np.float64(2.2991034984588623)}

Episode step 4970, time diff 0.7775905132293701, total time dif 455.5601487159729)
step: 4970 @ episode report: {'average_total_reward': np.float32(7.3433332), 'reward_variance': np.float32(1.3870736), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09024928137660027), 'actor_loss': np.float64(-0.9474231243133545), 'hyper_actor_loss': np.float64(0.008096609916538), 'behavior_loss': np.float64(2.105288290977478)}

Episode step 4980, time diff 0.773432731628418, total time dif 456.33773922920227)
step: 4980 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(2.5323162), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09295142441987991), 'actor_loss': np.float64(-0.9795817613601685), 'hyper_actor_loss': np.float64(0.007892247708514332), 'behavior_loss': np.float64(2.203853416442871)}

Episode step 4990, time diff 0.7882976531982422, total time dif 457.1111719608307)
step: 4990 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(1.3028595), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09951476082205772), 'actor_loss': np.float64(-0.9709925472736358), 'hyper_actor_loss': np.float64(0.007833837484940886), 'behavior_loss': np.float64(2.2644250273704527)}

Episode step 5000, time diff 0.7492451667785645, total time dif 457.89946961402893)
step: 5000 @ episode report: {'average_total_reward': np.float32(7.9555564), 'reward_variance': np.float32(3.141704), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09837659709155559), 'actor_loss': np.float64(-0.9671863973140716), 'hyper_actor_loss': np.float64(0.00780714894644916), 'behavior_loss': np.float64(2.2533801317214968)}

Episode step 5010, time diff 0.7738862037658691, total time dif 458.6487147808075)
step: 5010 @ episode report: {'average_total_reward': np.float32(6.7066665), 'reward_variance': np.float32(1.4375359), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08880983628332614), 'actor_loss': np.float64(-0.9712605476379395), 'hyper_actor_loss': np.float64(0.008025831682607532), 'behavior_loss': np.float64(2.3337872982025147)}

Episode step 5020, time diff 0.775599479675293, total time dif 459.42260098457336)
step: 5020 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(1.3521534), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0972595602273941), 'actor_loss': np.float64(-1.0030785083770752), 'hyper_actor_loss': np.float64(0.007985912542790175), 'behavior_loss': np.float64(2.0493743896484373)}

Episode step 5030, time diff 0.7279579639434814, total time dif 460.19820046424866)
step: 5030 @ episode report: {'average_total_reward': np.float32(6.406667), 'reward_variance': np.float32(3.6488438), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0867395255714655), 'actor_loss': np.float64(-0.9417831659317016), 'hyper_actor_loss': np.float64(0.00802228688262403), 'behavior_loss': np.float64(2.29699649810791)}

Episode step 5040, time diff 0.7539165019989014, total time dif 460.92615842819214)
step: 5040 @ episode report: {'average_total_reward': np.float32(7.767778), 'reward_variance': np.float32(2.085443), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09156254529953003), 'actor_loss': np.float64(-0.9579882562160492), 'hyper_actor_loss': np.float64(0.00817216276191175), 'behavior_loss': np.float64(2.2432274341583254)}

Episode step 5050, time diff 0.945955753326416, total time dif 461.68007493019104)
step: 5050 @ episode report: {'average_total_reward': np.float32(8.004445), 'reward_variance': np.float32(1.7259312), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08726256005465985), 'actor_loss': np.float64(-0.9570637881755829), 'hyper_actor_loss': np.float64(0.00809858194552362), 'behavior_loss': np.float64(2.241141378879547)}

Episode step 5060, time diff 0.7662336826324463, total time dif 462.62603068351746)
step: 5060 @ episode report: {'average_total_reward': np.float32(6.967778), 'reward_variance': np.float32(1.1870726), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08878805935382843), 'actor_loss': np.float64(-0.94573974609375), 'hyper_actor_loss': np.float64(0.007949027698487043), 'behavior_loss': np.float64(2.2161449790000916)}

Episode step 5070, time diff 0.7678134441375732, total time dif 463.3922643661499)
step: 5070 @ episode report: {'average_total_reward': np.float32(7.231112), 'reward_variance': np.float32(4.212933), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09165254049003124), 'actor_loss': np.float64(-0.975087147951126), 'hyper_actor_loss': np.float64(0.0078105215914547445), 'behavior_loss': np.float64(2.2268995523452757)}

Episode step 5080, time diff 0.7750368118286133, total time dif 464.1600778102875)
step: 5080 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(2.8500247), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08633729070425034), 'actor_loss': np.float64(-0.9407809257507325), 'hyper_actor_loss': np.float64(0.007682247646152973), 'behavior_loss': np.float64(2.2846311688423158)}

Episode step 5090, time diff 0.7550547122955322, total time dif 464.9351146221161)
step: 5090 @ episode report: {'average_total_reward': np.float32(7.167778), 'reward_variance': np.float32(1.0251718), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0815892443060875), 'actor_loss': np.float64(-0.9591027796268463), 'hyper_actor_loss': np.float64(0.007425494538620114), 'behavior_loss': np.float64(2.2234209775924683)}

Episode step 5100, time diff 0.8119609355926514, total time dif 465.6901693344116)
step: 5100 @ episode report: {'average_total_reward': np.float32(7.5922227), 'reward_variance': np.float32(3.243681), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0824727799743414), 'actor_loss': np.float64(-0.893687903881073), 'hyper_actor_loss': np.float64(0.007373525202274323), 'behavior_loss': np.float64(2.4584221839904785)}

Episode step 5110, time diff 0.769768476486206, total time dif 466.5021302700043)
step: 5110 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(0.8326683), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08257106728851796), 'actor_loss': np.float64(-0.9582851886749267), 'hyper_actor_loss': np.float64(0.007150418683886528), 'behavior_loss': np.float64(2.22859468460083)}

Episode step 5120, time diff 0.7502303123474121, total time dif 467.2718987464905)
step: 5120 @ episode report: {'average_total_reward': np.float32(7.3433332), 'reward_variance': np.float32(3.7287517), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08745074570178986), 'actor_loss': np.float64(-0.9475855648517608), 'hyper_actor_loss': np.float64(0.006845340179279446), 'behavior_loss': np.float64(2.1371578931808473)}

Episode step 5130, time diff 0.7556014060974121, total time dif 468.0221290588379)
step: 5130 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(2.6285732), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09693792909383774), 'actor_loss': np.float64(-0.9787240743637085), 'hyper_actor_loss': np.float64(0.006844258867204189), 'behavior_loss': np.float64(2.1597466826438905)}

Episode step 5140, time diff 0.7811684608459473, total time dif 468.7777304649353)
step: 5140 @ episode report: {'average_total_reward': np.float32(7.731111), 'reward_variance': np.float32(1.5077732), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09801902398467063), 'actor_loss': np.float64(-0.9787960946559906), 'hyper_actor_loss': np.float64(0.006690349150449038), 'behavior_loss': np.float64(2.1262477159500124)}

Episode step 5150, time diff 0.7462866306304932, total time dif 469.55889892578125)
step: 5150 @ episode report: {'average_total_reward': np.float32(6.694444), 'reward_variance': np.float32(1.2979815), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1014115571975708), 'actor_loss': np.float64(-0.9703691303730011), 'hyper_actor_loss': np.float64(0.006645349226891995), 'behavior_loss': np.float64(2.190416693687439)}

Episode step 5160, time diff 0.7642579078674316, total time dif 470.30518555641174)
step: 5160 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(2.454752), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09116137847304344), 'actor_loss': np.float64(-0.9860096931457519), 'hyper_actor_loss': np.float64(0.0067615334410220385), 'behavior_loss': np.float64(2.181508457660675)}

Episode step 5170, time diff 0.7664742469787598, total time dif 471.0694434642792)
step: 5170 @ episode report: {'average_total_reward': np.float32(7.406667), 'reward_variance': np.float32(2.6473138), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0829742819070816), 'actor_loss': np.float64(-0.9289034843444824), 'hyper_actor_loss': np.float64(0.006969090038910508), 'behavior_loss': np.float64(2.225834381580353)}

Episode step 5180, time diff 0.7706677913665771, total time dif 471.83591771125793)
step: 5180 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(2.4285884), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09303383901715279), 'actor_loss': np.float64(-0.9755825340747833), 'hyper_actor_loss': np.float64(0.0069811788853257895), 'behavior_loss': np.float64(2.1834095239639284)}

Episode step 5190, time diff 0.7487645149230957, total time dif 472.6065855026245)
step: 5190 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(2.1096158), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07631616890430451), 'actor_loss': np.float64(-0.9508253812789917), 'hyper_actor_loss': np.float64(0.006906062783673406), 'behavior_loss': np.float64(2.08651340007782)}

Episode step 5200, time diff 0.746131181716919, total time dif 473.3553500175476)
step: 5200 @ episode report: {'average_total_reward': np.float32(7.267778), 'reward_variance': np.float32(2.7848263), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0776911959052086), 'actor_loss': np.float64(-0.9318839192390442), 'hyper_actor_loss': np.float64(0.007037172187119722), 'behavior_loss': np.float64(2.3141022682189942)}

Episode step 5210, time diff 0.9085738658905029, total time dif 474.1014811992645)
step: 5210 @ episode report: {'average_total_reward': np.float32(7.567778), 'reward_variance': np.float32(1.9177644), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.2888885), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09908500984311104), 'actor_loss': np.float64(-0.9632302343845367), 'hyper_actor_loss': np.float64(0.007023813435807824), 'behavior_loss': np.float64(2.290378451347351)}

Episode step 5220, time diff 0.7554402351379395, total time dif 475.01005506515503)
step: 5220 @ episode report: {'average_total_reward': np.float32(5.908889), 'reward_variance': np.float32(0.7197977), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08094923943281174), 'actor_loss': np.float64(-0.9878319084644318), 'hyper_actor_loss': np.float64(0.0072452562861144544), 'behavior_loss': np.float64(2.152928555011749)}

Episode step 5230, time diff 0.7694971561431885, total time dif 475.76549530029297)
step: 5230 @ episode report: {'average_total_reward': np.float32(5.957778), 'reward_variance': np.float32(2.7231307), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0912245538085699), 'actor_loss': np.float64(-0.9496020972728729), 'hyper_actor_loss': np.float64(0.0075304304715245966), 'behavior_loss': np.float64(2.15970139503479)}

Episode step 5240, time diff 0.7723846435546875, total time dif 476.53499245643616)
step: 5240 @ episode report: {'average_total_reward': np.float32(6.5944443), 'reward_variance': np.float32(1.5653397), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09772785417735577), 'actor_loss': np.float64(-1.0041091799736024), 'hyper_actor_loss': np.float64(0.007916902005672456), 'behavior_loss': np.float64(2.1446119785308837)}

Episode step 5250, time diff 0.7984139919281006, total time dif 477.30737709999084)
step: 5250 @ episode report: {'average_total_reward': np.float32(6.131111), 'reward_variance': np.float32(2.836983), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1089427188038826), 'actor_loss': np.float64(-0.9750523924827575), 'hyper_actor_loss': np.float64(0.007978110667318105), 'behavior_loss': np.float64(2.0648785829544067)}

Episode step 5260, time diff 0.7738790512084961, total time dif 478.10579109191895)
step: 5260 @ episode report: {'average_total_reward': np.float32(5.7966666), 'reward_variance': np.float32(1.778273), 'max_total_reward': np.float32(8.655557), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08619283325970173), 'actor_loss': np.float64(-0.9787283778190613), 'hyper_actor_loss': np.float64(0.007410433329641819), 'behavior_loss': np.float64(2.081016421318054)}

Episode step 5270, time diff 0.76607346534729, total time dif 478.87967014312744)
step: 5270 @ episode report: {'average_total_reward': np.float32(5.608889), 'reward_variance': np.float32(2.6329832), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09212172329425812), 'actor_loss': np.float64(-0.9455339312553406), 'hyper_actor_loss': np.float64(0.006993946991860867), 'behavior_loss': np.float64(2.106428587436676)}

Episode step 5280, time diff 0.7581491470336914, total time dif 479.64574360847473)
step: 5280 @ episode report: {'average_total_reward': np.float32(7.3922224), 'reward_variance': np.float32(1.1388901), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09458834230899811), 'actor_loss': np.float64(-0.9826349318027496), 'hyper_actor_loss': np.float64(0.006739474786445499), 'behavior_loss': np.float64(2.298987329006195)}

Episode step 5290, time diff 0.7844810485839844, total time dif 480.4038927555084)
step: 5290 @ episode report: {'average_total_reward': np.float32(7.4044447), 'reward_variance': np.float32(2.5571666), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09349721893668175), 'actor_loss': np.float64(-0.9538196146488189), 'hyper_actor_loss': np.float64(0.006631285045295953), 'behavior_loss': np.float64(2.2710883021354675)}

Episode step 5300, time diff 0.7759585380554199, total time dif 481.1883738040924)
step: 5300 @ episode report: {'average_total_reward': np.float32(7.5800004), 'reward_variance': np.float32(1.5411808), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09898583739995956), 'actor_loss': np.float64(-0.9921302735805512), 'hyper_actor_loss': np.float64(0.006731851911172271), 'behavior_loss': np.float64(2.1919861316680906)}

Episode step 5310, time diff 0.8075957298278809, total time dif 481.9643323421478)
step: 5310 @ episode report: {'average_total_reward': np.float32(8.041112), 'reward_variance': np.float32(2.483557), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08550203703343869), 'actor_loss': np.float64(-0.965691864490509), 'hyper_actor_loss': np.float64(0.006787934899330139), 'behavior_loss': np.float64(2.3436213731765747)}

Episode step 5320, time diff 0.763068437576294, total time dif 482.7719280719757)
step: 5320 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(3.6858718), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08859035298228264), 'actor_loss': np.float64(-0.9704318284988404), 'hyper_actor_loss': np.float64(0.006977139273658395), 'behavior_loss': np.float64(2.066022515296936)}

Episode step 5330, time diff 0.7817387580871582, total time dif 483.534996509552)
step: 5330 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(2.562736), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09085589274764061), 'actor_loss': np.float64(-0.9566067516803741), 'hyper_actor_loss': np.float64(0.0069053396582603455), 'behavior_loss': np.float64(2.169937551021576)}

Episode step 5340, time diff 0.7916631698608398, total time dif 484.31673526763916)
step: 5340 @ episode report: {'average_total_reward': np.float32(5.8333335), 'reward_variance': np.float32(1.3656303), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09016365557909012), 'actor_loss': np.float64(-0.9783848226070404), 'hyper_actor_loss': np.float64(0.00694528422318399), 'behavior_loss': np.float64(2.218833565711975)}

Episode step 5350, time diff 0.7525787353515625, total time dif 485.1083984375)
step: 5350 @ episode report: {'average_total_reward': np.float32(6.082223), 'reward_variance': np.float32(0.7328941), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08932600729167461), 'actor_loss': np.float64(-0.9552706420421601), 'hyper_actor_loss': np.float64(0.0068514843005687), 'behavior_loss': np.float64(2.2511242032051086)}

Episode step 5360, time diff 0.7799241542816162, total time dif 485.86097717285156)
step: 5360 @ episode report: {'average_total_reward': np.float32(6.3966665), 'reward_variance': np.float32(2.3845692), 'max_total_reward': np.float32(9.533334), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09678107500076294), 'actor_loss': np.float64(-0.976602965593338), 'hyper_actor_loss': np.float64(0.006709908787161112), 'behavior_loss': np.float64(2.1912918210029604)}

Episode step 5370, time diff 0.7440264225006104, total time dif 486.6409013271332)
step: 5370 @ episode report: {'average_total_reward': np.float32(6.406667), 'reward_variance': np.float32(1.4936842), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1030995074659586), 'actor_loss': np.float64(-0.984239250421524), 'hyper_actor_loss': np.float64(0.006505842320621014), 'behavior_loss': np.float64(2.3934963583946227)}

Episode step 5380, time diff 0.9539148807525635, total time dif 487.3849277496338)
step: 5380 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(2.0058627), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08648150712251663), 'actor_loss': np.float64(-0.9483369052410126), 'hyper_actor_loss': np.float64(0.0062213285826146604), 'behavior_loss': np.float64(2.212599277496338)}

Episode step 5390, time diff 0.7442152500152588, total time dif 488.33884263038635)
step: 5390 @ episode report: {'average_total_reward': np.float32(7.4433336), 'reward_variance': np.float32(2.295147), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09559364169836045), 'actor_loss': np.float64(-0.9715105831623078), 'hyper_actor_loss': np.float64(0.006254346761852503), 'behavior_loss': np.float64(2.3804135799407957)}

Episode step 5400, time diff 0.757753849029541, total time dif 489.0830578804016)
step: 5400 @ episode report: {'average_total_reward': np.float32(6.3333335), 'reward_variance': np.float32(1.6881977), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08765761330723762), 'actor_loss': np.float64(-0.9671577155590058), 'hyper_actor_loss': np.float64(0.006090768286958337), 'behavior_loss': np.float64(2.292309749126434)}

Episode step 5410, time diff 0.770488977432251, total time dif 489.84081172943115)
step: 5410 @ episode report: {'average_total_reward': np.float32(6.731111), 'reward_variance': np.float32(4.365699), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08973409980535507), 'actor_loss': np.float64(-0.9614705860614776), 'hyper_actor_loss': np.float64(0.006081239040941), 'behavior_loss': np.float64(2.271225893497467)}

Episode step 5420, time diff 0.7930223941802979, total time dif 490.6113007068634)
step: 5420 @ episode report: {'average_total_reward': np.float32(6.4066668), 'reward_variance': np.float32(2.9256344), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09383098594844341), 'actor_loss': np.float64(-0.9744564890861511), 'hyper_actor_loss': np.float64(0.006238578213378787), 'behavior_loss': np.float64(2.2253623604774475)}

Episode step 5430, time diff 0.7496614456176758, total time dif 491.4043231010437)
step: 5430 @ episode report: {'average_total_reward': np.float32(6.2844443), 'reward_variance': np.float32(2.251758), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07880300134420395), 'actor_loss': np.float64(-0.9296978771686554), 'hyper_actor_loss': np.float64(0.006372451549395919), 'behavior_loss': np.float64(2.096859538555145)}

Episode step 5440, time diff 0.7658534049987793, total time dif 492.1539845466614)
step: 5440 @ episode report: {'average_total_reward': np.float32(6.967778), 'reward_variance': np.float32(3.3197892), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08983958289027213), 'actor_loss': np.float64(-0.9924758493900299), 'hyper_actor_loss': np.float64(0.006078005535528064), 'behavior_loss': np.float64(2.189024496078491)}

Episode step 5450, time diff 0.7492218017578125, total time dif 492.91983795166016)
step: 5450 @ episode report: {'average_total_reward': np.float32(6.418889), 'reward_variance': np.float32(1.2412611), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08302700519561768), 'actor_loss': np.float64(-0.9106320202350616), 'hyper_actor_loss': np.float64(0.005960480310022831), 'behavior_loss': np.float64(2.2834051847457886)}

Episode step 5460, time diff 0.7540538311004639, total time dif 493.66905975341797)
step: 5460 @ episode report: {'average_total_reward': np.float32(6.257778), 'reward_variance': np.float32(1.4390074), 'max_total_reward': np.float32(8.411111), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09125111252069473), 'actor_loss': np.float64(-0.9583502948284149), 'hyper_actor_loss': np.float64(0.005499674286693334), 'behavior_loss': np.float64(2.2806552410125733)}

Episode step 5470, time diff 0.7836103439331055, total time dif 494.42311358451843)
step: 5470 @ episode report: {'average_total_reward': np.float32(5.9700003), 'reward_variance': np.float32(1.5859028), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08977479562163353), 'actor_loss': np.float64(-0.9516400992870331), 'hyper_actor_loss': np.float64(0.005329895112663507), 'behavior_loss': np.float64(2.222796368598938)}

Episode step 5480, time diff 0.7561988830566406, total time dif 495.20672392845154)
step: 5480 @ episode report: {'average_total_reward': np.float32(6.7433333), 'reward_variance': np.float32(2.2384562), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07747179232537746), 'actor_loss': np.float64(-0.9502037167549133), 'hyper_actor_loss': np.float64(0.005153182614594698), 'behavior_loss': np.float64(2.269876742362976)}

Episode step 5490, time diff 0.7687022686004639, total time dif 495.9629228115082)
step: 5490 @ episode report: {'average_total_reward': np.float32(6.9822226), 'reward_variance': np.float32(1.6254618), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08525740653276444), 'actor_loss': np.float64(-0.9422394156455993), 'hyper_actor_loss': np.float64(0.005017212172970176), 'behavior_loss': np.float64(2.2372521996498107)}

Episode step 5500, time diff 0.816033124923706, total time dif 496.73162508010864)
step: 5500 @ episode report: {'average_total_reward': np.float32(5.496667), 'reward_variance': np.float32(2.0051124), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0852238267660141), 'actor_loss': np.float64(-0.955973482131958), 'hyper_actor_loss': np.float64(0.005095080705359578), 'behavior_loss': np.float64(2.099990200996399)}

Episode step 5510, time diff 0.7865099906921387, total time dif 497.54765820503235)
step: 5510 @ episode report: {'average_total_reward': np.float32(6.782222), 'reward_variance': np.float32(2.6996593), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09188246689736843), 'actor_loss': np.float64(-0.9772739470005035), 'hyper_actor_loss': np.float64(0.00526029928587377), 'behavior_loss': np.float64(2.2350746750831605)}

Episode step 5520, time diff 0.77713942527771, total time dif 498.3341681957245)
step: 5520 @ episode report: {'average_total_reward': np.float32(6.457778), 'reward_variance': np.float32(2.1930568), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08773175850510598), 'actor_loss': np.float64(-0.9524102151393891), 'hyper_actor_loss': np.float64(0.005450737057253718), 'behavior_loss': np.float64(2.3260183811187742)}

Episode step 5530, time diff 0.7826845645904541, total time dif 499.1113076210022)
step: 5530 @ episode report: {'average_total_reward': np.float32(6.594445), 'reward_variance': np.float32(1.4067225), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08642537221312523), 'actor_loss': np.float64(-0.9750611543655395), 'hyper_actor_loss': np.float64(0.005565695697441697), 'behavior_loss': np.float64(2.043013834953308)}

Episode step 5540, time diff 0.9231359958648682, total time dif 499.89399218559265)
step: 5540 @ episode report: {'average_total_reward': np.float32(6.382222), 'reward_variance': np.float32(0.6943261), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08627176657319069), 'actor_loss': np.float64(-0.9693216860294342), 'hyper_actor_loss': np.float64(0.0057006315793842076), 'behavior_loss': np.float64(2.1380730271339417)}

Episode step 5550, time diff 0.7732598781585693, total time dif 500.8171281814575)
step: 5550 @ episode report: {'average_total_reward': np.float32(6.145556), 'reward_variance': np.float32(1.3364064), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08885550945997238), 'actor_loss': np.float64(-0.9454927146434784), 'hyper_actor_loss': np.float64(0.005546657415106892), 'behavior_loss': np.float64(2.2406410336494447)}

Episode step 5560, time diff 0.760833740234375, total time dif 501.5903880596161)
step: 5560 @ episode report: {'average_total_reward': np.float32(5.821111), 'reward_variance': np.float32(1.3174431), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0791895866394043), 'actor_loss': np.float64(-0.9289752125740052), 'hyper_actor_loss': np.float64(0.005329330824315548), 'behavior_loss': np.float64(2.2819417834281923)}

Episode step 5570, time diff 0.769686222076416, total time dif 502.35122179985046)
step: 5570 @ episode report: {'average_total_reward': np.float32(6.582223), 'reward_variance': np.float32(1.8689436), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08824007138609886), 'actor_loss': np.float64(-0.9444720625877381), 'hyper_actor_loss': np.float64(0.0051097681280225515), 'behavior_loss': np.float64(2.2262492775917053)}

Episode step 5580, time diff 0.7778499126434326, total time dif 503.1209080219269)
step: 5580 @ episode report: {'average_total_reward': np.float32(6.645556), 'reward_variance': np.float32(1.6594679), 'max_total_reward': np.float32(8.411111), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0820144072175026), 'actor_loss': np.float64(-0.9698609232902526), 'hyper_actor_loss': np.float64(0.004953892761841416), 'behavior_loss': np.float64(2.0327292680740356)}

Episode step 5590, time diff 0.7960712909698486, total time dif 503.8987579345703)
step: 5590 @ episode report: {'average_total_reward': np.float32(6.318889), 'reward_variance': np.float32(1.6320512), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08617504462599754), 'actor_loss': np.float64(-0.9444082200527191), 'hyper_actor_loss': np.float64(0.005026012845337391), 'behavior_loss': np.float64(2.021641397476196)}

Episode step 5600, time diff 0.7691559791564941, total time dif 504.69482922554016)
step: 5600 @ episode report: {'average_total_reward': np.float32(5.708889), 'reward_variance': np.float32(0.95071095), 'max_total_reward': np.float32(7.4111114), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0829305112361908), 'actor_loss': np.float64(-0.9521668732166291), 'hyper_actor_loss': np.float64(0.005140419257804752), 'behavior_loss': np.float64(2.188681995868683)}

Episode step 5610, time diff 0.7919824123382568, total time dif 505.46398520469666)
step: 5610 @ episode report: {'average_total_reward': np.float32(5.296667), 'reward_variance': np.float32(0.6796557), 'max_total_reward': np.float32(6.6555567), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08365161791443824), 'actor_loss': np.float64(-0.9309264838695526), 'hyper_actor_loss': np.float64(0.005238878075033426), 'behavior_loss': np.float64(2.176192510128021)}

Episode step 5620, time diff 0.7823221683502197, total time dif 506.2559676170349)
step: 5620 @ episode report: {'average_total_reward': np.float32(5.7188888), 'reward_variance': np.float32(0.32790253), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0917057916522026), 'actor_loss': np.float64(-0.9728740632534028), 'hyper_actor_loss': np.float64(0.0051691918168216945), 'behavior_loss': np.float64(2.110115575790405)}

Episode step 5630, time diff 0.7776834964752197, total time dif 507.03828978538513)
step: 5630 @ episode report: {'average_total_reward': np.float32(6.682223), 'reward_variance': np.float32(1.442425), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08239961043000221), 'actor_loss': np.float64(-0.9565509974956512), 'hyper_actor_loss': np.float64(0.005003121308982372), 'behavior_loss': np.float64(1.999183750152588)}

Episode step 5640, time diff 0.7736036777496338, total time dif 507.81597328186035)
step: 5640 @ episode report: {'average_total_reward': np.float32(6.4700003), 'reward_variance': np.float32(0.8680507), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08662856370210648), 'actor_loss': np.float64(-0.9649881541728973), 'hyper_actor_loss': np.float64(0.004820520058274269), 'behavior_loss': np.float64(1.9268993973731994)}

Episode step 5650, time diff 0.7676699161529541, total time dif 508.58957695961)
step: 5650 @ episode report: {'average_total_reward': np.float32(6.231111), 'reward_variance': np.float32(2.7495756), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08922635838389396), 'actor_loss': np.float64(-0.9380671918392182), 'hyper_actor_loss': np.float64(0.0044971406925469635), 'behavior_loss': np.float64(2.1987165212631226)}

Episode step 5660, time diff 0.7649273872375488, total time dif 509.35724687576294)
step: 5660 @ episode report: {'average_total_reward': np.float32(6.533334), 'reward_variance': np.float32(1.6498772), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07637921385467053), 'actor_loss': np.float64(-0.9526778578758239), 'hyper_actor_loss': np.float64(0.0043823675252497194), 'behavior_loss': np.float64(2.1295435667037963)}

Episode step 5670, time diff 0.7588362693786621, total time dif 510.1221742630005)
step: 5670 @ episode report: {'average_total_reward': np.float32(7.094445), 'reward_variance': np.float32(3.7034125), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08508246429264546), 'actor_loss': np.float64(-0.9475288331508637), 'hyper_actor_loss': np.float64(0.0041986019117757675), 'behavior_loss': np.float64(2.0183654546737673)}

Episode step 5680, time diff 0.7778799533843994, total time dif 510.88101053237915)
step: 5680 @ episode report: {'average_total_reward': np.float32(7.204445), 'reward_variance': np.float32(3.002771), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09233426600694657), 'actor_loss': np.float64(-0.9569909751415253), 'hyper_actor_loss': np.float64(0.0040361058665439485), 'behavior_loss': np.float64(2.126510727405548)}

Episode step 5690, time diff 0.7561309337615967, total time dif 511.65889048576355)
step: 5690 @ episode report: {'average_total_reward': np.float32(6.8822227), 'reward_variance': np.float32(2.107462), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07390958070755005), 'actor_loss': np.float64(-0.925040864944458), 'hyper_actor_loss': np.float64(0.0040093837305903435), 'behavior_loss': np.float64(2.1175561428070067)}

Episode step 5700, time diff 0.9265789985656738, total time dif 512.4150214195251)
step: 5700 @ episode report: {'average_total_reward': np.float32(6.533334), 'reward_variance': np.float32(1.3341727), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09151088073849678), 'actor_loss': np.float64(-0.9689819574356079), 'hyper_actor_loss': np.float64(0.003943489282391965), 'behavior_loss': np.float64(1.9928755283355712)}

Episode step 5710, time diff 0.7955737113952637, total time dif 513.3416004180908)
step: 5710 @ episode report: {'average_total_reward': np.float32(6.243334), 'reward_variance': np.float32(1.8909496), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0807251401245594), 'actor_loss': np.float64(-0.9581614434719086), 'hyper_actor_loss': np.float64(0.003984529455192387), 'behavior_loss': np.float64(2.0371548414230345)}

Episode step 5720, time diff 0.7756295204162598, total time dif 514.1371741294861)
step: 5720 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(1.2923654), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08312689550220967), 'actor_loss': np.float64(-0.9320751667022705), 'hyper_actor_loss': np.float64(0.004032028908841312), 'behavior_loss': np.float64(2.030181336402893)}

Episode step 5730, time diff 0.7646081447601318, total time dif 514.9128036499023)
step: 5730 @ episode report: {'average_total_reward': np.float32(5.9844446), 'reward_variance': np.float32(1.227141), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08340068981051445), 'actor_loss': np.float64(-0.9620253503322601), 'hyper_actor_loss': np.float64(0.004185187933035195), 'behavior_loss': np.float64(2.0001346349716185)}

Episode step 5740, time diff 0.7924952507019043, total time dif 515.6774117946625)
step: 5740 @ episode report: {'average_total_reward': np.float32(6.7333336), 'reward_variance': np.float32(1.6727165), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0819462589919567), 'actor_loss': np.float64(-0.9527538120746613), 'hyper_actor_loss': np.float64(0.004211443266831339), 'behavior_loss': np.float64(2.008778476715088)}

Episode step 5750, time diff 0.7891976833343506, total time dif 516.4699070453644)
step: 5750 @ episode report: {'average_total_reward': np.float32(6.406667), 'reward_variance': np.float32(1.5241044), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07966665849089623), 'actor_loss': np.float64(-0.9340883374214173), 'hyper_actor_loss': np.float64(0.004563892632722855), 'behavior_loss': np.float64(2.1550004363059996)}

Episode step 5760, time diff 0.7631638050079346, total time dif 517.2591047286987)
step: 5760 @ episode report: {'average_total_reward': np.float32(6.5311117), 'reward_variance': np.float32(2.0625875), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08305701985955238), 'actor_loss': np.float64(-0.9688861489295959), 'hyper_actor_loss': np.float64(0.0044960238505154845), 'behavior_loss': np.float64(2.053130030632019)}

Episode step 5770, time diff 0.7991771697998047, total time dif 518.0222685337067)
step: 5770 @ episode report: {'average_total_reward': np.float32(6.906667), 'reward_variance': np.float32(1.2194617), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08383656069636344), 'actor_loss': np.float64(-0.9570197463035583), 'hyper_actor_loss': np.float64(0.004569369228556752), 'behavior_loss': np.float64(2.1128663659095763)}

Episode step 5780, time diff 0.7952470779418945, total time dif 518.8214457035065)
step: 5780 @ episode report: {'average_total_reward': np.float32(6.033334), 'reward_variance': np.float32(1.9761978), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08781050741672516), 'actor_loss': np.float64(-0.9612394630908966), 'hyper_actor_loss': np.float64(0.004828242352232337), 'behavior_loss': np.float64(2.0605407834053038)}

Episode step 5790, time diff 0.7644095420837402, total time dif 519.6166927814484)
step: 5790 @ episode report: {'average_total_reward': np.float32(5.7822227), 'reward_variance': np.float32(0.66923964), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07978450283408164), 'actor_loss': np.float64(-0.9440804362297058), 'hyper_actor_loss': np.float64(0.004947475623339415), 'behavior_loss': np.float64(2.041679322719574)}

Episode step 5800, time diff 0.7554278373718262, total time dif 520.3811023235321)
step: 5800 @ episode report: {'average_total_reward': np.float32(6.257778), 'reward_variance': np.float32(1.7168596), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0925930853933096), 'actor_loss': np.float64(-0.9526438653469086), 'hyper_actor_loss': np.float64(0.004722251230850816), 'behavior_loss': np.float64(2.064115595817566)}

Episode step 5810, time diff 0.7839012145996094, total time dif 521.1365301609039)
step: 5810 @ episode report: {'average_total_reward': np.float32(6.518889), 'reward_variance': np.float32(2.9417307), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08146094158291817), 'actor_loss': np.float64(-0.9391099274158478), 'hyper_actor_loss': np.float64(0.004493002220988274), 'behavior_loss': np.float64(2.063182008266449)}

Episode step 5820, time diff 0.7736430168151855, total time dif 521.9204313755035)
step: 5820 @ episode report: {'average_total_reward': np.float32(6.1700006), 'reward_variance': np.float32(1.3497794), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09562838971614837), 'actor_loss': np.float64(-0.9591685712337494), 'hyper_actor_loss': np.float64(0.004369884356856346), 'behavior_loss': np.float64(2.1208778023719788)}

Episode step 5830, time diff 0.7834503650665283, total time dif 522.6940743923187)
step: 5830 @ episode report: {'average_total_reward': np.float32(5.7333336), 'reward_variance': np.float32(2.2498028), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07882576994597912), 'actor_loss': np.float64(-0.9158123016357422), 'hyper_actor_loss': np.float64(0.004160639457404613), 'behavior_loss': np.float64(2.0931683778762817)}

Episode step 5840, time diff 0.7456133365631104, total time dif 523.4775247573853)
step: 5840 @ episode report: {'average_total_reward': np.float32(5.6333337), 'reward_variance': np.float32(2.402247), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.0444448), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08487588614225387), 'actor_loss': np.float64(-0.9565603852272033), 'hyper_actor_loss': np.float64(0.004174009920097888), 'behavior_loss': np.float64(1.9614891767501832)}

Episode step 5850, time diff 0.922466516494751, total time dif 524.2231380939484)
step: 5850 @ episode report: {'average_total_reward': np.float32(6.0211115), 'reward_variance': np.float32(4.341937), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07333366237580777), 'actor_loss': np.float64(-0.9492475628852844), 'hyper_actor_loss': np.float64(0.004154999321326613), 'behavior_loss': np.float64(1.9777090191841125)}

Episode step 5860, time diff 0.7694251537322998, total time dif 525.1456046104431)
step: 5860 @ episode report: {'average_total_reward': np.float32(6.131111), 'reward_variance': np.float32(2.3880947), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08715599924325942), 'actor_loss': np.float64(-0.9369706690311432), 'hyper_actor_loss': np.float64(0.004286689404398203), 'behavior_loss': np.float64(1.9789098143577575)}

Episode step 5870, time diff 0.7964017391204834, total time dif 525.9150297641754)
step: 5870 @ episode report: {'average_total_reward': np.float32(6.4822226), 'reward_variance': np.float32(1.3307952), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08857291787862778), 'actor_loss': np.float64(-0.977774703502655), 'hyper_actor_loss': np.float64(0.004545774636790156), 'behavior_loss': np.float64(2.0106889724731447)}

Episode step 5880, time diff 0.7405037879943848, total time dif 526.7114315032959)
step: 5880 @ episode report: {'average_total_reward': np.float32(6.4455557), 'reward_variance': np.float32(2.0229747), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08582240082323551), 'actor_loss': np.float64(-0.9214090228080749), 'hyper_actor_loss': np.float64(0.0046723420731723305), 'behavior_loss': np.float64(1.9886287331581116)}

Episode step 5890, time diff 0.8012654781341553, total time dif 527.4519352912903)
step: 5890 @ episode report: {'average_total_reward': np.float32(5.957778), 'reward_variance': np.float32(1.057773), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09020584300160409), 'actor_loss': np.float64(-0.9728929281234742), 'hyper_actor_loss': np.float64(0.004836708540096879), 'behavior_loss': np.float64(2.0966653704643248)}

Episode step 5900, time diff 0.7597761154174805, total time dif 528.2532007694244)
step: 5900 @ episode report: {'average_total_reward': np.float32(6.8188887), 'reward_variance': np.float32(1.005236), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08246035948395729), 'actor_loss': np.float64(-0.9417062640190125), 'hyper_actor_loss': np.float64(0.00484966984950006), 'behavior_loss': np.float64(1.9392626881599426)}

Episode step 5910, time diff 0.7890028953552246, total time dif 529.0129768848419)
step: 5910 @ episode report: {'average_total_reward': np.float32(5.347778), 'reward_variance': np.float32(1.9658283), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07859190814197063), 'actor_loss': np.float64(-0.9564604341983796), 'hyper_actor_loss': np.float64(0.004758657934144139), 'behavior_loss': np.float64(1.8612584352493287)}

Episode step 5920, time diff 0.7942473888397217, total time dif 529.8019797801971)
step: 5920 @ episode report: {'average_total_reward': np.float32(5.0600004), 'reward_variance': np.float32(0.7435358), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07804952375590801), 'actor_loss': np.float64(-0.9492721855640411), 'hyper_actor_loss': np.float64(0.004688967578113079), 'behavior_loss': np.float64(2.0157754182815553)}

Episode step 5930, time diff 0.7966649532318115, total time dif 530.5962271690369)
step: 5930 @ episode report: {'average_total_reward': np.float32(6.482222), 'reward_variance': np.float32(2.2694862), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07824990637600422), 'actor_loss': np.float64(-0.9418822228908539), 'hyper_actor_loss': np.float64(0.00450286683626473), 'behavior_loss': np.float64(1.9483339071273804)}

Episode step 5940, time diff 0.7746922969818115, total time dif 531.3928921222687)
step: 5940 @ episode report: {'average_total_reward': np.float32(6.7188897), 'reward_variance': np.float32(2.417705), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07979284040629864), 'actor_loss': np.float64(-0.9169614970684051), 'hyper_actor_loss': np.float64(0.004314958839677275), 'behavior_loss': np.float64(1.8415107727050781)}

Episode step 5950, time diff 0.7485201358795166, total time dif 532.1675844192505)
step: 5950 @ episode report: {'average_total_reward': np.float32(5.508889), 'reward_variance': np.float32(0.9914273), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08504058122634887), 'actor_loss': np.float64(-0.9603163301944733), 'hyper_actor_loss': np.float64(0.004145659157074988), 'behavior_loss': np.float64(1.897892916202545)}

Episode step 5960, time diff 0.8164167404174805, total time dif 532.91610455513)
step: 5960 @ episode report: {'average_total_reward': np.float32(5.7333336), 'reward_variance': np.float32(2.632864), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08508810810744763), 'actor_loss': np.float64(-0.9539443016052246), 'hyper_actor_loss': np.float64(0.004045216762460768), 'behavior_loss': np.float64(1.8782601833343506)}

Episode step 5970, time diff 0.7568874359130859, total time dif 533.7325212955475)
step: 5970 @ episode report: {'average_total_reward': np.float32(6.2333336), 'reward_variance': np.float32(1.295284), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08021969608962536), 'actor_loss': np.float64(-0.9366717338562012), 'hyper_actor_loss': np.float64(0.003896730113774538), 'behavior_loss': np.float64(1.8999629259109496)}

Episode step 5980, time diff 0.7664859294891357, total time dif 534.4894087314606)
step: 5980 @ episode report: {'average_total_reward': np.float32(5.982222), 'reward_variance': np.float32(0.82891876), 'max_total_reward': np.float32(7.5333333), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0847790315747261), 'actor_loss': np.float64(-0.9510019421577454), 'hyper_actor_loss': np.float64(0.0038322087377309797), 'behavior_loss': np.float64(1.8885716199874878)}

Episode step 5990, time diff 0.755408763885498, total time dif 535.2558946609497)
step: 5990 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(2.0864398), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09362647011876106), 'actor_loss': np.float64(-0.9505051493644714), 'hyper_actor_loss': np.float64(0.0034064326202496885), 'behavior_loss': np.float64(1.8543343424797059)}

Episode step 6000, time diff 0.7484292984008789, total time dif 536.0113034248352)
step: 6000 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(1.1852953), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08713319040834903), 'actor_loss': np.float64(-0.949985933303833), 'hyper_actor_loss': np.float64(0.003503702930174768), 'behavior_loss': np.float64(1.8100667238235473)}

Episode step 6010, time diff 0.8843965530395508, total time dif 536.7597327232361)
step: 6010 @ episode report: {'average_total_reward': np.float32(6.4700003), 'reward_variance': np.float32(1.8147173), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08104923330247402), 'actor_loss': np.float64(-0.9560278713703155), 'hyper_actor_loss': np.float64(0.003334742970764637), 'behavior_loss': np.float64(1.7473866105079652)}

Episode step 6020, time diff 0.7330143451690674, total time dif 537.6441292762756)
step: 6020 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(1.4619949), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08125850893557071), 'actor_loss': np.float64(-0.9409199953079224), 'hyper_actor_loss': np.float64(0.0034769224002957344), 'behavior_loss': np.float64(1.7860454440116882)}

Episode step 6030, time diff 0.738527774810791, total time dif 538.3771436214447)
step: 6030 @ episode report: {'average_total_reward': np.float32(6.5577784), 'reward_variance': np.float32(3.075255), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07442545257508755), 'actor_loss': np.float64(-0.9223981559276581), 'hyper_actor_loss': np.float64(0.003424443467520177), 'behavior_loss': np.float64(1.7931470990180969)}

Episode step 6040, time diff 0.7799167633056641, total time dif 539.1156713962555)
step: 6040 @ episode report: {'average_total_reward': np.float32(6.37), 'reward_variance': np.float32(0.9513594), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08799347914755344), 'actor_loss': np.float64(-0.9382426023483277), 'hyper_actor_loss': np.float64(0.003539059543982148), 'behavior_loss': np.float64(1.820992887020111)}

Episode step 6050, time diff 0.7843837738037109, total time dif 539.8955881595612)
step: 6050 @ episode report: {'average_total_reward': np.float32(7.3555555), 'reward_variance': np.float32(2.0858517), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07823806218802928), 'actor_loss': np.float64(-0.9575972437858582), 'hyper_actor_loss': np.float64(0.003636205545626581), 'behavior_loss': np.float64(1.8875873684883118)}

Episode step 6060, time diff 0.7598898410797119, total time dif 540.6799719333649)
step: 6060 @ episode report: {'average_total_reward': np.float32(7.3188887), 'reward_variance': np.float32(2.0230873), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07898322679102421), 'actor_loss': np.float64(-0.9037366926670074), 'hyper_actor_loss': np.float64(0.0035726889735087754), 'behavior_loss': np.float64(1.7280647516250611)}

Episode step 6070, time diff 0.7854161262512207, total time dif 541.4398617744446)
step: 6070 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(2.3706474), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08300923630595207), 'actor_loss': np.float64(-0.9465907156467438), 'hyper_actor_loss': np.float64(0.0036501169903203844), 'behavior_loss': np.float64(1.8066850423812866)}

Episode step 6080, time diff 0.7975442409515381, total time dif 542.2252779006958)
step: 6080 @ episode report: {'average_total_reward': np.float32(7.0944443), 'reward_variance': np.float32(1.1098579), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07467529028654099), 'actor_loss': np.float64(-0.954192864894867), 'hyper_actor_loss': np.float64(0.0037479571532458068), 'behavior_loss': np.float64(1.6295790195465087)}

Episode step 6090, time diff 0.8043906688690186, total time dif 543.0228221416473)
step: 6090 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(1.9693199), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07951728887856006), 'actor_loss': np.float64(-0.927648514509201), 'hyper_actor_loss': np.float64(0.0037420398788526655), 'behavior_loss': np.float64(1.7489485383033752)}

Episode step 6100, time diff 0.779620885848999, total time dif 543.8272128105164)
step: 6100 @ episode report: {'average_total_reward': np.float32(7.082223), 'reward_variance': np.float32(1.0456096), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08740295022726059), 'actor_loss': np.float64(-0.9559547543525696), 'hyper_actor_loss': np.float64(0.003775868797674775), 'behavior_loss': np.float64(1.783736789226532)}

Episode step 6110, time diff 0.8398833274841309, total time dif 544.6068336963654)
step: 6110 @ episode report: {'average_total_reward': np.float32(6.6555557), 'reward_variance': np.float32(2.253408), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08242275416851044), 'actor_loss': np.float64(-0.9398721694946289), 'hyper_actor_loss': np.float64(0.003994857566431165), 'behavior_loss': np.float64(1.7091133832931518)}

Episode step 6120, time diff 0.8451509475708008, total time dif 545.4467170238495)
step: 6120 @ episode report: {'average_total_reward': np.float32(7.194445), 'reward_variance': np.float32(0.90158653), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09232026860117912), 'actor_loss': np.float64(-0.961484694480896), 'hyper_actor_loss': np.float64(0.0039680162211880084), 'behavior_loss': np.float64(1.7424882531166077)}

Episode step 6130, time diff 0.8988049030303955, total time dif 546.2918679714203)
step: 6130 @ episode report: {'average_total_reward': np.float32(7.1433334), 'reward_variance': np.float32(1.151839), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0813524067401886), 'actor_loss': np.float64(-0.9345965683460236), 'hyper_actor_loss': np.float64(0.00407592193223536), 'behavior_loss': np.float64(1.737312376499176)}

Episode step 6140, time diff 0.8191721439361572, total time dif 547.1906728744507)
step: 6140 @ episode report: {'average_total_reward': np.float32(6.2211113), 'reward_variance': np.float32(2.3486784), 'max_total_reward': np.float32(8.655557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07566231116652489), 'actor_loss': np.float64(-0.9597396612167358), 'hyper_actor_loss': np.float64(0.0038683365099132063), 'behavior_loss': np.float64(1.605344784259796)}

Episode step 6150, time diff 0.7990691661834717, total time dif 548.0098450183868)
step: 6150 @ episode report: {'average_total_reward': np.float32(6.018889), 'reward_variance': np.float32(0.7547668), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08220902942121029), 'actor_loss': np.float64(-0.944608348608017), 'hyper_actor_loss': np.float64(0.003604648471809924), 'behavior_loss': np.float64(1.7009376168251038)}

Episode step 6160, time diff 0.780163049697876, total time dif 548.8089141845703)
step: 6160 @ episode report: {'average_total_reward': np.float32(7.2433333), 'reward_variance': np.float32(4.057074), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0713475577533245), 'actor_loss': np.float64(-0.9256049692630768), 'hyper_actor_loss': np.float64(0.0037469367729499937), 'behavior_loss': np.float64(1.731199526786804)}

Episode step 6170, time diff 0.9240632057189941, total time dif 549.5890772342682)
step: 6170 @ episode report: {'average_total_reward': np.float32(6.2822227), 'reward_variance': np.float32(2.8010676), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08336858227849006), 'actor_loss': np.float64(-0.9333442270755767), 'hyper_actor_loss': np.float64(0.003804656444117427), 'behavior_loss': np.float64(1.8494885325431825)}

Episode step 6180, time diff 0.7702572345733643, total time dif 550.5131404399872)
step: 6180 @ episode report: {'average_total_reward': np.float32(6.894445), 'reward_variance': np.float32(1.7646482), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07856708168983459), 'actor_loss': np.float64(-0.9584500968456269), 'hyper_actor_loss': np.float64(0.0037091482896357774), 'behavior_loss': np.float64(1.6824502825737)}

Episode step 6190, time diff 0.7666056156158447, total time dif 551.2833976745605)
step: 6190 @ episode report: {'average_total_reward': np.float32(6.357778), 'reward_variance': np.float32(2.3333042), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07408305220305919), 'actor_loss': np.float64(-0.9301782965660095), 'hyper_actor_loss': np.float64(0.003782689105719328), 'behavior_loss': np.float64(1.746748447418213)}

Episode step 6200, time diff 0.7659440040588379, total time dif 552.0500032901764)
step: 6200 @ episode report: {'average_total_reward': np.float32(6.1944447), 'reward_variance': np.float32(2.437291), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08082752674818039), 'actor_loss': np.float64(-0.9241255760192871), 'hyper_actor_loss': np.float64(0.003847273695282638), 'behavior_loss': np.float64(1.8295675039291381)}

Episode step 6210, time diff 0.8329148292541504, total time dif 552.8159472942352)
step: 6210 @ episode report: {'average_total_reward': np.float32(6.145556), 'reward_variance': np.float32(2.0097396), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07486737109720706), 'actor_loss': np.float64(-0.9431750476360321), 'hyper_actor_loss': np.float64(0.003914294205605984), 'behavior_loss': np.float64(1.79258291721344)}

Episode step 6220, time diff 0.8062291145324707, total time dif 553.6488621234894)
step: 6220 @ episode report: {'average_total_reward': np.float32(6.118889), 'reward_variance': np.float32(0.96612465), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08326088674366475), 'actor_loss': np.float64(-0.922753369808197), 'hyper_actor_loss': np.float64(0.003882257151417434), 'behavior_loss': np.float64(1.7284404516220093)}

Episode step 6230, time diff 0.7707293033599854, total time dif 554.4550912380219)
step: 6230 @ episode report: {'average_total_reward': np.float32(6.6577783), 'reward_variance': np.float32(1.5942422), 'max_total_reward': np.float32(9.533334), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08506311252713203), 'actor_loss': np.float64(-0.9609788656234741), 'hyper_actor_loss': np.float64(0.003876619762741029), 'behavior_loss': np.float64(1.736106538772583)}

Episode step 6240, time diff 0.7658579349517822, total time dif 555.2258205413818)
step: 6240 @ episode report: {'average_total_reward': np.float32(6.6700006), 'reward_variance': np.float32(2.3935328), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07846985869109631), 'actor_loss': np.float64(-0.9466214299201965), 'hyper_actor_loss': np.float64(0.004061325266957283), 'behavior_loss': np.float64(1.6706887841224671)}

Episode step 6250, time diff 0.7819416522979736, total time dif 555.9916784763336)
step: 6250 @ episode report: {'average_total_reward': np.float32(6.731111), 'reward_variance': np.float32(3.3063157), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07837613373994827), 'actor_loss': np.float64(-0.9486506104469299), 'hyper_actor_loss': np.float64(0.004162055905908346), 'behavior_loss': np.float64(1.6723329663276671)}

Episode step 6260, time diff 0.7640936374664307, total time dif 556.7736201286316)
step: 6260 @ episode report: {'average_total_reward': np.float32(6.582223), 'reward_variance': np.float32(1.4444987), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07898905985057354), 'actor_loss': np.float64(-0.9324303805828095), 'hyper_actor_loss': np.float64(0.004113614512607455), 'behavior_loss': np.float64(1.6949028968811035)}

Episode step 6270, time diff 0.7844605445861816, total time dif 557.537713766098)
step: 6270 @ episode report: {'average_total_reward': np.float32(6.8944445), 'reward_variance': np.float32(1.7297096), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0772991806268692), 'actor_loss': np.float64(-0.9532258212566376), 'hyper_actor_loss': np.float64(0.003939580707810819), 'behavior_loss': np.float64(1.7760692358016967)}

Episode step 6280, time diff 0.7619316577911377, total time dif 558.3221743106842)
step: 6280 @ episode report: {'average_total_reward': np.float32(5.7944446), 'reward_variance': np.float32(0.36373454), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07922192253172397), 'actor_loss': np.float64(-0.94754079580307), 'hyper_actor_loss': np.float64(0.004154242994263768), 'behavior_loss': np.float64(1.762460172176361)}

Episode step 6290, time diff 0.7333705425262451, total time dif 559.0841059684753)
step: 6290 @ episode report: {'average_total_reward': np.float32(6.9822226), 'reward_variance': np.float32(3.5995617), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08709247820079327), 'actor_loss': np.float64(-0.9543025135993958), 'hyper_actor_loss': np.float64(0.004377402970567346), 'behavior_loss': np.float64(1.6989207863807678)}

Episode step 6300, time diff 0.7733771800994873, total time dif 559.8174765110016)
step: 6300 @ episode report: {'average_total_reward': np.float32(5.6966667), 'reward_variance': np.float32(2.7164953), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08792076073586941), 'actor_loss': np.float64(-0.953009557723999), 'hyper_actor_loss': np.float64(0.004047225508838892), 'behavior_loss': np.float64(1.7118379473686218)}

Episode step 6310, time diff 0.773996114730835, total time dif 560.5908536911011)
step: 6310 @ episode report: {'average_total_reward': np.float32(5.857778), 'reward_variance': np.float32(1.3777974), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08080680929124355), 'actor_loss': np.float64(-0.9394097924232483), 'hyper_actor_loss': np.float64(0.003594368346966803), 'behavior_loss': np.float64(1.6395784974098206)}

Episode step 6320, time diff 0.7441978454589844, total time dif 561.3648498058319)
step: 6320 @ episode report: {'average_total_reward': np.float32(6.1700006), 'reward_variance': np.float32(1.1607419), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0752036951482296), 'actor_loss': np.float64(-0.9192028105258941), 'hyper_actor_loss': np.float64(0.003509992780163884), 'behavior_loss': np.float64(1.7314822554588318)}

Episode step 6330, time diff 0.9244439601898193, total time dif 562.1090476512909)
step: 6330 @ episode report: {'average_total_reward': np.float32(6.7188888), 'reward_variance': np.float32(3.1124957), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08538518100976944), 'actor_loss': np.float64(-0.958254587650299), 'hyper_actor_loss': np.float64(0.003455294296145439), 'behavior_loss': np.float64(1.7269465565681457)}

Episode step 6340, time diff 0.7809865474700928, total time dif 563.0334916114807)
step: 6340 @ episode report: {'average_total_reward': np.float32(7.853334), 'reward_variance': np.float32(5.4634767), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08576157167553902), 'actor_loss': np.float64(-0.9248569011688232), 'hyper_actor_loss': np.float64(0.003495649341493845), 'behavior_loss': np.float64(1.6810469388961793)}

Episode step 6350, time diff 0.7973179817199707, total time dif 563.8144781589508)
step: 6350 @ episode report: {'average_total_reward': np.float32(6.0577784), 'reward_variance': np.float32(1.600045), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07222747337073088), 'actor_loss': np.float64(-0.9371949791908264), 'hyper_actor_loss': np.float64(0.003309135069139302), 'behavior_loss': np.float64(1.6910485982894898)}

Episode step 6360, time diff 0.7774529457092285, total time dif 564.6117961406708)
step: 6360 @ episode report: {'average_total_reward': np.float32(6.955556), 'reward_variance': np.float32(2.162643), 'max_total_reward': np.float32(8.77778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09289520420134068), 'actor_loss': np.float64(-0.9519884288311005), 'hyper_actor_loss': np.float64(0.0031766530591994524), 'behavior_loss': np.float64(1.653260588645935)}

Episode step 6370, time diff 0.7896113395690918, total time dif 565.38924908638)
step: 6370 @ episode report: {'average_total_reward': np.float32(6.7433333), 'reward_variance': np.float32(1.4768507), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08490849733352661), 'actor_loss': np.float64(-0.9549538969993592), 'hyper_actor_loss': np.float64(0.003066745586693287), 'behavior_loss': np.float64(1.6396399974822997)}

Episode step 6380, time diff 0.802177906036377, total time dif 566.1788604259491)
step: 6380 @ episode report: {'average_total_reward': np.float32(7.304445), 'reward_variance': np.float32(4.1845245), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08493765629827976), 'actor_loss': np.float64(-0.9624160945415496), 'hyper_actor_loss': np.float64(0.0031599072273820637), 'behavior_loss': np.float64(1.5644001007080077)}

Episode step 6390, time diff 0.8161780834197998, total time dif 566.9810383319855)
step: 6390 @ episode report: {'average_total_reward': np.float32(7.3555555), 'reward_variance': np.float32(1.2813336), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0837907038629055), 'actor_loss': np.float64(-0.9504002928733826), 'hyper_actor_loss': np.float64(0.003023244487121701), 'behavior_loss': np.float64(1.678110408782959)}

Episode step 6400, time diff 0.7945022583007812, total time dif 567.7972164154053)
step: 6400 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(3.0710235), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08169969394803048), 'actor_loss': np.float64(-0.9317435920238495), 'hyper_actor_loss': np.float64(0.0029296670341864227), 'behavior_loss': np.float64(1.6019200801849365)}

Episode step 6410, time diff 0.7895011901855469, total time dif 568.591718673706)
step: 6410 @ episode report: {'average_total_reward': np.float32(7.182222), 'reward_variance': np.float32(1.3644497), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09157225675880909), 'actor_loss': np.float64(-0.9365453481674194), 'hyper_actor_loss': np.float64(0.0029013169929385184), 'behavior_loss': np.float64(1.695589017868042)}

Episode step 6420, time diff 0.7922742366790771, total time dif 569.3812198638916)
step: 6420 @ episode report: {'average_total_reward': np.float32(7.1066666), 'reward_variance': np.float32(1.9767954), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09677378684282303), 'actor_loss': np.float64(-0.9683932602405548), 'hyper_actor_loss': np.float64(0.0028764873975887896), 'behavior_loss': np.float64(1.62280775308609)}

Episode step 6430, time diff 0.8148086071014404, total time dif 570.1734941005707)
step: 6430 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(1.8273531), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0912021353840828), 'actor_loss': np.float64(-0.9796888649463653), 'hyper_actor_loss': np.float64(0.0028843072475865485), 'behavior_loss': np.float64(1.5919393181800843)}

Episode step 6440, time diff 0.7825422286987305, total time dif 570.9883027076721)
step: 6440 @ episode report: {'average_total_reward': np.float32(6.694444), 'reward_variance': np.float32(2.0401292), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07134460844099522), 'actor_loss': np.float64(-0.920603585243225), 'hyper_actor_loss': np.float64(0.003026243392378092), 'behavior_loss': np.float64(1.6057318091392516)}

Episode step 6450, time diff 0.7785186767578125, total time dif 571.7708449363708)
step: 6450 @ episode report: {'average_total_reward': np.float32(6.4822226), 'reward_variance': np.float32(1.7108688), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08516477458178998), 'actor_loss': np.float64(-0.9428671717643737), 'hyper_actor_loss': np.float64(0.003036212408915162), 'behavior_loss': np.float64(1.6016168355941773)}

Episode step 6460, time diff 0.764683723449707, total time dif 572.5493636131287)
step: 6460 @ episode report: {'average_total_reward': np.float32(5.7700005), 'reward_variance': np.float32(0.82479125), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07763360403478145), 'actor_loss': np.float64(-0.958332622051239), 'hyper_actor_loss': np.float64(0.002967742271721363), 'behavior_loss': np.float64(1.5816607356071473)}

Episode step 6470, time diff 0.8128354549407959, total time dif 573.3140473365784)
step: 6470 @ episode report: {'average_total_reward': np.float32(7.055556), 'reward_variance': np.float32(1.4809139), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07284559160470963), 'actor_loss': np.float64(-0.89320148229599), 'hyper_actor_loss': np.float64(0.0028345081023871898), 'behavior_loss': np.float64(1.6850934028625488)}

Episode step 6480, time diff 0.7864477634429932, total time dif 574.1268827915192)
step: 6480 @ episode report: {'average_total_reward': np.float32(7.043334), 'reward_variance': np.float32(0.70897424), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07978737205266953), 'actor_loss': np.float64(-0.9251655220985413), 'hyper_actor_loss': np.float64(0.002639067289419472), 'behavior_loss': np.float64(1.6457783818244933)}

Episode step 6490, time diff 0.9697055816650391, total time dif 574.9133305549622)
step: 6490 @ episode report: {'average_total_reward': np.float32(7.416667), 'reward_variance': np.float32(3.0838828), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0864452674984932), 'actor_loss': np.float64(-0.9598463773727417), 'hyper_actor_loss': np.float64(0.002491032215766609), 'behavior_loss': np.float64(1.6845531821250916)}

Episode step 6500, time diff 0.8090183734893799, total time dif 575.8830361366272)
step: 6500 @ episode report: {'average_total_reward': np.float32(7.1433334), 'reward_variance': np.float32(2.353369), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07833321802318097), 'actor_loss': np.float64(-0.927919453382492), 'hyper_actor_loss': np.float64(0.002453926461748779), 'behavior_loss': np.float64(1.6899697422981261)}

Episode step 6510, time diff 0.7996280193328857, total time dif 576.6920545101166)
step: 6510 @ episode report: {'average_total_reward': np.float32(6.457778), 'reward_variance': np.float32(1.8932794), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0772510427981615), 'actor_loss': np.float64(-0.9310140192508698), 'hyper_actor_loss': np.float64(0.002641733968630433), 'behavior_loss': np.float64(1.568954837322235)}

Episode step 6520, time diff 0.7922897338867188, total time dif 577.4916825294495)
step: 6520 @ episode report: {'average_total_reward': np.float32(5.347778), 'reward_variance': np.float32(1.3827667), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0769072998315096), 'actor_loss': np.float64(-0.9304221093654632), 'hyper_actor_loss': np.float64(0.002664332673884928), 'behavior_loss': np.float64(1.6722654938697814)}

Episode step 6530, time diff 0.7467765808105469, total time dif 578.2839722633362)
step: 6530 @ episode report: {'average_total_reward': np.float32(7.006667), 'reward_variance': np.float32(2.1695113), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08535660281777382), 'actor_loss': np.float64(-0.9315541744232178), 'hyper_actor_loss': np.float64(0.002667963784188032), 'behavior_loss': np.float64(1.7789608597755433)}

Episode step 6540, time diff 0.761760950088501, total time dif 579.0307488441467)
step: 6540 @ episode report: {'average_total_reward': np.float32(7.131111), 'reward_variance': np.float32(2.7008102), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0875104546546936), 'actor_loss': np.float64(-0.9231427788734436), 'hyper_actor_loss': np.float64(0.0025554083986207843), 'behavior_loss': np.float64(1.688754963874817)}

Episode step 6550, time diff 0.7468094825744629, total time dif 579.7925097942352)
step: 6550 @ episode report: {'average_total_reward': np.float32(6.506667), 'reward_variance': np.float32(0.9386719), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0866036742925644), 'actor_loss': np.float64(-0.9702169716358184), 'hyper_actor_loss': np.float64(0.002509218850173056), 'behavior_loss': np.float64(1.5445552229881288)}

Episode step 6560, time diff 0.7986843585968018, total time dif 580.5393192768097)
step: 6560 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(1.9928017), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808455232530833), 'actor_loss': np.float64(-0.9228640615940094), 'hyper_actor_loss': np.float64(0.0025058771949261426), 'behavior_loss': np.float64(1.6242899298667908)}

Episode step 6570, time diff 0.754857063293457, total time dif 581.3380036354065)
step: 6570 @ episode report: {'average_total_reward': np.float32(6.1577783), 'reward_variance': np.float32(1.3028593), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08113153092563152), 'actor_loss': np.float64(-0.9038101613521576), 'hyper_actor_loss': np.float64(0.0027282208669930697), 'behavior_loss': np.float64(1.6892623543739318)}

Episode step 6580, time diff 0.8129990100860596, total time dif 582.0928606987)
step: 6580 @ episode report: {'average_total_reward': np.float32(6.8555555), 'reward_variance': np.float32(3.6268888), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07939234003424644), 'actor_loss': np.float64(-0.9683944642543793), 'hyper_actor_loss': np.float64(0.002698238403536379), 'behavior_loss': np.float64(1.5951856970787048)}

Episode step 6590, time diff 0.7869365215301514, total time dif 582.905859708786)
step: 6590 @ episode report: {'average_total_reward': np.float32(7.7922225), 'reward_variance': np.float32(1.9940752), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08551147282123565), 'actor_loss': np.float64(-0.9159683406352996), 'hyper_actor_loss': np.float64(0.002659062738530338), 'behavior_loss': np.float64(1.6199774384498595)}

Episode step 6600, time diff 0.8199567794799805, total time dif 583.6927962303162)
step: 6600 @ episode report: {'average_total_reward': np.float32(6.8066664), 'reward_variance': np.float32(2.7432895), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07603064924478531), 'actor_loss': np.float64(-0.9225102126598358), 'hyper_actor_loss': np.float64(0.002616290468722582), 'behavior_loss': np.float64(1.6255162715911866)}

Episode step 6610, time diff 0.7874834537506104, total time dif 584.5127530097961)
step: 6610 @ episode report: {'average_total_reward': np.float32(7.5288887), 'reward_variance': np.float32(1.8493135), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08047209978103638), 'actor_loss': np.float64(-0.9359367489814758), 'hyper_actor_loss': np.float64(0.002565258997492492), 'behavior_loss': np.float64(1.5683825492858887)}

Episode step 6620, time diff 0.8152916431427002, total time dif 585.3002364635468)
step: 6620 @ episode report: {'average_total_reward': np.float32(6.794445), 'reward_variance': np.float32(2.1113896), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0714311458170414), 'actor_loss': np.float64(-0.9139411985874176), 'hyper_actor_loss': np.float64(0.0025834348052740097), 'behavior_loss': np.float64(1.5972664833068848)}

Episode step 6630, time diff 0.817389965057373, total time dif 586.1155281066895)
step: 6630 @ episode report: {'average_total_reward': np.float32(6.555556), 'reward_variance': np.float32(2.2531362), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08878740705549718), 'actor_loss': np.float64(-0.9469842731952667), 'hyper_actor_loss': np.float64(0.002526400797069073), 'behavior_loss': np.float64(1.5635652661323547)}

Episode step 6640, time diff 0.7839677333831787, total time dif 586.9329180717468)
step: 6640 @ episode report: {'average_total_reward': np.float32(6.918889), 'reward_variance': np.float32(1.4625694), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08517402857542038), 'actor_loss': np.float64(-0.9615567088127136), 'hyper_actor_loss': np.float64(0.002483878913335502), 'behavior_loss': np.float64(1.6101219415664674)}

Episode step 6650, time diff 0.9631845951080322, total time dif 587.71688580513)
step: 6650 @ episode report: {'average_total_reward': np.float32(7.653334), 'reward_variance': np.float32(3.5698714), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09830271452665329), 'actor_loss': np.float64(-0.9310138523578644), 'hyper_actor_loss': np.float64(0.0024259394500404595), 'behavior_loss': np.float64(1.5667455196380615)}

Episode step 6660, time diff 0.7778279781341553, total time dif 588.680070400238)
step: 6660 @ episode report: {'average_total_reward': np.float32(6.457778), 'reward_variance': np.float32(1.1306866), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06854421198368073), 'actor_loss': np.float64(-0.9533687829971313), 'hyper_actor_loss': np.float64(0.002419176371768117), 'behavior_loss': np.float64(1.5033405303955079)}

Episode step 6670, time diff 0.7455024719238281, total time dif 589.4578983783722)
step: 6670 @ episode report: {'average_total_reward': np.float32(7.3822227), 'reward_variance': np.float32(0.82849884), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08795698955655099), 'actor_loss': np.float64(-0.9321662485599518), 'hyper_actor_loss': np.float64(0.002337569766677916), 'behavior_loss': np.float64(1.5904634118080139)}

Episode step 6680, time diff 0.7776906490325928, total time dif 590.203400850296)
step: 6680 @ episode report: {'average_total_reward': np.float32(6.8188896), 'reward_variance': np.float32(1.6217048), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09226985648274422), 'actor_loss': np.float64(-0.9536315202713013), 'hyper_actor_loss': np.float64(0.002277002017945051), 'behavior_loss': np.float64(1.5723379969596862)}

Episode step 6690, time diff 0.7812037467956543, total time dif 590.9810914993286)
step: 6690 @ episode report: {'average_total_reward': np.float32(6.6066675), 'reward_variance': np.float32(2.3637338), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08792351223528386), 'actor_loss': np.float64(-0.969122713804245), 'hyper_actor_loss': np.float64(0.002345317369326949), 'behavior_loss': np.float64(1.6127564907073975)}

Episode step 6700, time diff 0.7850492000579834, total time dif 591.7622952461243)
step: 6700 @ episode report: {'average_total_reward': np.float32(7.006667), 'reward_variance': np.float32(1.9021537), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07572519481182098), 'actor_loss': np.float64(-0.9206283330917359), 'hyper_actor_loss': np.float64(0.0023354959208518266), 'behavior_loss': np.float64(1.5741572260856629)}

Episode step 6710, time diff 0.7713959217071533, total time dif 592.5473444461823)
step: 6710 @ episode report: {'average_total_reward': np.float32(7.0944443), 'reward_variance': np.float32(2.238056), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08548747263848781), 'actor_loss': np.float64(-0.9344744384288788), 'hyper_actor_loss': np.float64(0.0023138696793466806), 'behavior_loss': np.float64(1.6054954648017883)}

Episode step 6720, time diff 0.7873773574829102, total time dif 593.3187403678894)
step: 6720 @ episode report: {'average_total_reward': np.float32(6.6066675), 'reward_variance': np.float32(4.091462), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08598513044416904), 'actor_loss': np.float64(-0.940605241060257), 'hyper_actor_loss': np.float64(0.0022039343137294056), 'behavior_loss': np.float64(1.5393415808677673)}

Episode step 6730, time diff 0.7938199043273926, total time dif 594.1061177253723)
step: 6730 @ episode report: {'average_total_reward': np.float32(6.955556), 'reward_variance': np.float32(2.0968156), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08219434656202793), 'actor_loss': np.float64(-0.9299614310264588), 'hyper_actor_loss': np.float64(0.0022964116651564837), 'behavior_loss': np.float64(1.6607617259025573)}

Episode step 6740, time diff 0.7693741321563721, total time dif 594.8999376296997)
step: 6740 @ episode report: {'average_total_reward': np.float32(6.6577783), 'reward_variance': np.float32(1.1423656), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08063228614628315), 'actor_loss': np.float64(-0.9333797812461853), 'hyper_actor_loss': np.float64(0.0022213713731616736), 'behavior_loss': np.float64(1.5958290219306945)}

Episode step 6750, time diff 0.8133618831634521, total time dif 595.6693117618561)
step: 6750 @ episode report: {'average_total_reward': np.float32(7.006667), 'reward_variance': np.float32(2.64882), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07942320294678211), 'actor_loss': np.float64(-0.927688467502594), 'hyper_actor_loss': np.float64(0.0021518196212127806), 'behavior_loss': np.float64(1.4696560025215148)}

Episode step 6760, time diff 0.7927463054656982, total time dif 596.4826736450195)
step: 6760 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(1.2719624), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08040993139147759), 'actor_loss': np.float64(-0.9440622448921203), 'hyper_actor_loss': np.float64(0.002122935594525188), 'behavior_loss': np.float64(1.5898969531059266)}

Episode step 6770, time diff 0.8197677135467529, total time dif 597.2754199504852)
step: 6770 @ episode report: {'average_total_reward': np.float32(7.3433332), 'reward_variance': np.float32(3.044455), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07966960817575455), 'actor_loss': np.float64(-0.9307383060455322), 'hyper_actor_loss': np.float64(0.00211920291185379), 'behavior_loss': np.float64(1.5582931518554688)}

Episode step 6780, time diff 0.7917044162750244, total time dif 598.095187664032)
step: 6780 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(1.8260605), 'max_total_reward': np.float32(9.533334), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07100027240812778), 'actor_loss': np.float64(-0.919745284318924), 'hyper_actor_loss': np.float64(0.0021688987384550274), 'behavior_loss': np.float64(1.5653134346008302)}

Episode step 6790, time diff 0.7980344295501709, total time dif 598.886892080307)
step: 6790 @ episode report: {'average_total_reward': np.float32(7.0922227), 'reward_variance': np.float32(3.1629894), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08274263553321362), 'actor_loss': np.float64(-0.9474092423915863), 'hyper_actor_loss': np.float64(0.0022411096142604947), 'behavior_loss': np.float64(1.4907719135284423)}

Episode step 6800, time diff 0.7712993621826172, total time dif 599.6849265098572)
step: 6800 @ episode report: {'average_total_reward': np.float32(6.4066668), 'reward_variance': np.float32(1.4966718), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07587117552757264), 'actor_loss': np.float64(-0.9515030264854432), 'hyper_actor_loss': np.float64(0.002384384791366756), 'behavior_loss': np.float64(1.442084300518036)}

Episode step 6810, time diff 0.9401125907897949, total time dif 600.4562258720398)
step: 6810 @ episode report: {'average_total_reward': np.float32(6.8433332), 'reward_variance': np.float32(3.2545788), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08437321707606316), 'actor_loss': np.float64(-0.9640346884727478), 'hyper_actor_loss': np.float64(0.0024601839715614913), 'behavior_loss': np.float64(1.4898008584976197)}

Episode step 6820, time diff 0.8101835250854492, total time dif 601.3963384628296)
step: 6820 @ episode report: {'average_total_reward': np.float32(6.794445), 'reward_variance': np.float32(1.6949196), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07639151252806187), 'actor_loss': np.float64(-0.9358811557292939), 'hyper_actor_loss': np.float64(0.002672007703222334), 'behavior_loss': np.float64(1.5132575035095215)}

Episode step 6830, time diff 0.7960011959075928, total time dif 602.206521987915)
step: 6830 @ episode report: {'average_total_reward': np.float32(6.3944445), 'reward_variance': np.float32(2.0749938), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08343214839696884), 'actor_loss': np.float64(-0.9319647192955017), 'hyper_actor_loss': np.float64(0.002744705183431506), 'behavior_loss': np.float64(1.488589358329773)}

Episode step 6840, time diff 0.7715897560119629, total time dif 603.0025231838226)
step: 6840 @ episode report: {'average_total_reward': np.float32(6.1211114), 'reward_variance': np.float32(1.3627526), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06973450146615505), 'actor_loss': np.float64(-0.9514043092727661), 'hyper_actor_loss': np.float64(0.002714412659406662), 'behavior_loss': np.float64(1.486984956264496)}

Episode step 6850, time diff 0.7411777973175049, total time dif 603.7741129398346)
step: 6850 @ episode report: {'average_total_reward': np.float32(7.167778), 'reward_variance': np.float32(1.3289245), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07399322241544723), 'actor_loss': np.float64(-0.9326090216636658), 'hyper_actor_loss': np.float64(0.002740586828440428), 'behavior_loss': np.float64(1.4466266989707948)}

Episode step 6860, time diff 0.8227396011352539, total time dif 604.5152907371521)
step: 6860 @ episode report: {'average_total_reward': np.float32(6.0700006), 'reward_variance': np.float32(1.4209893), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09008942693471908), 'actor_loss': np.float64(-0.941270512342453), 'hyper_actor_loss': np.float64(0.0027768971864134072), 'behavior_loss': np.float64(1.5589379668235779)}

Episode step 6870, time diff 0.7965962886810303, total time dif 605.3380303382874)
step: 6870 @ episode report: {'average_total_reward': np.float32(7.216667), 'reward_variance': np.float32(3.123883), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08366400934755802), 'actor_loss': np.float64(-0.9462777674198151), 'hyper_actor_loss': np.float64(0.002725730394013226), 'behavior_loss': np.float64(1.5064684987068175)}

Episode step 6880, time diff 0.7824795246124268, total time dif 606.1346266269684)
step: 6880 @ episode report: {'average_total_reward': np.float32(6.8944445), 'reward_variance': np.float32(3.0579076), 'max_total_reward': np.float32(9.777777), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09268875792622566), 'actor_loss': np.float64(-0.9669570624828339), 'hyper_actor_loss': np.float64(0.0027891613775864244), 'behavior_loss': np.float64(1.496764588356018)}

Episode step 6890, time diff 0.7528653144836426, total time dif 606.9171061515808)
step: 6890 @ episode report: {'average_total_reward': np.float32(6.967778), 'reward_variance': np.float32(2.2938135), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09122188724577426), 'actor_loss': np.float64(-0.9560551822185517), 'hyper_actor_loss': np.float64(0.0029021968832239507), 'behavior_loss': np.float64(1.6215412855148315)}

Episode step 6900, time diff 0.7746272087097168, total time dif 607.6699714660645)
step: 6900 @ episode report: {'average_total_reward': np.float32(6.5700006), 'reward_variance': np.float32(2.1861987), 'max_total_reward': np.float32(8.777777), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0875798773020506), 'actor_loss': np.float64(-0.9369306445121766), 'hyper_actor_loss': np.float64(0.002797027654014528), 'behavior_loss': np.float64(1.5052292346954346)}

Episode step 6910, time diff 0.7700660228729248, total time dif 608.4445986747742)
step: 6910 @ episode report: {'average_total_reward': np.float32(7.553334), 'reward_variance': np.float32(3.4928355), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07457046844065189), 'actor_loss': np.float64(-0.9381175398826599), 'hyper_actor_loss': np.float64(0.0028939530020579696), 'behavior_loss': np.float64(1.5237828016281127)}

Episode step 6920, time diff 0.7872059345245361, total time dif 609.2146646976471)
step: 6920 @ episode report: {'average_total_reward': np.float32(6.7433333), 'reward_variance': np.float32(1.7012951), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07696146480739116), 'actor_loss': np.float64(-0.9385694324970245), 'hyper_actor_loss': np.float64(0.0028690536972135307), 'behavior_loss': np.float64(1.533750319480896)}

Episode step 6930, time diff 0.7442450523376465, total time dif 610.0018706321716)
step: 6930 @ episode report: {'average_total_reward': np.float32(6.8944445), 'reward_variance': np.float32(2.0853395), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08052680753171444), 'actor_loss': np.float64(-0.9340933859348297), 'hyper_actor_loss': np.float64(0.0029848183039575815), 'behavior_loss': np.float64(1.5038007736206054)}

Episode step 6940, time diff 0.7972574234008789, total time dif 610.7461156845093)
step: 6940 @ episode report: {'average_total_reward': np.float32(6.845556), 'reward_variance': np.float32(1.7107513), 'max_total_reward': np.float32(8.655555), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0912768855690956), 'actor_loss': np.float64(-0.9546037971973419), 'hyper_actor_loss': np.float64(0.002945767599157989), 'behavior_loss': np.float64(1.5123824596405029)}

Episode step 6950, time diff 0.8234338760375977, total time dif 611.5433731079102)
step: 6950 @ episode report: {'average_total_reward': np.float32(6.994446), 'reward_variance': np.float32(3.4068706), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08918774239718914), 'actor_loss': np.float64(-0.9640079021453858), 'hyper_actor_loss': np.float64(0.0032605775399133564), 'behavior_loss': np.float64(1.4446858406066894)}

Episode step 6960, time diff 0.7965927124023438, total time dif 612.3668069839478)
step: 6960 @ episode report: {'average_total_reward': np.float32(6.7555556), 'reward_variance': np.float32(1.2989633), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08555595017969608), 'actor_loss': np.float64(-0.9446036577224731), 'hyper_actor_loss': np.float64(0.0032523143803700806), 'behavior_loss': np.float64(1.493762743473053)}

Episode step 6970, time diff 0.9481940269470215, total time dif 613.1633996963501)
step: 6970 @ episode report: {'average_total_reward': np.float32(6.767779), 'reward_variance': np.float32(1.7178634), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09253194630146026), 'actor_loss': np.float64(-0.9257271528244019), 'hyper_actor_loss': np.float64(0.0030598206678405402), 'behavior_loss': np.float64(1.5709229469299317)}

Episode step 6980, time diff 0.7529778480529785, total time dif 614.1115937232971)
step: 6980 @ episode report: {'average_total_reward': np.float32(7.3188887), 'reward_variance': np.float32(1.2674582), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08427500016987324), 'actor_loss': np.float64(-0.9267981171607971), 'hyper_actor_loss': np.float64(0.0026490038493648172), 'behavior_loss': np.float64(1.4755684733390808)}

Episode step 6990, time diff 0.7998607158660889, total time dif 614.8645715713501)
step: 6990 @ episode report: {'average_total_reward': np.float32(6.37), 'reward_variance': np.float32(1.7823219), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08307171165943146), 'actor_loss': np.float64(-0.9434019505977631), 'hyper_actor_loss': np.float64(0.0025784449884667993), 'behavior_loss': np.float64(1.4529431223869325)}

Episode step 7000, time diff 0.8101098537445068, total time dif 615.6644322872162)
step: 7000 @ episode report: {'average_total_reward': np.float32(6.5577784), 'reward_variance': np.float32(0.8477481), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09320274740457535), 'actor_loss': np.float64(-0.9545873284339905), 'hyper_actor_loss': np.float64(0.0026243885746225714), 'behavior_loss': np.float64(1.468838882446289)}

Episode step 7010, time diff 0.7837846279144287, total time dif 616.4745421409607)
step: 7010 @ episode report: {'average_total_reward': np.float32(6.3066664), 'reward_variance': np.float32(2.2541296), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0845850482583046), 'actor_loss': np.float64(-0.9545534908771515), 'hyper_actor_loss': np.float64(0.0027574783889576793), 'behavior_loss': np.float64(1.442973554134369)}

Episode step 7020, time diff 0.8212509155273438, total time dif 617.2583267688751)
step: 7020 @ episode report: {'average_total_reward': np.float32(6.2822223), 'reward_variance': np.float32(1.4000051), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0960854422301054), 'actor_loss': np.float64(-0.9359963536262512), 'hyper_actor_loss': np.float64(0.002915485552512109), 'behavior_loss': np.float64(1.4751718878746032)}

Episode step 7030, time diff 0.8093042373657227, total time dif 618.0795776844025)
step: 7030 @ episode report: {'average_total_reward': np.float32(6.6433344), 'reward_variance': np.float32(3.431839), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08745091259479523), 'actor_loss': np.float64(-0.969609797000885), 'hyper_actor_loss': np.float64(0.0026017355266958475), 'behavior_loss': np.float64(1.4559133172035217)}

Episode step 7040, time diff 0.7946932315826416, total time dif 618.8888819217682)
step: 7040 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(2.628563), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08655079938471318), 'actor_loss': np.float64(-0.9289288401603699), 'hyper_actor_loss': np.float64(0.002348418557085097), 'behavior_loss': np.float64(1.4352633118629456)}

Episode step 7050, time diff 0.7624433040618896, total time dif 619.6835751533508)
step: 7050 @ episode report: {'average_total_reward': np.float32(7.131112), 'reward_variance': np.float32(2.694835), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08200975023210048), 'actor_loss': np.float64(-0.9409618973731995), 'hyper_actor_loss': np.float64(0.0022207419155165554), 'behavior_loss': np.float64(1.471003484725952)}

Episode step 7060, time diff 0.8009910583496094, total time dif 620.4460184574127)
step: 7060 @ episode report: {'average_total_reward': np.float32(8.216667), 'reward_variance': np.float32(1.8016608), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07261033579707146), 'actor_loss': np.float64(-0.9200477242469788), 'hyper_actor_loss': np.float64(0.0020634432206861676), 'behavior_loss': np.float64(1.4414787769317627)}

Episode step 7070, time diff 0.8251354694366455, total time dif 621.2470095157623)
step: 7070 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(1.9889584), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07863140925765037), 'actor_loss': np.float64(-0.930216783285141), 'hyper_actor_loss': np.float64(0.0019127741223201155), 'behavior_loss': np.float64(1.464398241043091)}

Episode step 7080, time diff 0.7735354900360107, total time dif 622.072144985199)
step: 7080 @ episode report: {'average_total_reward': np.float32(6.555556), 'reward_variance': np.float32(1.9463953), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07890638932585717), 'actor_loss': np.float64(-0.9267039120197296), 'hyper_actor_loss': np.float64(0.0019426562823355198), 'behavior_loss': np.float64(1.4253109693527222)}

Episode step 7090, time diff 0.8098254203796387, total time dif 622.845680475235)
step: 7090 @ episode report: {'average_total_reward': np.float32(7.2066674), 'reward_variance': np.float32(1.387536), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0870641078799963), 'actor_loss': np.float64(-0.9507809996604919), 'hyper_actor_loss': np.float64(0.0018840072792954744), 'behavior_loss': np.float64(1.3391396403312683)}

Episode step 7100, time diff 0.7848691940307617, total time dif 623.6555058956146)
step: 7100 @ episode report: {'average_total_reward': np.float32(7.455556), 'reward_variance': np.float32(2.5535557), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0782908894121647), 'actor_loss': np.float64(-0.9279293119907379), 'hyper_actor_loss': np.float64(0.0019028490060009062), 'behavior_loss': np.float64(1.5367399215698243)}

Episode step 7110, time diff 0.7940583229064941, total time dif 624.4403750896454)
step: 7110 @ episode report: {'average_total_reward': np.float32(6.2211113), 'reward_variance': np.float32(1.1990238), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08580510951578617), 'actor_loss': np.float64(-0.9316536664962769), 'hyper_actor_loss': np.float64(0.0019612999400123953), 'behavior_loss': np.float64(1.4187119126319885)}

Episode step 7120, time diff 0.8106272220611572, total time dif 625.2344334125519)
step: 7120 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(2.4799502), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07841957174241543), 'actor_loss': np.float64(-0.9517110645771026), 'hyper_actor_loss': np.float64(0.0021396742318756878), 'behavior_loss': np.float64(1.442241406440735)}

Episode step 7130, time diff 0.948613166809082, total time dif 626.045060634613)
step: 7130 @ episode report: {'average_total_reward': np.float32(6.931112), 'reward_variance': np.float32(0.6604644), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07897849716246128), 'actor_loss': np.float64(-0.9280101716518402), 'hyper_actor_loss': np.float64(0.0021045703324489295), 'behavior_loss': np.float64(1.438575553894043)}

Episode step 7140, time diff 0.7464499473571777, total time dif 626.9936738014221)
step: 7140 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(1.3917578), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07935207262635231), 'actor_loss': np.float64(-0.9398455202579499), 'hyper_actor_loss': np.float64(0.002160880947485566), 'behavior_loss': np.float64(1.4237018942832946)}

Episode step 7150, time diff 0.7445065975189209, total time dif 627.7401237487793)
step: 7150 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(2.7907763), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08043591566383838), 'actor_loss': np.float64(-0.9304251790046691), 'hyper_actor_loss': np.float64(0.002125613042153418), 'behavior_loss': np.float64(1.4984500408172607)}

Episode step 7160, time diff 0.7453110218048096, total time dif 628.4846303462982)
step: 7160 @ episode report: {'average_total_reward': np.float32(6.918889), 'reward_variance': np.float32(3.326469), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0733161572366953), 'actor_loss': np.float64(-0.9139583826065063), 'hyper_actor_loss': np.float64(0.0021212431252934038), 'behavior_loss': np.float64(1.4794217109680177)}

Episode step 7170, time diff 0.803534984588623, total time dif 629.229941368103)
step: 7170 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(1.8476362), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08058828413486481), 'actor_loss': np.float64(-0.924136620759964), 'hyper_actor_loss': np.float64(0.001987604517489672), 'behavior_loss': np.float64(1.3711765885353089)}

Episode step 7180, time diff 0.7482075691223145, total time dif 630.0334763526917)
step: 7180 @ episode report: {'average_total_reward': np.float32(7.4188895), 'reward_variance': np.float32(1.8671618), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07632996588945389), 'actor_loss': np.float64(-0.9679055988788605), 'hyper_actor_loss': np.float64(0.0019711356027983127), 'behavior_loss': np.float64(1.3803898215293884)}

Episode step 7190, time diff 0.7544291019439697, total time dif 630.781683921814)
step: 7190 @ episode report: {'average_total_reward': np.float32(7.741111), 'reward_variance': np.float32(1.9388406), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08170965015888214), 'actor_loss': np.float64(-0.9116537988185882), 'hyper_actor_loss': np.float64(0.001993380219209939), 'behavior_loss': np.float64(1.4521001696586608)}

Episode step 7200, time diff 0.7322502136230469, total time dif 631.5361130237579)
step: 7200 @ episode report: {'average_total_reward': np.float32(6.667778), 'reward_variance': np.float32(1.6851723), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0879086934030056), 'actor_loss': np.float64(-0.9493279695510864), 'hyper_actor_loss': np.float64(0.0019840741879306734), 'behavior_loss': np.float64(1.4831209063529969)}

Episode step 7210, time diff 0.762688398361206, total time dif 632.268363237381)
step: 7210 @ episode report: {'average_total_reward': np.float32(7.2433333), 'reward_variance': np.float32(2.7044303), 'max_total_reward': np.float32(9.777777), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08071686774492264), 'actor_loss': np.float64(-0.9350801706314087), 'hyper_actor_loss': np.float64(0.0019344129716046155), 'behavior_loss': np.float64(1.4377331495285035)}

Episode step 7220, time diff 0.7434403896331787, total time dif 633.0310516357422)
step: 7220 @ episode report: {'average_total_reward': np.float32(7.6800003), 'reward_variance': np.float32(2.8064404), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09219900891184807), 'actor_loss': np.float64(-0.9367620050907135), 'hyper_actor_loss': np.float64(0.0019084953120909632), 'behavior_loss': np.float64(1.3701034188270569)}

Episode step 7230, time diff 0.7323658466339111, total time dif 633.7744920253754)
step: 7230 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(3.3868415), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08371357843279839), 'actor_loss': np.float64(-0.9689578652381897), 'hyper_actor_loss': np.float64(0.0019511418882757424), 'behavior_loss': np.float64(1.4453572750091552)}

Episode step 7240, time diff 0.7514224052429199, total time dif 634.5068578720093)
step: 7240 @ episode report: {'average_total_reward': np.float32(7.267778), 'reward_variance': np.float32(0.28452963), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07893924899399281), 'actor_loss': np.float64(-0.9233153581619262), 'hyper_actor_loss': np.float64(0.001982057257555425), 'behavior_loss': np.float64(1.3648921012878419)}

Episode step 7250, time diff 0.7555906772613525, total time dif 635.2582802772522)
step: 7250 @ episode report: {'average_total_reward': np.float32(7.28), 'reward_variance': np.float32(3.1532788), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07597609311342239), 'actor_loss': np.float64(-0.9417365312576294), 'hyper_actor_loss': np.float64(0.0019985427148640155), 'behavior_loss': np.float64(1.4207019329071044)}

Episode step 7260, time diff 0.7528584003448486, total time dif 636.0138709545135)
step: 7260 @ episode report: {'average_total_reward': np.float32(8.102223), 'reward_variance': np.float32(5.826293), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08138019032776356), 'actor_loss': np.float64(-0.9474249899387359), 'hyper_actor_loss': np.float64(0.0020344145712442698), 'behavior_loss': np.float64(1.3499382138252258)}

Episode step 7270, time diff 0.7612085342407227, total time dif 636.7667293548584)
step: 7270 @ episode report: {'average_total_reward': np.float32(6.618889), 'reward_variance': np.float32(1.7832115), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08469093330204487), 'actor_loss': np.float64(-0.9701078176498413), 'hyper_actor_loss': np.float64(0.002170868287794292), 'behavior_loss': np.float64(1.313052487373352)}

Episode step 7280, time diff 0.7617528438568115, total time dif 637.5279378890991)
step: 7280 @ episode report: {'average_total_reward': np.float32(6.857778), 'reward_variance': np.float32(1.890514), 'max_total_reward': np.float32(8.655557), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07867163419723511), 'actor_loss': np.float64(-0.9371107339859008), 'hyper_actor_loss': np.float64(0.0022218353115022182), 'behavior_loss': np.float64(1.3742369890213013)}

Episode step 7290, time diff 0.9638543128967285, total time dif 638.2896907329559)
step: 7290 @ episode report: {'average_total_reward': np.float32(6.294444), 'reward_variance': np.float32(1.8195127), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0859658695757389), 'actor_loss': np.float64(-0.9365881264209748), 'hyper_actor_loss': np.float64(0.0022148176562041045), 'behavior_loss': np.float64(1.3670651197433472)}

Episode step 7300, time diff 0.8207998275756836, total time dif 639.2535450458527)
step: 7300 @ episode report: {'average_total_reward': np.float32(7.3188896), 'reward_variance': np.float32(1.2948903), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07277193032205105), 'actor_loss': np.float64(-0.9227044820785523), 'hyper_actor_loss': np.float64(0.002199305989779532), 'behavior_loss': np.float64(1.4083208918571473)}

Episode step 7310, time diff 0.7766177654266357, total time dif 640.0743448734283)
step: 7310 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(1.3815755), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07770035304129123), 'actor_loss': np.float64(-0.9493153393268585), 'hyper_actor_loss': np.float64(0.002134432038292289), 'behavior_loss': np.float64(1.343965971469879)}

Episode step 7320, time diff 0.8053929805755615, total time dif 640.850962638855)
step: 7320 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(3.8126926), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07110352367162705), 'actor_loss': np.float64(-0.9284038066864013), 'hyper_actor_loss': np.float64(0.0020875427406281235), 'behavior_loss': np.float64(1.353051447868347)}

Episode step 7330, time diff 0.7876505851745605, total time dif 641.6563556194305)
step: 7330 @ episode report: {'average_total_reward': np.float32(6.4700003), 'reward_variance': np.float32(2.0087423), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07351105958223343), 'actor_loss': np.float64(-0.9145283877849579), 'hyper_actor_loss': np.float64(0.0021070656832307575), 'behavior_loss': np.float64(1.390434741973877)}

Episode step 7340, time diff 0.7912638187408447, total time dif 642.4440062046051)
step: 7340 @ episode report: {'average_total_reward': np.float32(6.994445), 'reward_variance': np.float32(3.0143769), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07505549304187298), 'actor_loss': np.float64(-0.9381196439266205), 'hyper_actor_loss': np.float64(0.002162147220224142), 'behavior_loss': np.float64(1.289625310897827)}

Episode step 7350, time diff 0.7984828948974609, total time dif 643.235270023346)
step: 7350 @ episode report: {'average_total_reward': np.float32(6.5700006), 'reward_variance': np.float32(2.5034337), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0800836008042097), 'actor_loss': np.float64(-0.9434945046901703), 'hyper_actor_loss': np.float64(0.0021654476411640646), 'behavior_loss': np.float64(1.4447375655174255)}

Episode step 7360, time diff 0.7739598751068115, total time dif 644.0337529182434)
step: 7360 @ episode report: {'average_total_reward': np.float32(7.78), 'reward_variance': np.float32(2.4655752), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08520877361297607), 'actor_loss': np.float64(-0.9528742015361786), 'hyper_actor_loss': np.float64(0.0021514538675546646), 'behavior_loss': np.float64(1.3122025847434997)}

Episode step 7370, time diff 0.8286614418029785, total time dif 644.8077127933502)
step: 7370 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(2.8062577), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08116782829165459), 'actor_loss': np.float64(-0.9434822142124176), 'hyper_actor_loss': np.float64(0.00224637349601835), 'behavior_loss': np.float64(1.409874439239502)}

Episode step 7380, time diff 0.759850025177002, total time dif 645.6363742351532)
step: 7380 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(2.072934), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08674179017543793), 'actor_loss': np.float64(-0.934486573934555), 'hyper_actor_loss': np.float64(0.0021831321530044077), 'behavior_loss': np.float64(1.3512590408325196)}

Episode step 7390, time diff 0.8340482711791992, total time dif 646.3962242603302)
step: 7390 @ episode report: {'average_total_reward': np.float32(7.2433333), 'reward_variance': np.float32(1.7901833), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08174838945269584), 'actor_loss': np.float64(-0.9509908735752106), 'hyper_actor_loss': np.float64(0.002340694284066558), 'behavior_loss': np.float64(1.3551798343658448)}

Episode step 7400, time diff 0.7712383270263672, total time dif 647.2302725315094)
step: 7400 @ episode report: {'average_total_reward': np.float32(7.716667), 'reward_variance': np.float32(2.6247222), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08259453997015953), 'actor_loss': np.float64(-0.9500950872898102), 'hyper_actor_loss': np.float64(0.0022515275282785296), 'behavior_loss': np.float64(1.3679479241371155)}

Episode step 7410, time diff 0.7636759281158447, total time dif 648.0015108585358)
step: 7410 @ episode report: {'average_total_reward': np.float32(8.08), 'reward_variance': np.float32(1.7675011), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0834351010620594), 'actor_loss': np.float64(-0.9554527282714844), 'hyper_actor_loss': np.float64(0.002131566684693098), 'behavior_loss': np.float64(1.345589029788971)}

Episode step 7420, time diff 0.8016936779022217, total time dif 648.7651867866516)
step: 7420 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(1.2324643), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07504984699189662), 'actor_loss': np.float64(-0.9272607266902924), 'hyper_actor_loss': np.float64(0.0021923773689195513), 'behavior_loss': np.float64(1.2454666256904603)}

Episode step 7430, time diff 0.7867991924285889, total time dif 649.5668804645538)
step: 7430 @ episode report: {'average_total_reward': np.float32(7.104445), 'reward_variance': np.float32(3.117264), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0825983215123415), 'actor_loss': np.float64(-0.94517862200737), 'hyper_actor_loss': np.float64(0.002242793212644756), 'behavior_loss': np.float64(1.3575780749320985)}

Episode step 7440, time diff 0.7303907871246338, total time dif 650.3536796569824)
step: 7440 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(2.326974), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06819181069731713), 'actor_loss': np.float64(-0.9486665070056916), 'hyper_actor_loss': np.float64(0.0021131324232555927), 'behavior_loss': np.float64(1.2906592965126038)}

Episode step 7450, time diff 0.9353175163269043, total time dif 651.084070444107)
step: 7450 @ episode report: {'average_total_reward': np.float32(6.894445), 'reward_variance': np.float32(3.1890924), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08057598359882831), 'actor_loss': np.float64(-0.9441778898239136), 'hyper_actor_loss': np.float64(0.0021138665382750333), 'behavior_loss': np.float64(1.2874330043792725)}

Episode step 7460, time diff 0.7674016952514648, total time dif 652.019387960434)
step: 7460 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(2.567635), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06436010859906674), 'actor_loss': np.float64(-0.9444490671157837), 'hyper_actor_loss': np.float64(0.002181840850971639), 'behavior_loss': np.float64(1.232052755355835)}

Episode step 7470, time diff 0.7546558380126953, total time dif 652.7867896556854)
step: 7470 @ episode report: {'average_total_reward': np.float32(7.131111), 'reward_variance': np.float32(3.3956008), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07527323924005032), 'actor_loss': np.float64(-0.9364901781082153), 'hyper_actor_loss': np.float64(0.0023038648534566162), 'behavior_loss': np.float64(1.3202396988868714)}

Episode step 7480, time diff 0.8002839088439941, total time dif 653.5414454936981)
step: 7480 @ episode report: {'average_total_reward': np.float32(7.78), 'reward_variance': np.float32(0.80021733), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08117552772164345), 'actor_loss': np.float64(-0.955870759487152), 'hyper_actor_loss': np.float64(0.002501825662329793), 'behavior_loss': np.float64(1.34191392660141)}

Episode step 7490, time diff 0.7568769454956055, total time dif 654.3417294025421)
step: 7490 @ episode report: {'average_total_reward': np.float32(8.192223), 'reward_variance': np.float32(1.5277292), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08463969714939594), 'actor_loss': np.float64(-0.9457032740116119), 'hyper_actor_loss': np.float64(0.0026351137319579722), 'behavior_loss': np.float64(1.341895830631256)}

Episode step 7500, time diff 0.7498211860656738, total time dif 655.0986063480377)
step: 7500 @ episode report: {'average_total_reward': np.float32(6.7188888), 'reward_variance': np.float32(2.9104958), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08455687575042248), 'actor_loss': np.float64(-0.961236834526062), 'hyper_actor_loss': np.float64(0.002792111085727811), 'behavior_loss': np.float64(1.3235575675964355)}

Episode step 7510, time diff 0.7781682014465332, total time dif 655.8484275341034)
step: 7510 @ episode report: {'average_total_reward': np.float32(6.918889), 'reward_variance': np.float32(2.149383), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08714264482259751), 'actor_loss': np.float64(-0.9593706727027893), 'hyper_actor_loss': np.float64(0.0030919664539396765), 'behavior_loss': np.float64(1.3185951113700867)}

Episode step 7520, time diff 0.7546429634094238, total time dif 656.6265957355499)
step: 7520 @ episode report: {'average_total_reward': np.float32(6.5577784), 'reward_variance': np.float32(2.2343411), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08471729829907418), 'actor_loss': np.float64(-0.9519361555576324), 'hyper_actor_loss': np.float64(0.003155720978975296), 'behavior_loss': np.float64(1.2681036233901977)}

Episode step 7530, time diff 0.7751855850219727, total time dif 657.3812386989594)
step: 7530 @ episode report: {'average_total_reward': np.float32(6.8433332), 'reward_variance': np.float32(1.437566), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07501922808587551), 'actor_loss': np.float64(-0.9448244452476502), 'hyper_actor_loss': np.float64(0.003169018146581948), 'behavior_loss': np.float64(1.322211492061615)}

Episode step 7540, time diff 0.7993319034576416, total time dif 658.1564242839813)
step: 7540 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(1.5940695), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08148523345589638), 'actor_loss': np.float64(-0.9504079997539521), 'hyper_actor_loss': np.float64(0.003147838031873107), 'behavior_loss': np.float64(1.2383058786392211)}

Episode step 7550, time diff 0.7684121131896973, total time dif 658.955756187439)
step: 7550 @ episode report: {'average_total_reward': np.float32(7.3433332), 'reward_variance': np.float32(2.170135), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0773948922753334), 'actor_loss': np.float64(-0.946315324306488), 'hyper_actor_loss': np.float64(0.002842647535726428), 'behavior_loss': np.float64(1.2922605991363525)}

Episode step 7560, time diff 0.7897980213165283, total time dif 659.7241683006287)
step: 7560 @ episode report: {'average_total_reward': np.float32(6.631111), 'reward_variance': np.float32(2.4587855), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09300102926790714), 'actor_loss': np.float64(-0.9548996746540069), 'hyper_actor_loss': np.float64(0.0024316397495567797), 'behavior_loss': np.float64(1.3016246914863587)}

Episode step 7570, time diff 0.7786061763763428, total time dif 660.5139663219452)
step: 7570 @ episode report: {'average_total_reward': np.float32(6.37), 'reward_variance': np.float32(1.3483719), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07764810398221016), 'actor_loss': np.float64(-0.953756433725357), 'hyper_actor_loss': np.float64(0.0023122367914766074), 'behavior_loss': np.float64(1.2926472902297974)}

Episode step 7580, time diff 0.777184009552002, total time dif 661.2925724983215)
step: 7580 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(1.8335555), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07536462768912315), 'actor_loss': np.float64(-0.9116234958171845), 'hyper_actor_loss': np.float64(0.002088252210523933), 'behavior_loss': np.float64(1.2247411847114562)}

Episode step 7590, time diff 0.8145086765289307, total time dif 662.0697565078735)
step: 7590 @ episode report: {'average_total_reward': np.float32(6.27), 'reward_variance': np.float32(2.8745942), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07641706988215446), 'actor_loss': np.float64(-0.9494184732437134), 'hyper_actor_loss': np.float64(0.002078649157192558), 'behavior_loss': np.float64(1.365816342830658)}

Episode step 7600, time diff 0.7726178169250488, total time dif 662.8842651844025)
step: 7600 @ episode report: {'average_total_reward': np.float32(6.8066664), 'reward_variance': np.float32(1.1477336), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06921694539487362), 'actor_loss': np.float64(-0.9236363351345063), 'hyper_actor_loss': np.float64(0.0021330187446437776), 'behavior_loss': np.float64(1.2752962470054627)}

Episode step 7610, time diff 0.9320805072784424, total time dif 663.6568830013275)
step: 7610 @ episode report: {'average_total_reward': np.float32(6.955556), 'reward_variance': np.float32(1.3257042), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0900486458092928), 'actor_loss': np.float64(-0.9427785575389862), 'hyper_actor_loss': np.float64(0.0020693040336482228), 'behavior_loss': np.float64(1.291795790195465)}

Episode step 7620, time diff 0.768756628036499, total time dif 664.588963508606)
step: 7620 @ episode report: {'average_total_reward': np.float32(7.055556), 'reward_variance': np.float32(1.7053587), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08045794144272804), 'actor_loss': np.float64(-0.9421642780303955), 'hyper_actor_loss': np.float64(0.0019687210442498326), 'behavior_loss': np.float64(1.2652066349983215)}

Episode step 7630, time diff 0.7913789749145508, total time dif 665.3577201366425)
step: 7630 @ episode report: {'average_total_reward': np.float32(6.61889), 'reward_variance': np.float32(1.8869644), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07906125895678998), 'actor_loss': np.float64(-0.9362079083919526), 'hyper_actor_loss': np.float64(0.0020328154088929294), 'behavior_loss': np.float64(1.3822776317596435)}

Episode step 7640, time diff 0.7869706153869629, total time dif 666.149099111557)
step: 7640 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(1.2210977), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08082284815609456), 'actor_loss': np.float64(-0.958230060338974), 'hyper_actor_loss': np.float64(0.0022521797101944683), 'behavior_loss': np.float64(1.3047385454177856)}

Episode step 7650, time diff 0.7770063877105713, total time dif 666.936069726944)
step: 7650 @ episode report: {'average_total_reward': np.float32(6.631111), 'reward_variance': np.float32(1.2787113), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0699489589780569), 'actor_loss': np.float64(-0.9209155738353729), 'hyper_actor_loss': np.float64(0.002404858241789043), 'behavior_loss': np.float64(1.291377955675125)}

Episode step 7660, time diff 0.8032546043395996, total time dif 667.7130761146545)
step: 7660 @ episode report: {'average_total_reward': np.float32(6.618889), 'reward_variance': np.float32(2.9767668), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08299838826060295), 'actor_loss': np.float64(-0.9482555210590362), 'hyper_actor_loss': np.float64(0.0023637416772544382), 'behavior_loss': np.float64(1.2962664842605591)}

Episode step 7670, time diff 0.7725255489349365, total time dif 668.5163307189941)
step: 7670 @ episode report: {'average_total_reward': np.float32(6.5433335), 'reward_variance': np.float32(1.6659863), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09211908504366875), 'actor_loss': np.float64(-0.9779296338558197), 'hyper_actor_loss': np.float64(0.0025928381131961943), 'behavior_loss': np.float64(1.2845232367515564)}

Episode step 7680, time diff 0.795621395111084, total time dif 669.2888562679291)
step: 7680 @ episode report: {'average_total_reward': np.float32(5.2233334), 'reward_variance': np.float32(1.0340359), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08167489096522332), 'actor_loss': np.float64(-0.9503757059574127), 'hyper_actor_loss': np.float64(0.0030034645926207302), 'behavior_loss': np.float64(1.2545311808586121)}

Episode step 7690, time diff 0.7909281253814697, total time dif 670.0844776630402)
step: 7690 @ episode report: {'average_total_reward': np.float32(4.574445), 'reward_variance': np.float32(0.38676667), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07952312976121903), 'actor_loss': np.float64(-0.956906658411026), 'hyper_actor_loss': np.float64(0.003544225404039025), 'behavior_loss': np.float64(1.3722638130187987)}

Episode step 7700, time diff 0.7854862213134766, total time dif 670.8754057884216)
step: 7700 @ episode report: {'average_total_reward': np.float32(4.511111), 'reward_variance': np.float32(0.8605679), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0748790055513382), 'actor_loss': np.float64(-0.9400401711463928), 'hyper_actor_loss': np.float64(0.004152320837602019), 'behavior_loss': np.float64(1.1768009424209596)}

Episode step 7710, time diff 0.7628285884857178, total time dif 671.6608920097351)
step: 7710 @ episode report: {'average_total_reward': np.float32(4.362222), 'reward_variance': np.float32(0.49783197), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07687093727290631), 'actor_loss': np.float64(-0.971148407459259), 'hyper_actor_loss': np.float64(0.004355753352865577), 'behavior_loss': np.float64(1.2649492740631103)}

Episode step 7720, time diff 0.7880089282989502, total time dif 672.4237205982208)
step: 7720 @ episode report: {'average_total_reward': np.float32(4.45), 'reward_variance': np.float32(1.7657839), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08560257367789745), 'actor_loss': np.float64(-0.9333321869373321), 'hyper_actor_loss': np.float64(0.004617839679121971), 'behavior_loss': np.float64(1.337581253051758)}

Episode step 7730, time diff 0.8017642498016357, total time dif 673.2117295265198)
step: 7730 @ episode report: {'average_total_reward': np.float32(4.462222), 'reward_variance': np.float32(1.4041287), 'max_total_reward': np.float32(6.411112), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07742273360490799), 'actor_loss': np.float64(-0.9733146071434021), 'hyper_actor_loss': np.float64(0.004787151422351599), 'behavior_loss': np.float64(1.3134035110473632)}

Episode step 7740, time diff 0.7848901748657227, total time dif 674.0134937763214)
step: 7740 @ episode report: {'average_total_reward': np.float32(4.05), 'reward_variance': np.float32(1.3581295), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07889779135584832), 'actor_loss': np.float64(-0.9334339499473572), 'hyper_actor_loss': np.float64(0.005133018083870411), 'behavior_loss': np.float64(1.4251275062561035)}

Episode step 7750, time diff 0.7816493511199951, total time dif 674.7983839511871)
step: 7750 @ episode report: {'average_total_reward': np.float32(4.2644444), 'reward_variance': np.float32(1.8522669), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06839714422821999), 'actor_loss': np.float64(-0.9205886423587799), 'hyper_actor_loss': np.float64(0.005700027896091342), 'behavior_loss': np.float64(1.3406970500946045)}

Episode step 7760, time diff 0.7937037944793701, total time dif 675.5800333023071)
step: 7760 @ episode report: {'average_total_reward': np.float32(3.8255553), 'reward_variance': np.float32(1.150643), 'max_total_reward': np.float32(6.411111), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07359554395079612), 'actor_loss': np.float64(-0.9697410464286804), 'hyper_actor_loss': np.float64(0.009350175177678466), 'behavior_loss': np.float64(1.3305208683013916)}

Episode step 7770, time diff 0.9554922580718994, total time dif 676.3737370967865)
step: 7770 @ episode report: {'average_total_reward': np.float32(3.376667), 'reward_variance': np.float32(0.5850483), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08274171575903892), 'actor_loss': np.float64(-1.0710469841957093), 'hyper_actor_loss': np.float64(0.016916575934737922), 'behavior_loss': np.float64(1.2017223477363586)}

Episode step 7780, time diff 0.7611420154571533, total time dif 677.3292293548584)
step: 7780 @ episode report: {'average_total_reward': np.float32(2.9155557), 'reward_variance': np.float32(0.20896792), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08236129246652127), 'actor_loss': np.float64(-1.3836332321166993), 'hyper_actor_loss': np.float64(0.02554207816720009), 'behavior_loss': np.float64(1.141423338651657)}

Episode step 7790, time diff 0.7971339225769043, total time dif 678.0903713703156)
step: 7790 @ episode report: {'average_total_reward': np.float32(2.64), 'reward_variance': np.float32(0.37541243), 'max_total_reward': np.float32(3.9222224), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0812775868922472), 'actor_loss': np.float64(-1.6847392678260804), 'hyper_actor_loss': np.float64(0.031783543340861795), 'behavior_loss': np.float64(0.9157538890838623)}

Episode step 7800, time diff 0.8165326118469238, total time dif 678.8875052928925)
step: 7800 @ episode report: {'average_total_reward': np.float32(2.978889), 'reward_variance': np.float32(0.3197889), 'max_total_reward': np.float32(3.9222224), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07901670075953007), 'actor_loss': np.float64(-1.707111668586731), 'hyper_actor_loss': np.float64(0.03623822256922722), 'behavior_loss': np.float64(0.885971587896347)}

Episode step 7810, time diff 0.7724275588989258, total time dif 679.7040379047394)
step: 7810 @ episode report: {'average_total_reward': np.float32(2.8055558), 'reward_variance': np.float32(0.31368524), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07988579347729682), 'actor_loss': np.float64(-1.6038694620132445), 'hyper_actor_loss': np.float64(0.040794627368450166), 'behavior_loss': np.float64(0.8082641065120697)}

Episode step 7820, time diff 0.8106598854064941, total time dif 680.4764654636383)
step: 7820 @ episode report: {'average_total_reward': np.float32(2.3833337), 'reward_variance': np.float32(0.86563605), 'max_total_reward': np.float32(4.411112), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09093399941921235), 'actor_loss': np.float64(-1.6271621942520142), 'hyper_actor_loss': np.float64(0.04420210011303425), 'behavior_loss': np.float64(0.7100207090377808)}

Episode step 7830, time diff 0.7457716464996338, total time dif 681.2871253490448)
step: 7830 @ episode report: {'average_total_reward': np.float32(1.8322222), 'reward_variance': np.float32(0.36786294), 'max_total_reward': np.float32(2.9222221), 'min_total_reward': np.float32(0.9222222), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08264359980821609), 'actor_loss': np.float64(-1.700101613998413), 'hyper_actor_loss': np.float64(0.046860430389642715), 'behavior_loss': np.float64(0.6482280850410461)}

Episode step 7840, time diff 0.7660157680511475, total time dif 682.0328969955444)
step: 7840 @ episode report: {'average_total_reward': np.float32(1.5711112), 'reward_variance': np.float32(0.14444938), 'max_total_reward': np.float32(2.1666667), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08620215207338333), 'actor_loss': np.float64(-1.7752591133117677), 'hyper_actor_loss': np.float64(0.047703180462121964), 'behavior_loss': np.float64(0.6055885016918182)}

Episode step 7850, time diff 0.7739701271057129, total time dif 682.7989127635956)
step: 7850 @ episode report: {'average_total_reward': np.float32(0.7244445), 'reward_variance': np.float32(0.12661235), 'max_total_reward': np.float32(1.1666667), 'min_total_reward': np.float32(-0.07777779), 'average_n_step': np.float32(2.9), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07887751869857311), 'actor_loss': np.float64(-1.8186341285705567), 'hyper_actor_loss': np.float64(0.04759136289358139), 'behavior_loss': np.float64(0.5811181128025055)}

Episode step 7860, time diff 0.7559866905212402, total time dif 683.5728828907013)
step: 7860 @ episode report: {'average_total_reward': np.float32(0.62666667), 'reward_variance': np.float32(0.13714077), 'max_total_reward': np.float32(1.3111112), 'min_total_reward': np.float32(0.044444427), 'average_n_step': np.float32(2.9), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08542764596641064), 'actor_loss': np.float64(-1.8656151652336121), 'hyper_actor_loss': np.float64(0.04702444337308407), 'behavior_loss': np.float64(0.5992808640003204)}

Episode step 7870, time diff 0.7463259696960449, total time dif 684.3288695812225)
step: 7870 @ episode report: {'average_total_reward': np.float32(0.4122222), 'reward_variance': np.float32(0.18566544), 'max_total_reward': np.float32(1.0444444), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.6), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08835969492793083), 'actor_loss': np.float64(-1.5945686459541322), 'hyper_actor_loss': np.float64(0.04698940478265286), 'behavior_loss': np.float64(0.5800110578536988)}

Episode step 7880, time diff 0.7687342166900635, total time dif 685.0751955509186)
step: 7880 @ episode report: {'average_total_reward': np.float32(0.1511111), 'reward_variance': np.float32(0.07790616), 'max_total_reward': np.float32(0.92222226), 'min_total_reward': np.float32(-0.077777795), 'average_n_step': np.float32(2.4), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0923998299986124), 'actor_loss': np.float64(-1.283584177494049), 'hyper_actor_loss': np.float64(0.0482290543615818), 'behavior_loss': np.float64(0.5627928793430328)}

Episode step 7890, time diff 0.756319522857666, total time dif 685.8439297676086)
step: 7890 @ episode report: {'average_total_reward': np.float32(0.028888876), 'reward_variance': np.float32(0.029017285), 'max_total_reward': np.float32(0.3111111), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.4), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09527052864432335), 'actor_loss': np.float64(-1.1647470235824584), 'hyper_actor_loss': np.float64(0.05076683647930622), 'behavior_loss': np.float64(0.5567857027053833)}

Episode step 7900, time diff 0.7896058559417725, total time dif 686.6002492904663)
step: 7900 @ episode report: {'average_total_reward': np.float32(-0.01444446), 'reward_variance': np.float32(0.0141), 'max_total_reward': np.float32(0.3111111), 'min_total_reward': np.float32(-0.077777795), 'average_n_step': np.float32(2.1), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09292948171496392), 'actor_loss': np.float64(-0.97024205327034), 'hyper_actor_loss': np.float64(0.05261695645749569), 'behavior_loss': np.float64(0.5910641252994537)}

Episode step 7910, time diff 0.8031497001647949, total time dif 687.3898551464081)
step: 7910 @ episode report: {'average_total_reward': np.float32(0.28999996), 'reward_variance': np.float32(0.07783828), 'max_total_reward': np.float32(0.92222226), 'min_total_reward': np.float32(-0.07777779), 'average_n_step': np.float32(2.6), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09557691998779774), 'actor_loss': np.float64(-0.8932525336742401), 'hyper_actor_loss': np.float64(0.05437569320201874), 'behavior_loss': np.float64(0.6011911034584045)}

Episode step 7920, time diff 0.7615172863006592, total time dif 688.1930048465729)
step: 7920 @ episode report: {'average_total_reward': np.float32(0.56777775), 'reward_variance': np.float32(0.025245678), 'max_total_reward': np.float32(0.9222222), 'min_total_reward': np.float32(0.43333328), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07449878379702568), 'actor_loss': np.float64(-0.8668286144733429), 'hyper_actor_loss': np.float64(0.05639020018279552), 'behavior_loss': np.float64(0.5998601913452148)}

Episode step 7930, time diff 0.9376122951507568, total time dif 688.9545221328735)
step: 7930 @ episode report: {'average_total_reward': np.float32(1.0633335), 'reward_variance': np.float32(0.5496556), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.3111111), 'average_n_step': np.float32(3.3), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09748476631939411), 'actor_loss': np.float64(-0.8039120495319366), 'hyper_actor_loss': np.float64(0.054911117628216745), 'behavior_loss': np.float64(0.5155501157045365)}

Episode step 7940, time diff 0.7865138053894043, total time dif 689.8921344280243)
step: 7940 @ episode report: {'average_total_reward': np.float32(2.442222), 'reward_variance': np.float32(0.22352593), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(4.3), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09491148963570595), 'actor_loss': np.float64(-0.9501174569129944), 'hyper_actor_loss': np.float64(0.04989586025476456), 'behavior_loss': np.float64(0.4855256944894791)}

Episode step 7950, time diff 0.8236269950866699, total time dif 690.6786482334137)
step: 7950 @ episode report: {'average_total_reward': np.float32(2.891111), 'reward_variance': np.float32(0.4783903), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.1666665), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09270181581377983), 'actor_loss': np.float64(-0.8841767609119415), 'hyper_actor_loss': np.float64(0.044521146640181544), 'behavior_loss': np.float64(0.4886768668889999)}

Episode step 7960, time diff 0.8024418354034424, total time dif 691.5022752285004)
step: 7960 @ episode report: {'average_total_reward': np.float32(4.1011114), 'reward_variance': np.float32(1.4465297), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0786640491336584), 'actor_loss': np.float64(-0.9434862673282624), 'hyper_actor_loss': np.float64(0.040015192702412605), 'behavior_loss': np.float64(0.5336963564157486)}

Episode step 7970, time diff 0.8045859336853027, total time dif 692.3047170639038)
step: 7970 @ episode report: {'average_total_reward': np.float32(5.362222), 'reward_variance': np.float32(0.83644944), 'max_total_reward': np.float32(7.4111114), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07558320350944996), 'actor_loss': np.float64(-0.9154781401157379), 'hyper_actor_loss': np.float64(0.03742838837206364), 'behavior_loss': np.float64(0.5412117719650269)}

Episode step 7980, time diff 0.7794516086578369, total time dif 693.1093029975891)
step: 7980 @ episode report: {'average_total_reward': np.float32(4.5744452), 'reward_variance': np.float32(1.2326679), 'max_total_reward': np.float32(6.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07642856575548648), 'actor_loss': np.float64(-0.9248868286609649), 'hyper_actor_loss': np.float64(0.0352908480912447), 'behavior_loss': np.float64(0.554415512084961)}

Episode step 7990, time diff 0.7925910949707031, total time dif 693.888754606247)
step: 7990 @ episode report: {'average_total_reward': np.float32(5.7577777), 'reward_variance': np.float32(1.2573532), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0862686812877655), 'actor_loss': np.float64(-0.9563333988189697), 'hyper_actor_loss': np.float64(0.033060988411307335), 'behavior_loss': np.float64(0.5752000480890274)}

Episode step 8000, time diff 0.8279995918273926, total time dif 694.6813457012177)
step: 8000 @ episode report: {'average_total_reward': np.float32(5.7211113), 'reward_variance': np.float32(1.6974188), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09332210868597031), 'actor_loss': np.float64(-0.9569154202938079), 'hyper_actor_loss': np.float64(0.030795313976705076), 'behavior_loss': np.float64(0.6065957546234131)}

Episode step 8010, time diff 0.8031356334686279, total time dif 695.509345293045)
step: 8010 @ episode report: {'average_total_reward': np.float32(6.5699997), 'reward_variance': np.float32(1.8929398), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08823434226214885), 'actor_loss': np.float64(-0.927470600605011), 'hyper_actor_loss': np.float64(0.028857885301113127), 'behavior_loss': np.float64(0.6642845809459687)}

Episode step 8020, time diff 0.7854118347167969, total time dif 696.3124809265137)
step: 8020 @ episode report: {'average_total_reward': np.float32(7.9800005), 'reward_variance': np.float32(2.2296734), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07647630758583546), 'actor_loss': np.float64(-0.933326804637909), 'hyper_actor_loss': np.float64(0.026649737544357777), 'behavior_loss': np.float64(0.6465918004512787)}

Episode step 8030, time diff 0.7895021438598633, total time dif 697.0978927612305)
step: 8030 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(0.9986173), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08719597160816192), 'actor_loss': np.float64(-0.9360850393772125), 'hyper_actor_loss': np.float64(0.024552847631275652), 'behavior_loss': np.float64(0.7089761018753051)}

Episode step 8040, time diff 0.7728149890899658, total time dif 697.8873949050903)
step: 8040 @ episode report: {'average_total_reward': np.float32(8.055555), 'reward_variance': np.float32(1.0379999), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08757631927728653), 'actor_loss': np.float64(-0.9374770939350128), 'hyper_actor_loss': np.float64(0.022399157099425793), 'behavior_loss': np.float64(0.7454923033714295)}

Episode step 8050, time diff 0.7680563926696777, total time dif 698.6602098941803)
step: 8050 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(1.7503815), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0840641051530838), 'actor_loss': np.float64(-0.9127404987812042), 'hyper_actor_loss': np.float64(0.02074712924659252), 'behavior_loss': np.float64(0.7478928327560425)}

Episode step 8060, time diff 0.7631044387817383, total time dif 699.42826628685)
step: 8060 @ episode report: {'average_total_reward': np.float32(8.902223), 'reward_variance': np.float32(2.2655752), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09159669578075409), 'actor_loss': np.float64(-0.9593078434467316), 'hyper_actor_loss': np.float64(0.01876054350286722), 'behavior_loss': np.float64(0.7820374548435212)}

Episode step 8070, time diff 0.7491638660430908, total time dif 700.1913707256317)
step: 8070 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(2.0601096), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08228562138974667), 'actor_loss': np.float64(-0.9371067643165588), 'hyper_actor_loss': np.float64(0.01682712957262993), 'behavior_loss': np.float64(0.793720155954361)}

Episode step 8080, time diff 0.7672460079193115, total time dif 700.9405345916748)
step: 8080 @ episode report: {'average_total_reward': np.float32(8.002222), 'reward_variance': np.float32(2.3616498), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0906555786728859), 'actor_loss': np.float64(-0.9279606878757477), 'hyper_actor_loss': np.float64(0.01540944203734398), 'behavior_loss': np.float64(0.8286051571369171)}

Episode step 8090, time diff 0.8945159912109375, total time dif 701.7077805995941)
step: 8090 @ episode report: {'average_total_reward': np.float32(8.665556), 'reward_variance': np.float32(1.8580115), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08314025439321995), 'actor_loss': np.float64(-0.9419726371765137), 'hyper_actor_loss': np.float64(0.014157859887927771), 'behavior_loss': np.float64(0.8716020822525025)}

Episode step 8100, time diff 0.7320699691772461, total time dif 702.602296590805)
step: 8100 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(7.373983), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07886469140648841), 'actor_loss': np.float64(-0.9099715769290924), 'hyper_actor_loss': np.float64(0.013032056111842393), 'behavior_loss': np.float64(0.8542706429958343)}

Episode step 8110, time diff 0.7423579692840576, total time dif 703.3343665599823)
step: 8110 @ episode report: {'average_total_reward': np.float32(7.1922226), 'reward_variance': np.float32(1.7247422), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09171632751822471), 'actor_loss': np.float64(-0.9412951469421387), 'hyper_actor_loss': np.float64(0.012213442381471396), 'behavior_loss': np.float64(0.8587231576442719)}

Episode step 8120, time diff 0.7795794010162354, total time dif 704.0767245292664)
step: 8120 @ episode report: {'average_total_reward': np.float32(8.765556), 'reward_variance': np.float32(5.4717402), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08711728192865849), 'actor_loss': np.float64(-0.9479233026504517), 'hyper_actor_loss': np.float64(0.011102058831602335), 'behavior_loss': np.float64(0.924716341495514)}

Episode step 8130, time diff 0.7791965007781982, total time dif 704.8563039302826)
step: 8130 @ episode report: {'average_total_reward': np.float32(7.416667), 'reward_variance': np.float32(2.5586739), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08211425729095936), 'actor_loss': np.float64(-0.8997036695480347), 'hyper_actor_loss': np.float64(0.010227077640593052), 'behavior_loss': np.float64(0.960884815454483)}

Episode step 8140, time diff 0.7827279567718506, total time dif 705.6355004310608)
step: 8140 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(1.4310877), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07942057810723782), 'actor_loss': np.float64(-0.9277037978172302), 'hyper_actor_loss': np.float64(0.009483206178992986), 'behavior_loss': np.float64(0.9467580020427704)}

Episode step 8150, time diff 0.7693626880645752, total time dif 706.4182283878326)
step: 8150 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(2.0050483), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08555099219083787), 'actor_loss': np.float64(-0.9372351229190826), 'hyper_actor_loss': np.float64(0.009142582491040229), 'behavior_loss': np.float64(0.8871634721755981)}

Episode step 8160, time diff 0.7788197994232178, total time dif 707.1875910758972)
step: 8160 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(3.4687703), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08148143962025642), 'actor_loss': np.float64(-0.9403381526470185), 'hyper_actor_loss': np.float64(0.008317280653864145), 'behavior_loss': np.float64(0.9209548175334931)}

Episode step 8170, time diff 0.7962849140167236, total time dif 707.9664108753204)
step: 8170 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(1.2943017), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0904230758547783), 'actor_loss': np.float64(-0.9601936101913452), 'hyper_actor_loss': np.float64(0.008176254108548164), 'behavior_loss': np.float64(0.954550588130951)}

Episode step 8180, time diff 0.7550711631774902, total time dif 708.7626957893372)
step: 8180 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(3.3457985), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08568054474890233), 'actor_loss': np.float64(-0.9070743560791016), 'hyper_actor_loss': np.float64(0.007576035289093852), 'behavior_loss': np.float64(0.934012258052826)}

Episode step 8190, time diff 0.8016278743743896, total time dif 709.5177669525146)
step: 8190 @ episode report: {'average_total_reward': np.float32(9.002222), 'reward_variance': np.float32(3.82002), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.288889), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08693314641714096), 'actor_loss': np.float64(-0.9479590594768524), 'hyper_actor_loss': np.float64(0.007154346304014325), 'behavior_loss': np.float64(0.9490565359592438)}

Episode step 8200, time diff 0.7739996910095215, total time dif 710.319394826889)
step: 8200 @ episode report: {'average_total_reward': np.float32(9.090001), 'reward_variance': np.float32(1.1960357), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08661431893706321), 'actor_loss': np.float64(-0.9599708259105683), 'hyper_actor_loss': np.float64(0.00702998205088079), 'behavior_loss': np.float64(0.898559284210205)}

Episode step 8210, time diff 0.792529821395874, total time dif 711.0933945178986)
step: 8210 @ episode report: {'average_total_reward': np.float32(7.79), 'reward_variance': np.float32(2.3859363), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08563161864876748), 'actor_loss': np.float64(-0.9321312308311462), 'hyper_actor_loss': np.float64(0.0064330262131989), 'behavior_loss': np.float64(0.9744560241699218)}

Episode step 8220, time diff 0.7860770225524902, total time dif 711.8859243392944)
step: 8220 @ episode report: {'average_total_reward': np.float32(7.953334), 'reward_variance': np.float32(2.6420445), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08248960636556149), 'actor_loss': np.float64(-0.9416399955749511), 'hyper_actor_loss': np.float64(0.0062651438638567924), 'behavior_loss': np.float64(0.9812381386756897)}

Episode step 8230, time diff 0.7917420864105225, total time dif 712.6720013618469)
step: 8230 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(1.56534), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08951848931610584), 'actor_loss': np.float64(-0.9401101171970367), 'hyper_actor_loss': np.float64(0.005807935213670135), 'behavior_loss': np.float64(0.9851021111011505)}

Episode step 8240, time diff 0.7793903350830078, total time dif 713.4637434482574)
step: 8240 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(1.6251224), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08450719825923443), 'actor_loss': np.float64(-0.9314984500408172), 'hyper_actor_loss': np.float64(0.006044891104102135), 'behavior_loss': np.float64(0.9757353127002716)}

Episode step 8250, time diff 0.9607303142547607, total time dif 714.2431337833405)
step: 8250 @ episode report: {'average_total_reward': np.float32(8.602222), 'reward_variance': np.float32(2.9841924), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08168275877833367), 'actor_loss': np.float64(-0.9478240668773651), 'hyper_actor_loss': np.float64(0.0054727612063288685), 'behavior_loss': np.float64(0.9697552502155304)}

Episode step 8260, time diff 0.8450088500976562, total time dif 715.2038640975952)
step: 8260 @ episode report: {'average_total_reward': np.float32(8.628889), 'reward_variance': np.float32(3.8270175), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07618021331727505), 'actor_loss': np.float64(-0.9202334821224213), 'hyper_actor_loss': np.float64(0.005243422882631421), 'behavior_loss': np.float64(0.9684603929519653)}

Episode step 8270, time diff 0.7993860244750977, total time dif 716.0488729476929)
step: 8270 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(4.0051904), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08726937398314476), 'actor_loss': np.float64(-0.9358502566814423), 'hyper_actor_loss': np.float64(0.005072512617334724), 'behavior_loss': np.float64(0.982651311159134)}

Episode step 8280, time diff 0.8026602268218994, total time dif 716.848258972168)
step: 8280 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.0805442), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07547554187476635), 'actor_loss': np.float64(-0.9413616240024567), 'hyper_actor_loss': np.float64(0.005357410991564393), 'behavior_loss': np.float64(0.9466539204120636)}

Episode step 8290, time diff 0.7924556732177734, total time dif 717.6509191989899)
step: 8290 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(1.1064695), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08205470368266106), 'actor_loss': np.float64(-0.9484294533729554), 'hyper_actor_loss': np.float64(0.005346616031602025), 'behavior_loss': np.float64(0.9799585163593292)}

Episode step 8300, time diff 0.8053910732269287, total time dif 718.4433748722076)
step: 8300 @ episode report: {'average_total_reward': np.float32(7.8922224), 'reward_variance': np.float32(1.9145943), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08901977315545082), 'actor_loss': np.float64(-0.9525625050067902), 'hyper_actor_loss': np.float64(0.005583854159340262), 'behavior_loss': np.float64(0.9026840627193451)}

Episode step 8310, time diff 0.8117251396179199, total time dif 719.2487659454346)
step: 8310 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(2.7575495), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0931909054517746), 'actor_loss': np.float64(-0.9778836011886597), 'hyper_actor_loss': np.float64(0.005605871556326747), 'behavior_loss': np.float64(0.9324635207653046)}

Episode step 8320, time diff 0.7881066799163818, total time dif 720.0604910850525)
step: 8320 @ episode report: {'average_total_reward': np.float32(7.8655562), 'reward_variance': np.float32(2.284357), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0740048997104168), 'actor_loss': np.float64(-0.9408991992473602), 'hyper_actor_loss': np.float64(0.005573696969076991), 'behavior_loss': np.float64(0.9276615619659424)}

Episode step 8330, time diff 0.8028228282928467, total time dif 720.8485977649689)
step: 8330 @ episode report: {'average_total_reward': np.float32(8.914445), 'reward_variance': np.float32(2.164644), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08731724992394448), 'actor_loss': np.float64(-0.9367772579193115), 'hyper_actor_loss': np.float64(0.005620808387175202), 'behavior_loss': np.float64(0.9392008125782013)}

Episode step 8340, time diff 0.780322790145874, total time dif 721.6514205932617)
step: 8340 @ episode report: {'average_total_reward': np.float32(8.990001), 'reward_variance': np.float32(1.6158378), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09347614794969558), 'actor_loss': np.float64(-0.9907333552837372), 'hyper_actor_loss': np.float64(0.005502485809847713), 'behavior_loss': np.float64(0.9217837989330292)}

Episode step 8350, time diff 0.7955071926116943, total time dif 722.4317433834076)
step: 8350 @ episode report: {'average_total_reward': np.float32(8.653334), 'reward_variance': np.float32(1.3972294), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09250702410936355), 'actor_loss': np.float64(-0.9565145194530487), 'hyper_actor_loss': np.float64(0.005318799940869212), 'behavior_loss': np.float64(0.9394216775894165)}

Episode step 8360, time diff 0.7368650436401367, total time dif 723.2272505760193)
step: 8360 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(2.2479057), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08585165739059449), 'actor_loss': np.float64(-0.9217336714267731), 'hyper_actor_loss': np.float64(0.004928301181644201), 'behavior_loss': np.float64(0.9704863846302032)}

Episode step 8370, time diff 0.7539265155792236, total time dif 723.9641156196594)
step: 8370 @ episode report: {'average_total_reward': np.float32(8.614445), 'reward_variance': np.float32(2.9928164), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08801486641168595), 'actor_loss': np.float64(-0.9310264229774475), 'hyper_actor_loss': np.float64(0.004894663253799081), 'behavior_loss': np.float64(0.9562688648700715)}

Episode step 8380, time diff 0.7817544937133789, total time dif 724.7180421352386)
step: 8380 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(2.3637328), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08476265780627727), 'actor_loss': np.float64(-0.9465616405010223), 'hyper_actor_loss': np.float64(0.004679128993302584), 'behavior_loss': np.float64(0.9642850935459137)}

Episode step 8390, time diff 0.7927136421203613, total time dif 725.499796628952)
step: 8390 @ episode report: {'average_total_reward': np.float32(7.267778), 'reward_variance': np.float32(1.4097391), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09015626162290573), 'actor_loss': np.float64(-0.9543553113937377), 'hyper_actor_loss': np.float64(0.004515831544995308), 'behavior_loss': np.float64(0.9781114399433136)}

Episode step 8400, time diff 0.7816088199615479, total time dif 726.2925102710724)
step: 8400 @ episode report: {'average_total_reward': np.float32(7.4044447), 'reward_variance': np.float32(2.79808), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07230235263705254), 'actor_loss': np.float64(-0.9029051840305329), 'hyper_actor_loss': np.float64(0.004451892198994755), 'behavior_loss': np.float64(0.9824372947216033)}

Episode step 8410, time diff 1.1016499996185303, total time dif 727.0741190910339)
step: 8410 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(3.1507907), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07875635251402854), 'actor_loss': np.float64(-0.9296219229698182), 'hyper_actor_loss': np.float64(0.004279147181659937), 'behavior_loss': np.float64(0.9743058621883393)}

Episode step 8420, time diff 0.7424781322479248, total time dif 728.1757690906525)
step: 8420 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(1.6750677), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08676631450653076), 'actor_loss': np.float64(-0.9679409146308899), 'hyper_actor_loss': np.float64(0.004081415268592537), 'behavior_loss': np.float64(0.9145429313182831)}

Episode step 8430, time diff 0.8236808776855469, total time dif 728.9182472229004)
step: 8430 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(3.932595), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09747124835848808), 'actor_loss': np.float64(-0.9388955891132355), 'hyper_actor_loss': np.float64(0.004050886421464383), 'behavior_loss': np.float64(0.961816668510437)}

Episode step 8440, time diff 0.7789633274078369, total time dif 729.7419281005859)
step: 8440 @ episode report: {'average_total_reward': np.float32(7.8411117), 'reward_variance': np.float32(2.091038), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09068805053830147), 'actor_loss': np.float64(-0.9526096820831299), 'hyper_actor_loss': np.float64(0.004075276060029864), 'behavior_loss': np.float64(0.9698756396770477)}

Episode step 8450, time diff 0.7630999088287354, total time dif 730.5208914279938)
step: 8450 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(1.348174), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07803487069904805), 'actor_loss': np.float64(-0.9511931717395783), 'hyper_actor_loss': np.float64(0.003867568029090762), 'behavior_loss': np.float64(0.914363706111908)}

Episode step 8460, time diff 0.76120924949646, total time dif 731.2839913368225)
step: 8460 @ episode report: {'average_total_reward': np.float32(7.916667), 'reward_variance': np.float32(1.568253), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08552868850529194), 'actor_loss': np.float64(-0.9561276376247406), 'hyper_actor_loss': np.float64(0.004002620349638164), 'behavior_loss': np.float64(0.90162872672081)}

Episode step 8470, time diff 0.7915303707122803, total time dif 732.045200586319)
step: 8470 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(3.8737388), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08298855423927307), 'actor_loss': np.float64(-0.9288220643997193), 'hyper_actor_loss': np.float64(0.0040651008021086454), 'behavior_loss': np.float64(0.9586542427539826)}

Episode step 8480, time diff 0.7883975505828857, total time dif 732.8367309570312)
step: 8480 @ episode report: {'average_total_reward': np.float32(8.79), 'reward_variance': np.float32(1.2667027), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09215282052755355), 'actor_loss': np.float64(-0.9478739321231842), 'hyper_actor_loss': np.float64(0.003947601653635502), 'behavior_loss': np.float64(0.9341112375259399)}

Episode step 8490, time diff 0.7613406181335449, total time dif 733.6251285076141)
step: 8490 @ episode report: {'average_total_reward': np.float32(8.128889), 'reward_variance': np.float32(2.8164005), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.1666665), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09110298380255699), 'actor_loss': np.float64(-0.9689895629882812), 'hyper_actor_loss': np.float64(0.0038494949927553534), 'behavior_loss': np.float64(0.9450661897659302)}

Episode step 8500, time diff 0.7733726501464844, total time dif 734.3864691257477)
step: 8500 @ episode report: {'average_total_reward': np.float32(9.114446), 'reward_variance': np.float32(2.8884454), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09957242906093597), 'actor_loss': np.float64(-0.9467479348182678), 'hyper_actor_loss': np.float64(0.0038366654189303517), 'behavior_loss': np.float64(0.9663146495819092)}

Episode step 8510, time diff 0.7580089569091797, total time dif 735.1598417758942)
step: 8510 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(2.7179036), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08313994705677033), 'actor_loss': np.float64(-0.9554761826992035), 'hyper_actor_loss': np.float64(0.0036679649027064444), 'behavior_loss': np.float64(0.9512260913848877)}

Episode step 8520, time diff 0.7791562080383301, total time dif 735.9178507328033)
step: 8520 @ episode report: {'average_total_reward': np.float32(8.402223), 'reward_variance': np.float32(3.1014028), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08507048040628433), 'actor_loss': np.float64(-0.9210342705249787), 'hyper_actor_loss': np.float64(0.0036551473662257195), 'behavior_loss': np.float64(0.9925390839576721)}

Episode step 8530, time diff 0.7386062145233154, total time dif 736.6970069408417)
step: 8530 @ episode report: {'average_total_reward': np.float32(8.465556), 'reward_variance': np.float32(1.4968017), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09368515908718109), 'actor_loss': np.float64(-0.956112265586853), 'hyper_actor_loss': np.float64(0.003777135699056089), 'behavior_loss': np.float64(0.9817956268787384)}

Episode step 8540, time diff 0.7823073863983154, total time dif 737.435613155365)
step: 8540 @ episode report: {'average_total_reward': np.float32(7.916667), 'reward_variance': np.float32(1.6919329), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07700619548559189), 'actor_loss': np.float64(-0.9205702424049378), 'hyper_actor_loss': np.float64(0.00402428915258497), 'behavior_loss': np.float64(0.9734546661376953)}

Episode step 8550, time diff 0.7848329544067383, total time dif 738.2179205417633)
step: 8550 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(3.4904385), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08465459272265434), 'actor_loss': np.float64(-0.9196008384227753), 'hyper_actor_loss': np.float64(0.004256076063029468), 'behavior_loss': np.float64(0.8781394302845001)}

Episode step 8560, time diff 0.7747359275817871, total time dif 739.00275349617)
step: 8560 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(2.5991416), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09037004858255386), 'actor_loss': np.float64(-0.9656065881252289), 'hyper_actor_loss': np.float64(0.004634433984756469), 'behavior_loss': np.float64(0.8970476865768433)}

Episode step 8570, time diff 0.9083347320556641, total time dif 739.7774894237518)
step: 8570 @ episode report: {'average_total_reward': np.float32(8.638889), 'reward_variance': np.float32(1.223117), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09921281337738037), 'actor_loss': np.float64(-0.9540633141994477), 'hyper_actor_loss': np.float64(0.004646945372223854), 'behavior_loss': np.float64(0.8705642819404602)}

Episode step 8580, time diff 0.7669546604156494, total time dif 740.6858241558075)
step: 8580 @ episode report: {'average_total_reward': np.float32(8.165556), 'reward_variance': np.float32(1.4586285), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08655139803886414), 'actor_loss': np.float64(-0.9752188205718995), 'hyper_actor_loss': np.float64(0.004595746193081141), 'behavior_loss': np.float64(0.8364892959594726)}

Episode step 8590, time diff 0.7876050472259521, total time dif 741.4527788162231)
step: 8590 @ episode report: {'average_total_reward': np.float32(9.177778), 'reward_variance': np.float32(1.626049), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08305708542466164), 'actor_loss': np.float64(-0.9318110883235932), 'hyper_actor_loss': np.float64(0.004307076521217823), 'behavior_loss': np.float64(0.8992704093456269)}

Episode step 8600, time diff 0.7835404872894287, total time dif 742.2403838634491)
step: 8600 @ episode report: {'average_total_reward': np.float32(8.004445), 'reward_variance': np.float32(0.239116), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07661356665194034), 'actor_loss': np.float64(-0.9445165455341339), 'hyper_actor_loss': np.float64(0.004235869063995779), 'behavior_loss': np.float64(0.9218395292758942)}

Episode step 8610, time diff 0.7745544910430908, total time dif 743.0239243507385)
step: 8610 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.1986679), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09436394944787026), 'actor_loss': np.float64(-0.9466013014316559), 'hyper_actor_loss': np.float64(0.004116766736842692), 'behavior_loss': np.float64(0.9729871153831482)}

Episode step 8620, time diff 0.7993793487548828, total time dif 743.7984788417816)
step: 8620 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(1.5682485), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07513515166938305), 'actor_loss': np.float64(-0.9393498063087463), 'hyper_actor_loss': np.float64(0.004243923397734761), 'behavior_loss': np.float64(0.897911137342453)}

Episode step 8630, time diff 0.7978329658508301, total time dif 744.5978581905365)
step: 8630 @ episode report: {'average_total_reward': np.float32(7.553334), 'reward_variance': np.float32(2.6409588), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08218755424022675), 'actor_loss': np.float64(-0.9271309316158295), 'hyper_actor_loss': np.float64(0.0039747243514284495), 'behavior_loss': np.float64(0.9412541031837464)}

Episode step 8640, time diff 0.7840542793273926, total time dif 745.3956911563873)
step: 8640 @ episode report: {'average_total_reward': np.float32(8.165556), 'reward_variance': np.float32(0.785295), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08359919562935829), 'actor_loss': np.float64(-0.9383468687534332), 'hyper_actor_loss': np.float64(0.004009204055182636), 'behavior_loss': np.float64(0.9226345002651215)}

Episode step 8650, time diff 0.7754185199737549, total time dif 746.1797454357147)
step: 8650 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(2.6526535), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07848054766654969), 'actor_loss': np.float64(-0.9484059274196625), 'hyper_actor_loss': np.float64(0.004116077930666507), 'behavior_loss': np.float64(0.8782227516174317)}

Episode step 8660, time diff 0.775393009185791, total time dif 746.9551639556885)
step: 8660 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.9278581), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08037892803549766), 'actor_loss': np.float64(-0.9381108999252319), 'hyper_actor_loss': np.float64(0.004026551567949355), 'behavior_loss': np.float64(0.9012430310249329)}

Episode step 8670, time diff 0.7394204139709473, total time dif 747.7305569648743)
step: 8670 @ episode report: {'average_total_reward': np.float32(9.3144455), 'reward_variance': np.float32(3.6220512), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08531731851398945), 'actor_loss': np.float64(-0.9428202867507934), 'hyper_actor_loss': np.float64(0.004032414243556559), 'behavior_loss': np.float64(0.904601228237152)}

Episode step 8680, time diff 0.7882850170135498, total time dif 748.4699773788452)
step: 8680 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(5.294036), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09935502260923386), 'actor_loss': np.float64(-0.9876718759536743), 'hyper_actor_loss': np.float64(0.003943315218202769), 'behavior_loss': np.float64(0.9098688960075378)}

Episode step 8690, time diff 0.754267692565918, total time dif 749.2582623958588)
step: 8690 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(2.8062534), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09083790369331837), 'actor_loss': np.float64(-0.9364534854888916), 'hyper_actor_loss': np.float64(0.004201732762157917), 'behavior_loss': np.float64(0.9093936204910278)}

Episode step 8700, time diff 0.7745771408081055, total time dif 750.0125300884247)
step: 8700 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.8636059), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09262505397200585), 'actor_loss': np.float64(-0.9696845889091492), 'hyper_actor_loss': np.float64(0.004295417037792504), 'behavior_loss': np.float64(0.9011753559112549)}

Episode step 8710, time diff 0.7929284572601318, total time dif 750.7871072292328)
step: 8710 @ episode report: {'average_total_reward': np.float32(7.853334), 'reward_variance': np.float32(4.6758966), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07997026406228543), 'actor_loss': np.float64(-0.9504653573036194), 'hyper_actor_loss': np.float64(0.004387086816132068), 'behavior_loss': np.float64(0.8984532535076142)}

Episode step 8720, time diff 0.748748779296875, total time dif 751.5800356864929)
step: 8720 @ episode report: {'average_total_reward': np.float32(8.041111), 'reward_variance': np.float32(1.5064704), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08705216646194458), 'actor_loss': np.float64(-0.9503005564212799), 'hyper_actor_loss': np.float64(0.00474409987218678), 'behavior_loss': np.float64(0.8858416557312012)}

Episode step 8730, time diff 0.9377515316009521, total time dif 752.3287844657898)
step: 8730 @ episode report: {'average_total_reward': np.float32(7.8922224), 'reward_variance': np.float32(2.1694589), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10238264054059983), 'actor_loss': np.float64(-0.9865638673305511), 'hyper_actor_loss': np.float64(0.004668173100799322), 'behavior_loss': np.float64(0.8658865690231323)}

Episode step 8740, time diff 0.7327001094818115, total time dif 753.2665359973907)
step: 8740 @ episode report: {'average_total_reward': np.float32(7.8288903), 'reward_variance': np.float32(1.2295609), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(6.288889), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09018085300922393), 'actor_loss': np.float64(-0.9770546734333039), 'hyper_actor_loss': np.float64(0.00454892567358911), 'behavior_loss': np.float64(0.9071537494659424)}

Episode step 8750, time diff 0.7554507255554199, total time dif 753.9992361068726)
step: 8750 @ episode report: {'average_total_reward': np.float32(7.916667), 'reward_variance': np.float32(1.226105), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07478931695222854), 'actor_loss': np.float64(-0.9169838309288025), 'hyper_actor_loss': np.float64(0.004426311561837792), 'behavior_loss': np.float64(0.9314041852951049)}

Episode step 8760, time diff 0.7743027210235596, total time dif 754.754686832428)
step: 8760 @ episode report: {'average_total_reward': np.float32(7.8166666), 'reward_variance': np.float32(2.2261791), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09946382492780685), 'actor_loss': np.float64(-0.9542945265769959), 'hyper_actor_loss': np.float64(0.004475820949301124), 'behavior_loss': np.float64(0.9056975305080414)}

Episode step 8770, time diff 0.8021986484527588, total time dif 755.5289895534515)
step: 8770 @ episode report: {'average_total_reward': np.float32(7.7411118), 'reward_variance': np.float32(1.7068905), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09146614857017994), 'actor_loss': np.float64(-0.981693035364151), 'hyper_actor_loss': np.float64(0.004528468986973166), 'behavior_loss': np.float64(0.9215309500694275)}

Episode step 8780, time diff 0.762033224105835, total time dif 756.3311882019043)
step: 8780 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(2.523987), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07498314455151558), 'actor_loss': np.float64(-0.9162554502487182), 'hyper_actor_loss': np.float64(0.00436592516489327), 'behavior_loss': np.float64(0.89277583360672)}

Episode step 8790, time diff 0.7578370571136475, total time dif 757.0932214260101)
step: 8790 @ episode report: {'average_total_reward': np.float32(7.5555563), 'reward_variance': np.float32(1.958346), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08747557774186135), 'actor_loss': np.float64(-0.9605490803718567), 'hyper_actor_loss': np.float64(0.004185446840710938), 'behavior_loss': np.float64(0.8740319550037384)}

Episode step 8800, time diff 0.7442076206207275, total time dif 757.8510584831238)
step: 8800 @ episode report: {'average_total_reward': np.float32(7.2433333), 'reward_variance': np.float32(0.76872694), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08561794683337212), 'actor_loss': np.float64(-0.9547263145446777), 'hyper_actor_loss': np.float64(0.004050205531530082), 'behavior_loss': np.float64(0.9181414008140564)}

Episode step 8810, time diff 0.773740291595459, total time dif 758.5952661037445)
step: 8810 @ episode report: {'average_total_reward': np.float32(7.5922227), 'reward_variance': np.float32(3.6406932), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08434564024209976), 'actor_loss': np.float64(-0.912674343585968), 'hyper_actor_loss': np.float64(0.003947626939043403), 'behavior_loss': np.float64(0.9327066123485566)}

Episode step 8820, time diff 0.7947657108306885, total time dif 759.36900639534)
step: 8820 @ episode report: {'average_total_reward': np.float32(7.941111), 'reward_variance': np.float32(3.4965692), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08936869576573372), 'actor_loss': np.float64(-0.9605623364448548), 'hyper_actor_loss': np.float64(0.0038519931957125665), 'behavior_loss': np.float64(0.9297106087207794)}

Episode step 8830, time diff 0.7924225330352783, total time dif 760.1637721061707)
step: 8830 @ episode report: {'average_total_reward': np.float32(7.4188895), 'reward_variance': np.float32(4.299187), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08634404018521309), 'actor_loss': np.float64(-0.9502381443977356), 'hyper_actor_loss': np.float64(0.00404464490711689), 'behavior_loss': np.float64(0.9469222307205201)}

Episode step 8840, time diff 0.7692503929138184, total time dif 760.9561946392059)
step: 8840 @ episode report: {'average_total_reward': np.float32(8.016667), 'reward_variance': np.float32(1.4090188), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08462138697504998), 'actor_loss': np.float64(-0.9583985209465027), 'hyper_actor_loss': np.float64(0.004019729024730623), 'behavior_loss': np.float64(0.8868846297264099)}

Episode step 8850, time diff 0.7770543098449707, total time dif 761.7254450321198)
step: 8850 @ episode report: {'average_total_reward': np.float32(8.016667), 'reward_variance': np.float32(3.94771), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0760639451444149), 'actor_loss': np.float64(-0.9461505889892579), 'hyper_actor_loss': np.float64(0.0038734253961592914), 'behavior_loss': np.float64(0.8650954842567444)}

Episode step 8860, time diff 0.8213064670562744, total time dif 762.5024993419647)
step: 8860 @ episode report: {'average_total_reward': np.float32(7.2433333), 'reward_variance': np.float32(2.8111718), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09391377717256547), 'actor_loss': np.float64(-0.959618866443634), 'hyper_actor_loss': np.float64(0.003936070878989994), 'behavior_loss': np.float64(0.9010872662067413)}

Episode step 8870, time diff 0.7780709266662598, total time dif 763.323805809021)
step: 8870 @ episode report: {'average_total_reward': np.float32(8.39), 'reward_variance': np.float32(3.4875913), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08773578330874443), 'actor_loss': np.float64(-0.9933564782142639), 'hyper_actor_loss': np.float64(0.00410393769852817), 'behavior_loss': np.float64(0.8480838656425476)}

Episode step 8880, time diff 0.7614536285400391, total time dif 764.1018767356873)
step: 8880 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(1.8995432), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08915451113134623), 'actor_loss': np.float64(-0.9641964852809906), 'hyper_actor_loss': np.float64(0.00423086246009916), 'behavior_loss': np.float64(0.8746010065078735)}

Episode step 8890, time diff 0.9365947246551514, total time dif 764.8633303642273)
step: 8890 @ episode report: {'average_total_reward': np.float32(7.6288886), 'reward_variance': np.float32(2.101807), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08095297440886498), 'actor_loss': np.float64(-0.963003259897232), 'hyper_actor_loss': np.float64(0.00418570009060204), 'behavior_loss': np.float64(0.8093094885349273)}

Episode step 8900, time diff 0.7974705696105957, total time dif 765.7999250888824)
step: 8900 @ episode report: {'average_total_reward': np.float32(8.553334), 'reward_variance': np.float32(2.1217232), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07445553429424763), 'actor_loss': np.float64(-0.9439847886562347), 'hyper_actor_loss': np.float64(0.004626411711797118), 'behavior_loss': np.float64(0.8536859929561615)}

Episode step 8910, time diff 0.8186256885528564, total time dif 766.597395658493)
step: 8910 @ episode report: {'average_total_reward': np.float32(7.404444), 'reward_variance': np.float32(3.3217576), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07994698099792004), 'actor_loss': np.float64(-0.9519420564174652), 'hyper_actor_loss': np.float64(0.004564188700169325), 'behavior_loss': np.float64(0.822707349061966)}

Episode step 8920, time diff 0.785980224609375, total time dif 767.4160213470459)
step: 8920 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(4.0119452), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09787717908620834), 'actor_loss': np.float64(-0.9627824544906616), 'hyper_actor_loss': np.float64(0.0045814862009137865), 'behavior_loss': np.float64(0.887857186794281)}

Episode step 8930, time diff 0.7660996913909912, total time dif 768.2020015716553)
step: 8930 @ episode report: {'average_total_reward': np.float32(7.216667), 'reward_variance': np.float32(3.2964509), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08522078394889832), 'actor_loss': np.float64(-0.9596444547176362), 'hyper_actor_loss': np.float64(0.004556991532444954), 'behavior_loss': np.float64(0.8294771254062653)}

Episode step 8940, time diff 0.7859930992126465, total time dif 768.9681012630463)
step: 8940 @ episode report: {'average_total_reward': np.float32(8.316668), 'reward_variance': np.float32(2.2267969), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07661797590553761), 'actor_loss': np.float64(-0.9307156026363372), 'hyper_actor_loss': np.float64(0.004198700468987226), 'behavior_loss': np.float64(0.8655489563941956)}

Episode step 8950, time diff 0.79123854637146, total time dif 769.7540943622589)
step: 8950 @ episode report: {'average_total_reward': np.float32(7.5922227), 'reward_variance': np.float32(1.3443717), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08365408107638359), 'actor_loss': np.float64(-0.941589993238449), 'hyper_actor_loss': np.float64(0.0040514797670766715), 'behavior_loss': np.float64(0.8732366442680359)}

Episode step 8960, time diff 0.7560009956359863, total time dif 770.5453329086304)
step: 8960 @ episode report: {'average_total_reward': np.float32(7.9044447), 'reward_variance': np.float32(1.4886963), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09020182453095912), 'actor_loss': np.float64(-0.9607394278049469), 'hyper_actor_loss': np.float64(0.0038257412845268844), 'behavior_loss': np.float64(0.8454605281352997)}

Episode step 8970, time diff 0.7356069087982178, total time dif 771.3013339042664)
step: 8970 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(3.7965991), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08344817161560059), 'actor_loss': np.float64(-0.9597777128219604), 'hyper_actor_loss': np.float64(0.0036276815924793484), 'behavior_loss': np.float64(0.8872319996356964)}

Episode step 8980, time diff 0.7820024490356445, total time dif 772.0369408130646)
step: 8980 @ episode report: {'average_total_reward': np.float32(6.6577783), 'reward_variance': np.float32(0.97876054), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07914703413844108), 'actor_loss': np.float64(-0.9206079006195068), 'hyper_actor_loss': np.float64(0.003757041390053928), 'behavior_loss': np.float64(0.9090380012989044)}

Episode step 8990, time diff 0.8058505058288574, total time dif 772.8189432621002)
step: 8990 @ episode report: {'average_total_reward': np.float32(7.8288894), 'reward_variance': np.float32(2.0664997), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08458730317652226), 'actor_loss': np.float64(-0.9566921055316925), 'hyper_actor_loss': np.float64(0.0036383148515596988), 'behavior_loss': np.float64(0.8712773740291595)}

Episode step 9000, time diff 0.775399923324585, total time dif 773.6247937679291)
step: 9000 @ episode report: {'average_total_reward': np.float32(7.555556), 'reward_variance': np.float32(1.4850119), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07783906906843185), 'actor_loss': np.float64(-0.9559838235378265), 'hyper_actor_loss': np.float64(0.003549638530239463), 'behavior_loss': np.float64(0.8583997249603271)}

Episode step 9010, time diff 0.7841660976409912, total time dif 774.4001936912537)
step: 9010 @ episode report: {'average_total_reward': np.float32(7.6655555), 'reward_variance': np.float32(1.772727), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08605059459805489), 'actor_loss': np.float64(-0.9282675445079803), 'hyper_actor_loss': np.float64(0.003462353069335222), 'behavior_loss': np.float64(0.912070918083191)}

Episode step 9020, time diff 0.7440128326416016, total time dif 775.1843597888947)
step: 9020 @ episode report: {'average_total_reward': np.float32(7.231111), 'reward_variance': np.float32(3.3291068), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08399633578956127), 'actor_loss': np.float64(-0.9545731723308564), 'hyper_actor_loss': np.float64(0.0034385302104055883), 'behavior_loss': np.float64(0.848753696680069)}

Episode step 9030, time diff 0.7831170558929443, total time dif 775.9283726215363)
step: 9030 @ episode report: {'average_total_reward': np.float32(7.5800004), 'reward_variance': np.float32(0.78853834), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08270020484924316), 'actor_loss': np.float64(-0.9469351530075073), 'hyper_actor_loss': np.float64(0.0033364590955898164), 'behavior_loss': np.float64(0.8724275827407837)}

Episode step 9040, time diff 0.7523074150085449, total time dif 776.7114896774292)
step: 9040 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(2.5912888), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08102116659283638), 'actor_loss': np.float64(-0.9452169001102447), 'hyper_actor_loss': np.float64(0.0033189105335623024), 'behavior_loss': np.float64(0.8296408832073212)}

Episode step 9050, time diff 0.927304744720459, total time dif 777.4637970924377)
step: 9050 @ episode report: {'average_total_reward': np.float32(7.8922224), 'reward_variance': np.float32(2.8702233), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07669190615415573), 'actor_loss': np.float64(-0.9580693006515503), 'hyper_actor_loss': np.float64(0.003206804394721985), 'behavior_loss': np.float64(0.8477695524692536)}

Episode step 9060, time diff 0.783189058303833, total time dif 778.3911018371582)
step: 9060 @ episode report: {'average_total_reward': np.float32(8.43889), 'reward_variance': np.float32(3.515142), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09431065171957016), 'actor_loss': np.float64(-0.9390763223171235), 'hyper_actor_loss': np.float64(0.0031673386925831436), 'behavior_loss': np.float64(0.885505735874176)}

Episode step 9070, time diff 0.7707664966583252, total time dif 779.174290895462)
step: 9070 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(2.8160052), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08352051600813866), 'actor_loss': np.float64(-0.9711349964141845), 'hyper_actor_loss': np.float64(0.003197363903746009), 'behavior_loss': np.float64(0.8379431128501892)}

Episode step 9080, time diff 0.8072865009307861, total time dif 779.9450573921204)
step: 9080 @ episode report: {'average_total_reward': np.float32(7.653334), 'reward_variance': np.float32(2.6446617), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07290152199566365), 'actor_loss': np.float64(-0.9288536190986634), 'hyper_actor_loss': np.float64(0.00298791176173836), 'behavior_loss': np.float64(0.8408417522907257)}

Episode step 9090, time diff 0.8317570686340332, total time dif 780.7523438930511)
step: 9090 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(2.9129944), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08520007096230983), 'actor_loss': np.float64(-0.9290568590164184), 'hyper_actor_loss': np.float64(0.0030304827261716126), 'behavior_loss': np.float64(0.8646861493587494)}

Episode step 9100, time diff 0.8047959804534912, total time dif 781.5841009616852)
step: 9100 @ episode report: {'average_total_reward': np.float32(8.177778), 'reward_variance': np.float32(2.5098774), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07892908304929733), 'actor_loss': np.float64(-0.9731210887432098), 'hyper_actor_loss': np.float64(0.0031842801719903944), 'behavior_loss': np.float64(0.8171316504478454)}

Episode step 9110, time diff 0.7718534469604492, total time dif 782.3888969421387)
step: 9110 @ episode report: {'average_total_reward': np.float32(7.716667), 'reward_variance': np.float32(1.6720806), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07360221296548844), 'actor_loss': np.float64(-0.9376999318599701), 'hyper_actor_loss': np.float64(0.00308519066311419), 'behavior_loss': np.float64(0.8061502695083618)}

Episode step 9120, time diff 0.7799632549285889, total time dif 783.1607503890991)
step: 9120 @ episode report: {'average_total_reward': np.float32(8.6044445), 'reward_variance': np.float32(2.570252), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08562247753143311), 'actor_loss': np.float64(-0.9409342408180237), 'hyper_actor_loss': np.float64(0.0030397762777283786), 'behavior_loss': np.float64(0.8533157348632813)}

Episode step 9130, time diff 0.778900146484375, total time dif 783.9407136440277)
step: 9130 @ episode report: {'average_total_reward': np.float32(8.241112), 'reward_variance': np.float32(0.7476562), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08328860402107238), 'actor_loss': np.float64(-0.9586253762245178), 'hyper_actor_loss': np.float64(0.002944006724283099), 'behavior_loss': np.float64(0.8381946206092834)}

Episode step 9140, time diff 0.796912670135498, total time dif 784.7196137905121)
step: 9140 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(3.0800757), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0787055492401123), 'actor_loss': np.float64(-0.9572662591934205), 'hyper_actor_loss': np.float64(0.003049516142345965), 'behavior_loss': np.float64(0.8466326534748078)}

Episode step 9150, time diff 0.8010296821594238, total time dif 785.5165264606476)
step: 9150 @ episode report: {'average_total_reward': np.float32(8.526668), 'reward_variance': np.float32(4.1143265), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0827356033027172), 'actor_loss': np.float64(-0.9611030399799347), 'hyper_actor_loss': np.float64(0.0029847325524315236), 'behavior_loss': np.float64(0.7971889317035675)}

Episode step 9160, time diff 0.7997674942016602, total time dif 786.317556142807)
step: 9160 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(2.0968103), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0822503238916397), 'actor_loss': np.float64(-0.9532118976116181), 'hyper_actor_loss': np.float64(0.0031249335035681723), 'behavior_loss': np.float64(0.8375212848186493)}

Episode step 9170, time diff 0.7694981098175049, total time dif 787.1173236370087)
step: 9170 @ episode report: {'average_total_reward': np.float32(8.565557), 'reward_variance': np.float32(2.5529983), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07598213590681553), 'actor_loss': np.float64(-0.9213889956474304), 'hyper_actor_loss': np.float64(0.0029559060232713817), 'behavior_loss': np.float64(0.7732444047927857)}

Episode step 9180, time diff 0.837909460067749, total time dif 787.8868217468262)
step: 9180 @ episode report: {'average_total_reward': np.float32(7.567778), 'reward_variance': np.float32(0.82796156), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08288598209619522), 'actor_loss': np.float64(-0.9925580680370331), 'hyper_actor_loss': np.float64(0.0031985812122002242), 'behavior_loss': np.float64(0.8146574378013611)}

Episode step 9190, time diff 0.7766494750976562, total time dif 788.7247312068939)
step: 9190 @ episode report: {'average_total_reward': np.float32(8.777779), 'reward_variance': np.float32(5.145729), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08581583369523287), 'actor_loss': np.float64(-0.9514204323291778), 'hyper_actor_loss': np.float64(0.0033528519095852973), 'behavior_loss': np.float64(0.8235555231571198)}

Episode step 9200, time diff 0.7703917026519775, total time dif 789.5013806819916)
step: 9200 @ episode report: {'average_total_reward': np.float32(7.9777784), 'reward_variance': np.float32(1.0941732), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07727577388286591), 'actor_loss': np.float64(-0.9651066303253174), 'hyper_actor_loss': np.float64(0.0034416363574564456), 'behavior_loss': np.float64(0.8380513727664948)}

Episode step 9210, time diff 0.8120992183685303, total time dif 790.2717723846436)
step: 9210 @ episode report: {'average_total_reward': np.float32(8.204445), 'reward_variance': np.float32(0.5663015), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08925709202885627), 'actor_loss': np.float64(-0.9647869110107422), 'hyper_actor_loss': np.float64(0.0037171161035075783), 'behavior_loss': np.float64(0.8495608627796173)}

Episode step 9220, time diff 0.9210672378540039, total time dif 791.0838716030121)
step: 9220 @ episode report: {'average_total_reward': np.float32(6.88), 'reward_variance': np.float32(2.60602), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08163236491382123), 'actor_loss': np.float64(-0.9685440838336945), 'hyper_actor_loss': np.float64(0.004000942525453866), 'behavior_loss': np.float64(0.8436554312705994)}

Episode step 9230, time diff 0.767244815826416, total time dif 792.0049388408661)
step: 9230 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(2.2894025), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07911895699799061), 'actor_loss': np.float64(-0.9599605321884155), 'hyper_actor_loss': np.float64(0.0038388109067454936), 'behavior_loss': np.float64(0.8480135142803192)}

Episode step 9240, time diff 0.7908487319946289, total time dif 792.7721836566925)
step: 9240 @ episode report: {'average_total_reward': np.float32(6.594445), 'reward_variance': np.float32(1.1667968), 'max_total_reward': np.float32(8.28889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08065813444554806), 'actor_loss': np.float64(-0.9653375804424286), 'hyper_actor_loss': np.float64(0.004324227757751942), 'behavior_loss': np.float64(0.8477986872196197)}

Episode step 9250, time diff 0.8180825710296631, total time dif 793.5630323886871)
step: 9250 @ episode report: {'average_total_reward': np.float32(6.5700006), 'reward_variance': np.float32(2.268495), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07774880453944207), 'actor_loss': np.float64(-0.952988350391388), 'hyper_actor_loss': np.float64(0.004300802852958441), 'behavior_loss': np.float64(0.875578647851944)}

Episode step 9260, time diff 0.7759900093078613, total time dif 794.3811149597168)
step: 9260 @ episode report: {'average_total_reward': np.float32(6.594445), 'reward_variance': np.float32(1.9424263), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0769001729786396), 'actor_loss': np.float64(-0.9521912753582), 'hyper_actor_loss': np.float64(0.0045480668079108), 'behavior_loss': np.float64(0.8965614914894104)}

Episode step 9270, time diff 0.7955567836761475, total time dif 795.1571049690247)
step: 9270 @ episode report: {'average_total_reward': np.float32(7.331111), 'reward_variance': np.float32(0.45258775), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08345169015228748), 'actor_loss': np.float64(-0.9734833359718322), 'hyper_actor_loss': np.float64(0.0046155612450093034), 'behavior_loss': np.float64(0.8611172795295715)}

Episode step 9280, time diff 0.7756862640380859, total time dif 795.9526617527008)
step: 9280 @ episode report: {'average_total_reward': np.float32(6.418889), 'reward_variance': np.float32(0.6492359), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08077803328633308), 'actor_loss': np.float64(-0.9893288671970367), 'hyper_actor_loss': np.float64(0.004807645920664072), 'behavior_loss': np.float64(0.8849696755409241)}

Episode step 9290, time diff 0.7966351509094238, total time dif 796.7283480167389)
step: 9290 @ episode report: {'average_total_reward': np.float32(5.821111), 'reward_variance': np.float32(1.2136902), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0808281160891056), 'actor_loss': np.float64(-0.9734871804714202), 'hyper_actor_loss': np.float64(0.005056704627349973), 'behavior_loss': np.float64(0.8794178545475007)}

Episode step 9300, time diff 0.7975268363952637, total time dif 797.5249831676483)
step: 9300 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(2.1897044), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09334641695022583), 'actor_loss': np.float64(-0.9653002440929412), 'hyper_actor_loss': np.float64(0.005200109770521521), 'behavior_loss': np.float64(0.9074012994766235)}

Episode step 9310, time diff 0.8093678951263428, total time dif 798.3225100040436)
step: 9310 @ episode report: {'average_total_reward': np.float32(6.4822226), 'reward_variance': np.float32(1.0345482), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08748486451804638), 'actor_loss': np.float64(-0.9880782902240753), 'hyper_actor_loss': np.float64(0.005239908304065466), 'behavior_loss': np.float64(0.8668652176856995)}

Episode step 9320, time diff 0.768143892288208, total time dif 799.1318778991699)
step: 9320 @ episode report: {'average_total_reward': np.float32(6.9822226), 'reward_variance': np.float32(1.048376), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07973224185407161), 'actor_loss': np.float64(-0.9728887856006623), 'hyper_actor_loss': np.float64(0.005272743245586753), 'behavior_loss': np.float64(0.8619780838489532)}

Episode step 9330, time diff 0.7972290515899658, total time dif 799.9000217914581)
step: 9330 @ episode report: {'average_total_reward': np.float32(6.7700005), 'reward_variance': np.float32(0.62777925), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08372986167669297), 'actor_loss': np.float64(-0.9557581543922424), 'hyper_actor_loss': np.float64(0.005301766889169812), 'behavior_loss': np.float64(0.9356844365596771)}

Episode step 9340, time diff 0.7939767837524414, total time dif 800.6972508430481)
step: 9340 @ episode report: {'average_total_reward': np.float32(5.4722223), 'reward_variance': np.float32(0.9912408), 'max_total_reward': np.float32(7.4111114), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08731045126914978), 'actor_loss': np.float64(-0.9485481917858124), 'hyper_actor_loss': np.float64(0.005280730547383428), 'behavior_loss': np.float64(0.919064736366272)}

Episode step 9350, time diff 0.7746891975402832, total time dif 801.4912276268005)
step: 9350 @ episode report: {'average_total_reward': np.float32(5.708889), 'reward_variance': np.float32(1.5382173), 'max_total_reward': np.float32(7.4111114), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08320301920175552), 'actor_loss': np.float64(-0.9950380146503448), 'hyper_actor_loss': np.float64(0.005213321186602116), 'behavior_loss': np.float64(0.8704954266548157)}

Episode step 9360, time diff 0.7954552173614502, total time dif 802.2659168243408)
step: 9360 @ episode report: {'average_total_reward': np.float32(5.672222), 'reward_variance': np.float32(1.2854135), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08967594802379608), 'actor_loss': np.float64(-0.9491537868976593), 'hyper_actor_loss': np.float64(0.005245398357510566), 'behavior_loss': np.float64(0.8643221080303192)}

Episode step 9370, time diff 0.9355735778808594, total time dif 803.0613720417023)
step: 9370 @ episode report: {'average_total_reward': np.float32(6.8944445), 'reward_variance': np.float32(2.0758328), 'max_total_reward': np.float32(9.777777), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08623547255992889), 'actor_loss': np.float64(-0.9843678295612335), 'hyper_actor_loss': np.float64(0.005456356145441532), 'behavior_loss': np.float64(0.9174573361873627)}

Episode step 9380, time diff 0.7888820171356201, total time dif 803.9969456195831)
step: 9380 @ episode report: {'average_total_reward': np.float32(6.321111), 'reward_variance': np.float32(1.4736904), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07102028056979179), 'actor_loss': np.float64(-0.9571068823337555), 'hyper_actor_loss': np.float64(0.005303916055709124), 'behavior_loss': np.float64(0.819723653793335)}

Episode step 9390, time diff 0.7796216011047363, total time dif 804.7858276367188)
step: 9390 @ episode report: {'average_total_reward': np.float32(6.0700006), 'reward_variance': np.float32(1.2269641), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09373234622180462), 'actor_loss': np.float64(-0.9759251117706299), 'hyper_actor_loss': np.float64(0.004968860791996122), 'behavior_loss': np.float64(0.8837027728557587)}

Episode step 9400, time diff 0.7827320098876953, total time dif 805.5654492378235)
step: 9400 @ episode report: {'average_total_reward': np.float32(5.921111), 'reward_variance': np.float32(2.4675918), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09702569544315338), 'actor_loss': np.float64(-0.9635277509689331), 'hyper_actor_loss': np.float64(0.004809920117259026), 'behavior_loss': np.float64(0.9168595373630524)}

Episode step 9410, time diff 0.8006784915924072, total time dif 806.3481812477112)
step: 9410 @ episode report: {'average_total_reward': np.float32(6.46), 'reward_variance': np.float32(0.9317831), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08231383860111237), 'actor_loss': np.float64(-0.9593446433544159), 'hyper_actor_loss': np.float64(0.0045852117706090215), 'behavior_loss': np.float64(0.8777965486049653)}

Episode step 9420, time diff 0.7687854766845703, total time dif 807.1488597393036)
step: 9420 @ episode report: {'average_total_reward': np.float32(6.045556), 'reward_variance': np.float32(1.3593447), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08922876417636871), 'actor_loss': np.float64(-0.9597473621368409), 'hyper_actor_loss': np.float64(0.004271055362187326), 'behavior_loss': np.float64(0.8591603755950927)}

Episode step 9430, time diff 0.730292797088623, total time dif 807.9176452159882)
step: 9430 @ episode report: {'average_total_reward': np.float32(6.4188895), 'reward_variance': np.float32(1.3589649), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0968651868402958), 'actor_loss': np.float64(-0.9980828583240509), 'hyper_actor_loss': np.float64(0.004061472648754716), 'behavior_loss': np.float64(0.8993810474872589)}

Episode step 9440, time diff 0.7387771606445312, total time dif 808.6479380130768)
step: 9440 @ episode report: {'average_total_reward': np.float32(6.033334), 'reward_variance': np.float32(1.4879258), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08053594641387463), 'actor_loss': np.float64(-0.9369712710380554), 'hyper_actor_loss': np.float64(0.003998568351380527), 'behavior_loss': np.float64(0.8778670430183411)}

Episode step 9450, time diff 0.7607626914978027, total time dif 809.3867151737213)
step: 9450 @ episode report: {'average_total_reward': np.float32(5.9822226), 'reward_variance': np.float32(4.00408), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08883581310510635), 'actor_loss': np.float64(-0.9501386046409607), 'hyper_actor_loss': np.float64(0.0038721650140359997), 'behavior_loss': np.float64(0.8561046898365021)}

Episode step 9460, time diff 0.7919247150421143, total time dif 810.1474778652191)
step: 9460 @ episode report: {'average_total_reward': np.float32(6.3700004), 'reward_variance': np.float32(2.626224), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08800982609391213), 'actor_loss': np.float64(-0.9611591994762421), 'hyper_actor_loss': np.float64(0.003654341143555939), 'behavior_loss': np.float64(0.8854170441627502)}

Episode step 9470, time diff 0.7458860874176025, total time dif 810.9394025802612)
step: 9470 @ episode report: {'average_total_reward': np.float32(5.8822227), 'reward_variance': np.float32(1.4740546), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07577748745679855), 'actor_loss': np.float64(-0.938705849647522), 'hyper_actor_loss': np.float64(0.0033947919495403767), 'behavior_loss': np.float64(0.9083033740520478)}

Episode step 9480, time diff 0.760054349899292, total time dif 811.6852886676788)
step: 9480 @ episode report: {'average_total_reward': np.float32(5.7966666), 'reward_variance': np.float32(1.5418783), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07673687264323234), 'actor_loss': np.float64(-0.9428085505962371), 'hyper_actor_loss': np.float64(0.00336694298312068), 'behavior_loss': np.float64(0.8393930554389953)}

Episode step 9490, time diff 0.7779147624969482, total time dif 812.4453430175781)
step: 9490 @ episode report: {'average_total_reward': np.float32(5.957778), 'reward_variance': np.float32(1.7510319), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08216456100344657), 'actor_loss': np.float64(-0.9580899894237518), 'hyper_actor_loss': np.float64(0.003362448909319937), 'behavior_loss': np.float64(0.8085699915885926)}

Episode step 9500, time diff 0.7680847644805908, total time dif 813.2232577800751)
step: 9500 @ episode report: {'average_total_reward': np.float32(6.5066667), 'reward_variance': np.float32(1.7706219), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09073201939463615), 'actor_loss': np.float64(-0.9867039918899536), 'hyper_actor_loss': np.float64(0.0031642247922718525), 'behavior_loss': np.float64(0.8634917855262756)}

Episode step 9510, time diff 0.7689979076385498, total time dif 813.9913425445557)
step: 9510 @ episode report: {'average_total_reward': np.float32(6.506667), 'reward_variance': np.float32(1.7431908), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08376198671758175), 'actor_loss': np.float64(-0.9458787620067597), 'hyper_actor_loss': np.float64(0.0030772970290854572), 'behavior_loss': np.float64(0.8975371301174164)}

Episode step 9520, time diff 0.7540433406829834, total time dif 814.7603404521942)
step: 9520 @ episode report: {'average_total_reward': np.float32(6.6577783), 'reward_variance': np.float32(1.7284148), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07738026976585388), 'actor_loss': np.float64(-0.9274115264415741), 'hyper_actor_loss': np.float64(0.0030637636547908185), 'behavior_loss': np.float64(0.8602786540985108)}

Episode step 9530, time diff 0.9249742031097412, total time dif 815.5143837928772)
step: 9530 @ episode report: {'average_total_reward': np.float32(7.3411117), 'reward_variance': np.float32(2.1161003), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08195631802082062), 'actor_loss': np.float64(-0.9468079924583435), 'hyper_actor_loss': np.float64(0.003123683063313365), 'behavior_loss': np.float64(0.8788453817367554)}

Episode step 9540, time diff 0.7642941474914551, total time dif 816.4393579959869)
step: 9540 @ episode report: {'average_total_reward': np.float32(6.8555555), 'reward_variance': np.float32(4.0862727), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08261943385004997), 'actor_loss': np.float64(-0.9495009779930115), 'hyper_actor_loss': np.float64(0.002871378418058157), 'behavior_loss': np.float64(0.799042022228241)}

Episode step 9550, time diff 0.7784152030944824, total time dif 817.2036521434784)
step: 9550 @ episode report: {'average_total_reward': np.float32(7.877778), 'reward_variance': np.float32(7.569976), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08032772913575173), 'actor_loss': np.float64(-0.9516858220100403), 'hyper_actor_loss': np.float64(0.002671262645162642), 'behavior_loss': np.float64(0.8314879834651947)}

Episode step 9560, time diff 0.7720913887023926, total time dif 817.9820673465729)
step: 9560 @ episode report: {'average_total_reward': np.float32(7.5188894), 'reward_variance': np.float32(1.9022728), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06793178096413613), 'actor_loss': np.float64(-0.9319434344768525), 'hyper_actor_loss': np.float64(0.0024726920761168002), 'behavior_loss': np.float64(0.8409971058368683)}

Episode step 9570, time diff 0.7558293342590332, total time dif 818.7541587352753)
step: 9570 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(3.8460681), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.075727229565382), 'actor_loss': np.float64(-0.9098110914230346), 'hyper_actor_loss': np.float64(0.0023980429861694573), 'behavior_loss': np.float64(0.8414676487445831)}

Episode step 9580, time diff 0.7901914119720459, total time dif 819.5099880695343)
step: 9580 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(2.480341), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08578101620078087), 'actor_loss': np.float64(-0.947301185131073), 'hyper_actor_loss': np.float64(0.0021972009679302575), 'behavior_loss': np.float64(0.8137534260749817)}

Episode step 9590, time diff 0.833427906036377, total time dif 820.3001794815063)
step: 9590 @ episode report: {'average_total_reward': np.float32(7.231111), 'reward_variance': np.float32(3.159526), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08305680081248283), 'actor_loss': np.float64(-0.9758352041244507), 'hyper_actor_loss': np.float64(0.002127342147286981), 'behavior_loss': np.float64(0.8179653942584991)}

Episode step 9600, time diff 0.8048133850097656, total time dif 821.1336073875427)
step: 9600 @ episode report: {'average_total_reward': np.float32(8.028889), 'reward_variance': np.float32(3.0853636), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08516901470720768), 'actor_loss': np.float64(-0.9282799899578095), 'hyper_actor_loss': np.float64(0.0021099377307109536), 'behavior_loss': np.float64(0.8602733612060547)}

Episode step 9610, time diff 0.7985787391662598, total time dif 821.9384207725525)
step: 9610 @ episode report: {'average_total_reward': np.float32(7.831112), 'reward_variance': np.float32(1.8619702), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07753270417451859), 'actor_loss': np.float64(-0.9694856166839599), 'hyper_actor_loss': np.float64(0.0021076043602079155), 'behavior_loss': np.float64(0.7881886839866639)}

Episode step 9620, time diff 0.762028694152832, total time dif 822.7369995117188)
step: 9620 @ episode report: {'average_total_reward': np.float32(7.3922224), 'reward_variance': np.float32(3.9763477), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08088226467370987), 'actor_loss': np.float64(-0.9490065991878509), 'hyper_actor_loss': np.float64(0.0020204922067932783), 'behavior_loss': np.float64(0.8075391411781311)}

Episode step 9630, time diff 0.8202147483825684, total time dif 823.4990282058716)
step: 9630 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(2.6759262), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09163245186209679), 'actor_loss': np.float64(-0.9456754624843597), 'hyper_actor_loss': np.float64(0.0021722528850659727), 'behavior_loss': np.float64(0.8164114952087402)}

Episode step 9640, time diff 0.784430980682373, total time dif 824.3192429542542)
step: 9640 @ episode report: {'average_total_reward': np.float32(7.38), 'reward_variance': np.float32(1.2938716), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06880118288099765), 'actor_loss': np.float64(-0.9680775761604309), 'hyper_actor_loss': np.float64(0.0020750055206008255), 'behavior_loss': np.float64(0.7729898989200592)}

Episode step 9650, time diff 0.7802684307098389, total time dif 825.1036739349365)
step: 9650 @ episode report: {'average_total_reward': np.float32(8.077779), 'reward_variance': np.float32(1.7486918), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08889169879257679), 'actor_loss': np.float64(-0.9438751757144928), 'hyper_actor_loss': np.float64(0.0019834245438687505), 'behavior_loss': np.float64(0.8237312078475952)}

Episode step 9660, time diff 0.8015825748443604, total time dif 825.8839423656464)
step: 9660 @ episode report: {'average_total_reward': np.float32(6.745556), 'reward_variance': np.float32(0.675147), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08307836130261421), 'actor_loss': np.float64(-0.9470158815383911), 'hyper_actor_loss': np.float64(0.0019526360323652626), 'behavior_loss': np.float64(0.8671645998954773)}

Episode step 9670, time diff 0.7774143218994141, total time dif 826.6855249404907)
step: 9670 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(0.8210977), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08564176186919212), 'actor_loss': np.float64(-0.9310586988925934), 'hyper_actor_loss': np.float64(0.001904452289454639), 'behavior_loss': np.float64(0.7907995045185089)}

Episode step 9680, time diff 0.7870693206787109, total time dif 827.4629392623901)
step: 9680 @ episode report: {'average_total_reward': np.float32(7.6288896), 'reward_variance': np.float32(4.037512), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08481043726205825), 'actor_loss': np.float64(-0.9543504774570465), 'hyper_actor_loss': np.float64(0.0018824275699444114), 'behavior_loss': np.float64(0.8009835660457612)}

Episode step 9690, time diff 0.8873791694641113, total time dif 828.2500085830688)
step: 9690 @ episode report: {'average_total_reward': np.float32(7.006667), 'reward_variance': np.float32(2.2577832), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08979367166757583), 'actor_loss': np.float64(-0.9607393801212311), 'hyper_actor_loss': np.float64(0.0019288924871943892), 'behavior_loss': np.float64(0.848313057422638)}

Episode step 9700, time diff 0.7848341464996338, total time dif 829.137387752533)
step: 9700 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(4.258176), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09027468152344227), 'actor_loss': np.float64(-0.9511460840702057), 'hyper_actor_loss': np.float64(0.001884644164238125), 'behavior_loss': np.float64(0.8327313423156738)}

Episode step 9710, time diff 0.7464828491210938, total time dif 829.9222218990326)
step: 9710 @ episode report: {'average_total_reward': np.float32(6.9800005), 'reward_variance': np.float32(1.4815509), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08370693102478981), 'actor_loss': np.float64(-0.9460142314434051), 'hyper_actor_loss': np.float64(0.0018936250824481248), 'behavior_loss': np.float64(0.8071198642253876)}

Episode step 9720, time diff 0.7446506023406982, total time dif 830.6687047481537)
step: 9720 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(1.1143258), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07055427208542824), 'actor_loss': np.float64(-0.9547932922840119), 'hyper_actor_loss': np.float64(0.0018254660884849728), 'behavior_loss': np.float64(0.7863156259059906)}

Episode step 9730, time diff 0.7810506820678711, total time dif 831.4133553504944)
step: 9730 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(2.745335), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08009090498089791), 'actor_loss': np.float64(-0.9158933997154236), 'hyper_actor_loss': np.float64(0.0017574725090526045), 'behavior_loss': np.float64(0.797468638420105)}

Episode step 9740, time diff 0.7701106071472168, total time dif 832.1944060325623)
step: 9740 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(1.9697087), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0828409094363451), 'actor_loss': np.float64(-0.9610221028327942), 'hyper_actor_loss': np.float64(0.0018343313480727375), 'behavior_loss': np.float64(0.7983987331390381)}

Episode step 9750, time diff 0.8001070022583008, total time dif 832.9645166397095)
step: 9750 @ episode report: {'average_total_reward': np.float32(6.9066668), 'reward_variance': np.float32(2.2993138), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08909678272902966), 'actor_loss': np.float64(-0.9398725211620331), 'hyper_actor_loss': np.float64(0.0018145933281630278), 'behavior_loss': np.float64(0.8286672711372376)}

Episode step 9760, time diff 0.7870075702667236, total time dif 833.7646236419678)
step: 9760 @ episode report: {'average_total_reward': np.float32(7.2433343), 'reward_variance': np.float32(2.4829745), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08680219277739525), 'actor_loss': np.float64(-0.9436880052089691), 'hyper_actor_loss': np.float64(0.001761259778868407), 'behavior_loss': np.float64(0.8096806943416596)}

Episode step 9770, time diff 0.7545173168182373, total time dif 834.5516312122345)
step: 9770 @ episode report: {'average_total_reward': np.float32(7.38), 'reward_variance': np.float32(1.861995), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07287870086729527), 'actor_loss': np.float64(-0.9644107937812805), 'hyper_actor_loss': np.float64(0.0017697918578051032), 'behavior_loss': np.float64(0.7561596632003784)}

Episode step 9780, time diff 0.7584316730499268, total time dif 835.3061485290527)
step: 9780 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(2.2754686), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08552336022257805), 'actor_loss': np.float64(-0.9334320247173309), 'hyper_actor_loss': np.float64(0.0017973349313251674), 'behavior_loss': np.float64(0.8049175620079041)}

Episode step 9790, time diff 0.7565865516662598, total time dif 836.0645802021027)
step: 9790 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(2.6450317), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0880323089659214), 'actor_loss': np.float64(-0.9559437096118927), 'hyper_actor_loss': np.float64(0.0019148292602039873), 'behavior_loss': np.float64(0.7579608678817749)}

Episode step 9800, time diff 0.780637264251709, total time dif 836.8211667537689)
step: 9800 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(1.8071067), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08442736491560936), 'actor_loss': np.float64(-0.9726723372936249), 'hyper_actor_loss': np.float64(0.0019301768857985734), 'behavior_loss': np.float64(0.8151001095771789)}

Episode step 9810, time diff 0.768284797668457, total time dif 837.6018040180206)
step: 9810 @ episode report: {'average_total_reward': np.float32(6.1577783), 'reward_variance': np.float32(1.5931317), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08933517448604107), 'actor_loss': np.float64(-0.9483830213546753), 'hyper_actor_loss': np.float64(0.0018182500381954015), 'behavior_loss': np.float64(0.8247838914394379)}

Episode step 9820, time diff 0.7873349189758301, total time dif 838.3700888156891)
step: 9820 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(1.5846782), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08093433901667595), 'actor_loss': np.float64(-0.9426924288272858), 'hyper_actor_loss': np.float64(0.0018695551785640418), 'behavior_loss': np.float64(0.7874508798122406)}

Episode step 9830, time diff 0.7668097019195557, total time dif 839.1574237346649)
step: 9830 @ episode report: {'average_total_reward': np.float32(7.2433343), 'reward_variance': np.float32(1.8585298), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08998683989048004), 'actor_loss': np.float64(-0.9782964050769806), 'hyper_actor_loss': np.float64(0.0018165913643315434), 'behavior_loss': np.float64(0.8257773637771606)}

Episode step 9840, time diff 0.8057467937469482, total time dif 839.9242334365845)
step: 9840 @ episode report: {'average_total_reward': np.float32(7.3555555), 'reward_variance': np.float32(2.0145183), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08161482661962509), 'actor_loss': np.float64(-0.9435787558555603), 'hyper_actor_loss': np.float64(0.0017785774543881416), 'behavior_loss': np.float64(0.8081393718719483)}

Episode step 9850, time diff 1.0297746658325195, total time dif 840.7299802303314)
step: 9850 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(3.8999863), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09283800311386585), 'actor_loss': np.float64(-0.9367318511009216), 'hyper_actor_loss': np.float64(0.0017903503612615168), 'behavior_loss': np.float64(0.8409648180007935)}

Episode step 9860, time diff 0.7919385433197021, total time dif 841.7597548961639)
step: 9860 @ episode report: {'average_total_reward': np.float32(8.1044445), 'reward_variance': np.float32(1.4119805), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08416191637516021), 'actor_loss': np.float64(-0.9718485832214355), 'hyper_actor_loss': np.float64(0.0018184402724727988), 'behavior_loss': np.float64(0.8090428113937378)}

Episode step 9870, time diff 0.8074824810028076, total time dif 842.5516934394836)
step: 9870 @ episode report: {'average_total_reward': np.float32(7.343334), 'reward_variance': np.float32(1.2234681), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08191269487142563), 'actor_loss': np.float64(-0.942691856622696), 'hyper_actor_loss': np.float64(0.001732913334853947), 'behavior_loss': np.float64(0.7755839049816131)}

Episode step 9880, time diff 0.8084747791290283, total time dif 843.3591759204865)
step: 9880 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(2.389092), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07387817278504372), 'actor_loss': np.float64(-0.9624915897846222), 'hyper_actor_loss': np.float64(0.001792730181477964), 'behavior_loss': np.float64(0.794823169708252)}

Episode step 9890, time diff 0.7926714420318604, total time dif 844.1676506996155)
step: 9890 @ episode report: {'average_total_reward': np.float32(7.1433334), 'reward_variance': np.float32(2.452603), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08663165643811226), 'actor_loss': np.float64(-0.9308958172798156), 'hyper_actor_loss': np.float64(0.0017975668190047145), 'behavior_loss': np.float64(0.7757104456424713)}

Episode step 9900, time diff 0.8190896511077881, total time dif 844.9603221416473)
step: 9900 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(2.098391), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07816849015653134), 'actor_loss': np.float64(-0.9583573281764984), 'hyper_actor_loss': np.float64(0.0018819069606252014), 'behavior_loss': np.float64(0.805113422870636)}

Episode step 9910, time diff 0.8007631301879883, total time dif 845.7794117927551)
step: 9910 @ episode report: {'average_total_reward': np.float32(8.341112), 'reward_variance': np.float32(2.3141), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08168623484671116), 'actor_loss': np.float64(-0.9580349743366241), 'hyper_actor_loss': np.float64(0.0018310319166630507), 'behavior_loss': np.float64(0.7622650265693665)}

Episode step 9920, time diff 0.8080410957336426, total time dif 846.5801749229431)
step: 9920 @ episode report: {'average_total_reward': np.float32(6.9066668), 'reward_variance': np.float32(3.0838323), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06757508851587772), 'actor_loss': np.float64(-0.9596413969993591), 'hyper_actor_loss': np.float64(0.0017756579327397048), 'behavior_loss': np.float64(0.7159341096878051)}

Episode step 9930, time diff 0.8299784660339355, total time dif 847.3882160186768)
step: 9930 @ episode report: {'average_total_reward': np.float32(7.416667), 'reward_variance': np.float32(2.1920807), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07776962891221047), 'actor_loss': np.float64(-0.9428257703781128), 'hyper_actor_loss': np.float64(0.001709385111462325), 'behavior_loss': np.float64(0.7636754393577576)}

Episode step 9940, time diff 0.8212716579437256, total time dif 848.2181944847107)
step: 9940 @ episode report: {'average_total_reward': np.float32(8.116667), 'reward_variance': np.float32(2.1824257), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06999305225908756), 'actor_loss': np.float64(-0.9304593443870545), 'hyper_actor_loss': np.float64(0.0017415152629837393), 'behavior_loss': np.float64(0.8103566765785217)}

Episode step 9950, time diff 0.8199160099029541, total time dif 849.0394661426544)
step: 9950 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(2.0629685), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08186619132757186), 'actor_loss': np.float64(-0.9203066170215607), 'hyper_actor_loss': np.float64(0.001722734433133155), 'behavior_loss': np.float64(0.8141680657863617)}

Episode step 9960, time diff 0.8239250183105469, total time dif 849.8593821525574)
step: 9960 @ episode report: {'average_total_reward': np.float32(8.216667), 'reward_variance': np.float32(2.0165987), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07712083421647549), 'actor_loss': np.float64(-0.9483665645122528), 'hyper_actor_loss': np.float64(0.0017074575531296431), 'behavior_loss': np.float64(0.8065261363983154)}

Episode step 9970, time diff 0.9163455963134766, total time dif 850.6833071708679)
step: 9970 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(2.1976495), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07743680886924267), 'actor_loss': np.float64(-0.9416794300079345), 'hyper_actor_loss': np.float64(0.0016585833858698607), 'behavior_loss': np.float64(0.7657248198986053)}

Episode step 9980, time diff 0.8849406242370605, total time dif 851.5996527671814)
step: 9980 @ episode report: {'average_total_reward': np.float32(7.8533335), 'reward_variance': np.float32(3.2404888), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08439468443393708), 'actor_loss': np.float64(-0.9581242442131043), 'hyper_actor_loss': np.float64(0.0017647177446633577), 'behavior_loss': np.float64(0.7664180874824524)}

Episode step 9990, time diff 0.7562427520751953, total time dif 852.4845933914185)
step: 9990 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(1.8564001), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07676531560719013), 'actor_loss': np.float64(-0.9406073868274689), 'hyper_actor_loss': np.float64(0.00175251595210284), 'behavior_loss': np.float64(0.753447151184082)}

Episode step 10000, time diff 0.8011701107025146, total time dif 853.2408361434937)
step: 10000 @ episode report: {'average_total_reward': np.float32(8.32889), 'reward_variance': np.float32(3.75519), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06925286166369915), 'actor_loss': np.float64(-0.9464506566524505), 'hyper_actor_loss': np.float64(0.001725813583470881), 'behavior_loss': np.float64(0.7534379303455353)}

Episode step 10010, time diff 0.766880989074707, total time dif 854.0420062541962)
step: 10010 @ episode report: {'average_total_reward': np.float32(7.7411118), 'reward_variance': np.float32(3.332866), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07251844070851803), 'actor_loss': np.float64(-0.9337385356426239), 'hyper_actor_loss': np.float64(0.0017861121334135533), 'behavior_loss': np.float64(0.7167655169963837)}

Episode step 10020, time diff 0.9394032955169678, total time dif 854.8088872432709)
step: 10020 @ episode report: {'average_total_reward': np.float32(8.477778), 'reward_variance': np.float32(2.4693818), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07852858230471611), 'actor_loss': np.float64(-0.9553852021694184), 'hyper_actor_loss': np.float64(0.0017142696538940071), 'behavior_loss': np.float64(0.6976946651935577)}

Episode step 10030, time diff 0.809971809387207, total time dif 855.7482905387878)
step: 10030 @ episode report: {'average_total_reward': np.float32(8.165556), 'reward_variance': np.float32(1.598777), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07918052077293396), 'actor_loss': np.float64(-0.9472706496715546), 'hyper_actor_loss': np.float64(0.001718042476568371), 'behavior_loss': np.float64(0.799701702594757)}

Episode step 10040, time diff 0.7511177062988281, total time dif 856.558262348175)
step: 10040 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(2.456124), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08219688721001148), 'actor_loss': np.float64(-0.9388268947601318), 'hyper_actor_loss': np.float64(0.0016327193239703774), 'behavior_loss': np.float64(0.7300388634204864)}

Episode step 10050, time diff 0.7985844612121582, total time dif 857.3093800544739)
step: 10050 @ episode report: {'average_total_reward': np.float32(7.8288894), 'reward_variance': np.float32(1.7871904), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08445654585957527), 'actor_loss': np.float64(-0.9511141180992126), 'hyper_actor_loss': np.float64(0.0018164785578846931), 'behavior_loss': np.float64(0.7892560958862305)}

Episode step 10060, time diff 0.7759666442871094, total time dif 858.107964515686)
step: 10060 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.3293297), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09047148786485196), 'actor_loss': np.float64(-0.9741147994995117), 'hyper_actor_loss': np.float64(0.0017748374608345329), 'behavior_loss': np.float64(0.7597714543342591)}

Episode step 10070, time diff 0.758723258972168, total time dif 858.8839311599731)
step: 10070 @ episode report: {'average_total_reward': np.float32(8.653334), 'reward_variance': np.float32(3.4337), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08151891380548477), 'actor_loss': np.float64(-0.965101283788681), 'hyper_actor_loss': np.float64(0.0017890414455905557), 'behavior_loss': np.float64(0.7305876731872558)}

Episode step 10080, time diff 0.8037199974060059, total time dif 859.6426544189453)
step: 10080 @ episode report: {'average_total_reward': np.float32(8.565557), 'reward_variance': np.float32(1.3041096), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08065760023891926), 'actor_loss': np.float64(-0.9496476650238037), 'hyper_actor_loss': np.float64(0.0017721897340379656), 'behavior_loss': np.float64(0.7590863883495331)}

Episode step 10090, time diff 0.7814788818359375, total time dif 860.4463744163513)
step: 10090 @ episode report: {'average_total_reward': np.float32(8.441112), 'reward_variance': np.float32(2.743311), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0772531695663929), 'actor_loss': np.float64(-0.9468603372573853), 'hyper_actor_loss': np.float64(0.0017123206635005772), 'behavior_loss': np.float64(0.7543172061443328)}

Episode step 10100, time diff 0.7789149284362793, total time dif 861.2278532981873)
step: 10100 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(2.5232384), 'max_total_reward': np.float32(13.1444435), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07863791137933732), 'actor_loss': np.float64(-0.9481443166732788), 'hyper_actor_loss': np.float64(0.0017088601714931428), 'behavior_loss': np.float64(0.7377688765525818)}

Episode step 10110, time diff 0.7836697101593018, total time dif 862.0067682266235)
step: 10110 @ episode report: {'average_total_reward': np.float32(8.365556), 'reward_variance': np.float32(2.8465796), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08387048393487931), 'actor_loss': np.float64(-0.9520102918148041), 'hyper_actor_loss': np.float64(0.0015355789917521178), 'behavior_loss': np.float64(0.7571276485919952)}

Episode step 10120, time diff 0.7513947486877441, total time dif 862.7904379367828)
step: 10120 @ episode report: {'average_total_reward': np.float32(7.9655557), 'reward_variance': np.float32(3.2466035), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08199423998594284), 'actor_loss': np.float64(-0.942482203245163), 'hyper_actor_loss': np.float64(0.0016101321554742754), 'behavior_loss': np.float64(0.7548062741756439)}

Episode step 10130, time diff 0.790259838104248, total time dif 863.5418326854706)
step: 10130 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(2.7014031), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08720875829458237), 'actor_loss': np.float64(-0.9383223414421081), 'hyper_actor_loss': np.float64(0.0016528475331142544), 'behavior_loss': np.float64(0.7449649810791016)}

Episode step 10140, time diff 0.7929401397705078, total time dif 864.3320925235748)
step: 10140 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.9543022), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09804181978106499), 'actor_loss': np.float64(-0.9775230109691619), 'hyper_actor_loss': np.float64(0.0016332038096152246), 'behavior_loss': np.float64(0.7476118743419647)}

Episode step 10150, time diff 0.7793533802032471, total time dif 865.1250326633453)
step: 10150 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(4.331733), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0896987833082676), 'actor_loss': np.float64(-0.9636259913444519), 'hyper_actor_loss': np.float64(0.0016027348814532162), 'behavior_loss': np.float64(0.7677576839923859)}

Episode step 10160, time diff 0.7347385883331299, total time dif 865.9043860435486)
step: 10160 @ episode report: {'average_total_reward': np.float32(8.402224), 'reward_variance': np.float32(6.030663), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08019688837230206), 'actor_loss': np.float64(-0.9459415793418884), 'hyper_actor_loss': np.float64(0.0016805605497211217), 'behavior_loss': np.float64(0.7229000210762024)}

Episode step 10170, time diff 0.7461121082305908, total time dif 866.6391246318817)
step: 10170 @ episode report: {'average_total_reward': np.float32(8.975557), 'reward_variance': np.float32(2.194587), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09454996027052402), 'actor_loss': np.float64(-0.9625183403491974), 'hyper_actor_loss': np.float64(0.0016981782275252045), 'behavior_loss': np.float64(0.7723715245723725)}

Episode step 10180, time diff 0.8881199359893799, total time dif 867.3852367401123)
step: 10180 @ episode report: {'average_total_reward': np.float32(8.090001), 'reward_variance': np.float32(4.269444), 'max_total_reward': np.float32(12.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08737724274396896), 'actor_loss': np.float64(-0.9402899980545044), 'hyper_actor_loss': np.float64(0.00173765670042485), 'behavior_loss': np.float64(0.7751639008522033)}

Episode step 10190, time diff 0.784477710723877, total time dif 868.2733566761017)
step: 10190 @ episode report: {'average_total_reward': np.float32(8.253334), 'reward_variance': np.float32(1.2933041), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08052834384143352), 'actor_loss': np.float64(-0.9446474969387054), 'hyper_actor_loss': np.float64(0.0017562209512107074), 'behavior_loss': np.float64(0.7486187636852264)}

Episode step 10200, time diff 0.7675113677978516, total time dif 869.0578343868256)
step: 10200 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(3.3811417), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07515777200460434), 'actor_loss': np.float64(-0.9418402850627899), 'hyper_actor_loss': np.float64(0.001785887754522264), 'behavior_loss': np.float64(0.7087782323360443)}

Episode step 10210, time diff 0.775531530380249, total time dif 869.8253457546234)
step: 10210 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(2.5137582), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09692066162824631), 'actor_loss': np.float64(-0.968733960390091), 'hyper_actor_loss': np.float64(0.0019211425445973874), 'behavior_loss': np.float64(0.7266405284404754)}

Episode step 10220, time diff 0.7724294662475586, total time dif 870.6008772850037)
step: 10220 @ episode report: {'average_total_reward': np.float32(8.465556), 'reward_variance': np.float32(3.5177898), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07937091216444969), 'actor_loss': np.float64(-0.9562450885772705), 'hyper_actor_loss': np.float64(0.0016985477064736187), 'behavior_loss': np.float64(0.7197252452373505)}

Episode step 10230, time diff 0.7374942302703857, total time dif 871.3733067512512)
step: 10230 @ episode report: {'average_total_reward': np.float32(9.065557), 'reward_variance': np.float32(2.3025057), 'max_total_reward': np.float32(13.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07238308936357499), 'actor_loss': np.float64(-0.9153908371925354), 'hyper_actor_loss': np.float64(0.0016872571199201048), 'behavior_loss': np.float64(0.7070473253726959)}

Episode step 10240, time diff 0.7809946537017822, total time dif 872.1108009815216)
step: 10240 @ episode report: {'average_total_reward': np.float32(8.177778), 'reward_variance': np.float32(3.835086), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07373448386788369), 'actor_loss': np.float64(-0.9400370180606842), 'hyper_actor_loss': np.float64(0.0016117644147016108), 'behavior_loss': np.float64(0.7027406752109527)}

Episode step 10250, time diff 0.80861496925354, total time dif 872.8917956352234)
step: 10250 @ episode report: {'average_total_reward': np.float32(8.290001), 'reward_variance': np.float32(1.4710737), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08046250231564045), 'actor_loss': np.float64(-0.9500829577445984), 'hyper_actor_loss': np.float64(0.0016590647515840828), 'behavior_loss': np.float64(0.6796836137771607)}

Episode step 10260, time diff 0.7876501083374023, total time dif 873.7004106044769)
step: 10260 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(2.8519552), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07742001637816429), 'actor_loss': np.float64(-0.9465008080005646), 'hyper_actor_loss': np.float64(0.0015747317578643561), 'behavior_loss': np.float64(0.7153835356235504)}

Episode step 10270, time diff 0.7457559108734131, total time dif 874.4880607128143)
step: 10270 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(3.476598), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0740471288561821), 'actor_loss': np.float64(-0.9420532047748565), 'hyper_actor_loss': np.float64(0.0015692299813963473), 'behavior_loss': np.float64(0.7152716875076294)}

Episode step 10280, time diff 0.763317346572876, total time dif 875.2338166236877)
step: 10280 @ episode report: {'average_total_reward': np.float32(8.490001), 'reward_variance': np.float32(3.1553695), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08160416558384895), 'actor_loss': np.float64(-0.950365138053894), 'hyper_actor_loss': np.float64(0.0014365493669174611), 'behavior_loss': np.float64(0.6868529081344604)}

Episode step 10290, time diff 0.7653040885925293, total time dif 875.9971339702606)
step: 10290 @ episode report: {'average_total_reward': np.float32(8.277778), 'reward_variance': np.float32(2.0161233), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09035045094788074), 'actor_loss': np.float64(-0.9786621451377868), 'hyper_actor_loss': np.float64(0.0014029055018909276), 'behavior_loss': np.float64(0.6686570823192597)}

Episode step 10300, time diff 0.745861291885376, total time dif 876.7624380588531)
step: 10300 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.5558783), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07099123559892177), 'actor_loss': np.float64(-0.9605136454105377), 'hyper_actor_loss': np.float64(0.001382119138725102), 'behavior_loss': np.float64(0.6810481607913971)}

Episode step 10310, time diff 0.7485406398773193, total time dif 877.5082993507385)
step: 10310 @ episode report: {'average_total_reward': np.float32(8.787779), 'reward_variance': np.float32(5.2990003), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08719444014132023), 'actor_loss': np.float64(-0.9385496199131012), 'hyper_actor_loss': np.float64(0.001427645073272288), 'behavior_loss': np.float64(0.6947952806949615)}

Episode step 10320, time diff 0.8034913539886475, total time dif 878.2568399906158)
step: 10320 @ episode report: {'average_total_reward': np.float32(8.526667), 'reward_variance': np.float32(1.9541779), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08436125852167606), 'actor_loss': np.float64(-0.9635052919387818), 'hyper_actor_loss': np.float64(0.0014591071056202054), 'behavior_loss': np.float64(0.7139201521873474)}

Episode step 10330, time diff 0.8813731670379639, total time dif 879.0603313446045)
step: 10330 @ episode report: {'average_total_reward': np.float32(8.79), 'reward_variance': np.float32(1.7394924), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09280827566981316), 'actor_loss': np.float64(-0.9406416714191437), 'hyper_actor_loss': np.float64(0.0013985809637233615), 'behavior_loss': np.float64(0.6849108576774597)}

Episode step 10340, time diff 0.8451991081237793, total time dif 879.9417045116425)
step: 10340 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(4.8420305), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07204720601439477), 'actor_loss': np.float64(-0.9583480417728424), 'hyper_actor_loss': np.float64(0.0013857840676791966), 'behavior_loss': np.float64(0.6427097201347352)}

Episode step 10350, time diff 0.959627628326416, total time dif 880.7869036197662)
step: 10350 @ episode report: {'average_total_reward': np.float32(9.202223), 'reward_variance': np.float32(2.4543157), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06599820964038372), 'actor_loss': np.float64(-0.9304463982582092), 'hyper_actor_loss': np.float64(0.0014130108756944537), 'behavior_loss': np.float64(0.6429686665534973)}

Episode step 10360, time diff 0.767188549041748, total time dif 881.7465312480927)
step: 10360 @ episode report: {'average_total_reward': np.float32(8.63889), 'reward_variance': np.float32(4.721957), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0809317484498024), 'actor_loss': np.float64(-0.9393613874912262), 'hyper_actor_loss': np.float64(0.0013729639817029239), 'behavior_loss': np.float64(0.6516538262367249)}

Episode step 10370, time diff 0.7870743274688721, total time dif 882.5137197971344)
step: 10370 @ episode report: {'average_total_reward': np.float32(9.002223), 'reward_variance': np.float32(4.4454775), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07185472585260869), 'actor_loss': np.float64(-0.9515112817287446), 'hyper_actor_loss': np.float64(0.0014780209981836378), 'behavior_loss': np.float64(0.6769406735897064)}

Episode step 10380, time diff 0.807004451751709, total time dif 883.3007941246033)
step: 10380 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(2.6805694), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08481775261461735), 'actor_loss': np.float64(-0.9391443133354187), 'hyper_actor_loss': np.float64(0.0015015043900348246), 'behavior_loss': np.float64(0.6757022202014923)}

Episode step 10390, time diff 0.7964036464691162, total time dif 884.107798576355)
step: 10390 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(3.3682513), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0684301022440195), 'actor_loss': np.float64(-0.9568402767181396), 'hyper_actor_loss': np.float64(0.001591159391682595), 'behavior_loss': np.float64(0.6359907388687134)}

Episode step 10400, time diff 0.8153669834136963, total time dif 884.9042022228241)
step: 10400 @ episode report: {'average_total_reward': np.float32(9.124445), 'reward_variance': np.float32(4.6559715), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08019131347537041), 'actor_loss': np.float64(-0.9422903537750245), 'hyper_actor_loss': np.float64(0.0016241388162598013), 'behavior_loss': np.float64(0.6527275741100311)}

Episode step 10410, time diff 0.8056445121765137, total time dif 885.7195692062378)
step: 10410 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(1.9353287), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08168534561991692), 'actor_loss': np.float64(-0.9461474359035492), 'hyper_actor_loss': np.float64(0.001629158528521657), 'behavior_loss': np.float64(0.6768856704235077)}

Episode step 10420, time diff 0.7992513179779053, total time dif 886.5252137184143)
step: 10420 @ episode report: {'average_total_reward': np.float32(8.887778), 'reward_variance': np.float32(6.7775664), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08149999007582664), 'actor_loss': np.float64(-0.9654786169528962), 'hyper_actor_loss': np.float64(0.0016575346235185862), 'behavior_loss': np.float64(0.64436474442482)}

Episode step 10430, time diff 0.7778878211975098, total time dif 887.3244650363922)
step: 10430 @ episode report: {'average_total_reward': np.float32(8.314445), 'reward_variance': np.float32(2.63899), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07978053092956543), 'actor_loss': np.float64(-0.9352397382259369), 'hyper_actor_loss': np.float64(0.0016689866431988775), 'behavior_loss': np.float64(0.6486705034971237)}

Episode step 10440, time diff 0.768829345703125, total time dif 888.1023528575897)
step: 10440 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(1.5608652), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08609736151993275), 'actor_loss': np.float64(-0.9462586522102356), 'hyper_actor_loss': np.float64(0.0015919762896373867), 'behavior_loss': np.float64(0.6430848717689515)}

Episode step 10450, time diff 0.7803902626037598, total time dif 888.8711822032928)
step: 10450 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(3.55689), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07709474675357342), 'actor_loss': np.float64(-0.97099928855896), 'hyper_actor_loss': np.float64(0.001591299579013139), 'behavior_loss': np.float64(0.6168503284454345)}

Episode step 10460, time diff 0.7480432987213135, total time dif 889.6515724658966)
step: 10460 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(2.1358337), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07749831341207028), 'actor_loss': np.float64(-0.948309451341629), 'hyper_actor_loss': np.float64(0.001378366257995367), 'behavior_loss': np.float64(0.6204963505268097)}

Episode step 10470, time diff 0.8096857070922852, total time dif 890.3996157646179)
step: 10470 @ episode report: {'average_total_reward': np.float32(8.365557), 'reward_variance': np.float32(3.7009735), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07517154663801193), 'actor_loss': np.float64(-0.930854594707489), 'hyper_actor_loss': np.float64(0.001337813143618405), 'behavior_loss': np.float64(0.598260223865509)}

Episode step 10480, time diff 0.7493164539337158, total time dif 891.2093014717102)
step: 10480 @ episode report: {'average_total_reward': np.float32(8.414445), 'reward_variance': np.float32(3.2540748), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08272027373313903), 'actor_loss': np.float64(-0.9463593065738678), 'hyper_actor_loss': np.float64(0.0013378449250012637), 'behavior_loss': np.float64(0.5878950744867325)}

Episode step 10490, time diff 0.773655891418457, total time dif 891.9586179256439)
step: 10490 @ episode report: {'average_total_reward': np.float32(9.224446), 'reward_variance': np.float32(3.162391), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07721909359097481), 'actor_loss': np.float64(-0.975640618801117), 'hyper_actor_loss': np.float64(0.0012306636432185768), 'behavior_loss': np.float64(0.5655916213989258)}

Episode step 10500, time diff 0.7646498680114746, total time dif 892.7322738170624)
step: 10500 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.6180794), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0719422634691), 'actor_loss': np.float64(-0.9485894441604614), 'hyper_actor_loss': np.float64(0.0012761198333464563), 'behavior_loss': np.float64(0.6134011328220368)}

Episode step 10510, time diff 0.9019057750701904, total time dif 893.4969236850739)
step: 10510 @ episode report: {'average_total_reward': np.float32(8.390001), 'reward_variance': np.float32(1.2067517), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07581275179982186), 'actor_loss': np.float64(-0.9249408841133118), 'hyper_actor_loss': np.float64(0.0013116016751155257), 'behavior_loss': np.float64(0.5909185647964478)}

Episode step 10520, time diff 0.7792172431945801, total time dif 894.398829460144)
step: 10520 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(2.599271), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07329921387135982), 'actor_loss': np.float64(-0.9655522346496582), 'hyper_actor_loss': np.float64(0.001382283086422831), 'behavior_loss': np.float64(0.5845817744731903)}

Episode step 10530, time diff 0.7421250343322754, total time dif 895.1780467033386)
step: 10530 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(4.87258), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07721164301037789), 'actor_loss': np.float64(-0.9304989337921142), 'hyper_actor_loss': np.float64(0.0014717604848556221), 'behavior_loss': np.float64(0.5383564561605454)}

Episode step 10540, time diff 0.7615175247192383, total time dif 895.9201717376709)
step: 10540 @ episode report: {'average_total_reward': np.float32(8.128889), 'reward_variance': np.float32(5.3401537), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07963401935994625), 'actor_loss': np.float64(-0.9881974279880523), 'hyper_actor_loss': np.float64(0.0017068994231522084), 'behavior_loss': np.float64(0.5493845880031586)}

Episode step 10550, time diff 0.7611596584320068, total time dif 896.6816892623901)
step: 10550 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(2.2913382), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0785827424377203), 'actor_loss': np.float64(-0.9454155325889587), 'hyper_actor_loss': np.float64(0.0014672794961370528), 'behavior_loss': np.float64(0.5743253082036972)}

Episode step 10560, time diff 0.7480072975158691, total time dif 897.4428489208221)
step: 10560 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.4284738), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08323720321059228), 'actor_loss': np.float64(-0.951527065038681), 'hyper_actor_loss': np.float64(0.0013456088490784168), 'behavior_loss': np.float64(0.5864901781082154)}

Episode step 10570, time diff 0.7864315509796143, total time dif 898.190856218338)
step: 10570 @ episode report: {'average_total_reward': np.float32(8.441112), 'reward_variance': np.float32(1.629063), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09005274623632431), 'actor_loss': np.float64(-0.967449527978897), 'hyper_actor_loss': np.float64(0.0013061240315437317), 'behavior_loss': np.float64(0.5775505572557449)}

Episode step 10580, time diff 0.7267084121704102, total time dif 898.9772877693176)
step: 10580 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(5.510481), 'max_total_reward': np.float32(14.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07817709110677243), 'actor_loss': np.float64(-0.9647682011127472), 'hyper_actor_loss': np.float64(0.0012390378280542791), 'behavior_loss': np.float64(0.5582242846488953)}

Episode step 10590, time diff 0.7686283588409424, total time dif 899.703996181488)
step: 10590 @ episode report: {'average_total_reward': np.float32(8.402224), 'reward_variance': np.float32(3.3148842), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07993866391479969), 'actor_loss': np.float64(-0.9472547471523285), 'hyper_actor_loss': np.float64(0.0013549308641813695), 'behavior_loss': np.float64(0.5816542983055115)}

Episode step 10600, time diff 0.7979657649993896, total time dif 900.472624540329)
step: 10600 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(1.4849379), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07765870355069637), 'actor_loss': np.float64(-0.96189746260643), 'hyper_actor_loss': np.float64(0.0015858867554925382), 'behavior_loss': np.float64(0.5649154305458068)}

Episode step 10610, time diff 0.7259864807128906, total time dif 901.2705903053284)
step: 10610 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.8059059), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0750049777328968), 'actor_loss': np.float64(-0.9504325211048126), 'hyper_actor_loss': np.float64(0.0018273910274729133), 'behavior_loss': np.float64(0.5581222981214523)}

Episode step 10620, time diff 0.7452602386474609, total time dif 901.9965767860413)
step: 10620 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.1996046), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0717182695865631), 'actor_loss': np.float64(-0.9481382846832276), 'hyper_actor_loss': np.float64(0.001813378103543073), 'behavior_loss': np.float64(0.568792325258255)}

Episode step 10630, time diff 0.775183916091919, total time dif 902.7418370246887)
step: 10630 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(1.4998019), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08654247894883156), 'actor_loss': np.float64(-0.9374205112457276), 'hyper_actor_loss': np.float64(0.0015198632725514471), 'behavior_loss': np.float64(0.5880967438220978)}

Episode step 10640, time diff 0.7509493827819824, total time dif 903.5170209407806)
step: 10640 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(6.5044007), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08127502836287022), 'actor_loss': np.float64(-0.9614650726318359), 'hyper_actor_loss': np.float64(0.0013325355714187026), 'behavior_loss': np.float64(0.5809651196002961)}

Episode step 10650, time diff 0.7642946243286133, total time dif 904.2679703235626)
step: 10650 @ episode report: {'average_total_reward': np.float32(9.626668), 'reward_variance': np.float32(1.9522517), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0852753259241581), 'actor_loss': np.float64(-0.9534676909446717), 'hyper_actor_loss': np.float64(0.0013056230964139104), 'behavior_loss': np.float64(0.5519885033369064)}

Episode step 10660, time diff 0.7549576759338379, total time dif 905.0322649478912)
step: 10660 @ episode report: {'average_total_reward': np.float32(9.775557), 'reward_variance': np.float32(2.160365), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08710139766335487), 'actor_loss': np.float64(-0.9757567763328552), 'hyper_actor_loss': np.float64(0.00134600029559806), 'behavior_loss': np.float64(0.5270561188459396)}

Episode step 10670, time diff 0.7600269317626953, total time dif 905.7872226238251)
step: 10670 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(3.3976753), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07639405578374862), 'actor_loss': np.float64(-0.9576279997825623), 'hyper_actor_loss': np.float64(0.0013792698038741947), 'behavior_loss': np.float64(0.5375495076179504)}

Episode step 10680, time diff 0.7537882328033447, total time dif 906.5472495555878)
step: 10680 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.0939302), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08631643615663051), 'actor_loss': np.float64(-0.9616446197032928), 'hyper_actor_loss': np.float64(0.001450005965307355), 'behavior_loss': np.float64(0.5124104738235473)}

Episode step 10690, time diff 0.9632565975189209, total time dif 907.3010377883911)
step: 10690 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(5.0520115), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06827979534864426), 'actor_loss': np.float64(-0.9571958005428314), 'hyper_actor_loss': np.float64(0.001423793809954077), 'behavior_loss': np.float64(0.49284749627113345)}

Episode step 10700, time diff 0.8261504173278809, total time dif 908.26429438591)
step: 10700 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(3.5070236), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08134418800473213), 'actor_loss': np.float64(-0.9409952342510224), 'hyper_actor_loss': np.float64(0.001375683827791363), 'behavior_loss': np.float64(0.5417345643043519)}

Episode step 10710, time diff 0.7212982177734375, total time dif 909.0904448032379)
step: 10710 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.7816792), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07233998253941536), 'actor_loss': np.float64(-0.9600248336791992), 'hyper_actor_loss': np.float64(0.0012921968824230134), 'behavior_loss': np.float64(0.5202807307243347)}

Episode step 10720, time diff 0.7615025043487549, total time dif 909.8117430210114)
step: 10720 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(2.6499527), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08152509555220604), 'actor_loss': np.float64(-0.980035400390625), 'hyper_actor_loss': np.float64(0.0012889788136817514), 'behavior_loss': np.float64(0.5023983865976334)}

Episode step 10730, time diff 0.7608194351196289, total time dif 910.5732455253601)
step: 10730 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(4.7248945), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07970160469412804), 'actor_loss': np.float64(-0.9490661978721618), 'hyper_actor_loss': np.float64(0.0013403433375060558), 'behavior_loss': np.float64(0.5336856812238693)}

Episode step 10740, time diff 0.8010275363922119, total time dif 911.3340649604797)
step: 10740 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(2.0456493), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09019864462316037), 'actor_loss': np.float64(-0.9640331089496612), 'hyper_actor_loss': np.float64(0.0013980442890897393), 'behavior_loss': np.float64(0.5045594662427902)}

Episode step 10750, time diff 0.7782480716705322, total time dif 912.135092496872)
step: 10750 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.6687703), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08392571434378623), 'actor_loss': np.float64(-0.9781225740909576), 'hyper_actor_loss': np.float64(0.00158325363881886), 'behavior_loss': np.float64(0.5041011571884155)}

Episode step 10760, time diff 0.7824616432189941, total time dif 912.9133405685425)
step: 10760 @ episode report: {'average_total_reward': np.float32(8.987778), 'reward_variance': np.float32(3.0783813), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081647962331772), 'actor_loss': np.float64(-0.9503492474555969), 'hyper_actor_loss': np.float64(0.001634055923204869), 'behavior_loss': np.float64(0.49717758893966674)}

Episode step 10770, time diff 0.7730832099914551, total time dif 913.6958022117615)
step: 10770 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.61978), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07871395386755467), 'actor_loss': np.float64(-0.9518469393253326), 'hyper_actor_loss': np.float64(0.0017452182131819428), 'behavior_loss': np.float64(0.5104956597089767)}

Episode step 10780, time diff 0.7783784866333008, total time dif 914.4688854217529)
step: 10780 @ episode report: {'average_total_reward': np.float32(9.551112), 'reward_variance': np.float32(2.9883027), 'max_total_reward': np.float32(13.022223), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07563507221639157), 'actor_loss': np.float64(-0.951620751619339), 'hyper_actor_loss': np.float64(0.0017474233522079886), 'behavior_loss': np.float64(0.5011524468660354)}

Episode step 10790, time diff 0.7738432884216309, total time dif 915.2472639083862)
step: 10790 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.8491675), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07254962921142578), 'actor_loss': np.float64(-0.9457428216934204), 'hyper_actor_loss': np.float64(0.0018089518882334233), 'behavior_loss': np.float64(0.47829577028751374)}

Episode step 10800, time diff 0.7839560508728027, total time dif 916.0211071968079)
step: 10800 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(1.3340943), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07373109310865403), 'actor_loss': np.float64(-0.9655006289482116), 'hyper_actor_loss': np.float64(0.0016730520292185247), 'behavior_loss': np.float64(0.49075338840484617)}

Episode step 10810, time diff 0.7606825828552246, total time dif 916.8050632476807)
step: 10810 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(1.7573693), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08507140167057514), 'actor_loss': np.float64(-0.9752062439918519), 'hyper_actor_loss': np.float64(0.0013977708877064288), 'behavior_loss': np.float64(0.5234609723091126)}

Episode step 10820, time diff 0.7572124004364014, total time dif 917.5657458305359)
step: 10820 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.8671386), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07311865352094174), 'actor_loss': np.float64(-0.9555197775363922), 'hyper_actor_loss': np.float64(0.0013059875695034862), 'behavior_loss': np.float64(0.5044412970542907)}

Episode step 10830, time diff 0.7630114555358887, total time dif 918.3229582309723)
step: 10830 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(0.372079), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07901985421776772), 'actor_loss': np.float64(-0.9398342251777649), 'hyper_actor_loss': np.float64(0.001273381896317005), 'behavior_loss': np.float64(0.5169754385948181)}

Episode step 10840, time diff 0.7421166896820068, total time dif 919.0859696865082)
step: 10840 @ episode report: {'average_total_reward': np.float32(9.014444), 'reward_variance': np.float32(2.5525193), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08956316336989403), 'actor_loss': np.float64(-0.9537020444869995), 'hyper_actor_loss': np.float64(0.0011603421065956355), 'behavior_loss': np.float64(0.5475637763738632)}

Episode step 10850, time diff 0.8895158767700195, total time dif 919.8280863761902)
step: 10850 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.8766434), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08601585254073144), 'actor_loss': np.float64(-0.9862766206264496), 'hyper_actor_loss': np.float64(0.0010653554869350045), 'behavior_loss': np.float64(0.531143119931221)}

Episode step 10860, time diff 0.8067913055419922, total time dif 920.7176022529602)
step: 10860 @ episode report: {'average_total_reward': np.float32(9.387779), 'reward_variance': np.float32(2.5364306), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07545596472918988), 'actor_loss': np.float64(-0.961192524433136), 'hyper_actor_loss': np.float64(0.0011137491033878177), 'behavior_loss': np.float64(0.4997297883033752)}

Episode step 10870, time diff 0.7589600086212158, total time dif 921.5243935585022)
step: 10870 @ episode report: {'average_total_reward': np.float32(10.122223), 'reward_variance': np.float32(2.6361969), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0826478149741888), 'actor_loss': np.float64(-0.9475998401641845), 'hyper_actor_loss': np.float64(0.0011533814598806203), 'behavior_loss': np.float64(0.532423684000969)}

Episode step 10880, time diff 0.7440013885498047, total time dif 922.2833535671234)
step: 10880 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(2.051557), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08242935538291932), 'actor_loss': np.float64(-0.9518567085266113), 'hyper_actor_loss': np.float64(0.0010848803620319813), 'behavior_loss': np.float64(0.5381462723016739)}

Episode step 10890, time diff 0.7814066410064697, total time dif 923.0273549556732)
step: 10890 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(2.3750126), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07311273217201233), 'actor_loss': np.float64(-0.9520129442214966), 'hyper_actor_loss': np.float64(0.0010959722159896046), 'behavior_loss': np.float64(0.5078056544065476)}

Episode step 10900, time diff 0.7659754753112793, total time dif 923.8087615966797)
step: 10900 @ episode report: {'average_total_reward': np.float32(9.663335), 'reward_variance': np.float32(1.4686927), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06652678288519383), 'actor_loss': np.float64(-0.9384717345237732), 'hyper_actor_loss': np.float64(0.001059864368289709), 'behavior_loss': np.float64(0.5117453813552857)}

Episode step 10910, time diff 0.736119270324707, total time dif 924.574737071991)
step: 10910 @ episode report: {'average_total_reward': np.float32(9.500002), 'reward_variance': np.float32(2.5343213), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08160459194332362), 'actor_loss': np.float64(-0.9653118133544922), 'hyper_actor_loss': np.float64(0.0011215897568035871), 'behavior_loss': np.float64(0.5063540756702423)}

Episode step 10920, time diff 0.7371921539306641, total time dif 925.3108563423157)
step: 10920 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(3.4173646), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08790340423583984), 'actor_loss': np.float64(-0.957547253370285), 'hyper_actor_loss': np.float64(0.001153573766350746), 'behavior_loss': np.float64(0.5214152753353118)}

Episode step 10930, time diff 0.7789003849029541, total time dif 926.0480484962463)
step: 10930 @ episode report: {'average_total_reward': np.float32(9.33889), 'reward_variance': np.float32(1.956748), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08066469430923462), 'actor_loss': np.float64(-0.9806656658649444), 'hyper_actor_loss': np.float64(0.0011131634586490692), 'behavior_loss': np.float64(0.5027093321084977)}

Episode step 10940, time diff 0.7966172695159912, total time dif 926.8269488811493)
step: 10940 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(3.8087182), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07695552855730056), 'actor_loss': np.float64(-0.9691882371902466), 'hyper_actor_loss': np.float64(0.0011405963101424276), 'behavior_loss': np.float64(0.48038685917854307)}

Episode step 10950, time diff 0.7470409870147705, total time dif 927.6235661506653)
step: 10950 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(2.989241), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08515929989516735), 'actor_loss': np.float64(-0.9592087268829346), 'hyper_actor_loss': np.float64(0.0011935633374378085), 'behavior_loss': np.float64(0.5278832405805588)}

Episode step 10960, time diff 0.7575395107269287, total time dif 928.37060713768)
step: 10960 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(2.400203), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0855482742190361), 'actor_loss': np.float64(-0.9708625435829162), 'hyper_actor_loss': np.float64(0.001270769815891981), 'behavior_loss': np.float64(0.5140434533357621)}

Episode step 10970, time diff 0.741938591003418, total time dif 929.128146648407)
step: 10970 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(1.4975069), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08269263766705989), 'actor_loss': np.float64(-0.9736452281475068), 'hyper_actor_loss': np.float64(0.0014685493661090732), 'behavior_loss': np.float64(0.5181294441223144)}

Episode step 10980, time diff 0.7920811176300049, total time dif 929.8700852394104)
step: 10980 @ episode report: {'average_total_reward': np.float32(9.975557), 'reward_variance': np.float32(1.1107613), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07714259400963783), 'actor_loss': np.float64(-0.9568361222743988), 'hyper_actor_loss': np.float64(0.0015333447721786798), 'behavior_loss': np.float64(0.4915365517139435)}

Episode step 10990, time diff 0.7720601558685303, total time dif 930.6621663570404)
step: 10990 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(3.4963665), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0749447863548994), 'actor_loss': np.float64(-0.9835136115550995), 'hyper_actor_loss': np.float64(0.0016437202459201217), 'behavior_loss': np.float64(0.5026186794042588)}

Episode step 11000, time diff 0.7575099468231201, total time dif 931.4342265129089)
step: 11000 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(1.3837725), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06612684652209282), 'actor_loss': np.float64(-0.9542501151561738), 'hyper_actor_loss': np.float64(0.0016191735281608998), 'behavior_loss': np.float64(0.4960194110870361)}

Episode step 11010, time diff 0.7402222156524658, total time dif 932.191736459732)
step: 11010 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(2.0446477), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08066977225244046), 'actor_loss': np.float64(-0.9659837424755097), 'hyper_actor_loss': np.float64(0.001588941237423569), 'behavior_loss': np.float64(0.46906550228595734)}

Episode step 11020, time diff 0.9134657382965088, total time dif 932.9319586753845)
step: 11020 @ episode report: {'average_total_reward': np.float32(8.987778), 'reward_variance': np.float32(5.1587524), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07750936485826969), 'actor_loss': np.float64(-0.9763042092323303), 'hyper_actor_loss': np.float64(0.001559027365874499), 'behavior_loss': np.float64(0.5170481741428375)}

Episode step 11030, time diff 0.7650647163391113, total time dif 933.845424413681)
step: 11030 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(2.702649), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07118304967880248), 'actor_loss': np.float64(-0.9502090156078339), 'hyper_actor_loss': np.float64(0.0016202821047045291), 'behavior_loss': np.float64(0.5191341906785965)}

Episode step 11040, time diff 0.7988302707672119, total time dif 934.6104891300201)
step: 11040 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.202224), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08261060640215874), 'actor_loss': np.float64(-0.9716519176959991), 'hyper_actor_loss': np.float64(0.0015167850186116993), 'behavior_loss': np.float64(0.48102574050426483)}

Episode step 11050, time diff 0.8132953643798828, total time dif 935.4093194007874)
step: 11050 @ episode report: {'average_total_reward': np.float32(9.463333), 'reward_variance': np.float32(2.8432856), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08284963108599186), 'actor_loss': np.float64(-0.9976905345916748), 'hyper_actor_loss': np.float64(0.001548999035730958), 'behavior_loss': np.float64(0.5069556713104248)}

Episode step 11060, time diff 0.7605164051055908, total time dif 936.2226147651672)
step: 11060 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.0684009), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06950213946402073), 'actor_loss': np.float64(-0.9636104226112365), 'hyper_actor_loss': np.float64(0.0014355993131175636), 'behavior_loss': np.float64(0.48447381258010863)}

Episode step 11070, time diff 0.7591331005096436, total time dif 936.9831311702728)
step: 11070 @ episode report: {'average_total_reward': np.float32(9.351111), 'reward_variance': np.float32(1.9169433), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08388273678719997), 'actor_loss': np.float64(-0.9691129267215729), 'hyper_actor_loss': np.float64(0.0013487784890457988), 'behavior_loss': np.float64(0.4875702440738678)}

Episode step 11080, time diff 0.758338212966919, total time dif 937.7422642707825)
step: 11080 @ episode report: {'average_total_reward': np.float32(9.336667), 'reward_variance': np.float32(1.2946433), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07025600820779801), 'actor_loss': np.float64(-0.9859601795673371), 'hyper_actor_loss': np.float64(0.0014228979242034256), 'behavior_loss': np.float64(0.46567435562610626)}

Episode step 11090, time diff 0.7499163150787354, total time dif 938.5006024837494)
step: 11090 @ episode report: {'average_total_reward': np.float32(9.561111), 'reward_variance': np.float32(4.1640315), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07955625541508198), 'actor_loss': np.float64(-0.9617586076259613), 'hyper_actor_loss': np.float64(0.0014513602945953608), 'behavior_loss': np.float64(0.4802631437778473)}

Episode step 11100, time diff 0.7815284729003906, total time dif 939.2505187988281)
step: 11100 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(2.2237148), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08033820539712906), 'actor_loss': np.float64(-0.9822098076343536), 'hyper_actor_loss': np.float64(0.0014702449669130146), 'behavior_loss': np.float64(0.4700466424226761)}

Episode step 11110, time diff 0.7799324989318848, total time dif 940.0320472717285)
step: 11110 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(1.6595322), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06492996662855148), 'actor_loss': np.float64(-0.9579746127128601), 'hyper_actor_loss': np.float64(0.0013417723239399493), 'behavior_loss': np.float64(0.47130198776721954)}

Episode step 11120, time diff 0.7943823337554932, total time dif 940.8119797706604)
step: 11120 @ episode report: {'average_total_reward': np.float32(9.065557), 'reward_variance': np.float32(1.9907767), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07376704774796963), 'actor_loss': np.float64(-0.9425398230552673), 'hyper_actor_loss': np.float64(0.0011832574848085642), 'behavior_loss': np.float64(0.48244848251342776)}

Episode step 11130, time diff 0.7749185562133789, total time dif 941.6063621044159)
step: 11130 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(3.1124008), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06485857330262661), 'actor_loss': np.float64(-0.967763888835907), 'hyper_actor_loss': np.float64(0.0010847882251255215), 'behavior_loss': np.float64(0.4804195165634155)}

Episode step 11140, time diff 0.8027687072753906, total time dif 942.3812806606293)
step: 11140 @ episode report: {'average_total_reward': np.float32(8.765556), 'reward_variance': np.float32(0.21628253), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08088755272328854), 'actor_loss': np.float64(-0.9583522558212281), 'hyper_actor_loss': np.float64(0.0011291761533357204), 'behavior_loss': np.float64(0.4743708074092865)}

Episode step 11150, time diff 0.7708814144134521, total time dif 943.1840493679047)
step: 11150 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(1.517946), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062009752914309504), 'actor_loss': np.float64(-0.9537941515445709), 'hyper_actor_loss': np.float64(0.0011684250552207232), 'behavior_loss': np.float64(0.4603142201900482)}

Episode step 11160, time diff 0.7872655391693115, total time dif 943.9549307823181)
step: 11160 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(3.1682527), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06961667202413083), 'actor_loss': np.float64(-0.9529627621173858), 'hyper_actor_loss': np.float64(0.0013218248263001443), 'behavior_loss': np.float64(0.42244035601615904)}

Episode step 11170, time diff 0.7695565223693848, total time dif 944.7421963214874)
step: 11170 @ episode report: {'average_total_reward': np.float32(9.587779), 'reward_variance': np.float32(2.4524312), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06868376955389977), 'actor_loss': np.float64(-0.9616289556026458), 'hyper_actor_loss': np.float64(0.0015292959404177963), 'behavior_loss': np.float64(0.4510499000549316)}

Episode step 11180, time diff 0.9165709018707275, total time dif 945.5117528438568)
step: 11180 @ episode report: {'average_total_reward': np.float32(9.424445), 'reward_variance': np.float32(3.3267856), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06908283457159996), 'actor_loss': np.float64(-0.966878879070282), 'hyper_actor_loss': np.float64(0.0016521503101103007), 'behavior_loss': np.float64(0.4301676094532013)}

Episode step 11190, time diff 0.7495944499969482, total time dif 946.4283237457275)
step: 11190 @ episode report: {'average_total_reward': np.float32(9.463334), 'reward_variance': np.float32(1.6347917), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07590613476932048), 'actor_loss': np.float64(-0.9695758044719696), 'hyper_actor_loss': np.float64(0.001510256831534207), 'behavior_loss': np.float64(0.45294601023197173)}

Episode step 11200, time diff 0.7958285808563232, total time dif 947.1779181957245)
step: 11200 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(1.335409), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07231423817574978), 'actor_loss': np.float64(-0.9666824758052825), 'hyper_actor_loss': np.float64(0.0012690284638665617), 'behavior_loss': np.float64(0.46710140705108644)}

Episode step 11210, time diff 0.7657010555267334, total time dif 947.9737467765808)
step: 11210 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.3273537), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0835427213460207), 'actor_loss': np.float64(-0.9761255741119385), 'hyper_actor_loss': np.float64(0.0011047907639294863), 'behavior_loss': np.float64(0.46625572741031646)}

Episode step 11220, time diff 0.7323877811431885, total time dif 948.7394478321075)
step: 11220 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(2.4032655), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07026825919747352), 'actor_loss': np.float64(-0.9785707235336304), 'hyper_actor_loss': np.float64(0.0010643096291460096), 'behavior_loss': np.float64(0.47813098430633544)}

Episode step 11230, time diff 0.7557663917541504, total time dif 949.4718356132507)
step: 11230 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(4.910367), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08931817151606083), 'actor_loss': np.float64(-0.9572817921638489), 'hyper_actor_loss': np.float64(0.0011135807493701578), 'behavior_loss': np.float64(0.4586212158203125)}

Episode step 11240, time diff 0.7659988403320312, total time dif 950.2276020050049)
step: 11240 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.2679455), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0652827549725771), 'actor_loss': np.float64(-0.9894543528556824), 'hyper_actor_loss': np.float64(0.0010555413551628589), 'behavior_loss': np.float64(0.4693965673446655)}

Episode step 11250, time diff 0.7386636734008789, total time dif 950.9936008453369)
step: 11250 @ episode report: {'average_total_reward': np.float32(8.826666), 'reward_variance': np.float32(2.730326), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07195530608296394), 'actor_loss': np.float64(-0.9509217977523804), 'hyper_actor_loss': np.float64(0.0011387330654542894), 'behavior_loss': np.float64(0.4824623942375183)}

Episode step 11260, time diff 0.7738018035888672, total time dif 951.7322645187378)
step: 11260 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(0.93377274), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07417532727122307), 'actor_loss': np.float64(-0.9686208069324493), 'hyper_actor_loss': np.float64(0.0013683781609870494), 'behavior_loss': np.float64(0.4653333991765976)}

Episode step 11270, time diff 0.7486164569854736, total time dif 952.5060663223267)
step: 11270 @ episode report: {'average_total_reward': np.float32(9.163333), 'reward_variance': np.float32(2.9080012), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07377947196364402), 'actor_loss': np.float64(-0.9898622155189514), 'hyper_actor_loss': np.float64(0.0013412073371000589), 'behavior_loss': np.float64(0.48028501570224763)}

Episode step 11280, time diff 0.7530703544616699, total time dif 953.2546827793121)
step: 11280 @ episode report: {'average_total_reward': np.float32(9.387779), 'reward_variance': np.float32(2.2052464), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06728647872805596), 'actor_loss': np.float64(-0.9593330144882202), 'hyper_actor_loss': np.float64(0.0012291372404433787), 'behavior_loss': np.float64(0.45674256086349485)}

Episode step 11290, time diff 0.7624251842498779, total time dif 954.0077531337738)
step: 11290 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(4.325488), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07350363209843636), 'actor_loss': np.float64(-0.9766138136386872), 'hyper_actor_loss': np.float64(0.0012865632539615034), 'behavior_loss': np.float64(0.5114258855581284)}

Episode step 11300, time diff 0.7365150451660156, total time dif 954.7701783180237)
step: 11300 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.0429583), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07995815500617028), 'actor_loss': np.float64(-0.9812691390514374), 'hyper_actor_loss': np.float64(0.001303382113110274), 'behavior_loss': np.float64(0.4910504400730133)}

Episode step 11310, time diff 0.768047571182251, total time dif 955.5066933631897)
step: 11310 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(3.6738815), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07453578338027), 'actor_loss': np.float64(-0.9787454068660736), 'hyper_actor_loss': np.float64(0.001253049704246223), 'behavior_loss': np.float64(0.46213647425174714)}

Episode step 11320, time diff 0.7413740158081055, total time dif 956.274740934372)
step: 11320 @ episode report: {'average_total_reward': np.float32(9.3144455), 'reward_variance': np.float32(1.2484204), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08404859006404877), 'actor_loss': np.float64(-0.9692618489265442), 'hyper_actor_loss': np.float64(0.001265862735453993), 'behavior_loss': np.float64(0.5200099021196365)}

Episode step 11330, time diff 0.7329602241516113, total time dif 957.01611495018)
step: 11330 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(1.1669492), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06626425497233868), 'actor_loss': np.float64(-0.9730551064014434), 'hyper_actor_loss': np.float64(0.0012837561196647584), 'behavior_loss': np.float64(0.47045942544937136)}

Episode step 11340, time diff 0.9436841011047363, total time dif 957.7490751743317)
step: 11340 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(3.053685), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07362559400498866), 'actor_loss': np.float64(-0.9627324521541596), 'hyper_actor_loss': np.float64(0.0014234010595828295), 'behavior_loss': np.float64(0.47685871720314027)}

Episode step 11350, time diff 0.7818140983581543, total time dif 958.6927592754364)
step: 11350 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(1.7782242), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07543327137827874), 'actor_loss': np.float64(-0.9659503102302551), 'hyper_actor_loss': np.float64(0.0018821087898686529), 'behavior_loss': np.float64(0.4835434377193451)}

Episode step 11360, time diff 0.8121559619903564, total time dif 959.4745733737946)
step: 11360 @ episode report: {'average_total_reward': np.float32(8.377779), 'reward_variance': np.float32(1.50237), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08534423932433129), 'actor_loss': np.float64(-1.0001353859901427), 'hyper_actor_loss': np.float64(0.002167829382233322), 'behavior_loss': np.float64(0.46119447350502013)}

Episode step 11370, time diff 0.8123588562011719, total time dif 960.2867293357849)
step: 11370 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.58718), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0798149012029171), 'actor_loss': np.float64(-0.9924392223358154), 'hyper_actor_loss': np.float64(0.0016950635006651282), 'behavior_loss': np.float64(0.4937734842300415)}

Episode step 11380, time diff 0.7945761680603027, total time dif 961.0990881919861)
step: 11380 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(1.8376554), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06527498029172421), 'actor_loss': np.float64(-0.9489961743354798), 'hyper_actor_loss': np.float64(0.001774058339651674), 'behavior_loss': np.float64(0.4800837904214859)}

Episode step 11390, time diff 0.7874894142150879, total time dif 961.8936643600464)
step: 11390 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(4.9225445), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06414041854441166), 'actor_loss': np.float64(-0.9540834844112396), 'hyper_actor_loss': np.float64(0.002123866288457066), 'behavior_loss': np.float64(0.49069285690784453)}

Episode step 11400, time diff 0.7887003421783447, total time dif 962.6811537742615)
step: 11400 @ episode report: {'average_total_reward': np.float32(8.963335), 'reward_variance': np.float32(3.5993104), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07093925401568413), 'actor_loss': np.float64(-0.9605975687503815), 'hyper_actor_loss': np.float64(0.0023251156555488704), 'behavior_loss': np.float64(0.48051262497901914)}

Episode step 11410, time diff 0.7883307933807373, total time dif 963.4698541164398)
step: 11410 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(3.6616416), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07536082416772842), 'actor_loss': np.float64(-0.9818510711193085), 'hyper_actor_loss': np.float64(0.0019087031716480852), 'behavior_loss': np.float64(0.42659507095813753)}

Episode step 11420, time diff 0.8044285774230957, total time dif 964.2581849098206)
step: 11420 @ episode report: {'average_total_reward': np.float32(8.877778), 'reward_variance': np.float32(1.1673088), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07047302685678006), 'actor_loss': np.float64(-0.9904551327228546), 'hyper_actor_loss': np.float64(0.001318433601409197), 'behavior_loss': np.float64(0.49796896874904634)}

Episode step 11430, time diff 0.780602216720581, total time dif 965.0626134872437)
step: 11430 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(4.885922), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07727471217513085), 'actor_loss': np.float64(-0.9644057989120484), 'hyper_actor_loss': np.float64(0.0013001498416997492), 'behavior_loss': np.float64(0.47313764691352844)}

Episode step 11440, time diff 0.7578685283660889, total time dif 965.8432157039642)
step: 11440 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(3.6605747), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07788678966462612), 'actor_loss': np.float64(-0.9970950126647949), 'hyper_actor_loss': np.float64(0.002189683064352721), 'behavior_loss': np.float64(0.4734526664018631)}

Episode step 11450, time diff 0.7599256038665771, total time dif 966.6010842323303)
step: 11450 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(2.8115268), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06734379436820745), 'actor_loss': np.float64(-0.9911498248577117), 'hyper_actor_loss': np.float64(0.004464215785264969), 'behavior_loss': np.float64(0.45742118954658506)}

Episode step 11460, time diff 0.7815921306610107, total time dif 967.3610098361969)
step: 11460 @ episode report: {'average_total_reward': np.float32(9.775556), 'reward_variance': np.float32(1.3832796), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07586177214980125), 'actor_loss': np.float64(-0.9909955501556397), 'hyper_actor_loss': np.float64(0.005366261070594192), 'behavior_loss': np.float64(0.516772598028183)}

Episode step 11470, time diff 0.7938249111175537, total time dif 968.1426019668579)
step: 11470 @ episode report: {'average_total_reward': np.float32(8.316668), 'reward_variance': np.float32(0.78089464), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07866445742547512), 'actor_loss': np.float64(-1.0196024179458618), 'hyper_actor_loss': np.float64(0.006600436475127936), 'behavior_loss': np.float64(0.5286269575357437)}

Episode step 11480, time diff 0.7725272178649902, total time dif 968.9364268779755)
step: 11480 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(1.9370482), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060592289082705975), 'actor_loss': np.float64(-0.9987536311149597), 'hyper_actor_loss': np.float64(0.00800069239921868), 'behavior_loss': np.float64(0.5384318619966507)}

Episode step 11490, time diff 0.729489803314209, total time dif 969.7089540958405)
step: 11490 @ episode report: {'average_total_reward': np.float32(5.7455554), 'reward_variance': np.float32(1.0602086), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07836192809045314), 'actor_loss': np.float64(-0.9622822344303131), 'hyper_actor_loss': np.float64(0.008846246264874935), 'behavior_loss': np.float64(0.5083717852830887)}

Episode step 11500, time diff 0.7495262622833252, total time dif 970.4384438991547)
step: 11500 @ episode report: {'average_total_reward': np.float32(6.8188887), 'reward_variance': np.float32(2.6616309), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08113157078623771), 'actor_loss': np.float64(-0.9990359425544739), 'hyper_actor_loss': np.float64(0.007763266377151012), 'behavior_loss': np.float64(0.545277652144432)}

Episode step 11510, time diff 0.9422867298126221, total time dif 971.187970161438)
step: 11510 @ episode report: {'average_total_reward': np.float32(7.2066674), 'reward_variance': np.float32(1.1296837), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07597396336495876), 'actor_loss': np.float64(-0.9712157309055328), 'hyper_actor_loss': np.float64(0.0065587559714913365), 'behavior_loss': np.float64(0.5149035751819611)}

Episode step 11520, time diff 0.8053431510925293, total time dif 972.1302568912506)
step: 11520 @ episode report: {'average_total_reward': np.float32(5.884444), 'reward_variance': np.float32(1.6931902), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08090049587190151), 'actor_loss': np.float64(-0.9650605857372284), 'hyper_actor_loss': np.float64(0.005754082044586539), 'behavior_loss': np.float64(0.534704452753067)}

Episode step 11530, time diff 0.7370321750640869, total time dif 972.9356000423431)
step: 11530 @ episode report: {'average_total_reward': np.float32(6.533334), 'reward_variance': np.float32(0.71570367), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09200418591499329), 'actor_loss': np.float64(-0.9864012718200683), 'hyper_actor_loss': np.float64(0.005281276535242796), 'behavior_loss': np.float64(0.5174723714590073)}

Episode step 11540, time diff 0.7387402057647705, total time dif 973.6726322174072)
step: 11540 @ episode report: {'average_total_reward': np.float32(5.4722223), 'reward_variance': np.float32(2.3838093), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06834262870252132), 'actor_loss': np.float64(-0.9821574330329895), 'hyper_actor_loss': np.float64(0.0050750190392136576), 'behavior_loss': np.float64(0.5032381743192673)}

Episode step 11550, time diff 0.7631428241729736, total time dif 974.411372423172)
step: 11550 @ episode report: {'average_total_reward': np.float32(6.2066665), 'reward_variance': np.float32(0.7875359), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08041203022003174), 'actor_loss': np.float64(-0.9692824721336365), 'hyper_actor_loss': np.float64(0.004717170866206289), 'behavior_loss': np.float64(0.518169391155243)}

Episode step 11560, time diff 0.7814443111419678, total time dif 975.174515247345)
step: 11560 @ episode report: {'average_total_reward': np.float32(6.794445), 'reward_variance': np.float32(2.0156112), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06249865256249905), 'actor_loss': np.float64(-0.975078570842743), 'hyper_actor_loss': np.float64(0.004083095258101821), 'behavior_loss': np.float64(0.5372512221336365)}

Episode step 11570, time diff 0.7590439319610596, total time dif 975.9559595584869)
step: 11570 @ episode report: {'average_total_reward': np.float32(6.6699996), 'reward_variance': np.float32(2.1466432), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07569340504705906), 'actor_loss': np.float64(-0.935914409160614), 'hyper_actor_loss': np.float64(0.003471935517154634), 'behavior_loss': np.float64(0.5143903464078903)}

Episode step 11580, time diff 0.7721450328826904, total time dif 976.715003490448)
step: 11580 @ episode report: {'average_total_reward': np.float32(7.045556), 'reward_variance': np.float32(1.333369), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06530495919287205), 'actor_loss': np.float64(-0.974066025018692), 'hyper_actor_loss': np.float64(0.0030725743621587754), 'behavior_loss': np.float64(0.5376511067152023)}

Episode step 11590, time diff 0.7897069454193115, total time dif 977.4871485233307)
step: 11590 @ episode report: {'average_total_reward': np.float32(6.8555555), 'reward_variance': np.float32(2.2438276), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0787822462618351), 'actor_loss': np.float64(-0.9608621180057526), 'hyper_actor_loss': np.float64(0.0026176614919677377), 'behavior_loss': np.float64(0.5109913945198059)}

Episode step 11600, time diff 0.7611575126647949, total time dif 978.27685546875)
step: 11600 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(2.3003654), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08308879286050797), 'actor_loss': np.float64(-0.991884458065033), 'hyper_actor_loss': np.float64(0.002373486710712314), 'behavior_loss': np.float64(0.49270249903202057)}

Episode step 11610, time diff 0.7855045795440674, total time dif 979.0380129814148)
step: 11610 @ episode report: {'average_total_reward': np.float32(5.6577773), 'reward_variance': np.float32(1.3194523), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07556042298674584), 'actor_loss': np.float64(-0.9603048920631408), 'hyper_actor_loss': np.float64(0.0020169745432212947), 'behavior_loss': np.float64(0.5227739810943604)}

Episode step 11620, time diff 0.7640078067779541, total time dif 979.8235175609589)
step: 11620 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(1.2381341), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06631384305655956), 'actor_loss': np.float64(-0.9454762995243072), 'hyper_actor_loss': np.float64(0.0018712619435973465), 'behavior_loss': np.float64(0.5200771480798722)}

Episode step 11630, time diff 0.7896015644073486, total time dif 980.5875253677368)
step: 11630 @ episode report: {'average_total_reward': np.float32(6.618889), 'reward_variance': np.float32(1.8106434), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07710200361907482), 'actor_loss': np.float64(-0.9800157487392426), 'hyper_actor_loss': np.float64(0.0016195305273868144), 'behavior_loss': np.float64(0.49522957801818845)}

Episode step 11640, time diff 0.785546064376831, total time dif 981.3771269321442)
step: 11640 @ episode report: {'average_total_reward': np.float32(6.7211113), 'reward_variance': np.float32(3.6859863), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07012107372283935), 'actor_loss': np.float64(-0.9749989628791809), 'hyper_actor_loss': np.float64(0.001582382805645466), 'behavior_loss': np.float64(0.47986128032207487)}

Episode step 11650, time diff 0.7930457592010498, total time dif 982.162672996521)
step: 11650 @ episode report: {'average_total_reward': np.float32(6.555556), 'reward_variance': np.float32(1.9738277), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07950001135468483), 'actor_loss': np.float64(-0.9519872546195984), 'hyper_actor_loss': np.float64(0.0014556384296156466), 'behavior_loss': np.float64(0.5328047662973404)}

Episode step 11660, time diff 0.7677450180053711, total time dif 982.955718755722)
step: 11660 @ episode report: {'average_total_reward': np.float32(6.6066675), 'reward_variance': np.float32(1.3208201), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07935386933386326), 'actor_loss': np.float64(-0.9783237934112549), 'hyper_actor_loss': np.float64(0.0013739603920839727), 'behavior_loss': np.float64(0.48951880633831024)}

Episode step 11670, time diff 0.9084963798522949, total time dif 983.7234637737274)
step: 11670 @ episode report: {'average_total_reward': np.float32(7.3922224), 'reward_variance': np.float32(1.3877795), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07289803326129914), 'actor_loss': np.float64(-0.9704158902168274), 'hyper_actor_loss': np.float64(0.0013830898678861558), 'behavior_loss': np.float64(0.49311549961566925)}

Episode step 11680, time diff 0.7368631362915039, total time dif 984.6319601535797)
step: 11680 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(4.2410617), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07434338442981243), 'actor_loss': np.float64(-0.9547873497009277), 'hyper_actor_loss': np.float64(0.0013332388713024556), 'behavior_loss': np.float64(0.5039940357208252)}

Episode step 11690, time diff 0.7583503723144531, total time dif 985.3688232898712)
step: 11690 @ episode report: {'average_total_reward': np.float32(6.406667), 'reward_variance': np.float32(1.6797335), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0781956173479557), 'actor_loss': np.float64(-0.9789891004562378), 'hyper_actor_loss': np.float64(0.00131335238693282), 'behavior_loss': np.float64(0.5167486280202865)}

Episode step 11700, time diff 0.75862717628479, total time dif 986.1271736621857)
step: 11700 @ episode report: {'average_total_reward': np.float32(6.9188895), 'reward_variance': np.float32(1.9249395), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05949007272720337), 'actor_loss': np.float64(-0.9459448218345642), 'hyper_actor_loss': np.float64(0.00127551534678787), 'behavior_loss': np.float64(0.4756277233362198)}

Episode step 11710, time diff 0.7762501239776611, total time dif 986.8858008384705)
step: 11710 @ episode report: {'average_total_reward': np.float32(6.694444), 'reward_variance': np.float32(2.0261788), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0826668854802847), 'actor_loss': np.float64(-0.9445331573486329), 'hyper_actor_loss': np.float64(0.0012605705647729336), 'behavior_loss': np.float64(0.5119988322257996)}

Episode step 11720, time diff 0.7449295520782471, total time dif 987.6620509624481)
step: 11720 @ episode report: {'average_total_reward': np.float32(8.016667), 'reward_variance': np.float32(3.025488), 'max_total_reward': np.float32(10.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06811990514397621), 'actor_loss': np.float64(-0.9773720622062683), 'hyper_actor_loss': np.float64(0.001314411184284836), 'behavior_loss': np.float64(0.49202724993228913)}

Episode step 11730, time diff 0.7604537010192871, total time dif 988.4069805145264)
step: 11730 @ episode report: {'average_total_reward': np.float32(7.7777786), 'reward_variance': np.float32(3.6988406), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06878846548497677), 'actor_loss': np.float64(-0.9366823494434356), 'hyper_actor_loss': np.float64(0.001286811346653849), 'behavior_loss': np.float64(0.4784186780452728)}

Episode step 11740, time diff 0.777888298034668, total time dif 989.1674342155457)
step: 11740 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(2.2394624), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05684589445590973), 'actor_loss': np.float64(-0.9488214254379272), 'hyper_actor_loss': np.float64(0.0013293138355948031), 'behavior_loss': np.float64(0.4884566247463226)}

Episode step 11750, time diff 0.7676856517791748, total time dif 989.9453225135803)
step: 11750 @ episode report: {'average_total_reward': np.float32(8.514444), 'reward_variance': np.float32(1.72494), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0758206531405449), 'actor_loss': np.float64(-0.969800877571106), 'hyper_actor_loss': np.float64(0.0013415216701105237), 'behavior_loss': np.float64(0.472164386510849)}

Episode step 11760, time diff 0.7716326713562012, total time dif 990.7130081653595)
step: 11760 @ episode report: {'average_total_reward': np.float32(8.53889), 'reward_variance': np.float32(3.5522296), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0761489950120449), 'actor_loss': np.float64(-0.9960022926330566), 'hyper_actor_loss': np.float64(0.0013646475970745086), 'behavior_loss': np.float64(0.5053245186805725)}

Episode step 11770, time diff 0.7761960029602051, total time dif 991.4846408367157)
step: 11770 @ episode report: {'average_total_reward': np.float32(8.851112), 'reward_variance': np.float32(0.92879474), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06991357766091824), 'actor_loss': np.float64(-0.975921493768692), 'hyper_actor_loss': np.float64(0.0013019598671235144), 'behavior_loss': np.float64(0.4590582370758057)}

Episode step 11780, time diff 0.757413387298584, total time dif 992.2608368396759)
step: 11780 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(2.9711554), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06507933400571346), 'actor_loss': np.float64(-0.9626911401748657), 'hyper_actor_loss': np.float64(0.001295994652900845), 'behavior_loss': np.float64(0.45262558460235597)}

Episode step 11790, time diff 0.7819919586181641, total time dif 993.0182502269745)
step: 11790 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(2.2988198), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06467811204493046), 'actor_loss': np.float64(-0.9424524366855621), 'hyper_actor_loss': np.float64(0.0012423180742189288), 'behavior_loss': np.float64(0.5031467765569687)}

Episode step 11800, time diff 0.7586236000061035, total time dif 993.8002421855927)
step: 11800 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(4.134845), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07955138273537159), 'actor_loss': np.float64(-0.9639160096645355), 'hyper_actor_loss': np.float64(0.0011751364101655782), 'behavior_loss': np.float64(0.46690425276756287)}

Episode step 11810, time diff 0.761559247970581, total time dif 994.5588657855988)
step: 11810 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(2.507508), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08719100207090377), 'actor_loss': np.float64(-0.9861452281475067), 'hyper_actor_loss': np.float64(0.0011347416206263007), 'behavior_loss': np.float64(0.46671704649925233)}

Episode step 11820, time diff 0.7504041194915771, total time dif 995.3204250335693)
step: 11820 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(1.7957885), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07377386465668678), 'actor_loss': np.float64(-0.979608166217804), 'hyper_actor_loss': np.float64(0.0011431538383476435), 'behavior_loss': np.float64(0.4226331919431686)}

Episode step 11830, time diff 0.9462606906890869, total time dif 996.0708291530609)
step: 11830 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(5.2177153), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07784008122980594), 'actor_loss': np.float64(-0.9734759509563446), 'hyper_actor_loss': np.float64(0.0010369633499067276), 'behavior_loss': np.float64(0.44187813699245454)}

Episode step 11840, time diff 0.7681190967559814, total time dif 997.01708984375)
step: 11840 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(3.2805202), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07421087138354779), 'actor_loss': np.float64(-0.9800882160663604), 'hyper_actor_loss': np.float64(0.0010219974734354763), 'behavior_loss': np.float64(0.45928039848804475)}

Episode step 11850, time diff 0.7588162422180176, total time dif 997.785208940506)
step: 11850 @ episode report: {'average_total_reward': np.float32(9.324446), 'reward_variance': np.float32(4.9476504), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07210338935256004), 'actor_loss': np.float64(-0.9610676229000091), 'hyper_actor_loss': np.float64(0.0009715443826280534), 'behavior_loss': np.float64(0.4858975797891617)}

Episode step 11860, time diff 0.7494072914123535, total time dif 998.544025182724)
step: 11860 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(4.2814074), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06737371534109116), 'actor_loss': np.float64(-0.966229248046875), 'hyper_actor_loss': np.float64(0.0009594582254067064), 'behavior_loss': np.float64(0.4410252869129181)}

Episode step 11870, time diff 0.7605342864990234, total time dif 999.2934324741364)
step: 11870 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(1.0171803), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07176608629524708), 'actor_loss': np.float64(-0.9697988033294678), 'hyper_actor_loss': np.float64(0.0009233083401340991), 'behavior_loss': np.float64(0.45830214619636533)}

Episode step 11880, time diff 0.7939541339874268, total time dif 1000.0539667606354)
step: 11880 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(4.144816), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.533333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07750076986849308), 'actor_loss': np.float64(-0.9659213960170746), 'hyper_actor_loss': np.float64(0.0009208771109115333), 'behavior_loss': np.float64(0.462693789601326)}

Episode step 11890, time diff 0.7554891109466553, total time dif 1000.8479208946228)
step: 11890 @ episode report: {'average_total_reward': np.float32(9.551112), 'reward_variance': np.float32(4.2272153), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07676739618182182), 'actor_loss': np.float64(-0.9811369061470032), 'hyper_actor_loss': np.float64(0.0009674435306806118), 'behavior_loss': np.float64(0.45740693509578706)}

Episode step 11900, time diff 0.7601919174194336, total time dif 1001.6034100055695)
step: 11900 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(1.5224707), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08058300092816353), 'actor_loss': np.float64(-0.9549527704715729), 'hyper_actor_loss': np.float64(0.000934855715604499), 'behavior_loss': np.float64(0.4663466662168503)}

Episode step 11910, time diff 0.7433700561523438, total time dif 1002.3636019229889)
step: 11910 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(0.79375446), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0699316594749689), 'actor_loss': np.float64(-0.9710426688194275), 'hyper_actor_loss': np.float64(0.0009606618143152446), 'behavior_loss': np.float64(0.43719284534454345)}

Episode step 11920, time diff 0.7338824272155762, total time dif 1003.1069719791412)
step: 11920 @ episode report: {'average_total_reward': np.float32(9.887778), 'reward_variance': np.float32(2.2223315), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07805721461772919), 'actor_loss': np.float64(-0.9611770868301391), 'hyper_actor_loss': np.float64(0.0009279194637201726), 'behavior_loss': np.float64(0.47640261650085447)}

Episode step 11930, time diff 0.7241232395172119, total time dif 1003.8408544063568)
step: 11930 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.6955181), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07771551981568336), 'actor_loss': np.float64(-0.9733177781105041), 'hyper_actor_loss': np.float64(0.0009357932314742357), 'behavior_loss': np.float64(0.4389978736639023)}

Episode step 11940, time diff 0.7223947048187256, total time dif 1004.564977645874)
step: 11940 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(2.241661), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07795787677168846), 'actor_loss': np.float64(-0.9916748046875), 'hyper_actor_loss': np.float64(0.0009105166886001825), 'behavior_loss': np.float64(0.44071791470050814)}

Episode step 11950, time diff 0.751793384552002, total time dif 1005.2873723506927)
step: 11950 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(2.05602), 'max_total_reward': np.float32(12.022223), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08040275163948536), 'actor_loss': np.float64(-0.9700559258460999), 'hyper_actor_loss': np.float64(0.0009251837676856667), 'behavior_loss': np.float64(0.44845831096172334)}

Episode step 11960, time diff 0.7288026809692383, total time dif 1006.0391657352448)
step: 11960 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(1.3060857), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07106703147292137), 'actor_loss': np.float64(-0.9590105235576629), 'hyper_actor_loss': np.float64(0.000887254811823368), 'behavior_loss': np.float64(0.4267211467027664)}

Episode step 11970, time diff 0.756298303604126, total time dif 1006.767968416214)
step: 11970 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(1.979111), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07979251109063626), 'actor_loss': np.float64(-0.9895186960697174), 'hyper_actor_loss': np.float64(0.0008991717360913754), 'behavior_loss': np.float64(0.4211737155914307)}

Episode step 11980, time diff 0.7202348709106445, total time dif 1007.5242667198181)
step: 11980 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(0.73306656), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06787629425525665), 'actor_loss': np.float64(-0.9596514105796814), 'hyper_actor_loss': np.float64(0.0009238440426997841), 'behavior_loss': np.float64(0.46770249903202055)}

Episode step 11990, time diff 0.7392687797546387, total time dif 1008.2445015907288)
step: 11990 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(1.2127417), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08084594346582889), 'actor_loss': np.float64(-0.9532231032848358), 'hyper_actor_loss': np.float64(0.0009293221053667366), 'behavior_loss': np.float64(0.47013868391513824)}

Episode step 12000, time diff 0.9097118377685547, total time dif 1008.9837703704834)
step: 12000 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(2.4430423), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0769672341644764), 'actor_loss': np.float64(-0.9785420417785644), 'hyper_actor_loss': np.float64(0.0009670153085608035), 'behavior_loss': np.float64(0.4320910155773163)}

Episode step 12010, time diff 0.7653543949127197, total time dif 1009.893482208252)
step: 12010 @ episode report: {'average_total_reward': np.float32(8.32889), 'reward_variance': np.float32(1.9676106), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07152406647801399), 'actor_loss': np.float64(-0.9590108096599579), 'hyper_actor_loss': np.float64(0.0009649809449911118), 'behavior_loss': np.float64(0.465145006775856)}

Episode step 12020, time diff 0.7838687896728516, total time dif 1010.6588366031647)
step: 12020 @ episode report: {'average_total_reward': np.float32(9.875556), 'reward_variance': np.float32(1.6597732), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08000542744994163), 'actor_loss': np.float64(-0.9722882747650147), 'hyper_actor_loss': np.float64(0.0010366174450609834), 'behavior_loss': np.float64(0.41467538475990295)}

Episode step 12030, time diff 0.7671363353729248, total time dif 1011.4427053928375)
step: 12030 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(1.4259809), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07873547151684761), 'actor_loss': np.float64(-1.0047143816947937), 'hyper_actor_loss': np.float64(0.0010955419624224306), 'behavior_loss': np.float64(0.45278393626213076)}

Episode step 12040, time diff 0.7780506610870361, total time dif 1012.2098417282104)
step: 12040 @ episode report: {'average_total_reward': np.float32(10.285557), 'reward_variance': np.float32(3.3479276), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0825042262673378), 'actor_loss': np.float64(-0.9753137111663819), 'hyper_actor_loss': np.float64(0.0011444360483437777), 'behavior_loss': np.float64(0.41782946288585665)}

Episode step 12050, time diff 0.7784149646759033, total time dif 1012.9878923892975)
step: 12050 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(4.635261), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06851850189268589), 'actor_loss': np.float64(-0.9649475455284119), 'hyper_actor_loss': np.float64(0.0012242768425494432), 'behavior_loss': np.float64(0.47012740969657896)}

Episode step 12060, time diff 0.7427158355712891, total time dif 1013.7663073539734)
step: 12060 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.0883262), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06610607374459505), 'actor_loss': np.float64(-0.9427393734455108), 'hyper_actor_loss': np.float64(0.001273566880263388), 'behavior_loss': np.float64(0.43946529626846315)}

Episode step 12070, time diff 0.7662105560302734, total time dif 1014.5090231895447)
step: 12070 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(1.5697091), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07883746922016144), 'actor_loss': np.float64(-0.9715876579284668), 'hyper_actor_loss': np.float64(0.0012336605810560285), 'behavior_loss': np.float64(0.41833398342132566)}

Episode step 12080, time diff 0.7376844882965088, total time dif 1015.275233745575)
step: 12080 @ episode report: {'average_total_reward': np.float32(8.663335), 'reward_variance': np.float32(2.5545204), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06905231140553951), 'actor_loss': np.float64(-0.9815034210681916), 'hyper_actor_loss': np.float64(0.001245754596311599), 'behavior_loss': np.float64(0.419167622923851)}

Episode step 12090, time diff 0.7275989055633545, total time dif 1016.0129182338715)
step: 12090 @ episode report: {'average_total_reward': np.float32(9.412224), 'reward_variance': np.float32(2.2237153), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07667637914419174), 'actor_loss': np.float64(-0.963588273525238), 'hyper_actor_loss': np.float64(0.0011380791547708213), 'behavior_loss': np.float64(0.43824350237846377)}

Episode step 12100, time diff 0.7581644058227539, total time dif 1016.7405171394348)
step: 12100 @ episode report: {'average_total_reward': np.float32(9.73889), 'reward_variance': np.float32(0.9363766), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07814977951347828), 'actor_loss': np.float64(-0.9921493351459503), 'hyper_actor_loss': np.float64(0.0010042994399555027), 'behavior_loss': np.float64(0.4314005583524704)}

Episode step 12110, time diff 0.7243108749389648, total time dif 1017.4986815452576)
step: 12110 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(1.4248699), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07893821485340595), 'actor_loss': np.float64(-0.9723616898059845), 'hyper_actor_loss': np.float64(0.001023438252741471), 'behavior_loss': np.float64(0.4144546568393707)}

Episode step 12120, time diff 0.7607707977294922, total time dif 1018.2229924201965)
step: 12120 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(1.313493), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08155306205153465), 'actor_loss': np.float64(-0.9687422931194305), 'hyper_actor_loss': np.float64(0.0010010592464823275), 'behavior_loss': np.float64(0.43940130770206454)}

Episode step 12130, time diff 0.7590925693511963, total time dif 1018.983763217926)
step: 12130 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(3.5883572), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07885076850652695), 'actor_loss': np.float64(-0.9693006634712219), 'hyper_actor_loss': np.float64(0.000929465756053105), 'behavior_loss': np.float64(0.4330348640680313)}

Episode step 12140, time diff 0.7601075172424316, total time dif 1019.7428557872772)
step: 12140 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.0534817), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07947642803192138), 'actor_loss': np.float64(-0.9664342701435089), 'hyper_actor_loss': np.float64(0.000977713620522991), 'behavior_loss': np.float64(0.41756289899349214)}

Episode step 12150, time diff 0.7386863231658936, total time dif 1020.5029633045197)
step: 12150 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.2665143), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0815452490001917), 'actor_loss': np.float64(-0.9841783761978149), 'hyper_actor_loss': np.float64(0.0009890660701785236), 'behavior_loss': np.float64(0.42171856462955476)}

Episode step 12160, time diff 0.9515607357025146, total time dif 1021.2416496276855)
step: 12160 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(1.2029089), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07506694048643112), 'actor_loss': np.float64(-0.9743637323379517), 'hyper_actor_loss': np.float64(0.0010840653092600405), 'behavior_loss': np.float64(0.4112954825162888)}

Episode step 12170, time diff 0.7511260509490967, total time dif 1022.1932103633881)
step: 12170 @ episode report: {'average_total_reward': np.float32(8.665556), 'reward_variance': np.float32(2.13732), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07763678468763828), 'actor_loss': np.float64(-0.9732909560203552), 'hyper_actor_loss': np.float64(0.0012082629022188486), 'behavior_loss': np.float64(0.3979425966739655)}

Episode step 12180, time diff 0.7655470371246338, total time dif 1022.9443364143372)
step: 12180 @ episode report: {'average_total_reward': np.float32(9.3144455), 'reward_variance': np.float32(2.3197548), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08327382579445838), 'actor_loss': np.float64(-0.9994702458381652), 'hyper_actor_loss': np.float64(0.0014221272547729313), 'behavior_loss': np.float64(0.4240929037332535)}

Episode step 12190, time diff 0.8007733821868896, total time dif 1023.7098834514618)
step: 12190 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.5348098), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07267130427062511), 'actor_loss': np.float64(-0.975242680311203), 'hyper_actor_loss': np.float64(0.001777531870175153), 'behavior_loss': np.float64(0.41456915736198424)}

Episode step 12200, time diff 0.7623119354248047, total time dif 1024.5106568336487)
step: 12200 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(6.3837085), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07876215726137162), 'actor_loss': np.float64(-0.9641285836696625), 'hyper_actor_loss': np.float64(0.002038756129331887), 'behavior_loss': np.float64(0.4167431354522705)}

Episode step 12210, time diff 0.7830309867858887, total time dif 1025.2729687690735)
step: 12210 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.8246422), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07531556785106659), 'actor_loss': np.float64(-0.984039968252182), 'hyper_actor_loss': np.float64(0.00227708350867033), 'behavior_loss': np.float64(0.4371305763721466)}

Episode step 12220, time diff 0.7958986759185791, total time dif 1026.0559997558594)
step: 12220 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(2.4768746), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07810352928936481), 'actor_loss': np.float64(-0.9644069790840148), 'hyper_actor_loss': np.float64(0.002247172431088984), 'behavior_loss': np.float64(0.4165013074874878)}

Episode step 12230, time diff 0.7847752571105957, total time dif 1026.851898431778)
step: 12230 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.1838088), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0698122251778841), 'actor_loss': np.float64(-0.9637050747871398), 'hyper_actor_loss': np.float64(0.0022306717932224275), 'behavior_loss': np.float64(0.42345923483371734)}

Episode step 12240, time diff 0.7784469127655029, total time dif 1027.6366736888885)
step: 12240 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.7463964), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06776063442230225), 'actor_loss': np.float64(-0.972639137506485), 'hyper_actor_loss': np.float64(0.0021083161467686296), 'behavior_loss': np.float64(0.4024422228336334)}

Episode step 12250, time diff 0.7812750339508057, total time dif 1028.415120601654)
step: 12250 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(1.9672699), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07080438770353795), 'actor_loss': np.float64(-0.9885830461978913), 'hyper_actor_loss': np.float64(0.0020626543555408716), 'behavior_loss': np.float64(0.42164296805858614)}

Episode step 12260, time diff 0.7975773811340332, total time dif 1029.1963956356049)
step: 12260 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(2.8604245), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08024144731462002), 'actor_loss': np.float64(-0.9923559308052063), 'hyper_actor_loss': np.float64(0.0021375830518081786), 'behavior_loss': np.float64(0.3969075560569763)}

Episode step 12270, time diff 0.7498142719268799, total time dif 1029.993973016739)
step: 12270 @ episode report: {'average_total_reward': np.float32(10.348889), 'reward_variance': np.float32(2.4479067), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07747112661600113), 'actor_loss': np.float64(-0.9887362599372864), 'hyper_actor_loss': np.float64(0.0021693914430215957), 'behavior_loss': np.float64(0.4246574968099594)}

Episode step 12280, time diff 0.764967679977417, total time dif 1030.7437872886658)
step: 12280 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(3.520001), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06724564470350743), 'actor_loss': np.float64(-0.9622041940689087), 'hyper_actor_loss': np.float64(0.002222455944865942), 'behavior_loss': np.float64(0.40867567956447604)}

Episode step 12290, time diff 0.7799596786499023, total time dif 1031.5087549686432)
step: 12290 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(2.3868256), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07680038884282112), 'actor_loss': np.float64(-0.9916251420974731), 'hyper_actor_loss': np.float64(0.0022356062196195125), 'behavior_loss': np.float64(0.36990993320941923)}

Episode step 12300, time diff 0.7733089923858643, total time dif 1032.288714647293)
step: 12300 @ episode report: {'average_total_reward': np.float32(11.1466675), 'reward_variance': np.float32(1.2411307), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(9.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06261160597205162), 'actor_loss': np.float64(-0.9630827844142914), 'hyper_actor_loss': np.float64(0.0019719849457032978), 'behavior_loss': np.float64(0.41867789030075075)}

Episode step 12310, time diff 0.774479866027832, total time dif 1033.062023639679)
step: 12310 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.0641103), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06197512820363045), 'actor_loss': np.float64(-0.9217579126358032), 'hyper_actor_loss': np.float64(0.0015863887965679169), 'behavior_loss': np.float64(0.39719509780406953)}

Episode step 12320, time diff 0.757105827331543, total time dif 1033.8365035057068)
step: 12320 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(4.8694615), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07688903212547302), 'actor_loss': np.float64(-0.9827389478683471), 'hyper_actor_loss': np.float64(0.0016214025788940489), 'behavior_loss': np.float64(0.3968586653470993)}

Episode step 12330, time diff 0.8987987041473389, total time dif 1034.5936093330383)
step: 12330 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(4.9518886), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07890389151871205), 'actor_loss': np.float64(-0.9600759327411652), 'hyper_actor_loss': np.float64(0.0015274027013219894), 'behavior_loss': np.float64(0.39382455348968504)}

Episode step 12340, time diff 0.7478981018066406, total time dif 1035.4924080371857)
step: 12340 @ episode report: {'average_total_reward': np.float32(8.802222), 'reward_variance': np.float32(4.4952555), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07679296135902405), 'actor_loss': np.float64(-0.9800333678722382), 'hyper_actor_loss': np.float64(0.0014328919001854955), 'behavior_loss': np.float64(0.4212962418794632)}

Episode step 12350, time diff 0.777646541595459, total time dif 1036.2403061389923)
step: 12350 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.4804502), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06905375979840755), 'actor_loss': np.float64(-0.9658807873725891), 'hyper_actor_loss': np.float64(0.001419186347629875), 'behavior_loss': np.float64(0.3956896126270294)}

Episode step 12360, time diff 0.7541329860687256, total time dif 1037.0179526805878)
step: 12360 @ episode report: {'average_total_reward': np.float32(10.758889), 'reward_variance': np.float32(4.460176), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060227082669734956), 'actor_loss': np.float64(-0.9530023276805878), 'hyper_actor_loss': np.float64(0.0015146271442063152), 'behavior_loss': np.float64(0.37320027947425843)}

Episode step 12370, time diff 0.7932319641113281, total time dif 1037.7720856666565)
step: 12370 @ episode report: {'average_total_reward': np.float32(10.01), 'reward_variance': np.float32(3.645321), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07999634929001331), 'actor_loss': np.float64(-0.9871816098690033), 'hyper_actor_loss': np.float64(0.0016304322169162333), 'behavior_loss': np.float64(0.40321671664714814)}

Episode step 12380, time diff 0.7895257472991943, total time dif 1038.5653176307678)
step: 12380 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.6696064), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06709385327994824), 'actor_loss': np.float64(-0.9710994243621827), 'hyper_actor_loss': np.float64(0.0016848016413860023), 'behavior_loss': np.float64(0.39158024191856383)}

Episode step 12390, time diff 0.8014423847198486, total time dif 1039.354843378067)
step: 12390 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.72253), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09249870479106903), 'actor_loss': np.float64(-0.9798413932323455), 'hyper_actor_loss': np.float64(0.0019409155240282417), 'behavior_loss': np.float64(0.39328652918338775)}

Episode step 12400, time diff 0.7785539627075195, total time dif 1040.1562857627869)
step: 12400 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(1.7736107), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061618483066558837), 'actor_loss': np.float64(-0.994584983587265), 'hyper_actor_loss': np.float64(0.0020407634321600197), 'behavior_loss': np.float64(0.3352804481983185)}

Episode step 12410, time diff 0.7669405937194824, total time dif 1040.9348397254944)
step: 12410 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(3.8544736), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07761050630360841), 'actor_loss': np.float64(-0.9556316673755646), 'hyper_actor_loss': np.float64(0.0019628486479632556), 'behavior_loss': np.float64(0.39733709394931793)}

Episode step 12420, time diff 0.7681305408477783, total time dif 1041.7017803192139)
step: 12420 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(1.7492641), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08054916895925998), 'actor_loss': np.float64(-0.9836196959018707), 'hyper_actor_loss': np.float64(0.001839210547041148), 'behavior_loss': np.float64(0.36263965666294096)}

Episode step 12430, time diff 0.7638189792633057, total time dif 1042.4699108600616)
step: 12430 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(5.448233), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07229368686676026), 'actor_loss': np.float64(-0.9928295910358429), 'hyper_actor_loss': np.float64(0.0017030902090482414), 'behavior_loss': np.float64(0.37862181663513184)}

Episode step 12440, time diff 0.7668943405151367, total time dif 1043.233729839325)
step: 12440 @ episode report: {'average_total_reward': np.float32(11.66889), 'reward_variance': np.float32(3.9726872), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07589561305940151), 'actor_loss': np.float64(-0.9651904225349426), 'hyper_actor_loss': np.float64(0.0016116579528898001), 'behavior_loss': np.float64(0.40177385210990907)}

Episode step 12450, time diff 0.755486011505127, total time dif 1044.00062417984)
step: 12450 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.9295082), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0736784391105175), 'actor_loss': np.float64(-1.0021569371223449), 'hyper_actor_loss': np.float64(0.0016183321247808635), 'behavior_loss': np.float64(0.35372146368026736)}

Episode step 12460, time diff 0.7616257667541504, total time dif 1044.7561101913452)
step: 12460 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(5.845246), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07911421619355678), 'actor_loss': np.float64(-0.984975266456604), 'hyper_actor_loss': np.float64(0.0014609970967285335), 'behavior_loss': np.float64(0.3661098301410675)}

Episode step 12470, time diff 0.7798607349395752, total time dif 1045.5177359580994)
step: 12470 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.9788212), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07670213766396046), 'actor_loss': np.float64(-0.9820279121398926), 'hyper_actor_loss': np.float64(0.001388111722189933), 'behavior_loss': np.float64(0.35728880763053894)}

Episode step 12480, time diff 0.7576501369476318, total time dif 1046.297596693039)
step: 12480 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.056125), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0706606436520815), 'actor_loss': np.float64(-0.9893537580966949), 'hyper_actor_loss': np.float64(0.0012405134621076286), 'behavior_loss': np.float64(0.36424470543861387)}

Episode step 12490, time diff 0.7273173332214355, total time dif 1047.0552468299866)
step: 12490 @ episode report: {'average_total_reward': np.float32(11.532224), 'reward_variance': np.float32(2.6032956), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07451283596456051), 'actor_loss': np.float64(-0.9671799719333649), 'hyper_actor_loss': np.float64(0.0012986418907530606), 'behavior_loss': np.float64(0.366587883234024)}

Episode step 12500, time diff 0.921184778213501, total time dif 1047.782564163208)
step: 12500 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(3.4755433), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07375752031803132), 'actor_loss': np.float64(-0.9937994539737701), 'hyper_actor_loss': np.float64(0.0014417144819162786), 'behavior_loss': np.float64(0.36439178287982943)}

Episode step 12510, time diff 0.7685596942901611, total time dif 1048.7037489414215)
step: 12510 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(1.4862179), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07639221511781216), 'actor_loss': np.float64(-1.005272912979126), 'hyper_actor_loss': np.float64(0.0014265638776123524), 'behavior_loss': np.float64(0.3573922663927078)}

Episode step 12520, time diff 0.734203577041626, total time dif 1049.4723086357117)
step: 12520 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(4.742778), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06481790393590928), 'actor_loss': np.float64(-0.9648144781589508), 'hyper_actor_loss': np.float64(0.001443387067411095), 'behavior_loss': np.float64(0.35251704454421995)}

Episode step 12530, time diff 0.7856357097625732, total time dif 1050.2065122127533)
step: 12530 @ episode report: {'average_total_reward': np.float32(11.644445), 'reward_variance': np.float32(7.329975), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.083558614179492), 'actor_loss': np.float64(-0.9951387643814087), 'hyper_actor_loss': np.float64(0.0015607329667545855), 'behavior_loss': np.float64(0.38496565222740176)}

Episode step 12540, time diff 0.7591266632080078, total time dif 1050.9921479225159)
step: 12540 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(5.861478), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08372295275330544), 'actor_loss': np.float64(-0.9852904379367828), 'hyper_actor_loss': np.float64(0.0015148734441027046), 'behavior_loss': np.float64(0.37546891570091245)}

Episode step 12550, time diff 0.7732808589935303, total time dif 1051.7512745857239)
step: 12550 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(1.9243513), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07932790741324425), 'actor_loss': np.float64(-0.988852572441101), 'hyper_actor_loss': np.float64(0.0014084493974223732), 'behavior_loss': np.float64(0.3647761672735214)}

Episode step 12560, time diff 0.7529737949371338, total time dif 1052.5245554447174)
step: 12560 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(6.54931), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07786641158163547), 'actor_loss': np.float64(-0.993162888288498), 'hyper_actor_loss': np.float64(0.0013175522908568382), 'behavior_loss': np.float64(0.3868531554937363)}

Episode step 12570, time diff 0.7529313564300537, total time dif 1053.2775292396545)
step: 12570 @ episode report: {'average_total_reward': np.float32(11.2711115), 'reward_variance': np.float32(3.5909927), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07280798628926277), 'actor_loss': np.float64(-0.9456065595149994), 'hyper_actor_loss': np.float64(0.0010998957324773074), 'behavior_loss': np.float64(0.4031727880239487)}

Episode step 12580, time diff 0.7603335380554199, total time dif 1054.0304605960846)
step: 12580 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(0.8913143), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07845389991998672), 'actor_loss': np.float64(-0.984127938747406), 'hyper_actor_loss': np.float64(0.0010307946242392064), 'behavior_loss': np.float64(0.365837037563324)}

Episode step 12590, time diff 0.7651948928833008, total time dif 1054.79079413414)
step: 12590 @ episode report: {'average_total_reward': np.float32(10.997779), 'reward_variance': np.float32(3.734637), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07281942442059516), 'actor_loss': np.float64(-0.9984494686126709), 'hyper_actor_loss': np.float64(0.0010063889785669744), 'behavior_loss': np.float64(0.35076929032802584)}

Episode step 12600, time diff 0.7779130935668945, total time dif 1055.5559890270233)
step: 12600 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(4.325872), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07515089437365532), 'actor_loss': np.float64(-0.9654918432235717), 'hyper_actor_loss': np.float64(0.0009786059032194316), 'behavior_loss': np.float64(0.3888481199741364)}

Episode step 12610, time diff 0.7664680480957031, total time dif 1056.3339021205902)
step: 12610 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(3.3856342), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07387151457369327), 'actor_loss': np.float64(-0.9711686968803406), 'hyper_actor_loss': np.float64(0.0011851254967041314), 'behavior_loss': np.float64(0.3673625886440277)}

Episode step 12620, time diff 0.7714147567749023, total time dif 1057.100370168686)
step: 12620 @ episode report: {'average_total_reward': np.float32(10.45889), 'reward_variance': np.float32(2.0777042), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06794992536306381), 'actor_loss': np.float64(-0.9922299683094025), 'hyper_actor_loss': np.float64(0.0014602491399273276), 'behavior_loss': np.float64(0.37517629861831664)}

Episode step 12630, time diff 0.7930419445037842, total time dif 1057.8717849254608)
step: 12630 @ episode report: {'average_total_reward': np.float32(11.407778), 'reward_variance': np.float32(4.3230143), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06641994789242744), 'actor_loss': np.float64(-0.9663949966430664), 'hyper_actor_loss': np.float64(0.0014573344960808754), 'behavior_loss': np.float64(0.36860481798648836)}

Episode step 12640, time diff 0.8256785869598389, total time dif 1058.6648268699646)
step: 12640 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(5.348153), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07245322987437249), 'actor_loss': np.float64(-0.9856453120708466), 'hyper_actor_loss': np.float64(0.001506377023179084), 'behavior_loss': np.float64(0.37353948652744295)}

Episode step 12650, time diff 0.790363073348999, total time dif 1059.4905054569244)
step: 12650 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.9297333), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06220309510827064), 'actor_loss': np.float64(-0.9887715816497803), 'hyper_actor_loss': np.float64(0.0014631657511927187), 'behavior_loss': np.float64(0.37676323354244234)}

Episode step 12660, time diff 0.7977275848388672, total time dif 1060.2808685302734)
step: 12660 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.3408947), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07549624033272266), 'actor_loss': np.float64(-0.9736566841602325), 'hyper_actor_loss': np.float64(0.0015860217506997287), 'behavior_loss': np.float64(0.3715848386287689)}

Episode step 12670, time diff 0.9322335720062256, total time dif 1061.0785961151123)
step: 12670 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.8784645), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06904065646231175), 'actor_loss': np.float64(-1.0039330184459687), 'hyper_actor_loss': np.float64(0.0015119041316211223), 'behavior_loss': np.float64(0.38182693123817446)}

Episode step 12680, time diff 0.7668211460113525, total time dif 1062.0108296871185)
step: 12680 @ episode report: {'average_total_reward': np.float32(9.43889), 'reward_variance': np.float32(1.2936113), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07723678350448608), 'actor_loss': np.float64(-0.9801883161067962), 'hyper_actor_loss': np.float64(0.0015135946800000966), 'behavior_loss': np.float64(0.3892816573381424)}

Episode step 12690, time diff 0.7912938594818115, total time dif 1062.7776508331299)
step: 12690 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(1.9214576), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08494985476136208), 'actor_loss': np.float64(-0.9884576678276062), 'hyper_actor_loss': np.float64(0.0015048947650939226), 'behavior_loss': np.float64(0.39642880856990814)}

Episode step 12700, time diff 0.7865219116210938, total time dif 1063.5689446926117)
step: 12700 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(1.1373781), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07772081047296524), 'actor_loss': np.float64(-1.006828600168228), 'hyper_actor_loss': np.float64(0.001500546233728528), 'behavior_loss': np.float64(0.40094674825668336)}

Episode step 12710, time diff 0.7675127983093262, total time dif 1064.3554666042328)
step: 12710 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(2.0176091), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07750485576689244), 'actor_loss': np.float64(-0.9759905397891998), 'hyper_actor_loss': np.float64(0.001552452880423516), 'behavior_loss': np.float64(0.38871017694473264)}

Episode step 12720, time diff 0.7624626159667969, total time dif 1065.1229794025421)
step: 12720 @ episode report: {'average_total_reward': np.float32(8.938889), 'reward_variance': np.float32(3.3254876), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07371986545622349), 'actor_loss': np.float64(-1.006557047367096), 'hyper_actor_loss': np.float64(0.0016905351192690432), 'behavior_loss': np.float64(0.36765762269496916)}

Episode step 12730, time diff 0.7896304130554199, total time dif 1065.885442018509)
step: 12730 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(1.3698283), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06433177031576634), 'actor_loss': np.float64(-0.9765113294124603), 'hyper_actor_loss': np.float64(0.0014138494967482983), 'behavior_loss': np.float64(0.3717176020145416)}

Episode step 12740, time diff 0.7560708522796631, total time dif 1066.6750724315643)
step: 12740 @ episode report: {'average_total_reward': np.float32(7.331111), 'reward_variance': np.float32(0.7837727), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0790579542517662), 'actor_loss': np.float64(-0.9761733770370483), 'hyper_actor_loss': np.float64(0.0013153569772839547), 'behavior_loss': np.float64(0.4007457882165909)}

Episode step 12750, time diff 0.7939083576202393, total time dif 1067.431143283844)
step: 12750 @ episode report: {'average_total_reward': np.float32(7.504445), 'reward_variance': np.float32(1.3387707), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06710282266139984), 'actor_loss': np.float64(-0.9802886128425599), 'hyper_actor_loss': np.float64(0.0011586632463149726), 'behavior_loss': np.float64(0.40614074766635894)}

Episode step 12760, time diff 0.8039772510528564, total time dif 1068.2250516414642)
step: 12760 @ episode report: {'average_total_reward': np.float32(7.455556), 'reward_variance': np.float32(1.5764692), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07721818499267101), 'actor_loss': np.float64(-0.9619163393974304), 'hyper_actor_loss': np.float64(0.0010478323034476488), 'behavior_loss': np.float64(0.3768793195486069)}

Episode step 12770, time diff 0.7543838024139404, total time dif 1069.029028892517)
step: 12770 @ episode report: {'average_total_reward': np.float32(7.716667), 'reward_variance': np.float32(1.3104757), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06518284715712071), 'actor_loss': np.float64(-0.9966401875019073), 'hyper_actor_loss': np.float64(0.0009886361251119525), 'behavior_loss': np.float64(0.38591532707214354)}

Episode step 12780, time diff 0.7476203441619873, total time dif 1069.783412694931)
step: 12780 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(1.7882979), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06275578364729881), 'actor_loss': np.float64(-0.9702133297920227), 'hyper_actor_loss': np.float64(0.00096533143077977), 'behavior_loss': np.float64(0.3881897807121277)}

Episode step 12790, time diff 0.7494244575500488, total time dif 1070.531033039093)
step: 12790 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(1.7036743), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07864417433738709), 'actor_loss': np.float64(-0.9512239694595337), 'hyper_actor_loss': np.float64(0.0010538673086557537), 'behavior_loss': np.float64(0.40171518325805666)}

Episode step 12800, time diff 0.7631807327270508, total time dif 1071.280457496643)
step: 12800 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(2.005755), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07726628556847573), 'actor_loss': np.float64(-1.0071875214576722), 'hyper_actor_loss': np.float64(0.001401460380293429), 'behavior_loss': np.float64(0.38117348253726957)}

Episode step 12810, time diff 0.74143385887146, total time dif 1072.0436382293701)
step: 12810 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.3533694), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.079436020180583), 'actor_loss': np.float64(-0.9769271850585938), 'hyper_actor_loss': np.float64(0.001214641029946506), 'behavior_loss': np.float64(0.3729088634252548)}

Episode step 12820, time diff 0.7549784183502197, total time dif 1072.7850720882416)
step: 12820 @ episode report: {'average_total_reward': np.float32(10.697779), 'reward_variance': np.float32(3.9219952), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808396615087986), 'actor_loss': np.float64(-0.9688004672527313), 'hyper_actor_loss': np.float64(0.0009704350493848323), 'behavior_loss': np.float64(0.40109494626522063)}

Episode step 12830, time diff 0.9322929382324219, total time dif 1073.5400505065918)
step: 12830 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(3.8348947), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664810635149479), 'actor_loss': np.float64(-0.9447567880153656), 'hyper_actor_loss': np.float64(0.0008858421177137643), 'behavior_loss': np.float64(0.40286171734333037)}

Episode step 12840, time diff 0.7161200046539307, total time dif 1074.4723434448242)
step: 12840 @ episode report: {'average_total_reward': np.float32(9.387779), 'reward_variance': np.float32(2.4217153), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08209848403930664), 'actor_loss': np.float64(-0.9820270001888275), 'hyper_actor_loss': np.float64(0.0008378717524465173), 'behavior_loss': np.float64(0.4077057898044586)}

Episode step 12850, time diff 0.7601008415222168, total time dif 1075.1884634494781)
step: 12850 @ episode report: {'average_total_reward': np.float32(9.936668), 'reward_variance': np.float32(6.669632), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06496947798877954), 'actor_loss': np.float64(-0.9863481104373932), 'hyper_actor_loss': np.float64(0.0008493763220030814), 'behavior_loss': np.float64(0.3675653636455536)}

Episode step 12860, time diff 0.7781076431274414, total time dif 1075.9485642910004)
step: 12860 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(4.225561), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06446085628122092), 'actor_loss': np.float64(-0.9479974806308746), 'hyper_actor_loss': np.float64(0.000891471205977723), 'behavior_loss': np.float64(0.39400040209293363)}

Episode step 12870, time diff 0.783066987991333, total time dif 1076.7266719341278)
step: 12870 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(3.6974368), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08066929541528226), 'actor_loss': np.float64(-0.9721209049224854), 'hyper_actor_loss': np.float64(0.0009615013841539621), 'behavior_loss': np.float64(0.35675286054611205)}

Episode step 12880, time diff 0.7749028205871582, total time dif 1077.5097389221191)
step: 12880 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(1.7796848), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06867711953818798), 'actor_loss': np.float64(-0.9825145900249481), 'hyper_actor_loss': np.float64(0.0009853399475105106), 'behavior_loss': np.float64(0.3793663293123245)}

Episode step 12890, time diff 0.8193075656890869, total time dif 1078.2846417427063)
step: 12890 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(0.7583218), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07218780033290387), 'actor_loss': np.float64(-0.9731564283370971), 'hyper_actor_loss': np.float64(0.0008627836068626493), 'behavior_loss': np.float64(0.3512998431921005)}

Episode step 12900, time diff 0.8104429244995117, total time dif 1079.1039493083954)
step: 12900 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(1.8968147), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06370402500033379), 'actor_loss': np.float64(-0.976197499036789), 'hyper_actor_loss': np.float64(0.0007985703705344349), 'behavior_loss': np.float64(0.3674515515565872)}

Episode step 12910, time diff 0.782611608505249, total time dif 1079.914392232895)
step: 12910 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(1.1421926), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.069279008731246), 'actor_loss': np.float64(-0.9561162471771241), 'hyper_actor_loss': np.float64(0.0007357905618846416), 'behavior_loss': np.float64(0.3748217523097992)}

Episode step 12920, time diff 0.7819869518280029, total time dif 1080.6970038414001)
step: 12920 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.644693), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07869313172996044), 'actor_loss': np.float64(-0.9817487359046936), 'hyper_actor_loss': np.float64(0.000788908766116947), 'behavior_loss': np.float64(0.37197692394256593)}

Episode step 12930, time diff 0.774594783782959, total time dif 1081.4789907932281)
step: 12930 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(3.9374528), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08119766637682915), 'actor_loss': np.float64(-0.9930157780647277), 'hyper_actor_loss': np.float64(0.0007842163147870451), 'behavior_loss': np.float64(0.38609660863876344)}

Episode step 12940, time diff 0.7990920543670654, total time dif 1082.253585577011)
step: 12940 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(5.2760763), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07567143514752388), 'actor_loss': np.float64(-0.9789493381977081), 'hyper_actor_loss': np.float64(0.0007423747447319329), 'behavior_loss': np.float64(0.36647210717201234)}

Episode step 12950, time diff 0.7726912498474121, total time dif 1083.0526776313782)
step: 12950 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(2.4152305), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08680249974131585), 'actor_loss': np.float64(-0.9995496511459351), 'hyper_actor_loss': np.float64(0.000827914453111589), 'behavior_loss': np.float64(0.378781858086586)}

Episode step 12960, time diff 0.7910902500152588, total time dif 1083.8253688812256)
step: 12960 @ episode report: {'average_total_reward': np.float32(9.151111), 'reward_variance': np.float32(2.6549423), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08231737986207008), 'actor_loss': np.float64(-0.9754657804965973), 'hyper_actor_loss': np.float64(0.0009179967979434878), 'behavior_loss': np.float64(0.3884335786104202)}

Episode step 12970, time diff 0.7757711410522461, total time dif 1084.6164591312408)
step: 12970 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(6.058506), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07588421404361725), 'actor_loss': np.float64(-0.9870489716529847), 'hyper_actor_loss': np.float64(0.000967875577043742), 'behavior_loss': np.float64(0.37125149369239807)}

Episode step 12980, time diff 0.7657983303070068, total time dif 1085.392230272293)
step: 12980 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.9473453), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06979390382766723), 'actor_loss': np.float64(-0.9915946662425995), 'hyper_actor_loss': np.float64(0.0009456299419980496), 'behavior_loss': np.float64(0.36126460433006286)}

Episode step 12990, time diff 0.9143879413604736, total time dif 1086.1580286026)
step: 12990 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.4355356), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08524073585867882), 'actor_loss': np.float64(-0.9698127329349517), 'hyper_actor_loss': np.float64(0.0009149583347607404), 'behavior_loss': np.float64(0.36904022097587585)}

Episode step 13000, time diff 0.7649831771850586, total time dif 1087.0724165439606)
step: 13000 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(2.700643), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06702492609620095), 'actor_loss': np.float64(-0.9910973072052002), 'hyper_actor_loss': np.float64(0.0008550558472052216), 'behavior_loss': np.float64(0.3730981111526489)}

Episode step 13010, time diff 0.778033971786499, total time dif 1087.8373997211456)
step: 13010 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(3.0280943), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05847739540040493), 'actor_loss': np.float64(-0.9608496963977814), 'hyper_actor_loss': np.float64(0.0008992661081720143), 'behavior_loss': np.float64(0.34673941135406494)}

Episode step 13020, time diff 0.7630417346954346, total time dif 1088.6154336929321)
step: 13020 @ episode report: {'average_total_reward': np.float32(8.665556), 'reward_variance': np.float32(2.0884304), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06548305731266738), 'actor_loss': np.float64(-0.9676165401935577), 'hyper_actor_loss': np.float64(0.0010734103969298303), 'behavior_loss': np.float64(0.3504522293806076)}

Episode step 13030, time diff 0.8908052444458008, total time dif 1089.3784754276276)
step: 13030 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.4205977), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07270377613604069), 'actor_loss': np.float64(-1.0038015723228455), 'hyper_actor_loss': np.float64(0.0012656969367526472), 'behavior_loss': np.float64(0.38589766919612883)}

Episode step 13040, time diff 0.7769930362701416, total time dif 1090.2692806720734)
step: 13040 @ episode report: {'average_total_reward': np.float32(8.863333), 'reward_variance': np.float32(2.3358529), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0841690331697464), 'actor_loss': np.float64(-0.973514610528946), 'hyper_actor_loss': np.float64(0.0013180055539123714), 'behavior_loss': np.float64(0.35176063776016236)}

Episode step 13050, time diff 0.7626631259918213, total time dif 1091.0462737083435)
step: 13050 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(2.3288655), 'max_total_reward': np.float32(11.900002), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08378967717289924), 'actor_loss': np.float64(-1.017678141593933), 'hyper_actor_loss': np.float64(0.0011844528955407441), 'behavior_loss': np.float64(0.3533462226390839)}

Episode step 13060, time diff 0.7635424137115479, total time dif 1091.8089368343353)
step: 13060 @ episode report: {'average_total_reward': np.float32(9.326667), 'reward_variance': np.float32(1.770819), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06920934990048408), 'actor_loss': np.float64(-0.970265907049179), 'hyper_actor_loss': np.float64(0.0009850549278780818), 'behavior_loss': np.float64(0.3581786721944809)}

Episode step 13070, time diff 0.7814538478851318, total time dif 1092.5724792480469)
step: 13070 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.4905488), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06768247634172439), 'actor_loss': np.float64(-0.9631787717342377), 'hyper_actor_loss': np.float64(0.0008673788688611239), 'behavior_loss': np.float64(0.3492346227169037)}

Episode step 13080, time diff 0.7693853378295898, total time dif 1093.353933095932)
step: 13080 @ episode report: {'average_total_reward': np.float32(8.165556), 'reward_variance': np.float32(1.9623817), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05687289126217365), 'actor_loss': np.float64(-0.9753035187721253), 'hyper_actor_loss': np.float64(0.0008271948026958853), 'behavior_loss': np.float64(0.3825560539960861)}

Episode step 13090, time diff 0.7658052444458008, total time dif 1094.1233184337616)
step: 13090 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(5.4126964), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07259900011122226), 'actor_loss': np.float64(-0.9642188727855683), 'hyper_actor_loss': np.float64(0.0007860065146815032), 'behavior_loss': np.float64(0.35140059888362885)}

Episode step 13100, time diff 0.7677476406097412, total time dif 1094.8891236782074)
step: 13100 @ episode report: {'average_total_reward': np.float32(9.3144455), 'reward_variance': np.float32(3.0938535), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06435801461338997), 'actor_loss': np.float64(-0.9886131525039673), 'hyper_actor_loss': np.float64(0.0006929072784259916), 'behavior_loss': np.float64(0.33651762306690214)}

Episode step 13110, time diff 0.7604894638061523, total time dif 1095.6568713188171)
step: 13110 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(3.343808), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0743553563952446), 'actor_loss': np.float64(-0.9690139949321747), 'hyper_actor_loss': np.float64(0.0007019356940872967), 'behavior_loss': np.float64(0.34959199130535124)}

Episode step 13120, time diff 0.7776970863342285, total time dif 1096.4173607826233)
step: 13120 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.1927525), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06659287549555301), 'actor_loss': np.float64(-0.9812242090702057), 'hyper_actor_loss': np.float64(0.0006319747131783515), 'behavior_loss': np.float64(0.34715864062309265)}

Episode step 13130, time diff 0.7875714302062988, total time dif 1097.1950578689575)
step: 13130 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(6.2464247), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06614118665456772), 'actor_loss': np.float64(-0.9713257491588593), 'hyper_actor_loss': np.float64(0.0006804822362028063), 'behavior_loss': np.float64(0.32699393630027773)}

Episode step 13140, time diff 0.758141040802002, total time dif 1097.9826292991638)
step: 13140 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.2491956), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07133923508226872), 'actor_loss': np.float64(-0.9833714544773102), 'hyper_actor_loss': np.float64(0.0006954578158911318), 'behavior_loss': np.float64(0.34288405179977416)}

Episode step 13150, time diff 0.7522335052490234, total time dif 1098.7407703399658)
step: 13150 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(2.8111315), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06578949987888336), 'actor_loss': np.float64(-0.972951489686966), 'hyper_actor_loss': np.float64(0.0007023577054496855), 'behavior_loss': np.float64(0.34758117496967317)}

Episode step 13160, time diff 0.9470553398132324, total time dif 1099.4930038452148)
step: 13160 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.3383013), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07450488433241845), 'actor_loss': np.float64(-0.9687747716903686), 'hyper_actor_loss': np.float64(0.0006902645400259644), 'behavior_loss': np.float64(0.3664271056652069)}

Episode step 13170, time diff 0.8009324073791504, total time dif 1100.440059185028)
step: 13170 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(2.4664898), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0627733089029789), 'actor_loss': np.float64(-0.9780903697013855), 'hyper_actor_loss': np.float64(0.0007357121095992625), 'behavior_loss': np.float64(0.35897561013698576)}

Episode step 13180, time diff 0.7804789543151855, total time dif 1101.2409915924072)
step: 13180 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(1.6351557), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06567320451140404), 'actor_loss': np.float64(-0.9619933605194092), 'hyper_actor_loss': np.float64(0.0007956569083034992), 'behavior_loss': np.float64(0.3386620134115219)}

Episode step 13190, time diff 0.7733561992645264, total time dif 1102.0214705467224)
step: 13190 @ episode report: {'average_total_reward': np.float32(9.912224), 'reward_variance': np.float32(2.1919112), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06665562726557255), 'actor_loss': np.float64(-0.9740398049354553), 'hyper_actor_loss': np.float64(0.000712851679418236), 'behavior_loss': np.float64(0.34714228808879855)}

Episode step 13200, time diff 0.787839412689209, total time dif 1102.794826745987)
step: 13200 @ episode report: {'average_total_reward': np.float32(10.834444), 'reward_variance': np.float32(3.8356411), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07609023563563824), 'actor_loss': np.float64(-0.9867857217788696), 'hyper_actor_loss': np.float64(0.0007331314089242369), 'behavior_loss': np.float64(0.3508853405714035)}

Episode step 13210, time diff 0.7774841785430908, total time dif 1103.5826661586761)
step: 13210 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(2.233458), 'max_total_reward': np.float32(11.777778), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06676041856408119), 'actor_loss': np.float64(-0.9667564809322358), 'hyper_actor_loss': np.float64(0.0007195384940132498), 'behavior_loss': np.float64(0.3250733345746994)}

Episode step 13220, time diff 0.7707767486572266, total time dif 1104.3601503372192)
step: 13220 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(2.525338), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08135485574603081), 'actor_loss': np.float64(-0.9919557452201844), 'hyper_actor_loss': np.float64(0.0007434131228365004), 'behavior_loss': np.float64(0.33960957229137423)}

Episode step 13230, time diff 0.7543947696685791, total time dif 1105.1309270858765)
step: 13230 @ episode report: {'average_total_reward': np.float32(10.285557), 'reward_variance': np.float32(3.657655), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06701896041631698), 'actor_loss': np.float64(-0.9852202355861663), 'hyper_actor_loss': np.float64(0.000720509368693456), 'behavior_loss': np.float64(0.35272284150123595)}

Episode step 13240, time diff 0.804499626159668, total time dif 1105.885321855545)
step: 13240 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(5.9154696), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.288889), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06814890392124653), 'actor_loss': np.float64(-0.964725661277771), 'hyper_actor_loss': np.float64(0.0006918616476468742), 'behavior_loss': np.float64(0.35051119029521943)}

Episode step 13250, time diff 0.7844183444976807, total time dif 1106.6898214817047)
step: 13250 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(3.9813442), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07884724922478199), 'actor_loss': np.float64(-0.9980663657188416), 'hyper_actor_loss': np.float64(0.0006852673599496484), 'behavior_loss': np.float64(0.3484231114387512)}

Episode step 13260, time diff 0.7509617805480957, total time dif 1107.4742398262024)
step: 13260 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(3.7857647), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07938140965998172), 'actor_loss': np.float64(-0.9929794490337371), 'hyper_actor_loss': np.float64(0.0007660556701011955), 'behavior_loss': np.float64(0.3460975676774979)}

Episode step 13270, time diff 0.7720863819122314, total time dif 1108.2252016067505)
step: 13270 @ episode report: {'average_total_reward': np.float32(9.673334), 'reward_variance': np.float32(3.4830925), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06706856526434421), 'actor_loss': np.float64(-0.9767485678195953), 'hyper_actor_loss': np.float64(0.0007187615090515464), 'behavior_loss': np.float64(0.3404954940080643)}

Episode step 13280, time diff 0.7735002040863037, total time dif 1108.9972879886627)
step: 13280 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(1.0499212), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06869564093649387), 'actor_loss': np.float64(-0.9658243954181671), 'hyper_actor_loss': np.float64(0.0006666148838121444), 'behavior_loss': np.float64(0.3524121105670929)}

Episode step 13290, time diff 0.7611682415008545, total time dif 1109.770788192749)
step: 13290 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(4.891612), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08246340602636337), 'actor_loss': np.float64(-0.9800214052200318), 'hyper_actor_loss': np.float64(0.0007046564889606089), 'behavior_loss': np.float64(0.361609610915184)}

Episode step 13300, time diff 0.764805793762207, total time dif 1110.5319564342499)
step: 13300 @ episode report: {'average_total_reward': np.float32(8.714444), 'reward_variance': np.float32(4.9823713), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07314825542271138), 'actor_loss': np.float64(-1.006936252117157), 'hyper_actor_loss': np.float64(0.0007598954660352319), 'behavior_loss': np.float64(0.335554638504982)}

Episode step 13310, time diff 0.7568864822387695, total time dif 1111.296762228012)
step: 13310 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(3.495557), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06846378855407238), 'actor_loss': np.float64(-0.962586510181427), 'hyper_actor_loss': np.float64(0.0007285815838258714), 'behavior_loss': np.float64(0.3478934973478317)}

Episode step 13320, time diff 0.915827751159668, total time dif 1112.0536487102509)
step: 13320 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(2.8062532), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058475982025265695), 'actor_loss': np.float64(-0.9540883719921112), 'hyper_actor_loss': np.float64(0.0007792570861056447), 'behavior_loss': np.float64(0.3487548530101776)}

Episode step 13330, time diff 0.7945103645324707, total time dif 1112.9694764614105)
step: 13330 @ episode report: {'average_total_reward': np.float32(8.902224), 'reward_variance': np.float32(3.1728597), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07463685050606728), 'actor_loss': np.float64(-0.9860595047473908), 'hyper_actor_loss': np.float64(0.0012151798757258803), 'behavior_loss': np.float64(0.32741071879863737)}

Episode step 13340, time diff 0.7790491580963135, total time dif 1113.763986825943)
step: 13340 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(5.314346), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07231196276843548), 'actor_loss': np.float64(-0.9932729482650757), 'hyper_actor_loss': np.float64(0.001233317016158253), 'behavior_loss': np.float64(0.32455745339393616)}

Episode step 13350, time diff 0.7881820201873779, total time dif 1114.5430359840393)
step: 13350 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(2.4712546), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07621313631534576), 'actor_loss': np.float64(-0.9698495507240296), 'hyper_actor_loss': np.float64(0.0009435871033929288), 'behavior_loss': np.float64(0.3388864517211914)}

Episode step 13360, time diff 0.779364824295044, total time dif 1115.3312180042267)
step: 13360 @ episode report: {'average_total_reward': np.float32(8.738889), 'reward_variance': np.float32(2.517981), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.068705515563488), 'actor_loss': np.float64(-0.9782021760940551), 'hyper_actor_loss': np.float64(0.00075991241610609), 'behavior_loss': np.float64(0.3495378315448761)}

Episode step 13370, time diff 0.7504246234893799, total time dif 1116.1105828285217)
step: 13370 @ episode report: {'average_total_reward': np.float32(8.092222), 'reward_variance': np.float32(2.2133348), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05642849188297987), 'actor_loss': np.float64(-0.9776360511779785), 'hyper_actor_loss': np.float64(0.0006748245737981051), 'behavior_loss': np.float64(0.311240117251873)}

Episode step 13380, time diff 0.7517056465148926, total time dif 1116.861007452011)
step: 13380 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(3.6954627), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06850871052592993), 'actor_loss': np.float64(-0.9600234866142273), 'hyper_actor_loss': np.float64(0.0007009910419583321), 'behavior_loss': np.float64(0.3482606589794159)}

Episode step 13390, time diff 0.7497341632843018, total time dif 1117.612713098526)
step: 13390 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(3.5166917), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07215126939117908), 'actor_loss': np.float64(-0.9857151091098786), 'hyper_actor_loss': np.float64(0.0007355000940151513), 'behavior_loss': np.float64(0.3498091220855713)}

Episode step 13400, time diff 0.7668590545654297, total time dif 1118.3624472618103)
step: 13400 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(5.2429094), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055975521355867384), 'actor_loss': np.float64(-0.9739307880401611), 'hyper_actor_loss': np.float64(0.0007151887170039117), 'behavior_loss': np.float64(0.31696759164333344)}

Episode step 13410, time diff 0.7653040885925293, total time dif 1119.1293063163757)
step: 13410 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(3.3546925), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07485373429954052), 'actor_loss': np.float64(-0.9760660588741302), 'hyper_actor_loss': np.float64(0.0006763596436940133), 'behavior_loss': np.float64(0.3363359719514847)}

Episode step 13420, time diff 0.7541294097900391, total time dif 1119.8946104049683)
step: 13420 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.7816794), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07662078067660331), 'actor_loss': np.float64(-0.9894416689872741), 'hyper_actor_loss': np.float64(0.0006891092634759843), 'behavior_loss': np.float64(0.32816055715084075)}

Episode step 13430, time diff 0.7504730224609375, total time dif 1120.6487398147583)
step: 13430 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(1.5928015), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08254044763743877), 'actor_loss': np.float64(-0.9954403400421142), 'hyper_actor_loss': np.float64(0.0006762586184777319), 'behavior_loss': np.float64(0.32395547330379487)}

Episode step 13440, time diff 0.745612382888794, total time dif 1121.3992128372192)
step: 13440 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(4.8995867), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06896042469888926), 'actor_loss': np.float64(-0.9782057285308838), 'hyper_actor_loss': np.float64(0.0006253005121834576), 'behavior_loss': np.float64(0.3258770346641541)}

Episode step 13450, time diff 0.7618722915649414, total time dif 1122.144825220108)
step: 13450 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(2.1239867), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07348760478198528), 'actor_loss': np.float64(-0.9823892533779144), 'hyper_actor_loss': np.float64(0.0005982992704957723), 'behavior_loss': np.float64(0.3232809782028198)}

Episode step 13460, time diff 0.734687089920044, total time dif 1122.906697511673)
step: 13460 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(2.6304994), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08260192163288593), 'actor_loss': np.float64(-0.9962852537631989), 'hyper_actor_loss': np.float64(0.0006179812597110867), 'behavior_loss': np.float64(0.31446145474910736)}

Episode step 13470, time diff 0.757286787033081, total time dif 1123.641384601593)
step: 13470 @ episode report: {'average_total_reward': np.float32(10.2733345), 'reward_variance': np.float32(1.3232152), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06441823672503233), 'actor_loss': np.float64(-0.9782868146896362), 'hyper_actor_loss': np.float64(0.0006530725513584912), 'behavior_loss': np.float64(0.3300301909446716)}

Episode step 13480, time diff 0.9163033962249756, total time dif 1124.398671388626)
step: 13480 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.775289), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06956178918480874), 'actor_loss': np.float64(-0.9620082199573516), 'hyper_actor_loss': np.float64(0.0006278421031311154), 'behavior_loss': np.float64(0.3175007626414299)}

Episode step 13490, time diff 0.7422521114349365, total time dif 1125.314974784851)
step: 13490 @ episode report: {'average_total_reward': np.float32(9.48778), 'reward_variance': np.float32(1.685962), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07909298874437809), 'actor_loss': np.float64(-1.0001495242118836), 'hyper_actor_loss': np.float64(0.0006824710813816637), 'behavior_loss': np.float64(0.32567756474018095)}

Episode step 13500, time diff 0.763953685760498, total time dif 1126.057226896286)
step: 13500 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.2011416), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07348453514277935), 'actor_loss': np.float64(-0.9916882514953613), 'hyper_actor_loss': np.float64(0.0007000761630479247), 'behavior_loss': np.float64(0.3235861659049988)}

Episode step 13510, time diff 0.749110221862793, total time dif 1126.8211805820465)
step: 13510 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(3.8769145), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07878010161221027), 'actor_loss': np.float64(-0.9806697607040405), 'hyper_actor_loss': np.float64(0.0006517012137919664), 'behavior_loss': np.float64(0.3467555433511734)}

Episode step 13520, time diff 0.7324366569519043, total time dif 1127.5702908039093)
step: 13520 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.7871861), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06899641491472722), 'actor_loss': np.float64(-0.980901426076889), 'hyper_actor_loss': np.float64(0.0006534093932714313), 'behavior_loss': np.float64(0.3274476706981659)}

Episode step 13530, time diff 0.7605588436126709, total time dif 1128.3027274608612)
step: 13530 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(1.9901931), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07635357938706874), 'actor_loss': np.float64(-0.96324924826622), 'hyper_actor_loss': np.float64(0.0006617623963393271), 'behavior_loss': np.float64(0.3361454874277115)}

Episode step 13540, time diff 0.7566766738891602, total time dif 1129.0632863044739)
step: 13540 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(0.61568534), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05933247022330761), 'actor_loss': np.float64(-0.9716064810752869), 'hyper_actor_loss': np.float64(0.0006714484130498022), 'behavior_loss': np.float64(0.33620116412639617)}

Episode step 13550, time diff 0.7443420886993408, total time dif 1129.819962978363)
step: 13550 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(4.8904743), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07477345503866673), 'actor_loss': np.float64(-0.9664993584156036), 'hyper_actor_loss': np.float64(0.0006232158979400992), 'behavior_loss': np.float64(0.324780760705471)}

Episode step 13560, time diff 0.7512576580047607, total time dif 1130.5643050670624)
step: 13560 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(5.4056253), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06648461893200874), 'actor_loss': np.float64(-0.9942016839981079), 'hyper_actor_loss': np.float64(0.0005917396745644509), 'behavior_loss': np.float64(0.3001600742340088)}

Episode step 13570, time diff 0.7165939807891846, total time dif 1131.3155627250671)
step: 13570 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(2.0485692), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06112834364175797), 'actor_loss': np.float64(-0.9864019274711608), 'hyper_actor_loss': np.float64(0.0006044766283594072), 'behavior_loss': np.float64(0.3231698155403137)}

Episode step 13580, time diff 0.7408502101898193, total time dif 1132.0321567058563)
step: 13580 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(2.6341088), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.533335), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0658093474805355), 'actor_loss': np.float64(-0.9697026431560516), 'hyper_actor_loss': np.float64(0.0006823607953265309), 'behavior_loss': np.float64(0.3050790190696716)}

Episode step 13590, time diff 0.7427873611450195, total time dif 1132.7730069160461)
step: 13590 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.0371716), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07814642041921616), 'actor_loss': np.float64(-0.9943250477313995), 'hyper_actor_loss': np.float64(0.000702659465605393), 'behavior_loss': np.float64(0.32960230112075806)}

Episode step 13600, time diff 0.8039789199829102, total time dif 1133.5157942771912)
step: 13600 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(3.8157897), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06388377919793128), 'actor_loss': np.float64(-0.9745696008205413), 'hyper_actor_loss': np.float64(0.0006993246381171048), 'behavior_loss': np.float64(0.31150666773319247)}

Episode step 13610, time diff 0.7572441101074219, total time dif 1134.319773197174)
step: 13610 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(0.93299425), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06752554550766945), 'actor_loss': np.float64(-0.9691296875476837), 'hyper_actor_loss': np.float64(0.0006823610630817712), 'behavior_loss': np.float64(0.32368197441101076)}

Episode step 13620, time diff 0.7878715991973877, total time dif 1135.0770173072815)
step: 13620 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(6.003232), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0640631016343832), 'actor_loss': np.float64(-0.9765401661396027), 'hyper_actor_loss': np.float64(0.0006674229924101382), 'behavior_loss': np.float64(0.3291437476873398)}

Episode step 13630, time diff 0.7816877365112305, total time dif 1135.8648889064789)
step: 13630 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(4.5066924), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07848877497017384), 'actor_loss': np.float64(-0.9834972858428955), 'hyper_actor_loss': np.float64(0.0007198146544396877), 'behavior_loss': np.float64(0.3005248546600342)}

Episode step 13640, time diff 0.7834327220916748, total time dif 1136.64657664299)
step: 13640 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.1835746), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06624245643615723), 'actor_loss': np.float64(-0.9955227315425873), 'hyper_actor_loss': np.float64(0.0007387314166408032), 'behavior_loss': np.float64(0.305814191699028)}

Episode step 13650, time diff 0.9318900108337402, total time dif 1137.4300093650818)
step: 13650 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(2.9458222), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06760176979005336), 'actor_loss': np.float64(-0.9665041208267212), 'hyper_actor_loss': np.float64(0.0007075568893924356), 'behavior_loss': np.float64(0.3113647371530533)}

Episode step 13660, time diff 0.8125450611114502, total time dif 1138.3618993759155)
step: 13660 @ episode report: {'average_total_reward': np.float32(10.012224), 'reward_variance': np.float32(2.7677407), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07581199146807194), 'actor_loss': np.float64(-0.9854808747768402), 'hyper_actor_loss': np.float64(0.0006542740215081721), 'behavior_loss': np.float64(0.30096849501132966)}

Episode step 13670, time diff 0.8260519504547119, total time dif 1139.174444437027)
step: 13670 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.8720047), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0749576173722744), 'actor_loss': np.float64(-1.0151846766471864), 'hyper_actor_loss': np.float64(0.0005620833369903266), 'behavior_loss': np.float64(0.3176357179880142)}

Episode step 13680, time diff 1.003817081451416, total time dif 1140.0004963874817)
step: 13680 @ episode report: {'average_total_reward': np.float32(9.797778), 'reward_variance': np.float32(3.9003406), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0685824777930975), 'actor_loss': np.float64(-0.9707834780216217), 'hyper_actor_loss': np.float64(0.0005632270593196153), 'behavior_loss': np.float64(0.3158020257949829)}

Episode step 13690, time diff 1.017047643661499, total time dif 1141.004313468933)
step: 13690 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(4.5439563), 'max_total_reward': np.float32(15.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061070815287530424), 'actor_loss': np.float64(-0.9776320457458496), 'hyper_actor_loss': np.float64(0.0005549016961595044), 'behavior_loss': np.float64(0.298606276512146)}

Episode step 13700, time diff 0.822922945022583, total time dif 1142.0213611125946)
step: 13700 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.0309877), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0659908939152956), 'actor_loss': np.float64(-0.9676253855228424), 'hyper_actor_loss': np.float64(0.0005713826045393944), 'behavior_loss': np.float64(0.29431438744068145)}

Episode step 13710, time diff 0.8171064853668213, total time dif 1142.8442840576172)
step: 13710 @ episode report: {'average_total_reward': np.float32(9.785556), 'reward_variance': np.float32(7.2773347), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05757912844419479), 'actor_loss': np.float64(-0.9755531132221222), 'hyper_actor_loss': np.float64(0.0005725595459807664), 'behavior_loss': np.float64(0.2961576461791992)}

Episode step 13720, time diff 0.7925958633422852, total time dif 1143.661390542984)
step: 13720 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(2.3158574), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061304505728185174), 'actor_loss': np.float64(-0.9835174798965454), 'hyper_actor_loss': np.float64(0.0006007757328916341), 'behavior_loss': np.float64(0.30117298364639283)}

Episode step 13730, time diff 0.8093571662902832, total time dif 1144.4539864063263)
step: 13730 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(2.8367176), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08869352489709854), 'actor_loss': np.float64(-0.9820149898529053), 'hyper_actor_loss': np.float64(0.0007457156840246171), 'behavior_loss': np.float64(0.32077161967754364)}

Episode step 13740, time diff 0.8397510051727295, total time dif 1145.2633435726166)
step: 13740 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(7.337976), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07751233763992786), 'actor_loss': np.float64(-1.003189104795456), 'hyper_actor_loss': np.float64(0.0008275347121525556), 'behavior_loss': np.float64(0.3113835632801056)}

Episode step 13750, time diff 0.8072164058685303, total time dif 1146.1030945777893)
step: 13750 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(3.9408908), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06598040703684091), 'actor_loss': np.float64(-0.9619419932365417), 'hyper_actor_loss': np.float64(0.0007390130253043026), 'behavior_loss': np.float64(0.30545124411582947)}

Episode step 13760, time diff 0.8114771842956543, total time dif 1146.9103109836578)
step: 13760 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(0.82547665), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061890187114477156), 'actor_loss': np.float64(-0.964608633518219), 'hyper_actor_loss': np.float64(0.0006532484607305378), 'behavior_loss': np.float64(0.310828885436058)}

Episode step 13770, time diff 0.7871787548065186, total time dif 1147.7217881679535)
step: 13770 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.4935665), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.079840037971735), 'actor_loss': np.float64(-0.9788030564785004), 'hyper_actor_loss': np.float64(0.0006411944981664419), 'behavior_loss': np.float64(0.3128290921449661)}

Episode step 13780, time diff 0.7992987632751465, total time dif 1148.50896692276)
step: 13780 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(6.569384), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06817448288202285), 'actor_loss': np.float64(-0.9923350870609283), 'hyper_actor_loss': np.float64(0.0005952117440756411), 'behavior_loss': np.float64(0.28967461585998533)}

Episode step 13790, time diff 0.7874999046325684, total time dif 1149.3082656860352)
step: 13790 @ episode report: {'average_total_reward': np.float32(10.834444), 'reward_variance': np.float32(3.1184068), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06369763724505902), 'actor_loss': np.float64(-0.9780708491802216), 'hyper_actor_loss': np.float64(0.0005453932622913271), 'behavior_loss': np.float64(0.2957113027572632)}

Episode step 13800, time diff 0.7794091701507568, total time dif 1150.0957655906677)
step: 13800 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(5.1743007), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06796344146132469), 'actor_loss': np.float64(-0.9735230147838593), 'hyper_actor_loss': np.float64(0.0005285487626679241), 'behavior_loss': np.float64(0.3142347753047943)}

Episode step 13810, time diff 0.7947514057159424, total time dif 1150.8751747608185)
step: 13810 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(2.5999553), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07059556376188994), 'actor_loss': np.float64(-0.9780829966068267), 'hyper_actor_loss': np.float64(0.0005139706074260176), 'behavior_loss': np.float64(0.3024420291185379)}

Episode step 13820, time diff 0.9424786567687988, total time dif 1151.6699261665344)
step: 13820 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.6425188), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06830177903175354), 'actor_loss': np.float64(-0.9739840149879455), 'hyper_actor_loss': np.float64(0.0005176659731660038), 'behavior_loss': np.float64(0.3043080478906631)}

Episode step 13830, time diff 0.8009488582611084, total time dif 1152.6124048233032)
step: 13830 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(3.4618275), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0674564853310585), 'actor_loss': np.float64(-0.9747951447963714), 'hyper_actor_loss': np.float64(0.0004953829484293237), 'behavior_loss': np.float64(0.3008913040161133)}

Episode step 13840, time diff 0.8031461238861084, total time dif 1153.4133536815643)
step: 13840 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(5.0750318), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06289765797555447), 'actor_loss': np.float64(-0.9838140606880188), 'hyper_actor_loss': np.float64(0.00047473441227339206), 'behavior_loss': np.float64(0.29592452347278597)}

Episode step 13850, time diff 0.8309905529022217, total time dif 1154.2164998054504)
step: 13850 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(2.2195115), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061392467841506), 'actor_loss': np.float64(-0.9650289058685303), 'hyper_actor_loss': np.float64(0.0004957308963639661), 'behavior_loss': np.float64(0.30062250792980194)}

Episode step 13860, time diff 0.828758716583252, total time dif 1155.0474903583527)
step: 13860 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.5945923), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07007655948400497), 'actor_loss': np.float64(-0.9726405501365661), 'hyper_actor_loss': np.float64(0.0005607705214060843), 'behavior_loss': np.float64(0.3098327308893204)}

Episode step 13870, time diff 0.8065245151519775, total time dif 1155.876249074936)
step: 13870 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(1.366637), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07439662255346775), 'actor_loss': np.float64(-1.0032314956188202), 'hyper_actor_loss': np.float64(0.0005573518166784198), 'behavior_loss': np.float64(0.2921907424926758)}

Episode step 13880, time diff 0.7967157363891602, total time dif 1156.682773590088)
step: 13880 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(1.5452653), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06802899017930031), 'actor_loss': np.float64(-0.9995504915714264), 'hyper_actor_loss': np.float64(0.0005262238468276337), 'behavior_loss': np.float64(0.2983821749687195)}

Episode step 13890, time diff 0.7737486362457275, total time dif 1157.479489326477)
step: 13890 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.3143758), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06998469680547714), 'actor_loss': np.float64(-0.9810131192207336), 'hyper_actor_loss': np.float64(0.0005503429245436564), 'behavior_loss': np.float64(0.30324859619140626)}

Episode step 13900, time diff 0.8961648941040039, total time dif 1158.2532379627228)
step: 13900 @ episode report: {'average_total_reward': np.float32(11.881113), 'reward_variance': np.float32(2.8391883), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.070724480971694), 'actor_loss': np.float64(-0.9876441895961762), 'hyper_actor_loss': np.float64(0.0006126905442215502), 'behavior_loss': np.float64(0.3007812350988388)}

Episode step 13910, time diff 0.8690073490142822, total time dif 1159.1494028568268)
step: 13910 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(6.0171804), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0696150291711092), 'actor_loss': np.float64(-0.9950378179550171), 'hyper_actor_loss': np.float64(0.0005789423477835954), 'behavior_loss': np.float64(0.3067904308438301)}

Episode step 13920, time diff 0.8638718128204346, total time dif 1160.018410205841)
step: 13920 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(1.6784499), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07460985146462917), 'actor_loss': np.float64(-0.9789192914962769), 'hyper_actor_loss': np.float64(0.0004971016111085192), 'behavior_loss': np.float64(0.30641179382801054)}

Episode step 13930, time diff 1.012176752090454, total time dif 1160.8822820186615)
step: 13930 @ episode report: {'average_total_reward': np.float32(12.181112), 'reward_variance': np.float32(2.3278773), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07693468034267426), 'actor_loss': np.float64(-0.9816357553005218), 'hyper_actor_loss': np.float64(0.0005226797424256801), 'behavior_loss': np.float64(0.29178132563829423)}

Episode step 13940, time diff 1.025524377822876, total time dif 1161.894458770752)
step: 13940 @ episode report: {'average_total_reward': np.float32(11.66889), 'reward_variance': np.float32(1.7606621), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06293640770018101), 'actor_loss': np.float64(-0.997723788022995), 'hyper_actor_loss': np.float64(0.0005471829615999013), 'behavior_loss': np.float64(0.304082402586937)}

Episode step 13950, time diff 0.795107364654541, total time dif 1162.9199831485748)
step: 13950 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(4.109848), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06274074241518975), 'actor_loss': np.float64(-0.9548949241638184), 'hyper_actor_loss': np.float64(0.0006822888215538114), 'behavior_loss': np.float64(0.3044542342424393)}

Episode step 13960, time diff 0.8145091533660889, total time dif 1163.7150905132294)
step: 13960 @ episode report: {'average_total_reward': np.float32(11.444445), 'reward_variance': np.float32(4.908372), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07340641617774964), 'actor_loss': np.float64(-0.9840692102909088), 'hyper_actor_loss': np.float64(0.0007975439424626529), 'behavior_loss': np.float64(0.301336532831192)}

Episode step 13970, time diff 0.7819995880126953, total time dif 1164.5295996665955)
step: 13970 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(5.031765), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0738846804946661), 'actor_loss': np.float64(-1.0024050176143646), 'hyper_actor_loss': np.float64(0.0008700576378032565), 'behavior_loss': np.float64(0.3049591988325119)}

Episode step 13980, time diff 0.9765167236328125, total time dif 1165.3115992546082)
step: 13980 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(3.3393703), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07682131305336952), 'actor_loss': np.float64(-0.969244921207428), 'hyper_actor_loss': np.float64(0.0008412016031797975), 'behavior_loss': np.float64(0.3267680734395981)}

Episode step 13990, time diff 0.8219656944274902, total time dif 1166.288115978241)
step: 13990 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.4146378), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06719399616122246), 'actor_loss': np.float64(-0.9799042701721191), 'hyper_actor_loss': np.float64(0.0007782646920531988), 'behavior_loss': np.float64(0.30462982654571535)}

Episode step 14000, time diff 0.8803343772888184, total time dif 1167.1100816726685)
step: 14000 @ episode report: {'average_total_reward': np.float32(11.744446), 'reward_variance': np.float32(4.521902), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06670819632709027), 'actor_loss': np.float64(-0.9960128486156463), 'hyper_actor_loss': np.float64(0.0008978611615020782), 'behavior_loss': np.float64(0.3044990122318268)}

Episode step 14010, time diff 0.9986326694488525, total time dif 1167.9904160499573)
step: 14010 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.116259), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07663300149142742), 'actor_loss': np.float64(-0.9736442565917969), 'hyper_actor_loss': np.float64(0.0013546649366617202), 'behavior_loss': np.float64(0.3034119695425034)}

Episode step 14020, time diff 1.0980536937713623, total time dif 1168.9890487194061)
step: 14020 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.3905722), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05682426802814007), 'actor_loss': np.float64(-0.9903038680553437), 'hyper_actor_loss': np.float64(0.0010347973904572428), 'behavior_loss': np.float64(0.3060570597648621)}

Episode step 14030, time diff 0.9825911521911621, total time dif 1170.0871024131775)
step: 14030 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(4.3084764), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06091306358575821), 'actor_loss': np.float64(-0.9613539516925812), 'hyper_actor_loss': np.float64(0.0008404572785366326), 'behavior_loss': np.float64(0.30069863200187685)}

Episode step 14040, time diff 0.8340420722961426, total time dif 1171.0696935653687)
step: 14040 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(2.794153), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07514765150845051), 'actor_loss': np.float64(-0.9977849364280701), 'hyper_actor_loss': np.float64(0.000750893191434443), 'behavior_loss': np.float64(0.3141317397356033)}

Episode step 14050, time diff 0.8530402183532715, total time dif 1171.9037356376648)
step: 14050 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(3.9302917), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06952691227197647), 'actor_loss': np.float64(-0.9948887944221496), 'hyper_actor_loss': np.float64(0.0007103583833668381), 'behavior_loss': np.float64(0.29200449883937835)}

Episode step 14060, time diff 1.1822021007537842, total time dif 1172.756775856018)
step: 14060 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(1.271073), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07225914485752583), 'actor_loss': np.float64(-0.976670891046524), 'hyper_actor_loss': np.float64(0.0006488786661066115), 'behavior_loss': np.float64(0.30403667986392974)}

Episode step 14070, time diff 1.5021803379058838, total time dif 1173.9389779567719)
step: 14070 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(4.1088343), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06168416254222393), 'actor_loss': np.float64(-0.9858542203903198), 'hyper_actor_loss': np.float64(0.0005825124244438484), 'behavior_loss': np.float64(0.30407161116600034)}

Episode step 14080, time diff 1.2442927360534668, total time dif 1175.4411582946777)
step: 14080 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(5.2317643), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07273812517523766), 'actor_loss': np.float64(-0.9918667018413544), 'hyper_actor_loss': np.float64(0.0005611762579064816), 'behavior_loss': np.float64(0.30136153399944304)}

Episode step 14090, time diff 1.4531025886535645, total time dif 1176.6854510307312)
step: 14090 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.0521836), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07683302946388722), 'actor_loss': np.float64(-0.9987353324890137), 'hyper_actor_loss': np.float64(0.0005555826181080192), 'behavior_loss': np.float64(0.31033057570457456)}

Episode step 14100, time diff 1.1014583110809326, total time dif 1178.1385536193848)
step: 14100 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.3182719), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06036743950098753), 'actor_loss': np.float64(-0.9824478447437286), 'hyper_actor_loss': np.float64(0.0007166712370235473), 'behavior_loss': np.float64(0.3206965208053589)}

Episode step 14110, time diff 0.9121172428131104, total time dif 1179.2400119304657)
step: 14110 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.7054926), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06671540392562747), 'actor_loss': np.float64(-0.978726613521576), 'hyper_actor_loss': np.float64(0.0010435423988383262), 'behavior_loss': np.float64(0.3480511218309402)}

Episode step 14120, time diff 0.9691598415374756, total time dif 1180.1521291732788)
step: 14120 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(1.6601092), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07832474168390036), 'actor_loss': np.float64(-0.994168347120285), 'hyper_actor_loss': np.float64(0.0011334308539517224), 'behavior_loss': np.float64(0.3484240710735321)}

Episode step 14130, time diff 1.0402772426605225, total time dif 1181.1212890148163)
step: 14130 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(3.7619278), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07465803548693657), 'actor_loss': np.float64(-1.0052871406078339), 'hyper_actor_loss': np.float64(0.0010256641719024629), 'behavior_loss': np.float64(0.35535968542099)}

Episode step 14140, time diff 1.0708153247833252, total time dif 1182.1615662574768)
step: 14140 @ episode report: {'average_total_reward': np.float32(8.963335), 'reward_variance': np.float32(3.808274), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06481612175703048), 'actor_loss': np.float64(-0.9850627839565277), 'hyper_actor_loss': np.float64(0.0008998303092084825), 'behavior_loss': np.float64(0.3462680846452713)}

Episode step 14150, time diff 1.243879795074463, total time dif 1183.2323815822601)
step: 14150 @ episode report: {'average_total_reward': np.float32(8.975556), 'reward_variance': np.float32(2.4848597), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07556909173727036), 'actor_loss': np.float64(-0.9836587011814117), 'hyper_actor_loss': np.float64(0.0007795445737428964), 'behavior_loss': np.float64(0.3523661881685257)}

Episode step 14160, time diff 0.9189012050628662, total time dif 1184.4762613773346)
step: 14160 @ episode report: {'average_total_reward': np.float32(10.234446), 'reward_variance': np.float32(4.1041846), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07464600577950478), 'actor_loss': np.float64(-1.0060240149497985), 'hyper_actor_loss': np.float64(0.0006233403051737696), 'behavior_loss': np.float64(0.3523112773895264)}

Episode step 14170, time diff 0.8719959259033203, total time dif 1185.3951625823975)
step: 14170 @ episode report: {'average_total_reward': np.float32(11.1466675), 'reward_variance': np.float32(3.7648842), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07954182177782058), 'actor_loss': np.float64(-0.9786740720272065), 'hyper_actor_loss': np.float64(0.000494436247390695), 'behavior_loss': np.float64(0.337771400809288)}

Episode step 14180, time diff 0.9746332168579102, total time dif 1186.2671585083008)
step: 14180 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(4.076001), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08220092169940471), 'actor_loss': np.float64(-1.0049073219299316), 'hyper_actor_loss': np.float64(0.0004850552417337894), 'behavior_loss': np.float64(0.3304369330406189)}

Episode step 14190, time diff 0.8870546817779541, total time dif 1187.2417917251587)
step: 14190 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.9731362), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06888751089572906), 'actor_loss': np.float64(-1.0036987245082856), 'hyper_actor_loss': np.float64(0.0004227828641887754), 'behavior_loss': np.float64(0.3217518121004105)}

Episode step 14200, time diff 1.082486867904663, total time dif 1188.1288464069366)
step: 14200 @ episode report: {'average_total_reward': np.float32(10.883333), 'reward_variance': np.float32(2.9313884), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.065628950484097), 'actor_loss': np.float64(-0.9650125801563263), 'hyper_actor_loss': np.float64(0.0004491476312978193), 'behavior_loss': np.float64(0.32144154608249664)}

Episode step 14210, time diff 1.3610827922821045, total time dif 1189.2113332748413)
step: 14210 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.2915416), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06490426324307919), 'actor_loss': np.float64(-0.9793141841888428), 'hyper_actor_loss': np.float64(0.00043255113414488735), 'behavior_loss': np.float64(0.33868924975395204)}

Episode step 14220, time diff 0.8975160121917725, total time dif 1190.5724160671234)
step: 14220 @ episode report: {'average_total_reward': np.float32(9.338889), 'reward_variance': np.float32(3.5188956), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058828949183225634), 'actor_loss': np.float64(-0.9719584822654724), 'hyper_actor_loss': np.float64(0.000444288898142986), 'behavior_loss': np.float64(0.3370457708835602)}

Episode step 14230, time diff 0.8297836780548096, total time dif 1191.4699320793152)
step: 14230 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.5873103), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057680236920714376), 'actor_loss': np.float64(-0.9679133653640747), 'hyper_actor_loss': np.float64(0.000474230176769197), 'behavior_loss': np.float64(0.3277603298425674)}

Episode step 14240, time diff 0.8068602085113525, total time dif 1192.29971575737)
step: 14240 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(2.6127515), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0818157959729433), 'actor_loss': np.float64(-0.9765584707260132), 'hyper_actor_loss': np.float64(0.0005843081045895815), 'behavior_loss': np.float64(0.3301855862140656)}

Episode step 14250, time diff 1.4284462928771973, total time dif 1193.1065759658813)
step: 14250 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(2.247666), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06140977647155523), 'actor_loss': np.float64(-1.0073204636573792), 'hyper_actor_loss': np.float64(0.0005947428464423866), 'behavior_loss': np.float64(0.32304042875766753)}

Episode step 14260, time diff 1.5400574207305908, total time dif 1194.5350222587585)
step: 14260 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(2.1709921), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07192854043096304), 'actor_loss': np.float64(-0.95934499502182), 'hyper_actor_loss': np.float64(0.0005253241921309382), 'behavior_loss': np.float64(0.3334604799747467)}

Episode step 14270, time diff 1.1986167430877686, total time dif 1196.0750796794891)
step: 14270 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.751434), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07244539968669414), 'actor_loss': np.float64(-0.991995632648468), 'hyper_actor_loss': np.float64(0.0005269193643471226), 'behavior_loss': np.float64(0.3208869218826294)}

Episode step 14280, time diff 1.1038544178009033, total time dif 1197.273696422577)
step: 14280 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(1.8254877), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060446536540985106), 'actor_loss': np.float64(-0.9914339363574982), 'hyper_actor_loss': np.float64(0.0005219020356889814), 'behavior_loss': np.float64(0.31510519087314603)}

Episode step 14290, time diff 1.2586874961853027, total time dif 1198.3775508403778)
step: 14290 @ episode report: {'average_total_reward': np.float32(8.990001), 'reward_variance': np.float32(2.7205796), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056146424077451226), 'actor_loss': np.float64(-0.9576788961887359), 'hyper_actor_loss': np.float64(0.0004847131174756214), 'behavior_loss': np.float64(0.31522840559482573)}

Episode step 14300, time diff 1.3676486015319824, total time dif 1199.636238336563)
step: 14300 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(1.4076555), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07785136885941028), 'actor_loss': np.float64(-1.0013203263282775), 'hyper_actor_loss': np.float64(0.0004683440376538783), 'behavior_loss': np.float64(0.3246533006429672)}

Episode step 14310, time diff 1.5515425205230713, total time dif 1201.003886938095)
step: 14310 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.9907176), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06352843828499317), 'actor_loss': np.float64(-1.0009931862354278), 'hyper_actor_loss': np.float64(0.0005712947808206081), 'behavior_loss': np.float64(0.3179074198007584)}

Episode step 14320, time diff 2.3664886951446533, total time dif 1202.5554294586182)
step: 14320 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.3647408), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05740746967494488), 'actor_loss': np.float64(-0.9554193079471588), 'hyper_actor_loss': np.float64(0.0010445543623063714), 'behavior_loss': np.float64(0.310568705201149)}

Episode step 14330, time diff 1.4845349788665771, total time dif 1204.9219181537628)
step: 14330 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(1.7431895), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06921941377222537), 'actor_loss': np.float64(-0.9735259532928466), 'hyper_actor_loss': np.float64(0.0010411657742224634), 'behavior_loss': np.float64(0.31584362089633944)}

Episode step 14340, time diff 1.126044511795044, total time dif 1206.4064531326294)
step: 14340 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(2.2432656), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06239234991371632), 'actor_loss': np.float64(-0.9794536471366883), 'hyper_actor_loss': np.float64(0.000707123294705525), 'behavior_loss': np.float64(0.3133204966783524)}

Episode step 14350, time diff 0.9654519557952881, total time dif 1207.5324976444244)
step: 14350 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.0033104), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06955510787665845), 'actor_loss': np.float64(-0.989385849237442), 'hyper_actor_loss': np.float64(0.0006758274510502815), 'behavior_loss': np.float64(0.31642048954963686)}

Episode step 14360, time diff 1.0167484283447266, total time dif 1208.4979496002197)
step: 14360 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(3.4477088), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07608862631022931), 'actor_loss': np.float64(-0.99207763671875), 'hyper_actor_loss': np.float64(0.0006291320314630866), 'behavior_loss': np.float64(0.3089595794677734)}

Episode step 14370, time diff 1.059119701385498, total time dif 1209.5146980285645)
step: 14370 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(1.0313332), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0669502068310976), 'actor_loss': np.float64(-0.9954269051551818), 'hyper_actor_loss': np.float64(0.0005891087523195893), 'behavior_loss': np.float64(0.32332214117050173)}

Episode step 14380, time diff 1.1583976745605469, total time dif 1210.57381772995)
step: 14380 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(1.138771), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07867778241634368), 'actor_loss': np.float64(-0.9847459495067596), 'hyper_actor_loss': np.float64(0.0006133805174613372), 'behavior_loss': np.float64(0.3192250907421112)}

Episode step 14390, time diff 1.1172840595245361, total time dif 1211.7322154045105)
step: 14390 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(2.4690933), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06646346151828766), 'actor_loss': np.float64(-1.0074758231639862), 'hyper_actor_loss': np.float64(0.0008153280417900532), 'behavior_loss': np.float64(0.31913919746875763)}

Episode step 14400, time diff 1.2468407154083252, total time dif 1212.849499464035)
step: 14400 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.8365437), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07900557704269887), 'actor_loss': np.float64(-0.9838520288467407), 'hyper_actor_loss': np.float64(0.0006406088068615645), 'behavior_loss': np.float64(0.3237853854894638)}

Episode step 14410, time diff 1.1882009506225586, total time dif 1214.0963401794434)
step: 14410 @ episode report: {'average_total_reward': np.float32(9.09), 'reward_variance': np.float32(1.2419364), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0704707283526659), 'actor_loss': np.float64(-1.0030638873577118), 'hyper_actor_loss': np.float64(0.0005722885252907872), 'behavior_loss': np.float64(0.33111594021320345)}

Episode step 14420, time diff 1.0726642608642578, total time dif 1215.284541130066)
step: 14420 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(2.4398043), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06790610402822495), 'actor_loss': np.float64(-0.9767800807952881), 'hyper_actor_loss': np.float64(0.0005787578469607979), 'behavior_loss': np.float64(0.32177948355674746)}

Episode step 14430, time diff 0.9324162006378174, total time dif 1216.3572053909302)
step: 14430 @ episode report: {'average_total_reward': np.float32(7.8655562), 'reward_variance': np.float32(3.3556905), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0623335987329483), 'actor_loss': np.float64(-0.9688905477523804), 'hyper_actor_loss': np.float64(0.0005725555471144617), 'behavior_loss': np.float64(0.3290617763996124)}

Episode step 14440, time diff 0.9525871276855469, total time dif 1217.289621591568)
step: 14440 @ episode report: {'average_total_reward': np.float32(7.5555563), 'reward_variance': np.float32(2.7827907), 'max_total_reward': np.float32(10.655556), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07462692260742188), 'actor_loss': np.float64(-0.9932217836380005), 'hyper_actor_loss': np.float64(0.0006159965298138559), 'behavior_loss': np.float64(0.3242719531059265)}

Episode step 14450, time diff 0.8771266937255859, total time dif 1218.2422087192535)
step: 14450 @ episode report: {'average_total_reward': np.float32(8.08), 'reward_variance': np.float32(1.2058964), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0862324010580778), 'actor_loss': np.float64(-1.018406879901886), 'hyper_actor_loss': np.float64(0.0006190428743138909), 'behavior_loss': np.float64(0.3218841016292572)}

Episode step 14460, time diff 0.8556537628173828, total time dif 1219.1193354129791)
step: 14460 @ episode report: {'average_total_reward': np.float32(7.304445), 'reward_variance': np.float32(2.1420786), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06816881485283374), 'actor_loss': np.float64(-1.0115946650505065), 'hyper_actor_loss': np.float64(0.0005612878885585815), 'behavior_loss': np.float64(0.3338100552558899)}

Episode step 14470, time diff 0.9325323104858398, total time dif 1219.9749891757965)
step: 14470 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(1.5127702), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06988363899290562), 'actor_loss': np.float64(-0.9778894782066345), 'hyper_actor_loss': np.float64(0.0005481301166582852), 'behavior_loss': np.float64(0.3286019593477249)}

Episode step 14480, time diff 1.0846567153930664, total time dif 1220.9075214862823)
step: 14480 @ episode report: {'average_total_reward': np.float32(6.757778), 'reward_variance': np.float32(1.5112299), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06140141375362873), 'actor_loss': np.float64(-0.9917541146278381), 'hyper_actor_loss': np.float64(0.0005253613198874518), 'behavior_loss': np.float64(0.3348812311887741)}

Episode step 14490, time diff 0.8383581638336182, total time dif 1221.9921782016754)
step: 14490 @ episode report: {'average_total_reward': np.float32(7.1555567), 'reward_variance': np.float32(2.2819514), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0708239434286952), 'actor_loss': np.float64(-0.9758946061134338), 'hyper_actor_loss': np.float64(0.0005126330739585683), 'behavior_loss': np.float64(0.3278210937976837)}

Episode step 14500, time diff 0.9079780578613281, total time dif 1222.830536365509)
step: 14500 @ episode report: {'average_total_reward': np.float32(7.0922227), 'reward_variance': np.float32(1.3879026), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07797965332865715), 'actor_loss': np.float64(-1.0151274383068085), 'hyper_actor_loss': np.float64(0.00047673264634795487), 'behavior_loss': np.float64(0.32979067265987394)}

Episode step 14510, time diff 0.9036805629730225, total time dif 1223.7385144233704)
step: 14510 @ episode report: {'average_total_reward': np.float32(8.053333), 'reward_variance': np.float32(1.5717237), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0721719939261675), 'actor_loss': np.float64(-0.9928159594535828), 'hyper_actor_loss': np.float64(0.00046569808619096876), 'behavior_loss': np.float64(0.32933673858642576)}

Episode step 14520, time diff 0.8988406658172607, total time dif 1224.6421949863434)
step: 14520 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(1.615091), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06143442690372467), 'actor_loss': np.float64(-0.9655876100063324), 'hyper_actor_loss': np.float64(0.00043351894710212945), 'behavior_loss': np.float64(0.3311213344335556)}

Episode step 14530, time diff 1.0067121982574463, total time dif 1225.5410356521606)
step: 14530 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(2.336431), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07789018712937831), 'actor_loss': np.float64(-0.9879594385623932), 'hyper_actor_loss': np.float64(0.00041308883810415865), 'behavior_loss': np.float64(0.33112606704235076)}

Episode step 14540, time diff 0.971644401550293, total time dif 1226.547747850418)
step: 14540 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(3.3563323), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0686526469886303), 'actor_loss': np.float64(-0.9934085249900818), 'hyper_actor_loss': np.float64(0.00038559617241844534), 'behavior_loss': np.float64(0.3368764787912369)}

Episode step 14550, time diff 0.8979558944702148, total time dif 1227.5193922519684)
step: 14550 @ episode report: {'average_total_reward': np.float32(9.102222), 'reward_variance': np.float32(1.0356491), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0750187162309885), 'actor_loss': np.float64(-0.9952487885951996), 'hyper_actor_loss': np.float64(0.0006192366505274549), 'behavior_loss': np.float64(0.34473524391651156)}

Episode step 14560, time diff 0.9229576587677002, total time dif 1228.4173481464386)
step: 14560 @ episode report: {'average_total_reward': np.float32(5.596667), 'reward_variance': np.float32(1.5413841), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08107290305197239), 'actor_loss': np.float64(-0.9930966913700103), 'hyper_actor_loss': np.float64(0.0012572602892760188), 'behavior_loss': np.float64(0.3372210592031479)}

Episode step 14570, time diff 0.8868410587310791, total time dif 1229.3403058052063)
step: 14570 @ episode report: {'average_total_reward': np.float32(6.867778), 'reward_variance': np.float32(1.6707026), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06259798146784305), 'actor_loss': np.float64(-0.9773988008499146), 'hyper_actor_loss': np.float64(0.0017376225674524902), 'behavior_loss': np.float64(0.3213658958673477)}

Episode step 14580, time diff 1.0050108432769775, total time dif 1230.2271468639374)
step: 14580 @ episode report: {'average_total_reward': np.float32(10.310001), 'reward_variance': np.float32(2.6531475), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07737045772373677), 'actor_loss': np.float64(-1.0045643985271453), 'hyper_actor_loss': np.float64(0.0012127775116823614), 'behavior_loss': np.float64(0.3133863598108292)}

Episode step 14590, time diff 0.8995153903961182, total time dif 1231.2321577072144)
step: 14590 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(3.6125798), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07118870913982392), 'actor_loss': np.float64(-0.9918380081653595), 'hyper_actor_loss': np.float64(0.0008612921636085957), 'behavior_loss': np.float64(0.3105598598718643)}

Episode step 14600, time diff 0.9397909641265869, total time dif 1232.1316730976105)
step: 14600 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(10.455813), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664601694792509), 'actor_loss': np.float64(-0.9910198450088501), 'hyper_actor_loss': np.float64(0.0007429417863022536), 'behavior_loss': np.float64(0.3089151605963707)}

Episode step 14610, time diff 0.9099423885345459, total time dif 1233.071464061737)
step: 14610 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.504391), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06901805214583874), 'actor_loss': np.float64(-0.976000314950943), 'hyper_actor_loss': np.float64(0.0006682824634481222), 'behavior_loss': np.float64(0.3055187851190567)}

Episode step 14620, time diff 0.9355685710906982, total time dif 1233.9814064502716)
step: 14620 @ episode report: {'average_total_reward': np.float32(10.285555), 'reward_variance': np.float32(2.0102227), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07285949401557446), 'actor_loss': np.float64(-0.9837070345878601), 'hyper_actor_loss': np.float64(0.0006706610613036901), 'behavior_loss': np.float64(0.3113936573266983)}

Episode step 14630, time diff 0.8919076919555664, total time dif 1234.9169750213623)
step: 14630 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.419141), 'max_total_reward': np.float32(14.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07198521494865417), 'actor_loss': np.float64(-0.9959332466125488), 'hyper_actor_loss': np.float64(0.0006766462291125208), 'behavior_loss': np.float64(0.30867010951042173)}

Episode step 14640, time diff 0.9069113731384277, total time dif 1235.8088827133179)
step: 14640 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.4329927), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06923640016466379), 'actor_loss': np.float64(-0.9806955099105835), 'hyper_actor_loss': np.float64(0.0006993738294113428), 'behavior_loss': np.float64(0.3026592075824738)}

Episode step 14650, time diff 1.1077473163604736, total time dif 1236.7157940864563)
step: 14650 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(2.5881104), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0678941709920764), 'actor_loss': np.float64(-0.9779768645763397), 'hyper_actor_loss': np.float64(0.0008027241041418165), 'behavior_loss': np.float64(0.3098107397556305)}

Episode step 14660, time diff 0.9426453113555908, total time dif 1237.8235414028168)
step: 14660 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(1.6001284), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07142972461879253), 'actor_loss': np.float64(-0.9911141812801361), 'hyper_actor_loss': np.float64(0.0011320260004140438), 'behavior_loss': np.float64(0.31382368206977845)}

Episode step 14670, time diff 1.045619249343872, total time dif 1238.7661867141724)
step: 14670 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(7.64173), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07197786569595337), 'actor_loss': np.float64(-0.99504234790802), 'hyper_actor_loss': np.float64(0.0013999661430716514), 'behavior_loss': np.float64(0.30530022978782656)}

Episode step 14680, time diff 0.969759464263916, total time dif 1239.8118059635162)
step: 14680 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(7.3430634), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0750484000891447), 'actor_loss': np.float64(-0.9932743728160858), 'hyper_actor_loss': np.float64(0.0016764998086728155), 'behavior_loss': np.float64(0.31568631529808044)}

Episode step 14690, time diff 1.064026117324829, total time dif 1240.7815654277802)
step: 14690 @ episode report: {'average_total_reward': np.float32(11.981112), 'reward_variance': np.float32(2.2536561), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06654997356235981), 'actor_loss': np.float64(-0.9926766514778137), 'hyper_actor_loss': np.float64(0.0015916768810711802), 'behavior_loss': np.float64(0.3032362848520279)}

Episode step 14700, time diff 1.0447940826416016, total time dif 1241.845591545105)
step: 14700 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(1.9453943), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07886395044624805), 'actor_loss': np.float64(-0.9927846848964691), 'hyper_actor_loss': np.float64(0.001064096431946382), 'behavior_loss': np.float64(0.3128503441810608)}

Episode step 14710, time diff 0.9022707939147949, total time dif 1242.8903856277466)
step: 14710 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(2.521903), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06401253547519445), 'actor_loss': np.float64(-0.9648660123348236), 'hyper_actor_loss': np.float64(0.0007457537634763866), 'behavior_loss': np.float64(0.3033447712659836)}

Episode step 14720, time diff 1.0053844451904297, total time dif 1243.7926564216614)
step: 14720 @ episode report: {'average_total_reward': np.float32(11.383333), 'reward_variance': np.float32(1.4638827), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08011751547455788), 'actor_loss': np.float64(-0.9965624570846557), 'hyper_actor_loss': np.float64(0.0007063800934702158), 'behavior_loss': np.float64(0.29436555206775666)}

Episode step 14730, time diff 0.9771699905395508, total time dif 1244.7980408668518)
step: 14730 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(1.0710667), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08478052541613579), 'actor_loss': np.float64(-1.0248118400573731), 'hyper_actor_loss': np.float64(0.0007316017290577292), 'behavior_loss': np.float64(0.2888108789920807)}

Episode step 14740, time diff 1.0923478603363037, total time dif 1245.7752108573914)
step: 14740 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(3.6596603), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07367577813565732), 'actor_loss': np.float64(-0.996296501159668), 'hyper_actor_loss': np.float64(0.000901704712305218), 'behavior_loss': np.float64(0.2879014641046524)}

Episode step 14750, time diff 0.9740259647369385, total time dif 1246.8675587177277)
step: 14750 @ episode report: {'average_total_reward': np.float32(11.993334), 'reward_variance': np.float32(2.056005), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.022223), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06595805585384369), 'actor_loss': np.float64(-0.9871944844722748), 'hyper_actor_loss': np.float64(0.0010611625155434012), 'behavior_loss': np.float64(0.2890136867761612)}

Episode step 14760, time diff 1.10471510887146, total time dif 1247.8415846824646)
step: 14760 @ episode report: {'average_total_reward': np.float32(11.993334), 'reward_variance': np.float32(2.981216), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06675781235098839), 'actor_loss': np.float64(-0.978138542175293), 'hyper_actor_loss': np.float64(0.0011534395278431475), 'behavior_loss': np.float64(0.28897368013858793)}

Episode step 14770, time diff 1.150521993637085, total time dif 1248.946299791336)
step: 14770 @ episode report: {'average_total_reward': np.float32(10.922222), 'reward_variance': np.float32(5.922544), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061202398501336575), 'actor_loss': np.float64(-0.9910377860069275), 'hyper_actor_loss': np.float64(0.0013458029134199023), 'behavior_loss': np.float64(0.29011426866054535)}

Episode step 14780, time diff 1.0177996158599854, total time dif 1250.0968217849731)
step: 14780 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(1.9556751), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07055237106978893), 'actor_loss': np.float64(-0.9970990300178528), 'hyper_actor_loss': np.float64(0.0012498109834268689), 'behavior_loss': np.float64(0.2896580308675766)}

Episode step 14790, time diff 1.0184383392333984, total time dif 1251.1146214008331)
step: 14790 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.855551), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07407118491828442), 'actor_loss': np.float64(-0.997175931930542), 'hyper_actor_loss': np.float64(0.0010556545399595051), 'behavior_loss': np.float64(0.2779108673334122)}

Episode step 14800, time diff 1.0690817832946777, total time dif 1252.1330597400665)
step: 14800 @ episode report: {'average_total_reward': np.float32(11.295556), 'reward_variance': np.float32(4.714919), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06074910461902618), 'actor_loss': np.float64(-0.9951698958873749), 'hyper_actor_loss': np.float64(0.0008384393353480845), 'behavior_loss': np.float64(0.2869009330868721)}

Episode step 14810, time diff 1.0153331756591797, total time dif 1253.2021415233612)
step: 14810 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(5.8221006), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06779108755290508), 'actor_loss': np.float64(-0.9619245052337646), 'hyper_actor_loss': np.float64(0.0007681134622544051), 'behavior_loss': np.float64(0.27883853316307067)}

Episode step 14820, time diff 1.3067388534545898, total time dif 1254.2174746990204)
step: 14820 @ episode report: {'average_total_reward': np.float32(13.103334), 'reward_variance': np.float32(2.0187185), 'max_total_reward': np.float32(15.144445), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(13.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0611000832170248), 'actor_loss': np.float64(-0.9831378221511841), 'hyper_actor_loss': np.float64(0.00074851083336398), 'behavior_loss': np.float64(0.28091617822647097)}

Episode step 14830, time diff 0.9841392040252686, total time dif 1255.524213552475)
step: 14830 @ episode report: {'average_total_reward': np.float32(11.520001), 'reward_variance': np.float32(3.2051558), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06857089437544346), 'actor_loss': np.float64(-1.0008289456367492), 'hyper_actor_loss': np.float64(0.0006761157012078911), 'behavior_loss': np.float64(0.26625014543533326)}

Episode step 14840, time diff 1.0285634994506836, total time dif 1256.5083527565002)
step: 14840 @ episode report: {'average_total_reward': np.float32(11.2322235), 'reward_variance': np.float32(5.186185), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06003852151334286), 'actor_loss': np.float64(-0.9748783648014069), 'hyper_actor_loss': np.float64(0.0007609824766404927), 'behavior_loss': np.float64(0.2838087037205696)}

Episode step 14850, time diff 0.9793167114257812, total time dif 1257.536916255951)
step: 14850 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(1.1468754), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062356643937528135), 'actor_loss': np.float64(-0.9600948512554168), 'hyper_actor_loss': np.float64(0.0008268337172921747), 'behavior_loss': np.float64(0.2847586512565613)}

Episode step 14860, time diff 0.925168514251709, total time dif 1258.5162329673767)
step: 14860 @ episode report: {'average_total_reward': np.float32(12.317778), 'reward_variance': np.float32(1.0589929), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(11.144444), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06318872608244419), 'actor_loss': np.float64(-0.9874765455722809), 'hyper_actor_loss': np.float64(0.0008526997349690646), 'behavior_loss': np.float64(0.28271550238132476)}

Episode step 14870, time diff 0.9398007392883301, total time dif 1259.4414014816284)
step: 14870 @ episode report: {'average_total_reward': np.float32(11.756667), 'reward_variance': np.float32(3.8352458), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06953472234308719), 'actor_loss': np.float64(-0.9971418440341949), 'hyper_actor_loss': np.float64(0.0008479806187096983), 'behavior_loss': np.float64(0.2936321198940277)}

Episode step 14880, time diff 0.9437050819396973, total time dif 1260.3812022209167)
step: 14880 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.091407), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06325655104592443), 'actor_loss': np.float64(-0.9661380469799041), 'hyper_actor_loss': np.float64(0.0007338964380323887), 'behavior_loss': np.float64(0.2804870083928108)}

Episode step 14890, time diff 0.8987030982971191, total time dif 1261.3249073028564)
step: 14890 @ episode report: {'average_total_reward': np.float32(11.332224), 'reward_variance': np.float32(1.9568018), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0627055037766695), 'actor_loss': np.float64(-0.9817293226718903), 'hyper_actor_loss': np.float64(0.0006838383036665618), 'behavior_loss': np.float64(0.2862570911645889)}

Episode step 14900, time diff 0.9168109893798828, total time dif 1262.2236104011536)
step: 14900 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(2.1881785), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07248565219342709), 'actor_loss': np.float64(-0.9905394732952117), 'hyper_actor_loss': np.float64(0.0006208252627402544), 'behavior_loss': np.float64(0.27843265384435656)}

Episode step 14910, time diff 1.0180509090423584, total time dif 1263.1404213905334)
step: 14910 @ episode report: {'average_total_reward': np.float32(11.532223), 'reward_variance': np.float32(2.3209996), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07070802114903926), 'actor_loss': np.float64(-1.0003615498542786), 'hyper_actor_loss': np.float64(0.0005759752064477652), 'behavior_loss': np.float64(0.27682057619094846)}

Episode step 14920, time diff 0.9535312652587891, total time dif 1264.1584722995758)
step: 14920 @ episode report: {'average_total_reward': np.float32(11.917778), 'reward_variance': np.float32(2.7763507), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06172298826277256), 'actor_loss': np.float64(-0.9712623536586762), 'hyper_actor_loss': np.float64(0.0005887933657504618), 'behavior_loss': np.float64(0.287287113070488)}

Episode step 14930, time diff 1.047191858291626, total time dif 1265.1120035648346)
step: 14930 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(3.2797718), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06663597859442234), 'actor_loss': np.float64(-0.9833053529262543), 'hyper_actor_loss': np.float64(0.0005626536498311907), 'behavior_loss': np.float64(0.2749759525060654)}

Episode step 14940, time diff 0.999969482421875, total time dif 1266.1591954231262)
step: 14940 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(3.2390664), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07308149263262749), 'actor_loss': np.float64(-1.0077812433242799), 'hyper_actor_loss': np.float64(0.0005742742621805518), 'behavior_loss': np.float64(0.2866459786891937)}

Episode step 14950, time diff 1.010488510131836, total time dif 1267.159164905548)
step: 14950 @ episode report: {'average_total_reward': np.float32(10.795557), 'reward_variance': np.float32(1.5718322), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07672876939177513), 'actor_loss': np.float64(-0.9895920634269715), 'hyper_actor_loss': np.float64(0.0005562025413382799), 'behavior_loss': np.float64(0.2838592931628227)}

Episode step 14960, time diff 1.0104169845581055, total time dif 1268.16965341568)
step: 14960 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(1.7685139), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07755823843181134), 'actor_loss': np.float64(-0.9966043412685395), 'hyper_actor_loss': np.float64(0.0006465432234108448), 'behavior_loss': np.float64(0.2830022215843201)}

Episode step 14970, time diff 0.9650828838348389, total time dif 1269.180070400238)
step: 14970 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(1.5143969), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0657985532656312), 'actor_loss': np.float64(-0.9983988046646118), 'hyper_actor_loss': np.float64(0.0006271513469982892), 'behavior_loss': np.float64(0.2719714492559433)}

Episode step 14980, time diff 1.0294654369354248, total time dif 1270.1451532840729)
step: 14980 @ episode report: {'average_total_reward': np.float32(11.556667), 'reward_variance': np.float32(3.054084), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07515673935413361), 'actor_loss': np.float64(-0.9826995670795441), 'hyper_actor_loss': np.float64(0.0006322802393697203), 'behavior_loss': np.float64(0.2884161382913589)}

Episode step 14990, time diff 1.17543363571167, total time dif 1271.1746187210083)
step: 14990 @ episode report: {'average_total_reward': np.float32(12.081112), 'reward_variance': np.float32(3.881607), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.050796670466661455), 'actor_loss': np.float64(-0.9671708941459656), 'hyper_actor_loss': np.float64(0.0006236361863557249), 'behavior_loss': np.float64(0.28453495651483535)}

Episode step 15000, time diff 0.9618914127349854, total time dif 1272.35005235672)
step: 15000 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(0.7592149), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060304447263479236), 'actor_loss': np.float64(-0.9725855171680451), 'hyper_actor_loss': np.float64(0.0006744583253748715), 'behavior_loss': np.float64(0.288239386677742)}

Episode step 15010, time diff 0.9878947734832764, total time dif 1273.311943769455)
step: 15010 @ episode report: {'average_total_reward': np.float32(11.944445), 'reward_variance': np.float32(2.5231357), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06263596061617135), 'actor_loss': np.float64(-0.9933299481868744), 'hyper_actor_loss': np.float64(0.0007438274216838181), 'behavior_loss': np.float64(0.27656024396419526)}

Episode step 15020, time diff 1.0051226615905762, total time dif 1274.2998385429382)
step: 15020 @ episode report: {'average_total_reward': np.float32(11.556667), 'reward_variance': np.float32(2.9473448), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06683161109685898), 'actor_loss': np.float64(-0.9946601688861847), 'hyper_actor_loss': np.float64(0.0008883902570232749), 'behavior_loss': np.float64(0.26419693529605864)}

Episode step 15030, time diff 1.032348871231079, total time dif 1275.3049612045288)
step: 15030 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(3.5608258), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06508171036839486), 'actor_loss': np.float64(-0.993829733133316), 'hyper_actor_loss': np.float64(0.0009615990042220801), 'behavior_loss': np.float64(0.27075860649347305)}

Episode step 15040, time diff 0.9933679103851318, total time dif 1276.33731007576)
step: 15040 @ episode report: {'average_total_reward': np.float32(11.583334), 'reward_variance': np.float32(4.187241), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07511144876480103), 'actor_loss': np.float64(-0.9947262108325958), 'hyper_actor_loss': np.float64(0.0009020416124258191), 'behavior_loss': np.float64(0.272204726934433)}

Episode step 15050, time diff 0.9586372375488281, total time dif 1277.330677986145)
step: 15050 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(1.776193), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059642478451132776), 'actor_loss': np.float64(-0.9919878780841828), 'hyper_actor_loss': np.float64(0.0007537469442468137), 'behavior_loss': np.float64(0.28669515550136565)}

Episode step 15060, time diff 0.9338443279266357, total time dif 1278.2893152236938)
step: 15060 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(2.218598), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06260676849633455), 'actor_loss': np.float64(-0.9612004518508911), 'hyper_actor_loss': np.float64(0.0006606882379855961), 'behavior_loss': np.float64(0.2838164657354355)}

Episode step 15070, time diff 0.9234602451324463, total time dif 1279.2231595516205)
step: 15070 @ episode report: {'average_total_reward': np.float32(12.642222), 'reward_variance': np.float32(3.369749), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(13.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05605654828250408), 'actor_loss': np.float64(-0.9814590096473694), 'hyper_actor_loss': np.float64(0.0007042764336802065), 'behavior_loss': np.float64(0.2768241032958031)}

Episode step 15080, time diff 0.899418830871582, total time dif 1280.146619796753)
step: 15080 @ episode report: {'average_total_reward': np.float32(11.844446), 'reward_variance': np.float32(4.49289), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06718321712687611), 'actor_loss': np.float64(-0.9920373797416687), 'hyper_actor_loss': np.float64(0.0007863350212574006), 'behavior_loss': np.float64(0.2775808572769165)}

Episode step 15090, time diff 0.9847774505615234, total time dif 1281.0460386276245)
step: 15090 @ episode report: {'average_total_reward': np.float32(11.556668), 'reward_variance': np.float32(2.7199116), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06321541219949722), 'actor_loss': np.float64(-0.9767119467258454), 'hyper_actor_loss': np.float64(0.000811453367350623), 'behavior_loss': np.float64(0.2691031157970428)}

Episode step 15100, time diff 0.9302270412445068, total time dif 1282.030816078186)
step: 15100 @ episode report: {'average_total_reward': np.float32(12.678889), 'reward_variance': np.float32(3.223666), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(13.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06253464594483375), 'actor_loss': np.float64(-0.984712815284729), 'hyper_actor_loss': np.float64(0.0008313190715853125), 'behavior_loss': np.float64(0.27125130891799926)}

Episode step 15110, time diff 1.0486280918121338, total time dif 1282.9610431194305)
step: 15110 @ episode report: {'average_total_reward': np.float32(12.093335), 'reward_variance': np.float32(4.412696), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06858205422759056), 'actor_loss': np.float64(-0.9994908332824707), 'hyper_actor_loss': np.float64(0.0007650144398212433), 'behavior_loss': np.float64(0.27767023891210557)}

Episode step 15120, time diff 1.0975985527038574, total time dif 1284.0096712112427)
step: 15120 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(1.7662728), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06993235051631927), 'actor_loss': np.float64(-0.9924149692058564), 'hyper_actor_loss': np.float64(0.0007288757187779993), 'behavior_loss': np.float64(0.2766569942235947)}

Episode step 15130, time diff 0.9567773342132568, total time dif 1285.1072697639465)
step: 15130 @ episode report: {'average_total_reward': np.float32(12.205557), 'reward_variance': np.float32(4.7619576), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06669502258300782), 'actor_loss': np.float64(-0.9998203575611114), 'hyper_actor_loss': np.float64(0.0007182103814557195), 'behavior_loss': np.float64(0.2886816024780273)}

Episode step 15140, time diff 0.9773702621459961, total time dif 1286.0640470981598)
step: 15140 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(6.3613396), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0607906423509121), 'actor_loss': np.float64(-0.981923520565033), 'hyper_actor_loss': np.float64(0.0007266220112796873), 'behavior_loss': np.float64(0.275892673432827)}

Episode step 15150, time diff 0.9722537994384766, total time dif 1287.0414173603058)
step: 15150 @ episode report: {'average_total_reward': np.float32(11.244446), 'reward_variance': np.float32(3.3643951), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05260218661278486), 'actor_loss': np.float64(-0.9794256091117859), 'hyper_actor_loss': np.float64(0.0008223894925322384), 'behavior_loss': np.float64(0.27019843757152556)}

Episode step 15160, time diff 1.2007839679718018, total time dif 1288.0136711597443)
step: 15160 @ episode report: {'average_total_reward': np.float32(11.432222), 'reward_variance': np.float32(4.530999), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06317775025963783), 'actor_loss': np.float64(-0.9750918209552765), 'hyper_actor_loss': np.float64(0.0009163982118479908), 'behavior_loss': np.float64(0.279742431640625)}

Episode step 15170, time diff 0.9798500537872314, total time dif 1289.214455127716)
step: 15170 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(3.9863725), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06943114921450615), 'actor_loss': np.float64(-0.9938384652137756), 'hyper_actor_loss': np.float64(0.0010325582115910947), 'behavior_loss': np.float64(0.29364390969276427)}

Episode step 15180, time diff 0.9190642833709717, total time dif 1290.1943051815033)
step: 15180 @ episode report: {'average_total_reward': np.float32(12.31778), 'reward_variance': np.float32(1.9517839), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07380523569881917), 'actor_loss': np.float64(-0.9936980843544007), 'hyper_actor_loss': np.float64(0.0011328579159453512), 'behavior_loss': np.float64(0.2861766189336777)}

Episode step 15190, time diff 0.949209451675415, total time dif 1291.1133694648743)
step: 15190 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(2.0690923), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0670294489711523), 'actor_loss': np.float64(-0.988231098651886), 'hyper_actor_loss': np.float64(0.00113794436911121), 'behavior_loss': np.float64(0.302498397231102)}

Episode step 15200, time diff 0.9396133422851562, total time dif 1292.0625789165497)
step: 15200 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(2.3653846), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07144413217902183), 'actor_loss': np.float64(-0.9868209779262542), 'hyper_actor_loss': np.float64(0.001041572546819225), 'behavior_loss': np.float64(0.27567584067583084)}

Episode step 15210, time diff 1.0079896450042725, total time dif 1293.0021922588348)
step: 15210 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(6.975445), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061574846133589746), 'actor_loss': np.float64(-1.002144557237625), 'hyper_actor_loss': np.float64(0.0009132769075222314), 'behavior_loss': np.float64(0.29235700964927674)}

Episode step 15220, time diff 0.9213347434997559, total time dif 1294.010181903839)
step: 15220 @ episode report: {'average_total_reward': np.float32(12.330001), 'reward_variance': np.float32(2.761384), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07117370404303074), 'actor_loss': np.float64(-0.9702605426311492), 'hyper_actor_loss': np.float64(0.0009049033455085009), 'behavior_loss': np.float64(0.297303731739521)}

Episode step 15230, time diff 0.9594755172729492, total time dif 1294.9315166473389)
step: 15230 @ episode report: {'average_total_reward': np.float32(11.968889), 'reward_variance': np.float32(4.1503167), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0551990469917655), 'actor_loss': np.float64(-0.9980047941207886), 'hyper_actor_loss': np.float64(0.0008489639323670418), 'behavior_loss': np.float64(0.27819679826498034)}

Episode step 15240, time diff 1.0046038627624512, total time dif 1295.8909921646118)
step: 15240 @ episode report: {'average_total_reward': np.float32(11.76889), 'reward_variance': np.float32(3.5393283), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07493838928639888), 'actor_loss': np.float64(-0.9794860124588013), 'hyper_actor_loss': np.float64(0.000863835820928216), 'behavior_loss': np.float64(0.3045796602964401)}

Episode step 15250, time diff 1.0684125423431396, total time dif 1296.8955960273743)
step: 15250 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(3.0221276), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07029360122978687), 'actor_loss': np.float64(-0.9995007038116455), 'hyper_actor_loss': np.float64(0.0009056701324880123), 'behavior_loss': np.float64(0.2948965221643448)}

Episode step 15260, time diff 0.9983677864074707, total time dif 1297.9640085697174)
step: 15260 @ episode report: {'average_total_reward': np.float32(12.33), 'reward_variance': np.float32(2.200767), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0564126942306757), 'actor_loss': np.float64(-0.9751639842987061), 'hyper_actor_loss': np.float64(0.0009279566758777947), 'behavior_loss': np.float64(0.27471187710762024)}

Episode step 15270, time diff 1.0355660915374756, total time dif 1298.9623763561249)
step: 15270 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(2.0233579), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05959860756993294), 'actor_loss': np.float64(-0.9725358664989472), 'hyper_actor_loss': np.float64(0.0009428268007468432), 'behavior_loss': np.float64(0.2831667482852936)}

Episode step 15280, time diff 0.9769880771636963, total time dif 1299.9979424476624)
step: 15280 @ episode report: {'average_total_reward': np.float32(10.685556), 'reward_variance': np.float32(2.952273), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06256236173212529), 'actor_loss': np.float64(-0.9898372888565063), 'hyper_actor_loss': np.float64(0.000977886375039816), 'behavior_loss': np.float64(0.2741837829351425)}

Episode step 15290, time diff 1.1514263153076172, total time dif 1300.974930524826)
step: 15290 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(4.3193207), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05604254715144634), 'actor_loss': np.float64(-0.9863136649131775), 'hyper_actor_loss': np.float64(0.0010091680102050304), 'behavior_loss': np.float64(0.2733371779322624)}

Episode step 15300, time diff 1.1159684658050537, total time dif 1302.1263568401337)
step: 15300 @ episode report: {'average_total_reward': np.float32(11.495557), 'reward_variance': np.float32(7.002055), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06528884135186672), 'actor_loss': np.float64(-0.9890008509159088), 'hyper_actor_loss': np.float64(0.0010502049291972071), 'behavior_loss': np.float64(0.27235368341207505)}

Episode step 15310, time diff 0.9778711795806885, total time dif 1303.2423253059387)
step: 15310 @ episode report: {'average_total_reward': np.float32(11.981112), 'reward_variance': np.float32(4.372422), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06518000010401011), 'actor_loss': np.float64(-0.98144890666008), 'hyper_actor_loss': np.float64(0.0011310973204672337), 'behavior_loss': np.float64(0.2818943843245506)}

Episode step 15320, time diff 1.1147089004516602, total time dif 1304.2201964855194)
step: 15320 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(4.07842), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0702091570943594), 'actor_loss': np.float64(-0.9956314742565155), 'hyper_actor_loss': np.float64(0.0011638086871244013), 'behavior_loss': np.float64(0.2813865438103676)}

Episode step 15330, time diff 1.399228572845459, total time dif 1305.334905385971)
step: 15330 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.733443), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06529647149145604), 'actor_loss': np.float64(-1.004762876033783), 'hyper_actor_loss': np.float64(0.0012179906712844967), 'behavior_loss': np.float64(0.2867374524474144)}

Episode step 15340, time diff 1.0705506801605225, total time dif 1306.7341339588165)
step: 15340 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(3.4601502), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07526292242109775), 'actor_loss': np.float64(-0.9795091450214386), 'hyper_actor_loss': np.float64(0.0012149592977948487), 'behavior_loss': np.float64(0.29607344418764114)}

Episode step 15350, time diff 1.0624802112579346, total time dif 1307.804684638977)
step: 15350 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.9044254), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07065146788954735), 'actor_loss': np.float64(-1.0003496885299683), 'hyper_actor_loss': np.float64(0.0010986507055349647), 'behavior_loss': np.float64(0.27722572535276413)}

Episode step 15360, time diff 1.1525914669036865, total time dif 1308.867164850235)
step: 15360 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(3.2535348), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059060953184962274), 'actor_loss': np.float64(-1.0054775655269623), 'hyper_actor_loss': np.float64(0.0012373923789709806), 'behavior_loss': np.float64(0.2712766945362091)}

Episode step 15370, time diff 1.1370112895965576, total time dif 1310.0197563171387)
step: 15370 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(3.4221425), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05988570377230644), 'actor_loss': np.float64(-0.9659975230693817), 'hyper_actor_loss': np.float64(0.0016845338279381395), 'behavior_loss': np.float64(0.2803203076124191)}

Episode step 15380, time diff 1.130002737045288, total time dif 1311.1567676067352)
step: 15380 @ episode report: {'average_total_reward': np.float32(8.302222), 'reward_variance': np.float32(2.303798), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07002627551555633), 'actor_loss': np.float64(-0.9942757368087769), 'hyper_actor_loss': np.float64(0.0019016827689483761), 'behavior_loss': np.float64(0.2903663799166679)}

Episode step 15390, time diff 1.1170783042907715, total time dif 1312.2867703437805)
step: 15390 @ episode report: {'average_total_reward': np.float32(9.900001), 'reward_variance': np.float32(4.750173), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07380703650414944), 'actor_loss': np.float64(-1.0253352522850037), 'hyper_actor_loss': np.float64(0.001793121627997607), 'behavior_loss': np.float64(0.2941111817955971)}

Episode step 15400, time diff 1.130211591720581, total time dif 1313.4038486480713)
step: 15400 @ episode report: {'average_total_reward': np.float32(7.4288893), 'reward_variance': np.float32(4.5838566), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06736431196331978), 'actor_loss': np.float64(-1.0025550544261932), 'hyper_actor_loss': np.float64(0.0018997402512468398), 'behavior_loss': np.float64(0.2830623805522919)}

Episode step 15410, time diff 1.0056684017181396, total time dif 1314.5340602397919)
step: 15410 @ episode report: {'average_total_reward': np.float32(7.231112), 'reward_variance': np.float32(7.4070334), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07108146212995052), 'actor_loss': np.float64(-0.9933974385261536), 'hyper_actor_loss': np.float64(0.0017452292726375163), 'behavior_loss': np.float64(0.27225730419158933)}

Episode step 15420, time diff 1.0725438594818115, total time dif 1315.53972864151)
step: 15420 @ episode report: {'average_total_reward': np.float32(8.316668), 'reward_variance': np.float32(1.9973638), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06614529527723789), 'actor_loss': np.float64(-1.031057596206665), 'hyper_actor_loss': np.float64(0.0019196929410099984), 'behavior_loss': np.float64(0.27697878181934354)}

Episode step 15430, time diff 1.0105061531066895, total time dif 1316.6122725009918)
step: 15430 @ episode report: {'average_total_reward': np.float32(7.504445), 'reward_variance': np.float32(3.712401), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05925377644598484), 'actor_loss': np.float64(-1.013557243347168), 'hyper_actor_loss': np.float64(0.0022527049062773586), 'behavior_loss': np.float64(0.2740601822733879)}

Episode step 15440, time diff 0.9112763404846191, total time dif 1317.6227786540985)
step: 15440 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(4.883039), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05491927079856396), 'actor_loss': np.float64(-0.9891043245792389), 'hyper_actor_loss': np.float64(0.0024436152772977946), 'behavior_loss': np.float64(0.2874105989933014)}

Episode step 15450, time diff 0.9344511032104492, total time dif 1318.5340549945831)
step: 15450 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(1.1173202), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06017786003649235), 'actor_loss': np.float64(-0.9993343949317932), 'hyper_actor_loss': np.float64(0.002126239216886461), 'behavior_loss': np.float64(0.28268924355506897)}

Episode step 15460, time diff 1.0206153392791748, total time dif 1319.4685060977936)
step: 15460 @ episode report: {'average_total_reward': np.float32(7.018889), 'reward_variance': np.float32(2.5592859), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0673899620771408), 'actor_loss': np.float64(-1.011963999271393), 'hyper_actor_loss': np.float64(0.0020651552942581473), 'behavior_loss': np.float64(0.3025604099035263)}

Episode step 15470, time diff 1.028982400894165, total time dif 1320.4891214370728)
step: 15470 @ episode report: {'average_total_reward': np.float32(6.867778), 'reward_variance': np.float32(1.3365301), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07250988371670246), 'actor_loss': np.float64(-1.0037200272083282), 'hyper_actor_loss': np.float64(0.0019287158735096454), 'behavior_loss': np.float64(0.30776546597480775)}

Episode step 15480, time diff 0.9761409759521484, total time dif 1321.518103837967)
step: 15480 @ episode report: {'average_total_reward': np.float32(7.5288887), 'reward_variance': np.float32(3.032375), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06273480784147978), 'actor_loss': np.float64(-0.9843732118606567), 'hyper_actor_loss': np.float64(0.0018682566937059165), 'behavior_loss': np.float64(0.30126955807209016)}

Episode step 15490, time diff 1.165687084197998, total time dif 1322.494244813919)
step: 15490 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(1.3622029), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07295038625597954), 'actor_loss': np.float64(-1.0030617356300353), 'hyper_actor_loss': np.float64(0.0018085832474753261), 'behavior_loss': np.float64(0.32517187893390653)}

Episode step 15500, time diff 1.0152113437652588, total time dif 1323.659931898117)
step: 15500 @ episode report: {'average_total_reward': np.float32(7.853334), 'reward_variance': np.float32(2.8878484), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06529796831309795), 'actor_loss': np.float64(-0.9826790690422058), 'hyper_actor_loss': np.float64(0.0017252723220735789), 'behavior_loss': np.float64(0.30760557055473325)}

Episode step 15510, time diff 0.9761962890625, total time dif 1324.6751432418823)
step: 15510 @ episode report: {'average_total_reward': np.float32(9.587779), 'reward_variance': np.float32(2.3925796), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060965605080127716), 'actor_loss': np.float64(-0.9794823706150055), 'hyper_actor_loss': np.float64(0.001854794647078961), 'behavior_loss': np.float64(0.3091802716255188)}

Episode step 15520, time diff 0.9392526149749756, total time dif 1325.6513395309448)
step: 15520 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.1269736), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08161506168544293), 'actor_loss': np.float64(-1.0001977145671845), 'hyper_actor_loss': np.float64(0.0018224686151370405), 'behavior_loss': np.float64(0.3050220012664795)}

Episode step 15530, time diff 0.978809118270874, total time dif 1326.5905921459198)
step: 15530 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(1.8391619), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05702981762588024), 'actor_loss': np.float64(-0.9796465992927551), 'hyper_actor_loss': np.float64(0.0016468134592287243), 'behavior_loss': np.float64(0.2974701106548309)}

Episode step 15540, time diff 0.9081671237945557, total time dif 1327.5694012641907)
step: 15540 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(1.9108852), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07294304370880127), 'actor_loss': np.float64(-0.995996105670929), 'hyper_actor_loss': np.float64(0.0015861207153648137), 'behavior_loss': np.float64(0.29098246693611146)}

Episode step 15550, time diff 0.941866397857666, total time dif 1328.4775683879852)
step: 15550 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(2.3676596), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05528264343738556), 'actor_loss': np.float64(-0.9916751921176911), 'hyper_actor_loss': np.float64(0.0015679297270253302), 'behavior_loss': np.float64(0.29396034628152845)}

Episode step 15560, time diff 0.9606621265411377, total time dif 1329.419434785843)
step: 15560 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.3578026), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06269816979765892), 'actor_loss': np.float64(-0.9517487764358521), 'hyper_actor_loss': np.float64(0.0016101083485409618), 'behavior_loss': np.float64(0.30805512964725495)}

Episode step 15570, time diff 0.9680423736572266, total time dif 1330.380096912384)
step: 15570 @ episode report: {'average_total_reward': np.float32(10.434444), 'reward_variance': np.float32(3.3363814), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06549643017351628), 'actor_loss': np.float64(-0.9858219623565674), 'hyper_actor_loss': np.float64(0.0017462832969613374), 'behavior_loss': np.float64(0.2987171232700348)}

Episode step 15580, time diff 0.9842321872711182, total time dif 1331.3481392860413)
step: 15580 @ episode report: {'average_total_reward': np.float32(11.607779), 'reward_variance': np.float32(2.0344703), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06361961588263512), 'actor_loss': np.float64(-1.0019829094409942), 'hyper_actor_loss': np.float64(0.0016081571578979493), 'behavior_loss': np.float64(0.28927266150712966)}

Episode step 15590, time diff 1.1085717678070068, total time dif 1332.3323714733124)
step: 15590 @ episode report: {'average_total_reward': np.float32(11.25889), 'reward_variance': np.float32(2.1694586), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062373198196291925), 'actor_loss': np.float64(-0.9840485751628876), 'hyper_actor_loss': np.float64(0.0015422716038301586), 'behavior_loss': np.float64(0.2772997424006462)}

Episode step 15600, time diff 1.1554539203643799, total time dif 1333.4409432411194)
step: 15600 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(1.7486868), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0717228926718235), 'actor_loss': np.float64(-0.9981637537479401), 'hyper_actor_loss': np.float64(0.0014580142451450229), 'behavior_loss': np.float64(0.29255310595035555)}

Episode step 15610, time diff 1.109771490097046, total time dif 1334.5963971614838)
step: 15610 @ episode report: {'average_total_reward': np.float32(10.910001), 'reward_variance': np.float32(1.7158629), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07042567580938339), 'actor_loss': np.float64(-1.001126879453659), 'hyper_actor_loss': np.float64(0.0013547934242524207), 'behavior_loss': np.float64(0.3092411383986473)}

Episode step 15620, time diff 1.088578224182129, total time dif 1335.7061686515808)
step: 15620 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(1.5138468), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06826020143926144), 'actor_loss': np.float64(-0.9817891001701355), 'hyper_actor_loss': np.float64(0.0012369367759674788), 'behavior_loss': np.float64(0.296124792098999)}

Episode step 15630, time diff 1.0903403759002686, total time dif 1336.794746875763)
step: 15630 @ episode report: {'average_total_reward': np.float32(11.495557), 'reward_variance': np.float32(5.295783), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06515937522053719), 'actor_loss': np.float64(-0.9988470196723938), 'hyper_actor_loss': np.float64(0.0011580077232792973), 'behavior_loss': np.float64(0.2844834208488464)}

Episode step 15640, time diff 1.1628522872924805, total time dif 1337.8850872516632)
step: 15640 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(2.5165985), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07014190107584), 'actor_loss': np.float64(-0.996903932094574), 'hyper_actor_loss': np.float64(0.0010119765473064035), 'behavior_loss': np.float64(0.2971750766038895)}

Episode step 15650, time diff 1.118821382522583, total time dif 1339.0479395389557)
step: 15650 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(3.4055603), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.074354712292552), 'actor_loss': np.float64(-0.9892128229141235), 'hyper_actor_loss': np.float64(0.0009215207130182535), 'behavior_loss': np.float64(0.28290700614452363)}

Episode step 15660, time diff 1.2888157367706299, total time dif 1340.1667609214783)
step: 15660 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(0.90898925), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06323351040482521), 'actor_loss': np.float64(-0.9942398011684418), 'hyper_actor_loss': np.float64(0.0008537154644727707), 'behavior_loss': np.float64(0.2911934837698936)}

Episode step 15670, time diff 1.044764757156372, total time dif 1341.455576658249)
step: 15670 @ episode report: {'average_total_reward': np.float32(10.683334), 'reward_variance': np.float32(1.5520067), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058857477456331256), 'actor_loss': np.float64(-0.9703676521778106), 'hyper_actor_loss': np.float64(0.0008154717332217842), 'behavior_loss': np.float64(0.2742352902889252)}

Episode step 15680, time diff 1.0685694217681885, total time dif 1342.5003414154053)
step: 15680 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(2.0812106), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.288889), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06803971230983734), 'actor_loss': np.float64(-0.986033707857132), 'hyper_actor_loss': np.float64(0.0008100235543679446), 'behavior_loss': np.float64(0.28901381492614747)}

Episode step 15690, time diff 0.9671409130096436, total time dif 1343.5689108371735)
step: 15690 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(5.154638), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07647592537105083), 'actor_loss': np.float64(-1.007578146457672), 'hyper_actor_loss': np.float64(0.0007816789497155696), 'behavior_loss': np.float64(0.29753231406211855)}

Episode step 15700, time diff 0.9307975769042969, total time dif 1344.536051750183)
step: 15700 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(1.872081), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06897099483758211), 'actor_loss': np.float64(-0.9965362846851349), 'hyper_actor_loss': np.float64(0.0007444903196301312), 'behavior_loss': np.float64(0.28963613957166673)}

Episode step 15710, time diff 1.0158624649047852, total time dif 1345.4668493270874)
step: 15710 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.3454139), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.899999), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058676678128540515), 'actor_loss': np.float64(-0.9817960798740387), 'hyper_actor_loss': np.float64(0.000737852236488834), 'behavior_loss': np.float64(0.26941708475351334)}

Episode step 15720, time diff 1.0148882865905762, total time dif 1346.4827117919922)
step: 15720 @ episode report: {'average_total_reward': np.float32(11.993334), 'reward_variance': np.float32(2.1462767), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06725334450602531), 'actor_loss': np.float64(-0.9857320308685302), 'hyper_actor_loss': np.float64(0.0006802774441894144), 'behavior_loss': np.float64(0.3000920295715332)}

Episode step 15730, time diff 0.969109058380127, total time dif 1347.4976000785828)
step: 15730 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(3.6796308), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06137018501758575), 'actor_loss': np.float64(-0.9771318733692169), 'hyper_actor_loss': np.float64(0.0006841647904366255), 'behavior_loss': np.float64(0.3118650481104851)}

Episode step 15740, time diff 0.9223361015319824, total time dif 1348.466709136963)
step: 15740 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(1.7126474), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06908567175269127), 'actor_loss': np.float64(-0.9705225646495819), 'hyper_actor_loss': np.float64(0.0006622916960623115), 'behavior_loss': np.float64(0.3086395710706711)}

Episode step 15750, time diff 0.9318714141845703, total time dif 1349.3890452384949)
step: 15750 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(3.7619274), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06856329403817654), 'actor_loss': np.float64(-0.9977139353752136), 'hyper_actor_loss': np.float64(0.000692658091429621), 'behavior_loss': np.float64(0.2906217098236084)}

Episode step 15760, time diff 1.148841381072998, total time dif 1350.3209166526794)
step: 15760 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(3.6191554), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06709755733609199), 'actor_loss': np.float64(-0.9953120410442352), 'hyper_actor_loss': np.float64(0.0006971500639338047), 'behavior_loss': np.float64(0.28286526501178744)}

Episode step 15770, time diff 1.1190640926361084, total time dif 1351.4697580337524)
step: 15770 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.847093), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0714301835745573), 'actor_loss': np.float64(-0.9951360762119293), 'hyper_actor_loss': np.float64(0.0007018906646408141), 'behavior_loss': np.float64(0.28223406821489333)}

Episode step 15780, time diff 1.2048604488372803, total time dif 1352.5888221263885)
step: 15780 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(5.1324306), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06620477177202702), 'actor_loss': np.float64(-1.0000448286533357), 'hyper_actor_loss': np.float64(0.0006993191374931485), 'behavior_loss': np.float64(0.3038403481245041)}

Episode step 15790, time diff 1.07309889793396, total time dif 1353.7936825752258)
step: 15790 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.1072955), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06718582585453987), 'actor_loss': np.float64(-0.9873433947563172), 'hyper_actor_loss': np.float64(0.0006814164924435317), 'behavior_loss': np.float64(0.28529195189476014)}

Episode step 15800, time diff 1.1027863025665283, total time dif 1354.8667814731598)
step: 15800 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(1.3581499), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06899514570832252), 'actor_loss': np.float64(-0.9862406432628632), 'hyper_actor_loss': np.float64(0.0007392892090138048), 'behavior_loss': np.float64(0.3029545992612839)}

Episode step 15810, time diff 0.9604167938232422, total time dif 1355.9695677757263)
step: 15810 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(1.9238781), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06285016611218452), 'actor_loss': np.float64(-1.001233834028244), 'hyper_actor_loss': np.float64(0.0007496886944863945), 'behavior_loss': np.float64(0.29101183116436)}

Episode step 15820, time diff 1.0920727252960205, total time dif 1356.9299845695496)
step: 15820 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(1.9779999), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06932081282138824), 'actor_loss': np.float64(-0.9952614188194275), 'hyper_actor_loss': np.float64(0.0008107334957458079), 'behavior_loss': np.float64(0.2812396794557571)}

Episode step 15830, time diff 1.2288358211517334, total time dif 1358.0220572948456)
step: 15830 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(2.3653836), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058281140960752965), 'actor_loss': np.float64(-0.9936237633228302), 'hyper_actor_loss': np.float64(0.0009099611139390618), 'behavior_loss': np.float64(0.2796495959162712)}

Episode step 15840, time diff 0.9933295249938965, total time dif 1359.2508931159973)
step: 15840 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.6471627), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07433336526155472), 'actor_loss': np.float64(-0.9888828039169312), 'hyper_actor_loss': np.float64(0.0010191780922468752), 'behavior_loss': np.float64(0.3170574188232422)}

Episode step 15850, time diff 0.9830048084259033, total time dif 1360.2442226409912)
step: 15850 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(2.7597587), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060204706341028216), 'actor_loss': np.float64(-1.0048687875270843), 'hyper_actor_loss': np.float64(0.0010385322326328605), 'behavior_loss': np.float64(0.3015452787280083)}

Episode step 15860, time diff 1.0154037475585938, total time dif 1361.2272274494171)
step: 15860 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(3.3534093), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0730638138949871), 'actor_loss': np.float64(-0.9991261959075928), 'hyper_actor_loss': np.float64(0.0010323909926228225), 'behavior_loss': np.float64(0.30034138560295104)}

Episode step 15870, time diff 0.9998185634613037, total time dif 1362.2426311969757)
step: 15870 @ episode report: {'average_total_reward': np.float32(10.461111), 'reward_variance': np.float32(2.6789703), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06927424333989621), 'actor_loss': np.float64(-0.9914552986621856), 'hyper_actor_loss': np.float64(0.0010841163515578956), 'behavior_loss': np.float64(0.3042937681078911)}

Episode step 15880, time diff 0.9812836647033691, total time dif 1363.242449760437)
step: 15880 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(1.1457775), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06661032978445292), 'actor_loss': np.float64(-1.0094488382339477), 'hyper_actor_loss': np.float64(0.0011032869922928512), 'behavior_loss': np.float64(0.29863532781600954)}

Episode step 15890, time diff 1.0551855564117432, total time dif 1364.2237334251404)
step: 15890 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.5517979), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07562128975987434), 'actor_loss': np.float64(-1.0057860136032104), 'hyper_actor_loss': np.float64(0.001207711372990161), 'behavior_loss': np.float64(0.3010447919368744)}

Episode step 15900, time diff 1.0104174613952637, total time dif 1365.2789189815521)
step: 15900 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(2.85244), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.071888442710042), 'actor_loss': np.float64(-1.0121578693389892), 'hyper_actor_loss': np.float64(0.001391048275399953), 'behavior_loss': np.float64(0.29550653845071795)}

Episode step 15910, time diff 0.9859111309051514, total time dif 1366.2893364429474)
step: 15910 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(2.7772155), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07219205796718597), 'actor_loss': np.float64(-1.0061203002929688), 'hyper_actor_loss': np.float64(0.001427569251973182), 'behavior_loss': np.float64(0.2883146062493324)}

Episode step 15920, time diff 0.9587869644165039, total time dif 1367.2752475738525)
step: 15920 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(0.6314521), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05949143134057522), 'actor_loss': np.float64(-1.0210946559906007), 'hyper_actor_loss': np.float64(0.0013872369192540646), 'behavior_loss': np.float64(0.2916090950369835)}

Episode step 15930, time diff 0.9294028282165527, total time dif 1368.234034538269)
step: 15930 @ episode report: {'average_total_reward': np.float32(7.6166673), 'reward_variance': np.float32(4.415289), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06167342942208052), 'actor_loss': np.float64(-0.9705447375774383), 'hyper_actor_loss': np.float64(0.0012962386012077332), 'behavior_loss': np.float64(0.3046129524707794)}

Episode step 15940, time diff 1.0429208278656006, total time dif 1369.1634373664856)
step: 15940 @ episode report: {'average_total_reward': np.float32(8.465556), 'reward_variance': np.float32(1.9152702), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0702974334359169), 'actor_loss': np.float64(-1.0022509276866913), 'hyper_actor_loss': np.float64(0.001145233027637005), 'behavior_loss': np.float64(0.26174641996622083)}

Episode step 15950, time diff 1.0176711082458496, total time dif 1370.2063581943512)
step: 15950 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(3.0261033), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0625192265957594), 'actor_loss': np.float64(-1.0192854881286622), 'hyper_actor_loss': np.float64(0.0010810497449710965), 'behavior_loss': np.float64(0.2703075408935547)}

Episode step 15960, time diff 0.9743509292602539, total time dif 1371.224029302597)
step: 15960 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(4.817186), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06901179235428571), 'actor_loss': np.float64(-0.9670788705348968), 'hyper_actor_loss': np.float64(0.0009994489315431564), 'behavior_loss': np.float64(0.3096273273229599)}

Episode step 15970, time diff 0.9695758819580078, total time dif 1372.1983802318573)
step: 15970 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(1.4205976), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08057377114892006), 'actor_loss': np.float64(-0.9924830675125123), 'hyper_actor_loss': np.float64(0.00090899677015841), 'behavior_loss': np.float64(0.3122636616230011)}

Episode step 15980, time diff 1.029210090637207, total time dif 1373.1679561138153)
step: 15980 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(6.549304), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08193121328949929), 'actor_loss': np.float64(-1.0190371513366698), 'hyper_actor_loss': np.float64(0.0008106884895823895), 'behavior_loss': np.float64(0.29357788115739825)}

Episode step 15990, time diff 1.2989261150360107, total time dif 1374.1971662044525)
step: 15990 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.1707215), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0728746559470892), 'actor_loss': np.float64(-1.008780586719513), 'hyper_actor_loss': np.float64(0.0007766935683321208), 'behavior_loss': np.float64(0.2768251791596413)}

Episode step 16000, time diff 1.0937511920928955, total time dif 1375.4960923194885)
step: 16000 @ episode report: {'average_total_reward': np.float32(9.624446), 'reward_variance': np.float32(1.4864392), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06222158186137676), 'actor_loss': np.float64(-1.0042311012744904), 'hyper_actor_loss': np.float64(0.0007371716317720711), 'behavior_loss': np.float64(0.28625175952911375)}

Episode step 16010, time diff 1.126781940460205, total time dif 1376.5898435115814)
step: 16010 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.7451805), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05823371056467295), 'actor_loss': np.float64(-0.9589991986751556), 'hyper_actor_loss': np.float64(0.0007736148661933839), 'behavior_loss': np.float64(0.29333108216524123)}

Episode step 16020, time diff 1.0933358669281006, total time dif 1377.7166254520416)
step: 16020 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(3.7338881), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06496923565864562), 'actor_loss': np.float64(-0.9913106262683868), 'hyper_actor_loss': np.float64(0.0007157291809562593), 'behavior_loss': np.float64(0.298266327381134)}

Episode step 16030, time diff 1.0675311088562012, total time dif 1378.8099613189697)
step: 16030 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(3.0292418), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07461948320269585), 'actor_loss': np.float64(-0.9963467180728912), 'hyper_actor_loss': np.float64(0.000684681348502636), 'behavior_loss': np.float64(0.2872808426618576)}

Episode step 16040, time diff 1.0936229228973389, total time dif 1379.877492427826)
step: 16040 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(3.3602772), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05480255484580994), 'actor_loss': np.float64(-0.987472540140152), 'hyper_actor_loss': np.float64(0.0006797518697567284), 'behavior_loss': np.float64(0.28519182205200194)}

Episode step 16050, time diff 1.05731201171875, total time dif 1380.9711153507233)
step: 16050 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(4.6546197), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05571477459743619), 'actor_loss': np.float64(-0.9670403718948364), 'hyper_actor_loss': np.float64(0.000650525966193527), 'behavior_loss': np.float64(0.2931045711040497)}

Episode step 16060, time diff 1.0567851066589355, total time dif 1382.028427362442)
step: 16060 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(2.8544493), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06600692085921764), 'actor_loss': np.float64(-0.9909704983234405), 'hyper_actor_loss': np.float64(0.0006226729601621628), 'behavior_loss': np.float64(0.2936126619577408)}

Episode step 16070, time diff 1.0051357746124268, total time dif 1383.085212469101)
step: 16070 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(3.255328), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06150020286440849), 'actor_loss': np.float64(-0.9894843399524689), 'hyper_actor_loss': np.float64(0.0005911245418246836), 'behavior_loss': np.float64(0.28331094831228254)}

Episode step 16080, time diff 0.9198741912841797, total time dif 1384.0903482437134)
step: 16080 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(2.0616066), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06024029608815908), 'actor_loss': np.float64(-0.9834827601909637), 'hyper_actor_loss': np.float64(0.0005838602199219167), 'behavior_loss': np.float64(0.2948440104722977)}

Episode step 16090, time diff 1.203073263168335, total time dif 1385.0102224349976)
step: 16090 @ episode report: {'average_total_reward': np.float32(11.344445), 'reward_variance': np.float32(5.0005198), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07312354724854231), 'actor_loss': np.float64(-0.9844284892082215), 'hyper_actor_loss': np.float64(0.0005720900429878384), 'behavior_loss': np.float64(0.2864178568124771)}

Episode step 16100, time diff 0.9320309162139893, total time dif 1386.213295698166)
step: 16100 @ episode report: {'average_total_reward': np.float32(11.134445), 'reward_variance': np.float32(3.4515672), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07939630001783371), 'actor_loss': np.float64(-1.0120572805404664), 'hyper_actor_loss': np.float64(0.0006096912722568959), 'behavior_loss': np.float64(0.29585510194301606)}

Episode step 16110, time diff 0.9909665584564209, total time dif 1387.1453266143799)
step: 16110 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(4.016792), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06128139160573483), 'actor_loss': np.float64(-0.9875130236148835), 'hyper_actor_loss': np.float64(0.0005996009684167802), 'behavior_loss': np.float64(0.289509579539299)}

Episode step 16120, time diff 1.0236783027648926, total time dif 1388.1362931728363)
step: 16120 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(4.481148), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07095539942383766), 'actor_loss': np.float64(-0.9754741311073303), 'hyper_actor_loss': np.float64(0.0005649590399116278), 'behavior_loss': np.float64(0.31297959089279176)}

Episode step 16130, time diff 0.9793148040771484, total time dif 1389.1599714756012)
step: 16130 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(2.4670386), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06080796271562576), 'actor_loss': np.float64(-0.9865094900131226), 'hyper_actor_loss': np.float64(0.0005693089391570538), 'behavior_loss': np.float64(0.2832453832030296)}

Episode step 16140, time diff 1.1671037673950195, total time dif 1390.1392862796783)
step: 16140 @ episode report: {'average_total_reward': np.float32(10.112224), 'reward_variance': np.float32(2.482183), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07263198271393775), 'actor_loss': np.float64(-1.0024288177490235), 'hyper_actor_loss': np.float64(0.0005511773400940001), 'behavior_loss': np.float64(0.3032752275466919)}

Episode step 16150, time diff 1.2114369869232178, total time dif 1391.3063900470734)
step: 16150 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(3.513823), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07349115945398807), 'actor_loss': np.float64(-0.9929903149604797), 'hyper_actor_loss': np.float64(0.0005465895199449732), 'behavior_loss': np.float64(0.3063651159405708)}

Episode step 16160, time diff 1.2760844230651855, total time dif 1392.5178270339966)
step: 16160 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(0.7826711), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06485748514533043), 'actor_loss': np.float64(-0.9906937420368195), 'hyper_actor_loss': np.float64(0.0005523580795852468), 'behavior_loss': np.float64(0.29084928780794145)}

Episode step 16170, time diff 0.9788491725921631, total time dif 1393.7939114570618)
step: 16170 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(2.8230324), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06157133802771568), 'actor_loss': np.float64(-0.9897833883762359), 'hyper_actor_loss': np.float64(0.0005720719636883587), 'behavior_loss': np.float64(0.29015497118234634)}

Episode step 16180, time diff 1.1340680122375488, total time dif 1394.772760629654)
step: 16180 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.5348098), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06109466385096311), 'actor_loss': np.float64(-0.9924684286117553), 'hyper_actor_loss': np.float64(0.0005780166189651937), 'behavior_loss': np.float64(0.3086851954460144)}

Episode step 16190, time diff 1.1021511554718018, total time dif 1395.9068286418915)
step: 16190 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(4.0980864), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06287168860435485), 'actor_loss': np.float64(-0.9831809341907501), 'hyper_actor_loss': np.float64(0.0006681446277070791), 'behavior_loss': np.float64(0.3091172128915787)}

Episode step 16200, time diff 1.1328659057617188, total time dif 1397.0089797973633)
step: 16200 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(4.86405), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07046786211431026), 'actor_loss': np.float64(-0.9952631056308746), 'hyper_actor_loss': np.float64(0.0006897651008330285), 'behavior_loss': np.float64(0.31843367516994475)}

Episode step 16210, time diff 1.1651334762573242, total time dif 1398.141845703125)
step: 16210 @ episode report: {'average_total_reward': np.float32(10.497779), 'reward_variance': np.float32(2.3536747), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07359464094042778), 'actor_loss': np.float64(-1.0096307516098022), 'hyper_actor_loss': np.float64(0.0006708220054861159), 'behavior_loss': np.float64(0.30767929553985596)}

Episode step 16220, time diff 1.1338629722595215, total time dif 1399.3069791793823)
step: 16220 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.0805695), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05994330421090126), 'actor_loss': np.float64(-0.9838455855846405), 'hyper_actor_loss': np.float64(0.0007076884969137609), 'behavior_loss': np.float64(0.2975472629070282)}

Episode step 16230, time diff 1.0061767101287842, total time dif 1400.4408421516418)
step: 16230 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(1.7268152), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0704507116228342), 'actor_loss': np.float64(-0.994385176897049), 'hyper_actor_loss': np.float64(0.000696282705757767), 'behavior_loss': np.float64(0.3079779237508774)}

Episode step 16240, time diff 0.9971551895141602, total time dif 1401.4470188617706)
step: 16240 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(4.385328), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.2888894), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06381827779114246), 'actor_loss': np.float64(-0.9941833674907684), 'hyper_actor_loss': np.float64(0.0006727216474246234), 'behavior_loss': np.float64(0.2936215832829475)}

Episode step 16250, time diff 0.9856045246124268, total time dif 1402.4441740512848)
step: 16250 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(1.8061234), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06473272629082202), 'actor_loss': np.float64(-0.9895361423492431), 'hyper_actor_loss': np.float64(0.0006837631284724921), 'behavior_loss': np.float64(0.3221325069665909)}

Episode step 16260, time diff 1.0253551006317139, total time dif 1403.4297785758972)
step: 16260 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(1.8274829), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05020657144486904), 'actor_loss': np.float64(-0.9813042521476746), 'hyper_actor_loss': np.float64(0.0006716782168950886), 'behavior_loss': np.float64(0.27287954539060594)}

Episode step 16270, time diff 1.0041494369506836, total time dif 1404.455133676529)
step: 16270 @ episode report: {'average_total_reward': np.float32(8.990001), 'reward_variance': np.float32(6.048851), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05787037946283817), 'actor_loss': np.float64(-0.9817286252975463), 'hyper_actor_loss': np.float64(0.0006410278612747789), 'behavior_loss': np.float64(0.2793645143508911)}

Episode step 16280, time diff 0.9973182678222656, total time dif 1405.4592831134796)
step: 16280 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(2.7627375), 'max_total_reward': np.float32(13.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05746410302817821), 'actor_loss': np.float64(-0.983021467924118), 'hyper_actor_loss': np.float64(0.000658367550931871), 'behavior_loss': np.float64(0.2903939962387085)}

Episode step 16290, time diff 0.9613494873046875, total time dif 1406.4566013813019)
step: 16290 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(1.301493), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06391550209373235), 'actor_loss': np.float64(-0.9988714694976807), 'hyper_actor_loss': np.float64(0.0005905222147703171), 'behavior_loss': np.float64(0.2888659745454788)}

Episode step 16300, time diff 0.9874403476715088, total time dif 1407.4179508686066)
step: 16300 @ episode report: {'average_total_reward': np.float32(9.561111), 'reward_variance': np.float32(1.8313148), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06416482925415039), 'actor_loss': np.float64(-0.9909176230430603), 'hyper_actor_loss': np.float64(0.0005584400874795392), 'behavior_loss': np.float64(0.29881936609745025)}

Episode step 16310, time diff 0.9656829833984375, total time dif 1408.405391216278)
step: 16310 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(2.2112417), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07061395347118378), 'actor_loss': np.float64(-0.9902025401592255), 'hyper_actor_loss': np.float64(0.0005604017816949636), 'behavior_loss': np.float64(0.3044376909732819)}

Episode step 16320, time diff 0.9539072513580322, total time dif 1409.3710741996765)
step: 16320 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(1.3837892), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06962676793336868), 'actor_loss': np.float64(-1.0043471217155457), 'hyper_actor_loss': np.float64(0.0005517181765753776), 'behavior_loss': np.float64(0.30050172209739684)}

Episode step 16330, time diff 1.1132285594940186, total time dif 1410.3249814510345)
step: 16330 @ episode report: {'average_total_reward': np.float32(10.224445), 'reward_variance': np.float32(3.5683658), 'max_total_reward': np.float32(14.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0626514881849289), 'actor_loss': np.float64(-0.9806053638458252), 'hyper_actor_loss': np.float64(0.0005558452336117625), 'behavior_loss': np.float64(0.31200561821460726)}

Episode step 16340, time diff 0.9046907424926758, total time dif 1411.4382100105286)
step: 16340 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(5.1414843), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06567585207521916), 'actor_loss': np.float64(-0.9753469705581665), 'hyper_actor_loss': np.float64(0.0005760886881034822), 'behavior_loss': np.float64(0.3126712888479233)}

Episode step 16350, time diff 0.8924567699432373, total time dif 1412.3429007530212)
step: 16350 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(3.1946871), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0673267513513565), 'actor_loss': np.float64(-1.0020315170288085), 'hyper_actor_loss': np.float64(0.0006545091106090695), 'behavior_loss': np.float64(0.312952584028244)}

Episode step 16360, time diff 0.9854605197906494, total time dif 1413.2353575229645)
step: 16360 @ episode report: {'average_total_reward': np.float32(11.183333), 'reward_variance': np.float32(4.5858583), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05889427810907364), 'actor_loss': np.float64(-0.9739671587944031), 'hyper_actor_loss': np.float64(0.0007440748217049986), 'behavior_loss': np.float64(0.3064733505249023)}

Episode step 16370, time diff 0.9721343517303467, total time dif 1414.2208180427551)
step: 16370 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(1.9931816), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06555118262767792), 'actor_loss': np.float64(-0.9724573910236358), 'hyper_actor_loss': np.float64(0.0009248532413039356), 'behavior_loss': np.float64(0.31104871034622195)}

Episode step 16380, time diff 1.0834956169128418, total time dif 1415.1929523944855)
step: 16380 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(1.0779264), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0625061621889472), 'actor_loss': np.float64(-1.0023289501667023), 'hyper_actor_loss': np.float64(0.0009353435132652521), 'behavior_loss': np.float64(0.30452642738819125)}

Episode step 16390, time diff 1.0887091159820557, total time dif 1416.2764480113983)
step: 16390 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(0.8484214), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06359075270593166), 'actor_loss': np.float64(-0.99274240732193), 'hyper_actor_loss': np.float64(0.0008986853586975485), 'behavior_loss': np.float64(0.2885732874274254)}

Episode step 16400, time diff 1.2028281688690186, total time dif 1417.3651571273804)
step: 16400 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(1.1528502), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06001218445599079), 'actor_loss': np.float64(-0.9969734668731689), 'hyper_actor_loss': np.float64(0.0009212761942762882), 'behavior_loss': np.float64(0.3163018226623535)}

Episode step 16410, time diff 1.0547621250152588, total time dif 1418.5679852962494)
step: 16410 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(4.7772546), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07349213734269142), 'actor_loss': np.float64(-0.9943570971488953), 'hyper_actor_loss': np.float64(0.0008610870223492384), 'behavior_loss': np.float64(0.3020552068948746)}

Episode step 16420, time diff 1.0918669700622559, total time dif 1419.6227474212646)
step: 16420 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(4.09445), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07007126100361347), 'actor_loss': np.float64(-1.010741114616394), 'hyper_actor_loss': np.float64(0.0008918534440454096), 'behavior_loss': np.float64(0.2992692679166794)}

Episode step 16430, time diff 1.1022682189941406, total time dif 1420.714614391327)
step: 16430 @ episode report: {'average_total_reward': np.float32(11.332224), 'reward_variance': np.float32(2.0665298), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06964705623686314), 'actor_loss': np.float64(-1.0020157873630524), 'hyper_actor_loss': np.float64(0.0008539305534213781), 'behavior_loss': np.float64(0.3119907945394516)}

Episode step 16440, time diff 1.1078271865844727, total time dif 1421.816882610321)
step: 16440 @ episode report: {'average_total_reward': np.float32(10.573334), 'reward_variance': np.float32(2.4389431), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06892068684101105), 'actor_loss': np.float64(-0.999214482307434), 'hyper_actor_loss': np.float64(0.0008559176814742387), 'behavior_loss': np.float64(0.2964122980833054)}

Episode step 16450, time diff 1.0817251205444336, total time dif 1422.9247097969055)
step: 16450 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(1.0029082), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08184018544852734), 'actor_loss': np.float64(-1.009993326663971), 'hyper_actor_loss': np.float64(0.0008397747704293578), 'behavior_loss': np.float64(0.31216626465320585)}

Episode step 16460, time diff 1.1058342456817627, total time dif 1424.00643491745)
step: 16460 @ episode report: {'average_total_reward': np.float32(10.922223), 'reward_variance': np.float32(2.414742), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0735424965620041), 'actor_loss': np.float64(-1.0208764553070069), 'hyper_actor_loss': np.float64(0.0007974898209795356), 'behavior_loss': np.float64(0.31784889101982117)}

Episode step 16470, time diff 1.1772878170013428, total time dif 1425.1122691631317)
step: 16470 @ episode report: {'average_total_reward': np.float32(10.085557), 'reward_variance': np.float32(3.9913354), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06010204181075096), 'actor_loss': np.float64(-0.9744724214076996), 'hyper_actor_loss': np.float64(0.00088298634509556), 'behavior_loss': np.float64(0.30638886988162994)}

Episode step 16480, time diff 1.112501621246338, total time dif 1426.289556980133)
step: 16480 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(2.7355578), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06344360392540693), 'actor_loss': np.float64(-0.9835946261882782), 'hyper_actor_loss': np.float64(0.001102257810998708), 'behavior_loss': np.float64(0.3295141965150833)}

Episode step 16490, time diff 1.2703824043273926, total time dif 1427.4020586013794)
step: 16490 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(1.4874125), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0671585563570261), 'actor_loss': np.float64(-1.0100451946258544), 'hyper_actor_loss': np.float64(0.001095949299633503), 'behavior_loss': np.float64(0.313372465968132)}

Episode step 16500, time diff 1.1349000930786133, total time dif 1428.6724410057068)
step: 16500 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.1911654), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07619741559028625), 'actor_loss': np.float64(-1.0172264218330382), 'hyper_actor_loss': np.float64(0.001047869271133095), 'behavior_loss': np.float64(0.32084697782993316)}

Episode step 16510, time diff 1.1166932582855225, total time dif 1429.8073410987854)
step: 16510 @ episode report: {'average_total_reward': np.float32(11.420001), 'reward_variance': np.float32(3.0879703), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07090226914733648), 'actor_loss': np.float64(-1.0006905972957612), 'hyper_actor_loss': np.float64(0.0010082272870931774), 'behavior_loss': np.float64(0.33463951349258425)}

Episode step 16520, time diff 1.163520097732544, total time dif 1430.924034357071)
step: 16520 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(2.8369992), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06103171482682228), 'actor_loss': np.float64(-0.9825350940227509), 'hyper_actor_loss': np.float64(0.0010328149888664483), 'behavior_loss': np.float64(0.33152776062488554)}

Episode step 16530, time diff 1.1114842891693115, total time dif 1432.0875544548035)
step: 16530 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.8416599), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07616690546274185), 'actor_loss': np.float64(-0.9986101567745209), 'hyper_actor_loss': np.float64(0.0009514609235338866), 'behavior_loss': np.float64(0.2986651808023453)}

Episode step 16540, time diff 1.1580009460449219, total time dif 1433.1990387439728)
step: 16540 @ episode report: {'average_total_reward': np.float32(10.895556), 'reward_variance': np.float32(1.2552893), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0757819164544344), 'actor_loss': np.float64(-1.0238112211227417), 'hyper_actor_loss': np.float64(0.0008687542635016143), 'behavior_loss': np.float64(0.31614032089710237)}

Episode step 16550, time diff 0.951758861541748, total time dif 1434.3570396900177)
step: 16550 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(2.5792053), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06991803459823132), 'actor_loss': np.float64(-0.9890860021114349), 'hyper_actor_loss': np.float64(0.0007512905052863061), 'behavior_loss': np.float64(0.33341154754161834)}

Episode step 16560, time diff 0.9678134918212891, total time dif 1435.3087985515594)
step: 16560 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(3.2679303), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07643305584788322), 'actor_loss': np.float64(-0.99231858253479), 'hyper_actor_loss': np.float64(0.0007354069792199879), 'behavior_loss': np.float64(0.3391678065061569)}

Episode step 16570, time diff 0.9622752666473389, total time dif 1436.2766120433807)
step: 16570 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(1.806124), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07192590311169625), 'actor_loss': np.float64(-1.0278712630271911), 'hyper_actor_loss': np.float64(0.000749738048762083), 'behavior_loss': np.float64(0.3055875837802887)}

Episode step 16580, time diff 0.9493746757507324, total time dif 1437.238887310028)
step: 16580 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(3.2296004), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08502839915454388), 'actor_loss': np.float64(-1.0184544205665589), 'hyper_actor_loss': np.float64(0.0007818044978193939), 'behavior_loss': np.float64(0.3229108974337578)}

Episode step 16590, time diff 1.020143747329712, total time dif 1438.1882619857788)
step: 16590 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(2.4823756), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06781307458877564), 'actor_loss': np.float64(-1.007010668516159), 'hyper_actor_loss': np.float64(0.000746613770024851), 'behavior_loss': np.float64(0.323083633184433)}

Episode step 16600, time diff 1.2834067344665527, total time dif 1439.2084057331085)
step: 16600 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(0.5441731), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0582082150503993), 'actor_loss': np.float64(-0.976395708322525), 'hyper_actor_loss': np.float64(0.0006242433737497777), 'behavior_loss': np.float64(0.3046431303024292)}

Episode step 16610, time diff 1.006394863128662, total time dif 1440.491812467575)
step: 16610 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(1.8964508), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05218166783452034), 'actor_loss': np.float64(-0.982251501083374), 'hyper_actor_loss': np.float64(0.0006202132906764745), 'behavior_loss': np.float64(0.2808865889906883)}

Episode step 16620, time diff 0.9250626564025879, total time dif 1441.4982073307037)
step: 16620 @ episode report: {'average_total_reward': np.float32(9.326668), 'reward_variance': np.float32(2.0745738), 'max_total_reward': np.float32(11.022224), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07370433323085308), 'actor_loss': np.float64(-1.0020214796066285), 'hyper_actor_loss': np.float64(0.0005776995269116015), 'behavior_loss': np.float64(0.31789183914661406)}

Episode step 16630, time diff 1.0785856246948242, total time dif 1442.4232699871063)
step: 16630 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(1.3138566), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06025891862809658), 'actor_loss': np.float64(-0.9920822381973267), 'hyper_actor_loss': np.float64(0.0005254354618955404), 'behavior_loss': np.float64(0.31207293570041655)}

Episode step 16640, time diff 1.137542486190796, total time dif 1443.5018556118011)
step: 16640 @ episode report: {'average_total_reward': np.float32(10.236667), 'reward_variance': np.float32(2.7951863), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057260519452393056), 'actor_loss': np.float64(-0.9809104263782501), 'hyper_actor_loss': np.float64(0.000540110853035003), 'behavior_loss': np.float64(0.32367916107177735)}

Episode step 16650, time diff 1.1231045722961426, total time dif 1444.639398097992)
step: 16650 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(2.3427103), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06124639920890331), 'actor_loss': np.float64(-0.9933577597141265), 'hyper_actor_loss': np.float64(0.0005726398318074644), 'behavior_loss': np.float64(0.3144484609365463)}

Episode step 16660, time diff 1.1847994327545166, total time dif 1445.762502670288)
step: 16660 @ episode report: {'average_total_reward': np.float32(9.014444), 'reward_variance': np.float32(1.1863962), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05952720642089844), 'actor_loss': np.float64(-0.9891900062561035), 'hyper_actor_loss': np.float64(0.0005586068262346088), 'behavior_loss': np.float64(0.3026831552386284)}

Episode step 16670, time diff 1.0313184261322021, total time dif 1446.9473021030426)
step: 16670 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(3.2105732), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07127950079739094), 'actor_loss': np.float64(-0.9828851461410523), 'hyper_actor_loss': np.float64(0.0005627437145449221), 'behavior_loss': np.float64(0.31173695623874664)}

Episode step 16680, time diff 0.9663338661193848, total time dif 1447.9786205291748)
step: 16680 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(2.1671567), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05199456587433815), 'actor_loss': np.float64(-0.997645914554596), 'hyper_actor_loss': np.float64(0.0005374175641918554), 'behavior_loss': np.float64(0.31652858257293703)}

Episode step 16690, time diff 0.9886107444763184, total time dif 1448.9449543952942)
step: 16690 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.3303409), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062224782258272174), 'actor_loss': np.float64(-0.9625054478645325), 'hyper_actor_loss': np.float64(0.0005317539442330598), 'behavior_loss': np.float64(0.3189027816057205)}

Episode step 16700, time diff 1.2283475399017334, total time dif 1449.9335651397705)
step: 16700 @ episode report: {'average_total_reward': np.float32(10.446667), 'reward_variance': np.float32(2.7904882), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05678685270249843), 'actor_loss': np.float64(-0.988175505399704), 'hyper_actor_loss': np.float64(0.0005053728353232145), 'behavior_loss': np.float64(0.32621570229530333)}

Episode step 16710, time diff 1.1549406051635742, total time dif 1451.1619126796722)
step: 16710 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.7172792), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07868170700967311), 'actor_loss': np.float64(-0.999107962846756), 'hyper_actor_loss': np.float64(0.0005224222870310769), 'behavior_loss': np.float64(0.3140281707048416)}

Episode step 16720, time diff 1.153254508972168, total time dif 1452.3168532848358)
step: 16720 @ episode report: {'average_total_reward': np.float32(10.8955555), 'reward_variance': np.float32(3.8014865), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06362984143197536), 'actor_loss': np.float64(-1.0125054001808167), 'hyper_actor_loss': np.float64(0.0005776478094048799), 'behavior_loss': np.float64(0.2955386683344841)}

Episode step 16730, time diff 1.0798029899597168, total time dif 1453.470107793808)
step: 16730 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(2.0690923), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0532441433519125), 'actor_loss': np.float64(-0.9893519163131714), 'hyper_actor_loss': np.float64(0.0005954330204986036), 'behavior_loss': np.float64(0.3029758036136627)}

Episode step 16740, time diff 1.0607104301452637, total time dif 1454.5499107837677)
step: 16740 @ episode report: {'average_total_reward': np.float32(11.620001), 'reward_variance': np.float32(3.4395013), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07519718855619431), 'actor_loss': np.float64(-0.9897281348705291), 'hyper_actor_loss': np.float64(0.0006169594533275812), 'behavior_loss': np.float64(0.3168856456875801)}

Episode step 16750, time diff 1.0834331512451172, total time dif 1455.610621213913)
step: 16750 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(0.69798625), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07348574995994568), 'actor_loss': np.float64(-1.0181580424308776), 'hyper_actor_loss': np.float64(0.0006455655209720134), 'behavior_loss': np.float64(0.3055761486291885)}

Episode step 16760, time diff 1.331256628036499, total time dif 1456.694054365158)
step: 16760 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(3.9667954), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06474429108202458), 'actor_loss': np.float64(-0.99741051197052), 'hyper_actor_loss': np.float64(0.0006224858516361564), 'behavior_loss': np.float64(0.3016015365719795)}

Episode step 16770, time diff 1.1801230907440186, total time dif 1458.0253109931946)
step: 16770 @ episode report: {'average_total_reward': np.float32(10.922223), 'reward_variance': np.float32(3.3673832), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06143484972417355), 'actor_loss': np.float64(-0.9822826504707336), 'hyper_actor_loss': np.float64(0.0005683673487510532), 'behavior_loss': np.float64(0.2995430439710617)}

Episode step 16780, time diff 1.082951545715332, total time dif 1459.2054340839386)
step: 16780 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(6.044054), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07120933122932911), 'actor_loss': np.float64(-0.999730360507965), 'hyper_actor_loss': np.float64(0.0005375449603889137), 'behavior_loss': np.float64(0.29513280987739565)}

Episode step 16790, time diff 1.177743911743164, total time dif 1460.288385629654)
step: 16790 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(2.3628662), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06189535576850176), 'actor_loss': np.float64(-1.0026555895805358), 'hyper_actor_loss': np.float64(0.0005453993042465299), 'behavior_loss': np.float64(0.31314178109169005)}

Episode step 16800, time diff 1.1997997760772705, total time dif 1461.466129541397)
step: 16800 @ episode report: {'average_total_reward': np.float32(10.910001), 'reward_variance': np.float32(4.604208), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05239605735987425), 'actor_loss': np.float64(-0.9793453097343445), 'hyper_actor_loss': np.float64(0.0005929586593993008), 'behavior_loss': np.float64(0.3231364369392395)}

Episode step 16810, time diff 1.1286556720733643, total time dif 1462.6659293174744)
step: 16810 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.322266), 'max_total_reward': np.float32(12.1444435), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0634840339422226), 'actor_loss': np.float64(-0.976260632276535), 'hyper_actor_loss': np.float64(0.0006288229080382735), 'behavior_loss': np.float64(0.33815303444862366)}

Episode step 16820, time diff 1.1024463176727295, total time dif 1463.7945849895477)
step: 16820 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(1.3693197), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06827814504504204), 'actor_loss': np.float64(-0.9930597126483918), 'hyper_actor_loss': np.float64(0.0006106301036197693), 'behavior_loss': np.float64(0.3017314046621323)}

Episode step 16830, time diff 1.3119466304779053, total time dif 1464.8970313072205)
step: 16830 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(4.8296347), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0849584374576807), 'actor_loss': np.float64(-1.0339226722717285), 'hyper_actor_loss': np.float64(0.0005770851799752563), 'behavior_loss': np.float64(0.30141419768333433)}

Episode step 16840, time diff 1.1527998447418213, total time dif 1466.2089779376984)
step: 16840 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.6980846), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0728521965444088), 'actor_loss': np.float64(-1.0089202344417572), 'hyper_actor_loss': np.float64(0.000531598343513906), 'behavior_loss': np.float64(0.30993800759315493)}

Episode step 16850, time diff 1.5351057052612305, total time dif 1467.3617777824402)
step: 16850 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.7896054), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06061271652579307), 'actor_loss': np.float64(-0.9774940490722657), 'hyper_actor_loss': np.float64(0.0005087908386485652), 'behavior_loss': np.float64(0.31276724189519883)}

Episode step 16860, time diff 1.2199726104736328, total time dif 1468.8968834877014)
step: 16860 @ episode report: {'average_total_reward': np.float32(10.907779), 'reward_variance': np.float32(4.424397), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059896521642804144), 'actor_loss': np.float64(-0.9792925119400024), 'hyper_actor_loss': np.float64(0.0005221595783950761), 'behavior_loss': np.float64(0.3051377236843109)}

Episode step 16870, time diff 1.2726266384124756, total time dif 1470.116856098175)
step: 16870 @ episode report: {'average_total_reward': np.float32(9.224444), 'reward_variance': np.float32(3.1898222), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06034586764872074), 'actor_loss': np.float64(-0.9925999701023102), 'hyper_actor_loss': np.float64(0.0005181316781090572), 'behavior_loss': np.float64(0.3001908972859383)}

Episode step 16880, time diff 1.1285004615783691, total time dif 1471.3894827365875)
step: 16880 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.0307274), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07112878300249577), 'actor_loss': np.float64(-0.9852413654327392), 'hyper_actor_loss': np.float64(0.0005699876986909657), 'behavior_loss': np.float64(0.3138593673706055)}

Episode step 16890, time diff 1.2791874408721924, total time dif 1472.517983198166)
step: 16890 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(3.0614438), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0645497364923358), 'actor_loss': np.float64(-0.9960020899772644), 'hyper_actor_loss': np.float64(0.0005168319039512426), 'behavior_loss': np.float64(0.30594093948602674)}

Episode step 16900, time diff 1.1273193359375, total time dif 1473.797170639038)
step: 16900 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(6.1054773), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07828969359397889), 'actor_loss': np.float64(-1.0101187229156494), 'hyper_actor_loss': np.float64(0.0004941826307913288), 'behavior_loss': np.float64(0.30536976754665374)}

Episode step 16910, time diff 1.0951430797576904, total time dif 1474.9244899749756)
step: 16910 @ episode report: {'average_total_reward': np.float32(11.693334), 'reward_variance': np.float32(3.379858), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0720208466053009), 'actor_loss': np.float64(-1.007165825366974), 'hyper_actor_loss': np.float64(0.0004819042980670929), 'behavior_loss': np.float64(0.3215352386236191)}

Episode step 16920, time diff 1.0133564472198486, total time dif 1476.0196330547333)
step: 16920 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(2.03445), 'max_total_reward': np.float32(13.1444435), 'min_total_reward': np.float32(8.411112), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07310421951115131), 'actor_loss': np.float64(-1.0022370994091034), 'hyper_actor_loss': np.float64(0.0004664332896936685), 'behavior_loss': np.float64(0.32951439917087555)}

Episode step 16930, time diff 1.0421147346496582, total time dif 1477.0329895019531)
step: 16930 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(2.7597327), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.411112), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07734176069498062), 'actor_loss': np.float64(-0.9974411725997925), 'hyper_actor_loss': np.float64(0.0004925002809613943), 'behavior_loss': np.float64(0.3327164053916931)}

Episode step 16940, time diff 1.1638116836547852, total time dif 1478.0751042366028)
step: 16940 @ episode report: {'average_total_reward': np.float32(11.507778), 'reward_variance': np.float32(3.804718), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07398686334490776), 'actor_loss': np.float64(-1.018785333633423), 'hyper_actor_loss': np.float64(0.0005374092754209415), 'behavior_loss': np.float64(0.3026792347431183)}

Episode step 16950, time diff 1.1686809062957764, total time dif 1479.2389159202576)
step: 16950 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(1.6160015), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06675329804420471), 'actor_loss': np.float64(-1.0100422620773315), 'hyper_actor_loss': np.float64(0.0006408231129171327), 'behavior_loss': np.float64(0.32360618412494657)}

Episode step 16960, time diff 1.249713659286499, total time dif 1480.4075968265533)
step: 16960 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(1.0873286), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07717958502471448), 'actor_loss': np.float64(-1.003095132112503), 'hyper_actor_loss': np.float64(0.0008715870906598866), 'behavior_loss': np.float64(0.3295285224914551)}

Episode step 16970, time diff 1.2791132926940918, total time dif 1481.6573104858398)
step: 16970 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(2.3382244), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061747950688004496), 'actor_loss': np.float64(-1.002487987279892), 'hyper_actor_loss': np.float64(0.0008150761015713215), 'behavior_loss': np.float64(0.32189137041568755)}

Episode step 16980, time diff 1.0677132606506348, total time dif 1482.936423778534)
step: 16980 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(1.3823072), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0685841303318739), 'actor_loss': np.float64(-0.9875165402889252), 'hyper_actor_loss': np.float64(0.0006403528444934636), 'behavior_loss': np.float64(0.3088193416595459)}

Episode step 16990, time diff 1.1461405754089355, total time dif 1484.0041370391846)
step: 16990 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(1.5323259), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06811174601316453), 'actor_loss': np.float64(-1.0054361164569854), 'hyper_actor_loss': np.float64(0.0005075679044239223), 'behavior_loss': np.float64(0.34449527561664584)}

Episode step 17000, time diff 1.3710379600524902, total time dif 1485.1502776145935)
step: 17000 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(0.7009739), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06401066333055497), 'actor_loss': np.float64(-0.9819105088710784), 'hyper_actor_loss': np.float64(0.0004648445319617167), 'behavior_loss': np.float64(0.3070010423660278)}

Episode step 17010, time diff 1.2950994968414307, total time dif 1486.521315574646)
step: 17010 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.4521985), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06375802718102933), 'actor_loss': np.float64(-0.9846574425697326), 'hyper_actor_loss': np.float64(0.00042617547733243556), 'behavior_loss': np.float64(0.320329812169075)}

Episode step 17020, time diff 1.1085646152496338, total time dif 1487.8164150714874)
step: 17020 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(1.859532), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0654809033498168), 'actor_loss': np.float64(-0.9961077928543091), 'hyper_actor_loss': np.float64(0.00043719021196011456), 'behavior_loss': np.float64(0.2867762565612793)}

Episode step 17030, time diff 1.2307710647583008, total time dif 1488.924979686737)
step: 17030 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(4.33566), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07208742834627628), 'actor_loss': np.float64(-1.0085053622722626), 'hyper_actor_loss': np.float64(0.0004411152214743197), 'behavior_loss': np.float64(0.31586443930864333)}

Episode step 17040, time diff 1.2844820022583008, total time dif 1490.1557507514954)
step: 17040 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.1353145), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060844316706061365), 'actor_loss': np.float64(-0.9783164143562317), 'hyper_actor_loss': np.float64(0.00042318458145018667), 'behavior_loss': np.float64(0.31338544487953185)}

Episode step 17050, time diff 1.2966017723083496, total time dif 1491.4402327537537)
step: 17050 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(4.278993), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058248305693268776), 'actor_loss': np.float64(-0.9742054998874664), 'hyper_actor_loss': np.float64(0.0004185503930784762), 'behavior_loss': np.float64(0.3060323089361191)}

Episode step 17060, time diff 1.1915364265441895, total time dif 1492.736834526062)
step: 17060 @ episode report: {'average_total_reward': np.float32(8.526667), 'reward_variance': np.float32(1.0259806), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06824534609913827), 'actor_loss': np.float64(-1.000676542520523), 'hyper_actor_loss': np.float64(0.00044739697768818586), 'behavior_loss': np.float64(0.3044593960046768)}

Episode step 17070, time diff 1.093559741973877, total time dif 1493.9283709526062)
step: 17070 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(4.168175), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0671298585832119), 'actor_loss': np.float64(-1.0001872181892395), 'hyper_actor_loss': np.float64(0.0004501019255258143), 'behavior_loss': np.float64(0.33185779452323916)}

Episode step 17080, time diff 1.0560715198516846, total time dif 1495.02193069458)
step: 17080 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(4.0870986), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05451681017875672), 'actor_loss': np.float64(-0.9812855422496796), 'hyper_actor_loss': np.float64(0.0004836634878301993), 'behavior_loss': np.float64(0.288222374022007)}

Episode step 17090, time diff 0.9357712268829346, total time dif 1496.0780022144318)
step: 17090 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(2.8220105), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062114284560084346), 'actor_loss': np.float64(-1.0033673405647279), 'hyper_actor_loss': np.float64(0.0005012293957406655), 'behavior_loss': np.float64(0.2965726599097252)}

Episode step 17100, time diff 0.9870302677154541, total time dif 1497.0137734413147)
step: 17100 @ episode report: {'average_total_reward': np.float32(10.000001), 'reward_variance': np.float32(2.4237046), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07101628072559833), 'actor_loss': np.float64(-1.0105066299438477), 'hyper_actor_loss': np.float64(0.0004978022770956159), 'behavior_loss': np.float64(0.3124637931585312)}

Episode step 17110, time diff 1.2168104648590088, total time dif 1498.0008037090302)
step: 17110 @ episode report: {'average_total_reward': np.float32(9.763334), 'reward_variance': np.float32(3.5906193), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05399924237281084), 'actor_loss': np.float64(-0.9834002435207367), 'hyper_actor_loss': np.float64(0.0005178095219889655), 'behavior_loss': np.float64(0.32622556686401366)}

Episode step 17120, time diff 1.1594483852386475, total time dif 1499.2176141738892)
step: 17120 @ episode report: {'average_total_reward': np.float32(8.351111), 'reward_variance': np.float32(3.9464488), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06734175998717547), 'actor_loss': np.float64(-0.9851601481437683), 'hyper_actor_loss': np.float64(0.0005359903996577486), 'behavior_loss': np.float64(0.3202491909265518)}

Episode step 17130, time diff 1.3033668994903564, total time dif 1500.3770625591278)
step: 17130 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(5.162852), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05737114436924458), 'actor_loss': np.float64(-0.991526198387146), 'hyper_actor_loss': np.float64(0.00048718189646024257), 'behavior_loss': np.float64(0.3377796083688736)}

Episode step 17140, time diff 1.009751319885254, total time dif 1501.6804294586182)
step: 17140 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(4.288988), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0680095098912716), 'actor_loss': np.float64(-0.984628963470459), 'hyper_actor_loss': np.float64(0.00047609543544240295), 'behavior_loss': np.float64(0.2940377056598663)}

Episode step 17150, time diff 1.072122573852539, total time dif 1502.6901807785034)
step: 17150 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(3.3658047), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05909147188067436), 'actor_loss': np.float64(-0.9952717959880829), 'hyper_actor_loss': np.float64(0.0004879307438386604), 'behavior_loss': np.float64(0.328671059012413)}

Episode step 17160, time diff 1.1680912971496582, total time dif 1503.762303352356)
step: 17160 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(2.77576), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07286996021866798), 'actor_loss': np.float64(-0.9794861853122712), 'hyper_actor_loss': np.float64(0.000475181607180275), 'behavior_loss': np.float64(0.32844712138175963)}

Episode step 17170, time diff 1.1256887912750244, total time dif 1504.9303946495056)
step: 17170 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(5.3702183), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.049751606211066245), 'actor_loss': np.float64(-0.9971709012985229), 'hyper_actor_loss': np.float64(0.0004959161393344403), 'behavior_loss': np.float64(0.30025395154953005)}

Episode step 17180, time diff 0.9765548706054688, total time dif 1506.0560834407806)
step: 17180 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(3.874124), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061739420890808104), 'actor_loss': np.float64(-0.9753455936908721), 'hyper_actor_loss': np.float64(0.0004768892627907917), 'behavior_loss': np.float64(0.3156112313270569)}

Episode step 17190, time diff 0.9107024669647217, total time dif 1507.032638311386)
step: 17190 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(1.1383013), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06519094407558441), 'actor_loss': np.float64(-0.9955007433891296), 'hyper_actor_loss': np.float64(0.0004447798972250894), 'behavior_loss': np.float64(0.33390330970287324)}

Episode step 17200, time diff 1.1847150325775146, total time dif 1507.9433407783508)
step: 17200 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(3.7267864), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06019640155136585), 'actor_loss': np.float64(-0.9934626996517182), 'hyper_actor_loss': np.float64(0.00044851425336673857), 'behavior_loss': np.float64(0.28320560455322263)}

Episode step 17210, time diff 0.9394207000732422, total time dif 1509.1280558109283)
step: 17210 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(3.1701741), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06816219501197338), 'actor_loss': np.float64(-1.0134076833724976), 'hyper_actor_loss': np.float64(0.00042056088568642735), 'behavior_loss': np.float64(0.31595455706119535)}

Episode step 17220, time diff 1.0132873058319092, total time dif 1510.0674765110016)
step: 17220 @ episode report: {'average_total_reward': np.float32(7.753333), 'reward_variance': np.float32(1.4892051), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07946589328348637), 'actor_loss': np.float64(-0.9997286677360535), 'hyper_actor_loss': np.float64(0.0004842258611461148), 'behavior_loss': np.float64(0.323479625582695)}

Episode step 17230, time diff 1.1219260692596436, total time dif 1511.0807638168335)
step: 17230 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(2.8660264), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808750741183758), 'actor_loss': np.float64(-1.0077030777931213), 'hyper_actor_loss': np.float64(0.0004884526308160276), 'behavior_loss': np.float64(0.3162654846906662)}

Episode step 17240, time diff 1.2600867748260498, total time dif 1512.2026898860931)
step: 17240 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(1.6834685), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07090603448450565), 'actor_loss': np.float64(-1.0019449293613434), 'hyper_actor_loss': np.float64(0.0004628135822713375), 'behavior_loss': np.float64(0.31184772551059725)}

Episode step 17250, time diff 1.0393590927124023, total time dif 1513.4627766609192)
step: 17250 @ episode report: {'average_total_reward': np.float32(9.126668), 'reward_variance': np.float32(2.539412), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060550708509981635), 'actor_loss': np.float64(-0.9931423664093018), 'hyper_actor_loss': np.float64(0.0005171289056306704), 'behavior_loss': np.float64(0.31418738067150115)}

Episode step 17260, time diff 1.0529861450195312, total time dif 1514.5021357536316)
step: 17260 @ episode report: {'average_total_reward': np.float32(9.163333), 'reward_variance': np.float32(3.6801002), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061820102669298646), 'actor_loss': np.float64(-0.9839857637882232), 'hyper_actor_loss': np.float64(0.0006168777938000858), 'behavior_loss': np.float64(0.3285135507583618)}

Episode step 17270, time diff 1.0748560428619385, total time dif 1515.5551218986511)
step: 17270 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(2.3192792), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06585485506802798), 'actor_loss': np.float64(-0.9888824582099914), 'hyper_actor_loss': np.float64(0.0006317520223092288), 'behavior_loss': np.float64(0.3337398707866669)}

Episode step 17280, time diff 0.912055253982544, total time dif 1516.629977941513)
step: 17280 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(1.5797151), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058847392350435256), 'actor_loss': np.float64(-0.9901369094848633), 'hyper_actor_loss': np.float64(0.0006349659932311624), 'behavior_loss': np.float64(0.3078128948807716)}

Episode step 17290, time diff 0.8857026100158691, total time dif 1517.5420331954956)
step: 17290 @ episode report: {'average_total_reward': np.float32(8.538889), 'reward_variance': np.float32(2.8868709), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0700557291507721), 'actor_loss': np.float64(-0.9939131438732147), 'hyper_actor_loss': np.float64(0.000602307339431718), 'behavior_loss': np.float64(0.3437220424413681)}

Episode step 17300, time diff 0.9307501316070557, total time dif 1518.4277358055115)
step: 17300 @ episode report: {'average_total_reward': np.float32(10.036668), 'reward_variance': np.float32(4.0639524), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05923719070851803), 'actor_loss': np.float64(-0.9803123235702514), 'hyper_actor_loss': np.float64(0.0006181126518640667), 'behavior_loss': np.float64(0.3074263155460358)}

Episode step 17310, time diff 0.8544328212738037, total time dif 1519.3584859371185)
step: 17310 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(2.5241427), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06082945838570595), 'actor_loss': np.float64(-0.9820515096187592), 'hyper_actor_loss': np.float64(0.0005568842258071527), 'behavior_loss': np.float64(0.3204765111207962)}

Episode step 17320, time diff 0.8613901138305664, total time dif 1520.2129187583923)
step: 17320 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(1.8206131), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07322440035641194), 'actor_loss': np.float64(-0.9955617010593414), 'hyper_actor_loss': np.float64(0.0005280115990899503), 'behavior_loss': np.float64(0.3232940942049026)}

Episode step 17330, time diff 1.0997436046600342, total time dif 1521.074308872223)
step: 17330 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.1132836), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06744777373969554), 'actor_loss': np.float64(-0.9957206428050995), 'hyper_actor_loss': np.float64(0.00043804155429825185), 'behavior_loss': np.float64(0.29337343126535415)}

Episode step 17340, time diff 0.902477502822876, total time dif 1522.174052476883)
step: 17340 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(1.6115263), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06303245685994625), 'actor_loss': np.float64(-0.9884377121925354), 'hyper_actor_loss': np.float64(0.0004071664297953248), 'behavior_loss': np.float64(0.3227952986955643)}

Episode step 17350, time diff 0.8899457454681396, total time dif 1523.0765299797058)
step: 17350 @ episode report: {'average_total_reward': np.float32(10.061112), 'reward_variance': np.float32(2.1144512), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06315077152103185), 'actor_loss': np.float64(-0.9912398517131805), 'hyper_actor_loss': np.float64(0.0004004627640824765), 'behavior_loss': np.float64(0.3059534773230553)}

Episode step 17360, time diff 0.86311936378479, total time dif 1523.966475725174)
step: 17360 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.86398), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06279221586883069), 'actor_loss': np.float64(-0.998036128282547), 'hyper_actor_loss': np.float64(0.0004079747595824301), 'behavior_loss': np.float64(0.31627882122993467)}

Episode step 17370, time diff 0.8049399852752686, total time dif 1524.8295950889587)
step: 17370 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(1.8908345), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06846879571676254), 'actor_loss': np.float64(-0.983297860622406), 'hyper_actor_loss': np.float64(0.0004074360796948895), 'behavior_loss': np.float64(0.3178157925605774)}

Episode step 17380, time diff 0.8014018535614014, total time dif 1525.634535074234)
step: 17380 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(3.0817533), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06110063306987286), 'actor_loss': np.float64(-1.0027795076370238), 'hyper_actor_loss': np.float64(0.0004209150356473401), 'behavior_loss': np.float64(0.2983185455203056)}

Episode step 17390, time diff 0.9136238098144531, total time dif 1526.4359369277954)
step: 17390 @ episode report: {'average_total_reward': np.float32(9.663334), 'reward_variance': np.float32(2.1968904), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06345526035875082), 'actor_loss': np.float64(-0.989215487241745), 'hyper_actor_loss': np.float64(0.0004130735353101045), 'behavior_loss': np.float64(0.3096471935510635)}

Episode step 17400, time diff 0.7982223033905029, total time dif 1527.3495607376099)
step: 17400 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(4.3782415), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06280819550156594), 'actor_loss': np.float64(-0.9866517782211304), 'hyper_actor_loss': np.float64(0.0004272492980817333), 'behavior_loss': np.float64(0.32724183797836304)}

Episode step 17410, time diff 0.8011813163757324, total time dif 1528.1477830410004)
step: 17410 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(2.0174022), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0734636690467596), 'actor_loss': np.float64(-0.999562269449234), 'hyper_actor_loss': np.float64(0.0003991498553659767), 'behavior_loss': np.float64(0.3093206316232681)}

Episode step 17420, time diff 0.8162927627563477, total time dif 1528.948964357376)
step: 17420 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(1.7778137), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07162113822996616), 'actor_loss': np.float64(-1.008452045917511), 'hyper_actor_loss': np.float64(0.0003871088847517967), 'behavior_loss': np.float64(0.3174093395471573)}

Episode step 17430, time diff 0.8596186637878418, total time dif 1529.7652571201324)
step: 17430 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.272489), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06360359266400337), 'actor_loss': np.float64(-0.9853722929954529), 'hyper_actor_loss': np.float64(0.0003795988275669515), 'behavior_loss': np.float64(0.3324554979801178)}

Episode step 17440, time diff 0.8921360969543457, total time dif 1530.6248757839203)
step: 17440 @ episode report: {'average_total_reward': np.float32(9.912224), 'reward_variance': np.float32(1.9644814), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062034950405359265), 'actor_loss': np.float64(-0.9890778720378876), 'hyper_actor_loss': np.float64(0.0003897066955687478), 'behavior_loss': np.float64(0.31414694488048556)}

Episode step 17450, time diff 0.8364527225494385, total time dif 1531.5170118808746)
step: 17450 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(3.4949632), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07067871801555156), 'actor_loss': np.float64(-0.9882049441337586), 'hyper_actor_loss': np.float64(0.000363681698217988), 'behavior_loss': np.float64(0.3575061082839966)}

Episode step 17460, time diff 0.8350892066955566, total time dif 1532.353464603424)
step: 17460 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(3.764272), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06302931234240532), 'actor_loss': np.float64(-0.9957827806472779), 'hyper_actor_loss': np.float64(0.0003209288406651467), 'behavior_loss': np.float64(0.2984733283519745)}

Episode step 17470, time diff 0.7631289958953857, total time dif 1533.1885538101196)
step: 17470 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(3.0190425), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06768295541405678), 'actor_loss': np.float64(-0.9781368136405945), 'hyper_actor_loss': np.float64(0.00036913352960254997), 'behavior_loss': np.float64(0.3249421030282974)}

Episode step 17480, time diff 0.840040922164917, total time dif 1533.951682806015)
step: 17480 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(1.4729431), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07444198876619339), 'actor_loss': np.float64(-0.9890044748783111), 'hyper_actor_loss': np.float64(0.0003812004841165617), 'behavior_loss': np.float64(0.3210555285215378)}

Episode step 17490, time diff 0.8239407539367676, total time dif 1534.79172372818)
step: 17490 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(3.3877082), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06723816804587841), 'actor_loss': np.float64(-0.9972437262535095), 'hyper_actor_loss': np.float64(0.00037516460579354314), 'behavior_loss': np.float64(0.3204171508550644)}

Episode step 17500, time diff 1.0143897533416748, total time dif 1535.6156644821167)
step: 17500 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(2.3536844), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07153712175786495), 'actor_loss': np.float64(-0.9958720862865448), 'hyper_actor_loss': np.float64(0.00036534579703584313), 'behavior_loss': np.float64(0.29379733353853227)}

Episode step 17510, time diff 0.8349404335021973, total time dif 1536.6300542354584)
step: 17510 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(1.0963751), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061711077392101285), 'actor_loss': np.float64(-0.987372201681137), 'hyper_actor_loss': np.float64(0.0003341458534123376), 'behavior_loss': np.float64(0.3214379966259003)}

Episode step 17520, time diff 0.8259897232055664, total time dif 1537.4649946689606)
step: 17520 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(3.7314763), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0577697578817606), 'actor_loss': np.float64(-0.9752756178379058), 'hyper_actor_loss': np.float64(0.00033960142463911324), 'behavior_loss': np.float64(0.30484133660793306)}

Episode step 17530, time diff 0.8134281635284424, total time dif 1538.2909843921661)
step: 17530 @ episode report: {'average_total_reward': np.float32(8.451111), 'reward_variance': np.float32(4.852968), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0715192511677742), 'actor_loss': np.float64(-0.9979020595550537), 'hyper_actor_loss': np.float64(0.0003506330220261589), 'behavior_loss': np.float64(0.30403422713279726)}

Episode step 17540, time diff 0.8206734657287598, total time dif 1539.1044125556946)
step: 17540 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(1.7104756), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06656268574297428), 'actor_loss': np.float64(-1.0058690905570984), 'hyper_actor_loss': np.float64(0.0003512881638016552), 'behavior_loss': np.float64(0.3069668233394623)}

Episode step 17550, time diff 0.833014965057373, total time dif 1539.9250860214233)
step: 17550 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(5.1494923), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06029981002211571), 'actor_loss': np.float64(-0.9832100749015809), 'hyper_actor_loss': np.float64(0.00034597217454575003), 'behavior_loss': np.float64(0.289944651722908)}

Episode step 17560, time diff 0.843040943145752, total time dif 1540.7581009864807)
step: 17560 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(1.4210664), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677349504083395), 'actor_loss': np.float64(-0.9824199914932251), 'hyper_actor_loss': np.float64(0.00032961276883725077), 'behavior_loss': np.float64(0.31340465843677523)}

Episode step 17570, time diff 0.8180246353149414, total time dif 1541.6011419296265)
step: 17570 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(1.1633346), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06474868170917034), 'actor_loss': np.float64(-0.9964730739593506), 'hyper_actor_loss': np.float64(0.0003357202600454912), 'behavior_loss': np.float64(0.2821656197309494)}

Episode step 17580, time diff 0.8042376041412354, total time dif 1542.4191665649414)
step: 17580 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(1.9898142), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05569109320640564), 'actor_loss': np.float64(-0.9928340792655945), 'hyper_actor_loss': np.float64(0.0003318183677038178), 'behavior_loss': np.float64(0.2933136343955994)}

Episode step 17590, time diff 0.8353016376495361, total time dif 1543.2234041690826)
step: 17590 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(1.1334329), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05989309921860695), 'actor_loss': np.float64(-0.9595571100711823), 'hyper_actor_loss': np.float64(0.0003411395417060703), 'behavior_loss': np.float64(0.32574295699596406)}

Episode step 17600, time diff 0.8021900653839111, total time dif 1544.0587058067322)
step: 17600 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(1.8052452), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06255072243511677), 'actor_loss': np.float64(-0.9790341973304748), 'hyper_actor_loss': np.float64(0.0003436580707784742), 'behavior_loss': np.float64(0.3070970445871353)}

Episode step 17610, time diff 0.8090853691101074, total time dif 1544.860895872116)
step: 17610 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.699042), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07091053053736687), 'actor_loss': np.float64(-0.9920177578926086), 'hyper_actor_loss': np.float64(0.0003397657274035737), 'behavior_loss': np.float64(0.31950927078723906)}

Episode step 17620, time diff 0.8260476589202881, total time dif 1545.6699812412262)
step: 17620 @ episode report: {'average_total_reward': np.float32(10.646667), 'reward_variance': np.float32(2.537797), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04963527247309685), 'actor_loss': np.float64(-0.9791697084903717), 'hyper_actor_loss': np.float64(0.0003515572112519294), 'behavior_loss': np.float64(0.2892424315214157)}

Episode step 17630, time diff 0.7983510494232178, total time dif 1546.4960289001465)
step: 17630 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.0795872), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059507342614233495), 'actor_loss': np.float64(-0.9728592038154602), 'hyper_actor_loss': np.float64(0.0003286059101810679), 'behavior_loss': np.float64(0.2910355806350708)}

Episode step 17640, time diff 0.8933265209197998, total time dif 1547.2943799495697)
step: 17640 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(4.573378), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07004190236330032), 'actor_loss': np.float64(-1.0138095319271088), 'hyper_actor_loss': np.float64(0.0003833879483863711), 'behavior_loss': np.float64(0.2762963637709618)}

Episode step 17650, time diff 0.8756728172302246, total time dif 1548.1877064704895)
step: 17650 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.3132846), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05481038857251406), 'actor_loss': np.float64(-0.9876277148723602), 'hyper_actor_loss': np.float64(0.0003408226009923965), 'behavior_loss': np.float64(0.31582565158605574)}

Episode step 17660, time diff 1.0157537460327148, total time dif 1549.0633792877197)
step: 17660 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(4.8730636), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07756760716438293), 'actor_loss': np.float64(-0.9634235560894012), 'hyper_actor_loss': np.float64(0.0003510765760438517), 'behavior_loss': np.float64(0.3026730760931969)}

Episode step 17670, time diff 0.8740742206573486, total time dif 1550.0791330337524)
step: 17670 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.0335557), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06039434410631657), 'actor_loss': np.float64(-1.0203723430633544), 'hyper_actor_loss': np.float64(0.0003196642908733338), 'behavior_loss': np.float64(0.2851338073611259)}

Episode step 17680, time diff 0.9208135604858398, total time dif 1550.9532072544098)
step: 17680 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(3.747937), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06029511857777834), 'actor_loss': np.float64(-0.9745314478874206), 'hyper_actor_loss': np.float64(0.0003162612294545397), 'behavior_loss': np.float64(0.2872097000479698)}

Episode step 17690, time diff 0.9855053424835205, total time dif 1551.8740208148956)
step: 17690 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(1.7003748), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06561720445752144), 'actor_loss': np.float64(-0.9823958456516266), 'hyper_actor_loss': np.float64(0.000315930022043176), 'behavior_loss': np.float64(0.28936774134635923)}

Episode step 17700, time diff 0.9386556148529053, total time dif 1552.8595261573792)
step: 17700 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(5.362744), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07527473904192447), 'actor_loss': np.float64(-1.0236076593399048), 'hyper_actor_loss': np.float64(0.0003160988533636555), 'behavior_loss': np.float64(0.3090143859386444)}

Episode step 17710, time diff 0.935382604598999, total time dif 1553.798181772232)
step: 17710 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(0.4894188), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07038882039487362), 'actor_loss': np.float64(-1.0067974388599397), 'hyper_actor_loss': np.float64(0.00032241489680018276), 'behavior_loss': np.float64(0.3121701955795288)}

Episode step 17720, time diff 0.895211935043335, total time dif 1554.733564376831)
step: 17720 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(4.367336), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06559001915156841), 'actor_loss': np.float64(-0.9781030118465424), 'hyper_actor_loss': np.float64(0.0003491233044769615), 'behavior_loss': np.float64(0.3012715667486191)}

Episode step 17730, time diff 0.8558554649353027, total time dif 1555.6287763118744)
step: 17730 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(2.9638035), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07597723342478276), 'actor_loss': np.float64(-1.0104506969451905), 'hyper_actor_loss': np.float64(0.00034615141048561783), 'behavior_loss': np.float64(0.3105055779218674)}

Episode step 17740, time diff 0.8426594734191895, total time dif 1556.4846317768097)
step: 17740 @ episode report: {'average_total_reward': np.float32(9.077778), 'reward_variance': np.float32(3.4748886), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058999210223555566), 'actor_loss': np.float64(-0.9974621653556823), 'hyper_actor_loss': np.float64(0.0005603493307717144), 'behavior_loss': np.float64(0.30187232196331026)}

Episode step 17750, time diff 0.865135669708252, total time dif 1557.3272912502289)
step: 17750 @ episode report: {'average_total_reward': np.float32(7.667778), 'reward_variance': np.float32(4.486233), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06547796614468097), 'actor_loss': np.float64(-0.9851381599903106), 'hyper_actor_loss': np.float64(0.0004923959786538035), 'behavior_loss': np.float64(0.2845247179269791)}

Episode step 17760, time diff 1.004225492477417, total time dif 1558.1924269199371)
step: 17760 @ episode report: {'average_total_reward': np.float32(6.4066668), 'reward_variance': np.float32(3.0822523), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.068430832400918), 'actor_loss': np.float64(-0.9956789255142212), 'hyper_actor_loss': np.float64(0.0004242266179062426), 'behavior_loss': np.float64(0.31212429106235506)}

Episode step 17770, time diff 0.8138923645019531, total time dif 1559.1966524124146)
step: 17770 @ episode report: {'average_total_reward': np.float32(6.694444), 'reward_variance': np.float32(3.84166), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07001109477132558), 'actor_loss': np.float64(-0.9923983216285706), 'hyper_actor_loss': np.float64(0.0003566368541214615), 'behavior_loss': np.float64(0.3172346591949463)}

Episode step 17780, time diff 0.9789488315582275, total time dif 1560.0105447769165)
step: 17780 @ episode report: {'average_total_reward': np.float32(7.0066667), 'reward_variance': np.float32(2.9281285), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664336659014225), 'actor_loss': np.float64(-0.9958266019821167), 'hyper_actor_loss': np.float64(0.0003871893364703283), 'behavior_loss': np.float64(0.30713523328304293)}

Episode step 17790, time diff 1.0628600120544434, total time dif 1560.9894936084747)
step: 17790 @ episode report: {'average_total_reward': np.float32(6.3333335), 'reward_variance': np.float32(2.0637527), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06739093959331513), 'actor_loss': np.float64(-0.992030245065689), 'hyper_actor_loss': np.float64(0.0003705294599058107), 'behavior_loss': np.float64(0.3209327161312103)}

Episode step 17800, time diff 1.0152308940887451, total time dif 1562.0523536205292)
step: 17800 @ episode report: {'average_total_reward': np.float32(5.7822227), 'reward_variance': np.float32(1.7729927), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06055099293589592), 'actor_loss': np.float64(-0.9796908259391784), 'hyper_actor_loss': np.float64(0.00036079118435736743), 'behavior_loss': np.float64(0.32997637093067167)}

Episode step 17810, time diff 0.9915077686309814, total time dif 1563.067584514618)
step: 17810 @ episode report: {'average_total_reward': np.float32(6.0455556), 'reward_variance': np.float32(1.6281593), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06942411661148071), 'actor_loss': np.float64(-0.9918147563934326), 'hyper_actor_loss': np.float64(0.0003729100048076361), 'behavior_loss': np.float64(0.2870361328125)}

Episode step 17820, time diff 1.074709177017212, total time dif 1564.059092283249)
step: 17820 @ episode report: {'average_total_reward': np.float32(6.8555555), 'reward_variance': np.float32(2.3675056), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05489476956427097), 'actor_loss': np.float64(-0.997667109966278), 'hyper_actor_loss': np.float64(0.000375324793276377), 'behavior_loss': np.float64(0.2987934619188309)}

Episode step 17830, time diff 0.9351775646209717, total time dif 1565.133801460266)
step: 17830 @ episode report: {'average_total_reward': np.float32(5.6966667), 'reward_variance': np.float32(1.1453842), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061841707676649094), 'actor_loss': np.float64(-0.9688516557216644), 'hyper_actor_loss': np.float64(0.00039822971739340575), 'behavior_loss': np.float64(0.32020014226436616)}

Episode step 17840, time diff 0.9032373428344727, total time dif 1566.068979024887)
step: 17840 @ episode report: {'average_total_reward': np.float32(6.5577784), 'reward_variance': np.float32(2.9351063), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0672588225454092), 'actor_loss': np.float64(-1.0040936470031738), 'hyper_actor_loss': np.float64(0.0004220462084049359), 'behavior_loss': np.float64(0.3138511151075363)}

Episode step 17850, time diff 0.8060877323150635, total time dif 1566.9722163677216)
step: 17850 @ episode report: {'average_total_reward': np.float32(7.4677787), 'reward_variance': np.float32(5.79327), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05826906226575375), 'actor_loss': np.float64(-0.98546884059906), 'hyper_actor_loss': np.float64(0.00042810944141820074), 'behavior_loss': np.float64(0.3257842600345612)}

Episode step 17860, time diff 0.8322410583496094, total time dif 1567.7783041000366)
step: 17860 @ episode report: {'average_total_reward': np.float32(8.59), 'reward_variance': np.float32(3.42216), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06737733036279678), 'actor_loss': np.float64(-0.9641501009464264), 'hyper_actor_loss': np.float64(0.0003786349348956719), 'behavior_loss': np.float64(0.33227840065956116)}

Episode step 17870, time diff 0.811774730682373, total time dif 1568.6105451583862)
step: 17870 @ episode report: {'average_total_reward': np.float32(7.965556), 'reward_variance': np.float32(3.8191724), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058411114290356635), 'actor_loss': np.float64(-0.9980004310607911), 'hyper_actor_loss': np.float64(0.00036352383613120766), 'behavior_loss': np.float64(0.3192525625228882)}

Episode step 17880, time diff 0.8640477657318115, total time dif 1569.4223198890686)
step: 17880 @ episode report: {'average_total_reward': np.float32(9.375557), 'reward_variance': np.float32(3.5297737), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07030033357441426), 'actor_loss': np.float64(-0.9775484323501586), 'hyper_actor_loss': np.float64(0.0003443609923124313), 'behavior_loss': np.float64(0.31156103909015653)}

Episode step 17890, time diff 0.820056676864624, total time dif 1570.2863676548004)
step: 17890 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(2.099542), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06038007084280252), 'actor_loss': np.float64(-0.9881092488765717), 'hyper_actor_loss': np.float64(0.00032429911952931435), 'behavior_loss': np.float64(0.31530116498470306)}

Episode step 17900, time diff 0.7914628982543945, total time dif 1571.106424331665)
step: 17900 @ episode report: {'average_total_reward': np.float32(8.902224), 'reward_variance': np.float32(1.571328), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06898595951497555), 'actor_loss': np.float64(-0.9799020051956177), 'hyper_actor_loss': np.float64(0.0003114890801953152), 'behavior_loss': np.float64(0.31433664858341215)}

Episode step 17910, time diff 0.8278648853302002, total time dif 1571.8978872299194)
step: 17910 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.1731217), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06270826198160648), 'actor_loss': np.float64(-0.9985536456108093), 'hyper_actor_loss': np.float64(0.0003102238348219544), 'behavior_loss': np.float64(0.2927591517567635)}

Episode step 17920, time diff 0.9705612659454346, total time dif 1572.7257521152496)
step: 17920 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(3.9605718), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06528021171689033), 'actor_loss': np.float64(-0.9921219527721405), 'hyper_actor_loss': np.float64(0.0002965216466691345), 'behavior_loss': np.float64(0.3185356616973877)}

Episode step 17930, time diff 0.9644961357116699, total time dif 1573.696313381195)
step: 17930 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.7488105), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056042372807860376), 'actor_loss': np.float64(-0.975644463300705), 'hyper_actor_loss': np.float64(0.0002956976240966469), 'behavior_loss': np.float64(0.2952857449650764)}

Episode step 17940, time diff 0.9206876754760742, total time dif 1574.6608095169067)
step: 17940 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.866943), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05859468095004559), 'actor_loss': np.float64(-0.9667962908744812), 'hyper_actor_loss': np.float64(0.0002986542269354686), 'behavior_loss': np.float64(0.31312904953956605)}

Episode step 17950, time diff 0.7872517108917236, total time dif 1575.5814971923828)
step: 17950 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(2.0835576), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06249901242554188), 'actor_loss': np.float64(-0.9730124592781066), 'hyper_actor_loss': np.float64(0.0003014383837580681), 'behavior_loss': np.float64(0.3113953351974487)}

Episode step 17960, time diff 0.8291025161743164, total time dif 1576.3687489032745)
step: 17960 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.2606192), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06161792688071728), 'actor_loss': np.float64(-0.9911847531795501), 'hyper_actor_loss': np.float64(0.0003169122530380264), 'behavior_loss': np.float64(0.3114355802536011)}

Episode step 17970, time diff 0.859661340713501, total time dif 1577.1978514194489)
step: 17970 @ episode report: {'average_total_reward': np.float32(8.863335), 'reward_variance': np.float32(1.7418289), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06684457585215568), 'actor_loss': np.float64(-0.9876044511795044), 'hyper_actor_loss': np.float64(0.00031090234115254133), 'behavior_loss': np.float64(0.3056504249572754)}

Episode step 17980, time diff 0.804025411605835, total time dif 1578.0575127601624)
step: 17980 @ episode report: {'average_total_reward': np.float32(7.7922225), 'reward_variance': np.float32(3.239977), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06579363122582435), 'actor_loss': np.float64(-0.9876292049884796), 'hyper_actor_loss': np.float64(0.0003177056758431718), 'behavior_loss': np.float64(0.3163180351257324)}

Episode step 17990, time diff 1.0130565166473389, total time dif 1578.8615381717682)
step: 17990 @ episode report: {'average_total_reward': np.float32(7.2433333), 'reward_variance': np.float32(1.5597641), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06466177739202976), 'actor_loss': np.float64(-0.9934765815734863), 'hyper_actor_loss': np.float64(0.00030740102811250837), 'behavior_loss': np.float64(0.31756353974342344)}

Episode step 18000, time diff 0.8481626510620117, total time dif 1579.8745946884155)
step: 18000 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(2.8924003), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0667639471590519), 'actor_loss': np.float64(-0.9781238615512848), 'hyper_actor_loss': np.float64(0.000314786346280016), 'behavior_loss': np.float64(0.2973696976900101)}

Episode step 18010, time diff 0.8782780170440674, total time dif 1580.7227573394775)
step: 18010 @ episode report: {'average_total_reward': np.float32(6.1700006), 'reward_variance': np.float32(2.8874826), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06429412886500359), 'actor_loss': np.float64(-0.9829765498638153), 'hyper_actor_loss': np.float64(0.0003141785564366728), 'behavior_loss': np.float64(0.33517105877399445)}

Episode step 18020, time diff 0.894202709197998, total time dif 1581.6010353565216)
step: 18020 @ episode report: {'average_total_reward': np.float32(6.3844447), 'reward_variance': np.float32(2.2803512), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0703220620751381), 'actor_loss': np.float64(-0.9909654080867767), 'hyper_actor_loss': np.float64(0.00034547943796496837), 'behavior_loss': np.float64(0.30211571156978606)}

Episode step 18030, time diff 0.911566972732544, total time dif 1582.4952380657196)
step: 18030 @ episode report: {'average_total_reward': np.float32(5.808889), 'reward_variance': np.float32(2.1552548), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06264880131930113), 'actor_loss': np.float64(-1.0011145532131196), 'hyper_actor_loss': np.float64(0.00035010275605600326), 'behavior_loss': np.float64(0.3115643084049225)}

Episode step 18040, time diff 0.8949766159057617, total time dif 1583.4068050384521)
step: 18040 @ episode report: {'average_total_reward': np.float32(5.0477777), 'reward_variance': np.float32(0.9028905), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06813369430601597), 'actor_loss': np.float64(-0.9885976672172546), 'hyper_actor_loss': np.float64(0.00035119898384436963), 'behavior_loss': np.float64(0.3097618520259857)}

Episode step 18050, time diff 0.8084897994995117, total time dif 1584.301781654358)
step: 18050 @ episode report: {'average_total_reward': np.float32(4.998889), 'reward_variance': np.float32(0.7198384), 'max_total_reward': np.float32(6.411112), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07382559515535832), 'actor_loss': np.float64(-0.9901724517345428), 'hyper_actor_loss': np.float64(0.0003482276079012081), 'behavior_loss': np.float64(0.3287098199129105)}

Episode step 18060, time diff 0.8162062168121338, total time dif 1585.1102714538574)
step: 18060 @ episode report: {'average_total_reward': np.float32(4.884444), 'reward_variance': np.float32(0.7071407), 'max_total_reward': np.float32(6.5333333), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06413117796182632), 'actor_loss': np.float64(-0.9830916106700898), 'hyper_actor_loss': np.float64(0.0003098521672654897), 'behavior_loss': np.float64(0.32331925332546235)}

Episode step 18070, time diff 0.7803826332092285, total time dif 1585.9264776706696)
step: 18070 @ episode report: {'average_total_reward': np.float32(5.957778), 'reward_variance': np.float32(1.3430567), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07162553295493126), 'actor_loss': np.float64(-0.9909659326076508), 'hyper_actor_loss': np.float64(0.0003281540877651423), 'behavior_loss': np.float64(0.3213346630334854)}

Episode step 18080, time diff 0.7965066432952881, total time dif 1586.7068603038788)
step: 18080 @ episode report: {'average_total_reward': np.float32(6.1066675), 'reward_variance': np.float32(3.144919), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06167218349874019), 'actor_loss': np.float64(-0.9867593705654144), 'hyper_actor_loss': np.float64(0.000392934720730409), 'behavior_loss': np.float64(0.3132708042860031)}

Episode step 18090, time diff 0.862694263458252, total time dif 1587.503366947174)
step: 18090 @ episode report: {'average_total_reward': np.float32(7.8655562), 'reward_variance': np.float32(2.7088015), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07844992317259311), 'actor_loss': np.float64(-0.9876602411270141), 'hyper_actor_loss': np.float64(0.00036817974178120493), 'behavior_loss': np.float64(0.32967221140861513)}

Episode step 18100, time diff 0.820814847946167, total time dif 1588.3660612106323)
step: 18100 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(1.4944063), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.070367525331676), 'actor_loss': np.float64(-0.9751162827014923), 'hyper_actor_loss': np.float64(0.00028800954460166394), 'behavior_loss': np.float64(0.32869575917720795)}

Episode step 18110, time diff 0.8269708156585693, total time dif 1589.1868760585785)
step: 18110 @ episode report: {'average_total_reward': np.float32(9.075557), 'reward_variance': np.float32(3.7258728), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05840152055025101), 'actor_loss': np.float64(-0.9833329081535339), 'hyper_actor_loss': np.float64(0.00027824353892356155), 'behavior_loss': np.float64(0.3159052848815918)}

Episode step 18120, time diff 0.8212063312530518, total time dif 1590.013846874237)
step: 18120 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(4.3604064), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06290133260190486), 'actor_loss': np.float64(-0.9804832100868225), 'hyper_actor_loss': np.float64(0.0002869410585844889), 'behavior_loss': np.float64(0.33685178458690646)}

Episode step 18130, time diff 0.8350000381469727, total time dif 1590.83505320549)
step: 18130 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(3.411635), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053591382689774036), 'actor_loss': np.float64(-0.9627552449703216), 'hyper_actor_loss': np.float64(0.0002912534138886258), 'behavior_loss': np.float64(0.32782813608646394)}

Episode step 18140, time diff 0.7810342311859131, total time dif 1591.670053243637)
step: 18140 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.4869645), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07287257201969624), 'actor_loss': np.float64(-0.9742741167545319), 'hyper_actor_loss': np.float64(0.00030636007722932844), 'behavior_loss': np.float64(0.34478779733181)}

Episode step 18150, time diff 0.9993610382080078, total time dif 1592.451087474823)
step: 18150 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.5408204), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06987108252942562), 'actor_loss': np.float64(-0.9921192824840546), 'hyper_actor_loss': np.float64(0.0003115318191703409), 'behavior_loss': np.float64(0.321281886100769)}

Episode step 18160, time diff 0.8147194385528564, total time dif 1593.450448513031)
step: 18160 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(2.8012109), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07460774332284928), 'actor_loss': np.float64(-0.9894975781440735), 'hyper_actor_loss': np.float64(0.0002951188536826521), 'behavior_loss': np.float64(0.33911789059638975)}

Episode step 18170, time diff 0.7836058139801025, total time dif 1594.2651679515839)
step: 18170 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.9299326), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07336243130266666), 'actor_loss': np.float64(-0.9844291985034943), 'hyper_actor_loss': np.float64(0.000291724520502612), 'behavior_loss': np.float64(0.35345191359519956)}

Episode step 18180, time diff 0.8234245777130127, total time dif 1595.048773765564)
step: 18180 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(3.5356362), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05387974251061678), 'actor_loss': np.float64(-0.9701618850231171), 'hyper_actor_loss': np.float64(0.0003123748872894794), 'behavior_loss': np.float64(0.3348877400159836)}

Episode step 18190, time diff 0.7839639186859131, total time dif 1595.872198343277)
step: 18190 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.8367178), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05155334807932377), 'actor_loss': np.float64(-0.953745287656784), 'hyper_actor_loss': np.float64(0.00034210446174256506), 'behavior_loss': np.float64(0.35083492994308474)}

Episode step 18200, time diff 0.8085927963256836, total time dif 1596.656162261963)
step: 18200 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(4.165265), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06565954871475696), 'actor_loss': np.float64(-0.9788698375225067), 'hyper_actor_loss': np.float64(0.0003498356702039018), 'behavior_loss': np.float64(0.33599472641944883)}

Episode step 18210, time diff 0.8515701293945312, total time dif 1597.4647550582886)
step: 18210 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(5.4756556), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05721147283911705), 'actor_loss': np.float64(-0.9798546075820923), 'hyper_actor_loss': np.float64(0.0003437870414927602), 'behavior_loss': np.float64(0.3371404141187668)}

Episode step 18220, time diff 0.7623071670532227, total time dif 1598.316325187683)
step: 18220 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(1.9491556), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05724194943904877), 'actor_loss': np.float64(-0.9654563903808594), 'hyper_actor_loss': np.float64(0.0003274171001976356), 'behavior_loss': np.float64(0.32587067484855653)}

Episode step 18230, time diff 0.8128218650817871, total time dif 1599.0786323547363)
step: 18230 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(3.5622668), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055715576745569706), 'actor_loss': np.float64(-0.9754727005958557), 'hyper_actor_loss': np.float64(0.0003258742799516767), 'behavior_loss': np.float64(0.33161412328481676)}

Episode step 18240, time diff 0.7924468517303467, total time dif 1599.8914542198181)
step: 18240 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(3.6097054), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06264080591499806), 'actor_loss': np.float64(-0.9748651206493377), 'hyper_actor_loss': np.float64(0.00035640482674352824), 'behavior_loss': np.float64(0.3276993453502655)}

Episode step 18250, time diff 0.7874228954315186, total time dif 1600.6839010715485)
step: 18250 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(3.8920124), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07026810385286808), 'actor_loss': np.float64(-0.9830009639263153), 'hyper_actor_loss': np.float64(0.00035647956538014113), 'behavior_loss': np.float64(0.3338160037994385)}

Episode step 18260, time diff 0.7966740131378174, total time dif 1601.47132396698)
step: 18260 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.1273541), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06567583382129669), 'actor_loss': np.float64(-0.9779186844825745), 'hyper_actor_loss': np.float64(0.00036867505696136504), 'behavior_loss': np.float64(0.34679036140441893)}

Episode step 18270, time diff 0.7935488224029541, total time dif 1602.2679979801178)
step: 18270 @ episode report: {'average_total_reward': np.float32(11.371112), 'reward_variance': np.float32(3.7115123), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07047718279063701), 'actor_loss': np.float64(-0.9811589300632477), 'hyper_actor_loss': np.float64(0.0004367728601209819), 'behavior_loss': np.float64(0.3545455992221832)}

Episode step 18280, time diff 0.8330132961273193, total time dif 1603.0615468025208)
step: 18280 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(1.7817348), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06247032880783081), 'actor_loss': np.float64(-0.977020388841629), 'hyper_actor_loss': np.float64(0.0003975143772549927), 'behavior_loss': np.float64(0.34772693514823916)}

Episode step 18290, time diff 0.7990875244140625, total time dif 1603.894560098648)
step: 18290 @ episode report: {'average_total_reward': np.float32(10.797778), 'reward_variance': np.float32(4.2758956), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06216316930949688), 'actor_loss': np.float64(-0.9717500150203705), 'hyper_actor_loss': np.float64(0.00037980723427608607), 'behavior_loss': np.float64(0.3380635529756546)}

Episode step 18300, time diff 0.7920310497283936, total time dif 1604.6936476230621)
step: 18300 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.728765), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060345759615302086), 'actor_loss': np.float64(-0.9719029486179351), 'hyper_actor_loss': np.float64(0.00038720905431546273), 'behavior_loss': np.float64(0.33626759946346285)}

Episode step 18310, time diff 0.8092544078826904, total time dif 1605.4856786727905)
step: 18310 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.1757026), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07074648700654507), 'actor_loss': np.float64(-0.9828467011451721), 'hyper_actor_loss': np.float64(0.00034967435349244627), 'behavior_loss': np.float64(0.3450393557548523)}

Episode step 18320, time diff 0.9683020114898682, total time dif 1606.2949330806732)
step: 18320 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(2.3838391), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05360156036913395), 'actor_loss': np.float64(-0.9809457063674927), 'hyper_actor_loss': np.float64(0.00034686166909523306), 'behavior_loss': np.float64(0.32821000516414645)}

Episode step 18330, time diff 0.8268928527832031, total time dif 1607.263235092163)
step: 18330 @ episode report: {'average_total_reward': np.float32(11.320001), 'reward_variance': np.float32(3.5592797), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0644802637398243), 'actor_loss': np.float64(-0.9668492615222931), 'hyper_actor_loss': np.float64(0.0003749463241547346), 'behavior_loss': np.float64(0.3192215323448181)}

Episode step 18340, time diff 0.8206343650817871, total time dif 1608.0901279449463)
step: 18340 @ episode report: {'average_total_reward': np.float32(11.607778), 'reward_variance': np.float32(7.548323), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06793879643082619), 'actor_loss': np.float64(-0.9873109459877014), 'hyper_actor_loss': np.float64(0.00037222793325781823), 'behavior_loss': np.float64(0.33858729898929596)}

Episode step 18350, time diff 0.8464174270629883, total time dif 1608.910762310028)
step: 18350 @ episode report: {'average_total_reward': np.float32(12.305557), 'reward_variance': np.float32(3.186328), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05916009657084942), 'actor_loss': np.float64(-0.9911991953849792), 'hyper_actor_loss': np.float64(0.00041806182125583293), 'behavior_loss': np.float64(0.33201884776353835)}

Episode step 18360, time diff 0.8608627319335938, total time dif 1609.757179737091)
step: 18360 @ episode report: {'average_total_reward': np.float32(11.495557), 'reward_variance': np.float32(1.5999311), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07555492892861367), 'actor_loss': np.float64(-0.9746441841125488), 'hyper_actor_loss': np.float64(0.00040550162375438956), 'behavior_loss': np.float64(0.335032194852829)}

Episode step 18370, time diff 0.8741364479064941, total time dif 1610.6180424690247)
step: 18370 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.794347), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06459978278726339), 'actor_loss': np.float64(-0.9890409409999847), 'hyper_actor_loss': np.float64(0.0003252066788263619), 'behavior_loss': np.float64(0.35427031815052035)}

Episode step 18380, time diff 0.879296064376831, total time dif 1611.4921789169312)
step: 18380 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(1.4231174), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081652469933034), 'actor_loss': np.float64(-0.9814378976821899), 'hyper_actor_loss': np.float64(0.00031408298236783593), 'behavior_loss': np.float64(0.32722158432006837)}

Episode step 18390, time diff 0.8252313137054443, total time dif 1612.371474981308)
step: 18390 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(0.936267), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0680898804217577), 'actor_loss': np.float64(-0.9974756836891174), 'hyper_actor_loss': np.float64(0.0003175681602442637), 'behavior_loss': np.float64(0.33942619562149046)}

Episode step 18400, time diff 0.8809912204742432, total time dif 1613.1967062950134)
step: 18400 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(4.8322735), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07814887650310993), 'actor_loss': np.float64(-0.9693592071533204), 'hyper_actor_loss': np.float64(0.00031263678101822734), 'behavior_loss': np.float64(0.3611206620931625)}

Episode step 18410, time diff 0.8458881378173828, total time dif 1614.0776975154877)
step: 18410 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(2.8741), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07308735884726048), 'actor_loss': np.float64(-0.9955294191837311), 'hyper_actor_loss': np.float64(0.0003012106870301068), 'behavior_loss': np.float64(0.32954799830913545)}

Episode step 18420, time diff 0.9483458995819092, total time dif 1614.923585653305)
step: 18420 @ episode report: {'average_total_reward': np.float32(10.846666), 'reward_variance': np.float32(2.4694762), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07125792726874351), 'actor_loss': np.float64(-0.9905824065208435), 'hyper_actor_loss': np.float64(0.000294457288691774), 'behavior_loss': np.float64(0.3363602846860886)}

Episode step 18430, time diff 0.8632712364196777, total time dif 1615.871931552887)
step: 18430 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(6.838095), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0620353989303112), 'actor_loss': np.float64(-0.9785676240921021), 'hyper_actor_loss': np.float64(0.00031049101962707935), 'behavior_loss': np.float64(0.3368842571973801)}

Episode step 18440, time diff 0.8708689212799072, total time dif 1616.7352027893066)
step: 18440 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.4949183), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0684744231402874), 'actor_loss': np.float64(-0.9818843901157379), 'hyper_actor_loss': np.float64(0.00030007667373865843), 'behavior_loss': np.float64(0.3080084800720215)}

Episode step 18450, time diff 0.9399242401123047, total time dif 1617.6060717105865)
step: 18450 @ episode report: {'average_total_reward': np.float32(11.420001), 'reward_variance': np.float32(5.571328), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07397670969367028), 'actor_loss': np.float64(-1.002078378200531), 'hyper_actor_loss': np.float64(0.00029495231865439564), 'behavior_loss': np.float64(0.3404203623533249)}

Episode step 18460, time diff 0.9016101360321045, total time dif 1618.5459959506989)
step: 18460 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(2.4850633), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06445012651383877), 'actor_loss': np.float64(-0.972412395477295), 'hyper_actor_loss': np.float64(0.000320456808549352), 'behavior_loss': np.float64(0.3358806908130646)}

Episode step 18470, time diff 0.8555908203125, total time dif 1619.447606086731)
step: 18470 @ episode report: {'average_total_reward': np.float32(11.1466675), 'reward_variance': np.float32(5.131996), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07429680936038494), 'actor_loss': np.float64(-0.9814569473266601), 'hyper_actor_loss': np.float64(0.00032636054675094783), 'behavior_loss': np.float64(0.33640168607234955)}

Episode step 18480, time diff 0.9098076820373535, total time dif 1620.3031969070435)
step: 18480 @ episode report: {'average_total_reward': np.float32(11.420001), 'reward_variance': np.float32(2.3323412), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0763592291623354), 'actor_loss': np.float64(-1.0010296642780303), 'hyper_actor_loss': np.float64(0.0002979698736453429), 'behavior_loss': np.float64(0.3539375215768814)}

Episode step 18490, time diff 1.0609710216522217, total time dif 1621.2130045890808)
step: 18490 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(4.4606924), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07074637562036515), 'actor_loss': np.float64(-0.9984392642974853), 'hyper_actor_loss': np.float64(0.00030470039346255364), 'behavior_loss': np.float64(0.3365760535001755)}

Episode step 18500, time diff 0.8227999210357666, total time dif 1622.273975610733)
step: 18500 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(0.9896353), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07007781080901623), 'actor_loss': np.float64(-0.984858363866806), 'hyper_actor_loss': np.float64(0.00030385318968910723), 'behavior_loss': np.float64(0.34117637276649476)}

Episode step 18510, time diff 0.8256256580352783, total time dif 1623.0967755317688)
step: 18510 @ episode report: {'average_total_reward': np.float32(12.156668), 'reward_variance': np.float32(1.8372713), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07284018099308014), 'actor_loss': np.float64(-0.9792591810226441), 'hyper_actor_loss': np.float64(0.0003035052999621257), 'behavior_loss': np.float64(0.3517428785562515)}

Episode step 18520, time diff 0.8649966716766357, total time dif 1623.922401189804)
step: 18520 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(5.197345), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060462775826454165), 'actor_loss': np.float64(-0.9752020120620728), 'hyper_actor_loss': np.float64(0.0003329286730149761), 'behavior_loss': np.float64(0.34323233366012573)}

Episode step 18530, time diff 0.8194656372070312, total time dif 1624.7873978614807)
step: 18530 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(2.3733346), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07098792418837548), 'actor_loss': np.float64(-0.9711087107658386), 'hyper_actor_loss': np.float64(0.00033071350189857186), 'behavior_loss': np.float64(0.35320129096508024)}

Episode step 18540, time diff 0.8755078315734863, total time dif 1625.6068634986877)
step: 18540 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(9.370526), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07715671397745609), 'actor_loss': np.float64(-0.9997779309749604), 'hyper_actor_loss': np.float64(0.00029662984306924047), 'behavior_loss': np.float64(0.3198682129383087)}

Episode step 18550, time diff 0.8771400451660156, total time dif 1626.4823713302612)
step: 18550 @ episode report: {'average_total_reward': np.float32(11.520001), 'reward_variance': np.float32(5.110416), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.1666665), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06571191549301147), 'actor_loss': np.float64(-0.9940681278705596), 'hyper_actor_loss': np.float64(0.00031871718820184467), 'behavior_loss': np.float64(0.34476245641708375)}

Episode step 18560, time diff 0.8398227691650391, total time dif 1627.3595113754272)
step: 18560 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(5.60736), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0801557157188654), 'actor_loss': np.float64(-0.9713334977626801), 'hyper_actor_loss': np.float64(0.0004564317816402763), 'behavior_loss': np.float64(0.36483265459537506)}

Episode step 18570, time diff 0.9082765579223633, total time dif 1628.1993341445923)
step: 18570 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.0529397), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07010645121335983), 'actor_loss': np.float64(-1.0079528570175171), 'hyper_actor_loss': np.float64(0.00048568892525509), 'behavior_loss': np.float64(0.35162085592746734)}

Episode step 18580, time diff 0.8852341175079346, total time dif 1629.1076107025146)
step: 18580 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(1.8995135), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05825054310262203), 'actor_loss': np.float64(-0.9806245326995849), 'hyper_actor_loss': np.float64(0.0003636020497651771), 'behavior_loss': np.float64(0.3680535048246384)}

Episode step 18590, time diff 0.8626883029937744, total time dif 1629.9928448200226)
step: 18590 @ episode report: {'average_total_reward': np.float32(11.395556), 'reward_variance': np.float32(2.0668948), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07398491464555264), 'actor_loss': np.float64(-0.9585170328617096), 'hyper_actor_loss': np.float64(0.0003381052432814613), 'behavior_loss': np.float64(0.33421704173088074)}

Episode step 18600, time diff 0.8484346866607666, total time dif 1630.8555331230164)
step: 18600 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(3.1288693), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061330271884799004), 'actor_loss': np.float64(-1.0082375347614287), 'hyper_actor_loss': np.float64(0.00027435953670646996), 'behavior_loss': np.float64(0.30115694403648374)}

Episode step 18610, time diff 0.9039266109466553, total time dif 1631.7039678096771)
step: 18610 @ episode report: {'average_total_reward': np.float32(11.007778), 'reward_variance': np.float32(1.8459513), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05564122125506401), 'actor_loss': np.float64(-0.9817743241786957), 'hyper_actor_loss': np.float64(0.00028273184434510765), 'behavior_loss': np.float64(0.33356539309024813)}

Episode step 18620, time diff 0.9249062538146973, total time dif 1632.6078944206238)
step: 18620 @ episode report: {'average_total_reward': np.float32(12.2300005), 'reward_variance': np.float32(1.6241243), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08188258707523347), 'actor_loss': np.float64(-0.976999831199646), 'hyper_actor_loss': np.float64(0.0002781291594146751), 'behavior_loss': np.float64(0.3386110246181488)}

Episode step 18630, time diff 0.9289655685424805, total time dif 1633.5328006744385)
step: 18630 @ episode report: {'average_total_reward': np.float32(11.744446), 'reward_variance': np.float32(5.275532), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08480056114494801), 'actor_loss': np.float64(-1.0180813550949097), 'hyper_actor_loss': np.float64(0.000270295818336308), 'behavior_loss': np.float64(0.3103362530469894)}

Episode step 18640, time diff 1.0303187370300293, total time dif 1634.461766242981)
step: 18640 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.1089997), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0731439832597971), 'actor_loss': np.float64(-0.9976216852664948), 'hyper_actor_loss': np.float64(0.0002611354721011594), 'behavior_loss': np.float64(0.35201391875743865)}

Episode step 18650, time diff 1.134803295135498, total time dif 1635.492084980011)
step: 18650 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.3937644), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05932989977300167), 'actor_loss': np.float64(-0.97075856924057), 'hyper_actor_loss': np.float64(0.00026658944407245146), 'behavior_loss': np.float64(0.3305211067199707)}

Episode step 18660, time diff 1.3140411376953125, total time dif 1636.6268882751465)
step: 18660 @ episode report: {'average_total_reward': np.float32(10.8977785), 'reward_variance': np.float32(1.4216743), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07277499102056026), 'actor_loss': np.float64(-0.9865120768547058), 'hyper_actor_loss': np.float64(0.0002692524722078815), 'behavior_loss': np.float64(0.34204149544239043)}

Episode step 18670, time diff 1.1363861560821533, total time dif 1637.9409294128418)
step: 18670 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(1.0883262), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06508190929889679), 'actor_loss': np.float64(-0.992612338066101), 'hyper_actor_loss': np.float64(0.00029775671428069474), 'behavior_loss': np.float64(0.3591756194829941)}

Episode step 18680, time diff 1.1123042106628418, total time dif 1639.077315568924)
step: 18680 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(3.027018), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06973250806331635), 'actor_loss': np.float64(-0.9698203325271606), 'hyper_actor_loss': np.float64(0.00035668321361299605), 'behavior_loss': np.float64(0.36900708079338074)}

Episode step 18690, time diff 1.1556060314178467, total time dif 1640.1896197795868)
step: 18690 @ episode report: {'average_total_reward': np.float32(10.858889), 'reward_variance': np.float32(4.9938784), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07296128235757351), 'actor_loss': np.float64(-0.987544983625412), 'hyper_actor_loss': np.float64(0.0003828482236713171), 'behavior_loss': np.float64(0.3671234607696533)}

Episode step 18700, time diff 1.1369059085845947, total time dif 1641.3452258110046)
step: 18700 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.2565248), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05737997479736805), 'actor_loss': np.float64(-0.9801037430763244), 'hyper_actor_loss': np.float64(0.0003336797031806782), 'behavior_loss': np.float64(0.33657858073711394)}

Episode step 18710, time diff 1.162207841873169, total time dif 1642.4821317195892)
step: 18710 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(3.276785), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07215378284454346), 'actor_loss': np.float64(-0.9743550062179566), 'hyper_actor_loss': np.float64(0.00037291447806637735), 'behavior_loss': np.float64(0.35468685030937197)}

Episode step 18720, time diff 1.1815922260284424, total time dif 1643.6443395614624)
step: 18720 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(5.493533), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07174549624323845), 'actor_loss': np.float64(-0.9885279595851898), 'hyper_actor_loss': np.float64(0.00041883293888531623), 'behavior_loss': np.float64(0.36332681477069856)}

Episode step 18730, time diff 1.1468195915222168, total time dif 1644.8259317874908)
step: 18730 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(1.8485689), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06288928575813771), 'actor_loss': np.float64(-0.9907459139823913), 'hyper_actor_loss': np.float64(0.00042579003784339877), 'behavior_loss': np.float64(0.3485980272293091)}

Episode step 18740, time diff 1.1341946125030518, total time dif 1645.972751379013)
step: 18740 @ episode report: {'average_total_reward': np.float32(11.707778), 'reward_variance': np.float32(3.1520257), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0725297074764967), 'actor_loss': np.float64(-0.9842097997665405), 'hyper_actor_loss': np.float64(0.0004150373657466844), 'behavior_loss': np.float64(0.32787263989448545)}

Episode step 18750, time diff 1.124873399734497, total time dif 1647.106945991516)
step: 18750 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.8844948), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07038362547755242), 'actor_loss': np.float64(-0.9979731619358063), 'hyper_actor_loss': np.float64(0.0004076036770129576), 'behavior_loss': np.float64(0.3309101402759552)}

Episode step 18760, time diff 1.1920781135559082, total time dif 1648.2318193912506)
step: 18760 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(2.1487956), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06062116920948028), 'actor_loss': np.float64(-0.9841756403446198), 'hyper_actor_loss': np.float64(0.00036969008797314016), 'behavior_loss': np.float64(0.34146833419799805)}

Episode step 18770, time diff 1.1900908946990967, total time dif 1649.4238975048065)
step: 18770 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(1.3970635), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07427877187728882), 'actor_loss': np.float64(-0.9877593874931335), 'hyper_actor_loss': np.float64(0.00038649944472126665), 'behavior_loss': np.float64(0.34974345266819)}

Episode step 18780, time diff 1.223125696182251, total time dif 1650.6139883995056)
step: 18780 @ episode report: {'average_total_reward': np.float32(11.483335), 'reward_variance': np.float32(2.1500077), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(9.655555), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07056948859244586), 'actor_loss': np.float64(-1.0006128430366517), 'hyper_actor_loss': np.float64(0.0003490258619422093), 'behavior_loss': np.float64(0.34448306560516356)}

Episode step 18790, time diff 1.2372961044311523, total time dif 1651.8371140956879)
step: 18790 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(3.4570327), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07007663175463677), 'actor_loss': np.float64(-0.9866955816745758), 'hyper_actor_loss': np.float64(0.0003265242819907144), 'behavior_loss': np.float64(0.344262021780014)}

Episode step 18800, time diff 1.236567735671997, total time dif 1653.074410200119)
step: 18800 @ episode report: {'average_total_reward': np.float32(12.056667), 'reward_variance': np.float32(3.0644307), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0693787194788456), 'actor_loss': np.float64(-0.9831933975219727), 'hyper_actor_loss': np.float64(0.0003257522970670834), 'behavior_loss': np.float64(0.3563908040523529)}

Episode step 18810, time diff 1.2953052520751953, total time dif 1654.310977935791)
step: 18810 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(4.982865), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06171622909605503), 'actor_loss': np.float64(-0.978389436006546), 'hyper_actor_loss': np.float64(0.00031942015630193055), 'behavior_loss': np.float64(0.35053011775016785)}

Episode step 18820, time diff 1.4768083095550537, total time dif 1655.6062831878662)
step: 18820 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(2.8020847), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06658035777509212), 'actor_loss': np.float64(-0.977560567855835), 'hyper_actor_loss': np.float64(0.00029783805075567216), 'behavior_loss': np.float64(0.34970634877681733)}

Episode step 18830, time diff 1.3523428440093994, total time dif 1657.0830914974213)
step: 18830 @ episode report: {'average_total_reward': np.float32(10.983334), 'reward_variance': np.float32(2.1758342), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07280046902596951), 'actor_loss': np.float64(-0.9918201804161072), 'hyper_actor_loss': np.float64(0.00029802130884490905), 'behavior_loss': np.float64(0.363082492351532)}

Episode step 18840, time diff 1.3909704685211182, total time dif 1658.4354343414307)
step: 18840 @ episode report: {'average_total_reward': np.float32(11.607779), 'reward_variance': np.float32(3.9681747), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06435460429638624), 'actor_loss': np.float64(-0.9825919508934021), 'hyper_actor_loss': np.float64(0.0002746152720646933), 'behavior_loss': np.float64(0.34157003462314606)}

Episode step 18850, time diff 1.3239266872406006, total time dif 1659.8264048099518)
step: 18850 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(0.94203025), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07158408276736736), 'actor_loss': np.float64(-0.992585027217865), 'hyper_actor_loss': np.float64(0.00028503382636699823), 'behavior_loss': np.float64(0.3388667404651642)}

Episode step 18860, time diff 1.30609130859375, total time dif 1661.1503314971924)
step: 18860 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(1.2229922), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07354996576905251), 'actor_loss': np.float64(-0.9947080135345459), 'hyper_actor_loss': np.float64(0.0002888374699978158), 'behavior_loss': np.float64(0.3612985372543335)}

Episode step 18870, time diff 1.3086838722229004, total time dif 1662.4564228057861)
step: 18870 @ episode report: {'average_total_reward': np.float32(10.336667), 'reward_variance': np.float32(2.7580016), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05860813185572624), 'actor_loss': np.float64(-0.9818018138408661), 'hyper_actor_loss': np.float64(0.0002970055618789047), 'behavior_loss': np.float64(0.36826473474502563)}

Episode step 18880, time diff 1.3281855583190918, total time dif 1663.765106678009)
step: 18880 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(4.2202525), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06480161584913731), 'actor_loss': np.float64(-0.9616042792797088), 'hyper_actor_loss': np.float64(0.0003287310304585844), 'behavior_loss': np.float64(0.34510785937309263)}

Episode step 18890, time diff 1.3577518463134766, total time dif 1665.0932922363281)
step: 18890 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(2.7454147), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06982228066772223), 'actor_loss': np.float64(-1.00134996175766), 'hyper_actor_loss': np.float64(0.0003378172608790919), 'behavior_loss': np.float64(0.3776366949081421)}

Episode step 18900, time diff 1.420562505722046, total time dif 1666.4510440826416)
step: 18900 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.1289451), 'max_total_reward': np.float32(12.022223), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05704321525990963), 'actor_loss': np.float64(-0.9866061091423035), 'hyper_actor_loss': np.float64(0.00032431723666377367), 'behavior_loss': np.float64(0.351476189494133)}

Episode step 18910, time diff 1.4937505722045898, total time dif 1667.8716065883636)
step: 18910 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(2.3697793), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07778688371181489), 'actor_loss': np.float64(-0.9743043124675751), 'hyper_actor_loss': np.float64(0.00030679616902489214), 'behavior_loss': np.float64(0.36614049673080445)}

Episode step 18920, time diff 1.6737701892852783, total time dif 1669.3653571605682)
step: 18920 @ episode report: {'average_total_reward': np.float32(11.344446), 'reward_variance': np.float32(2.6513333), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06583187505602836), 'actor_loss': np.float64(-0.9887301564216614), 'hyper_actor_loss': np.float64(0.0003387268545338884), 'behavior_loss': np.float64(0.3716380953788757)}

Episode step 18930, time diff 1.6809985637664795, total time dif 1671.0391273498535)
step: 18930 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(4.7074637), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.2888894), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08185988813638687), 'actor_loss': np.float64(-0.997857391834259), 'hyper_actor_loss': np.float64(0.0003414689184864983), 'behavior_loss': np.float64(0.37136845886707304)}

Episode step 18940, time diff 1.417111873626709, total time dif 1672.72012591362)
step: 18940 @ episode report: {'average_total_reward': np.float32(11.76889), 'reward_variance': np.float32(4.456564), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07041240893304349), 'actor_loss': np.float64(-0.9951258897781372), 'hyper_actor_loss': np.float64(0.0003831045964034274), 'behavior_loss': np.float64(0.3885611712932587)}

Episode step 18950, time diff 1.465285062789917, total time dif 1674.1372377872467)
step: 18950 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(1.4921725), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07983772531151771), 'actor_loss': np.float64(-0.992994076013565), 'hyper_actor_loss': np.float64(0.0005120264191646129), 'behavior_loss': np.float64(0.3616158694028854)}

Episode step 18960, time diff 1.4344065189361572, total time dif 1675.6025228500366)
step: 18960 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(1.7663323), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06532761715352535), 'actor_loss': np.float64(-1.012891376018524), 'hyper_actor_loss': np.float64(0.0005348452454200014), 'behavior_loss': np.float64(0.36070024967193604)}

Episode step 18970, time diff 1.4904820919036865, total time dif 1677.0369293689728)
step: 18970 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(3.2779808), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06425707750022411), 'actor_loss': np.float64(-0.9750782012939453), 'hyper_actor_loss': np.float64(0.00047143840347416697), 'behavior_loss': np.float64(0.3543843239545822)}

Episode step 18980, time diff 1.5449516773223877, total time dif 1678.5274114608765)
step: 18980 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.6615262), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07013203352689742), 'actor_loss': np.float64(-0.9918783605098724), 'hyper_actor_loss': np.float64(0.0004548666503978893), 'behavior_loss': np.float64(0.38291302919387815)}

Episode step 18990, time diff 1.7639989852905273, total time dif 1680.0723631381989)
step: 18990 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(3.642999), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06823759712278843), 'actor_loss': np.float64(-0.9890838146209717), 'hyper_actor_loss': np.float64(0.0004518212401308119), 'behavior_loss': np.float64(0.41440813839435575)}

Episode step 19000, time diff 1.5838537216186523, total time dif 1681.8363621234894)
step: 19000 @ episode report: {'average_total_reward': np.float32(10.1), 'reward_variance': np.float32(1.1639755), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.079397177323699), 'actor_loss': np.float64(-0.983291631937027), 'hyper_actor_loss': np.float64(0.0004970880982000381), 'behavior_loss': np.float64(0.37281920909881594)}

Episode step 19010, time diff 1.630586862564087, total time dif 1683.420215845108)
step: 19010 @ episode report: {'average_total_reward': np.float32(9.263333), 'reward_variance': np.float32(1.4525442), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07080910615622997), 'actor_loss': np.float64(-0.9956271886825562), 'hyper_actor_loss': np.float64(0.00045406821591313927), 'behavior_loss': np.float64(0.39220036268234254)}

Episode step 19020, time diff 1.6374309062957764, total time dif 1685.0508027076721)
step: 19020 @ episode report: {'average_total_reward': np.float32(8.814445), 'reward_variance': np.float32(3.2071872), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06219227071851492), 'actor_loss': np.float64(-0.9788687229156494), 'hyper_actor_loss': np.float64(0.0003754499339265749), 'behavior_loss': np.float64(0.4023631036281586)}

Episode step 19030, time diff 1.6533777713775635, total time dif 1686.688233613968)
step: 19030 @ episode report: {'average_total_reward': np.float32(9.4388895), 'reward_variance': np.float32(3.214377), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06442642994225026), 'actor_loss': np.float64(-0.9813906610012054), 'hyper_actor_loss': np.float64(0.00036692449066322297), 'behavior_loss': np.float64(0.37595023214817047)}

Episode step 19040, time diff 1.6512939929962158, total time dif 1688.3416113853455)
step: 19040 @ episode report: {'average_total_reward': np.float32(8.114445), 'reward_variance': np.float32(5.5129647), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06546760462224484), 'actor_loss': np.float64(-0.9853234171867371), 'hyper_actor_loss': np.float64(0.0003588682360714301), 'behavior_loss': np.float64(0.37664628624916074)}

Episode step 19050, time diff 1.6286585330963135, total time dif 1689.9929053783417)
step: 19050 @ episode report: {'average_total_reward': np.float32(7.953334), 'reward_variance': np.float32(1.579674), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06616738848388196), 'actor_loss': np.float64(-0.9862509608268738), 'hyper_actor_loss': np.float64(0.00037597348855342714), 'behavior_loss': np.float64(0.37270013689994813)}

Episode step 19060, time diff 1.7324495315551758, total time dif 1691.621563911438)
step: 19060 @ episode report: {'average_total_reward': np.float32(8.565557), 'reward_variance': np.float32(1.7589748), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06257861852645874), 'actor_loss': np.float64(-0.9895657479763031), 'hyper_actor_loss': np.float64(0.0003606262995162979), 'behavior_loss': np.float64(0.3891347497701645)}

Episode step 19070, time diff 1.7965621948242188, total time dif 1693.3540134429932)
step: 19070 @ episode report: {'average_total_reward': np.float32(8.253334), 'reward_variance': np.float32(1.6354516), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06355975903570651), 'actor_loss': np.float64(-0.9795145571231842), 'hyper_actor_loss': np.float64(0.00033029103360604495), 'behavior_loss': np.float64(0.38196735382080077)}

Episode step 19080, time diff 2.0129711627960205, total time dif 1695.1505756378174)
step: 19080 @ episode report: {'average_total_reward': np.float32(7.604445), 'reward_variance': np.float32(1.5383012), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06682473570108413), 'actor_loss': np.float64(-0.9658977508544921), 'hyper_actor_loss': np.float64(0.00033049851190298797), 'behavior_loss': np.float64(0.3924741119146347)}

Episode step 19090, time diff 1.906637191772461, total time dif 1697.1635468006134)
step: 19090 @ episode report: {'average_total_reward': np.float32(8.092222), 'reward_variance': np.float32(1.6133343), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.288889), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07391445450484753), 'actor_loss': np.float64(-0.9932716667652131), 'hyper_actor_loss': np.float64(0.0003542195918271318), 'behavior_loss': np.float64(0.39955560564994813)}

Episode step 19100, time diff 1.8301165103912354, total time dif 1699.0701839923859)
step: 19100 @ episode report: {'average_total_reward': np.float32(7.7655554), 'reward_variance': np.float32(3.0542827), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07613800130784512), 'actor_loss': np.float64(-0.9935874164104461), 'hyper_actor_loss': np.float64(0.00034476510772947223), 'behavior_loss': np.float64(0.38040336668491365)}

Episode step 19110, time diff 1.817122220993042, total time dif 1700.900300502777)
step: 19110 @ episode report: {'average_total_reward': np.float32(8.626667), 'reward_variance': np.float32(2.4804497), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07205536216497421), 'actor_loss': np.float64(-1.0032429695129395), 'hyper_actor_loss': np.float64(0.0003283127036411315), 'behavior_loss': np.float64(0.39052545130252836)}

Episode step 19120, time diff 1.7504112720489502, total time dif 1702.7174227237701)
step: 19120 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(1.4169987), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06632102690637112), 'actor_loss': np.float64(-0.9821747422218323), 'hyper_actor_loss': np.float64(0.00035512077156454327), 'behavior_loss': np.float64(0.3647447407245636)}

Episode step 19130, time diff 1.6954128742218018, total time dif 1704.467833995819)
step: 19130 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(2.0396614), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06917350254952907), 'actor_loss': np.float64(-0.9931654095649719), 'hyper_actor_loss': np.float64(0.00037209743168205024), 'behavior_loss': np.float64(0.37632605731487273)}

Episode step 19140, time diff 1.7802200317382812, total time dif 1706.163246870041)
step: 19140 @ episode report: {'average_total_reward': np.float32(8.402224), 'reward_variance': np.float32(2.6739702), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0727062277495861), 'actor_loss': np.float64(-0.9960034608840942), 'hyper_actor_loss': np.float64(0.00030045133316889403), 'behavior_loss': np.float64(0.4055859476327896)}

Episode step 19150, time diff 1.934802770614624, total time dif 1707.9434669017792)
step: 19150 @ episode report: {'average_total_reward': np.float32(8.402224), 'reward_variance': np.float32(2.452514), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05625589974224567), 'actor_loss': np.float64(-0.9750701248645782), 'hyper_actor_loss': np.float64(0.0002828765340382233), 'behavior_loss': np.float64(0.37577562034130096)}

Episode step 19160, time diff 1.717759132385254, total time dif 1709.8782696723938)
step: 19160 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(2.9838285), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07658986710011959), 'actor_loss': np.float64(-0.9837652206420898), 'hyper_actor_loss': np.float64(0.0002799541238346137), 'behavior_loss': np.float64(0.35341785550117494)}

Episode step 19170, time diff 1.8495562076568604, total time dif 1711.596028804779)
step: 19170 @ episode report: {'average_total_reward': np.float32(9.114445), 'reward_variance': np.float32(1.1956556), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06209571771323681), 'actor_loss': np.float64(-0.9973330795764923), 'hyper_actor_loss': np.float64(0.0002843667854904197), 'behavior_loss': np.float64(0.3787727147340775)}

Episode step 19180, time diff 1.7346727848052979, total time dif 1713.445585012436)
step: 19180 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(2.4284637), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06759683564305305), 'actor_loss': np.float64(-0.9906976997852326), 'hyper_actor_loss': np.float64(0.00027132552640978246), 'behavior_loss': np.float64(0.376376149058342)}

Episode step 19190, time diff 1.7468550205230713, total time dif 1715.1802577972412)
step: 19190 @ episode report: {'average_total_reward': np.float32(8.738889), 'reward_variance': np.float32(2.890549), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06677932888269425), 'actor_loss': np.float64(-0.9858839392662049), 'hyper_actor_loss': np.float64(0.000277611066121608), 'behavior_loss': np.float64(0.37096880972385404)}

Episode step 19200, time diff 1.7845044136047363, total time dif 1716.9271128177643)
step: 19200 @ episode report: {'average_total_reward': np.float32(8.253333), 'reward_variance': np.float32(2.1526859), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06407845765352249), 'actor_loss': np.float64(-0.9906368434429169), 'hyper_actor_loss': np.float64(0.0002521204558433965), 'behavior_loss': np.float64(0.3722353607416153)}

Episode step 19210, time diff 1.6834118366241455, total time dif 1718.711617231369)
step: 19210 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(1.9098877), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06898902021348477), 'actor_loss': np.float64(-0.9778802931308747), 'hyper_actor_loss': np.float64(0.0002669456633157097), 'behavior_loss': np.float64(0.37612763941287997)}

Episode step 19220, time diff 1.7904038429260254, total time dif 1720.3950290679932)
step: 19220 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(2.6015747), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07323117181658745), 'actor_loss': np.float64(-0.9945302367210388), 'hyper_actor_loss': np.float64(0.0003156854334520176), 'behavior_loss': np.float64(0.352218297123909)}

Episode step 19230, time diff 3.3821029663085938, total time dif 1722.1854329109192)
step: 19230 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(3.1469455), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06262814216315746), 'actor_loss': np.float64(-1.0045171976089478), 'hyper_actor_loss': np.float64(0.00028533109871204944), 'behavior_loss': np.float64(0.366918158531189)}

Episode step 19240, time diff 3.144017219543457, total time dif 1725.5675358772278)
step: 19240 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(2.370841), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06940596178174019), 'actor_loss': np.float64(-0.9749322652816772), 'hyper_actor_loss': np.float64(0.0002730552456341684), 'behavior_loss': np.float64(0.34841513335704805)}

Episode step 19250, time diff 2.294478178024292, total time dif 1728.7115530967712)
step: 19250 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(2.7089884), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05505430269986391), 'actor_loss': np.float64(-0.986499035358429), 'hyper_actor_loss': np.float64(0.0002515173066058196), 'behavior_loss': np.float64(0.36194968819618223)}

Episode step 19260, time diff 2.0445291996002197, total time dif 1731.0060312747955)
step: 19260 @ episode report: {'average_total_reward': np.float32(9.463333), 'reward_variance': np.float32(3.7320995), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07125962041318416), 'actor_loss': np.float64(-0.9647517919540405), 'hyper_actor_loss': np.float64(0.0002697782576433383), 'behavior_loss': np.float64(0.3674640566110611)}

Episode step 19270, time diff 2.1804165840148926, total time dif 1733.0505604743958)
step: 19270 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(6.8427973), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0737602360546589), 'actor_loss': np.float64(-0.9998794317245483), 'hyper_actor_loss': np.float64(0.00027658087929012255), 'behavior_loss': np.float64(0.38190551698207853)}

Episode step 19280, time diff 2.074063301086426, total time dif 1735.2309770584106)
step: 19280 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(4.0925045), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08619055934250355), 'actor_loss': np.float64(-1.0061789989471435), 'hyper_actor_loss': np.float64(0.0002492989631718956), 'behavior_loss': np.float64(0.33996522426605225)}

Episode step 19290, time diff 3.037046194076538, total time dif 1737.305040359497)
step: 19290 @ episode report: {'average_total_reward': np.float32(8.23889), 'reward_variance': np.float32(3.9640305), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.073219321295619), 'actor_loss': np.float64(-1.0040367662906646), 'hyper_actor_loss': np.float64(0.00023911531607154757), 'behavior_loss': np.float64(0.3745471268892288)}

Episode step 19300, time diff 2.1642839908599854, total time dif 1740.3420865535736)
step: 19300 @ episode report: {'average_total_reward': np.float32(8.39), 'reward_variance': np.float32(1.1009989), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07093909159302711), 'actor_loss': np.float64(-0.9939815700054169), 'hyper_actor_loss': np.float64(0.00026830553833860903), 'behavior_loss': np.float64(0.35659153163433077)}

Episode step 19310, time diff 1.8841676712036133, total time dif 1742.5063705444336)
step: 19310 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(1.5134926), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07891377098858357), 'actor_loss': np.float64(-0.9824308395385742), 'hyper_actor_loss': np.float64(0.0002705490987864323), 'behavior_loss': np.float64(0.3759315550327301)}

Episode step 19320, time diff 2.087843418121338, total time dif 1744.3905382156372)
step: 19320 @ episode report: {'average_total_reward': np.float32(8.79), 'reward_variance': np.float32(3.0682333), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07214679345488548), 'actor_loss': np.float64(-0.9998546302318573), 'hyper_actor_loss': np.float64(0.0002638412246596999), 'behavior_loss': np.float64(0.37496383786201476)}

Episode step 19330, time diff 1.9708366394042969, total time dif 1746.4783816337585)
step: 19330 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(0.99635696), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07137441039085388), 'actor_loss': np.float64(-0.9989189147949219), 'hyper_actor_loss': np.float64(0.000338892953004688), 'behavior_loss': np.float64(0.3691782206296921)}

Episode step 19340, time diff 1.9181969165802002, total time dif 1748.4492182731628)
step: 19340 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.329802), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06875681132078171), 'actor_loss': np.float64(-0.9999415040016174), 'hyper_actor_loss': np.float64(0.0009575764706823975), 'behavior_loss': np.float64(0.37902897596359253)}

Episode step 19350, time diff 1.9323608875274658, total time dif 1750.367415189743)
step: 19350 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.8045628), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07026777416467667), 'actor_loss': np.float64(-0.9758817672729492), 'hyper_actor_loss': np.float64(0.0013653578935191035), 'behavior_loss': np.float64(0.36623617112636564)}

Episode step 19360, time diff 2.001096487045288, total time dif 1752.2997760772705)
step: 19360 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(3.8022335), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06561153568327427), 'actor_loss': np.float64(-0.9816475093364716), 'hyper_actor_loss': np.float64(0.0011589965433813632), 'behavior_loss': np.float64(0.38357571363449094)}

Episode step 19370, time diff 1.9487075805664062, total time dif 1754.3008725643158)
step: 19370 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(1.104785), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05801863595843315), 'actor_loss': np.float64(-0.9667932987213135), 'hyper_actor_loss': np.float64(0.0006364926492096856), 'behavior_loss': np.float64(0.3733405232429504)}

Episode step 19380, time diff 2.230835437774658, total time dif 1756.2495801448822)
step: 19380 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.7696251), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07003788836300373), 'actor_loss': np.float64(-0.9894170522689819), 'hyper_actor_loss': np.float64(0.00038923344982322305), 'behavior_loss': np.float64(0.37895157039165495)}

Episode step 19390, time diff 1.8992407321929932, total time dif 1758.4804155826569)
step: 19390 @ episode report: {'average_total_reward': np.float32(11.046668), 'reward_variance': np.float32(2.8124146), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.070401132106781), 'actor_loss': np.float64(-0.9937210202217102), 'hyper_actor_loss': np.float64(0.00028885921637993305), 'behavior_loss': np.float64(0.38229995369911196)}

Episode step 19400, time diff 2.111666679382324, total time dif 1760.3796563148499)
step: 19400 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(3.277532), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06978476420044899), 'actor_loss': np.float64(-0.9878104984760284), 'hyper_actor_loss': np.float64(0.0002879726642277092), 'behavior_loss': np.float64(0.37056815028190615)}

Episode step 19410, time diff 1.972402572631836, total time dif 1762.4913229942322)
step: 19410 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.5608754), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06719969399273396), 'actor_loss': np.float64(-0.9899547815322876), 'hyper_actor_loss': np.float64(0.0003018132614670321), 'behavior_loss': np.float64(0.39009983241558077)}

Episode step 19420, time diff 1.8885219097137451, total time dif 1764.463725566864)
step: 19420 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(0.78963506), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057021908648312095), 'actor_loss': np.float64(-0.9662215769290924), 'hyper_actor_loss': np.float64(0.00029367460520006715), 'behavior_loss': np.float64(0.39092021584510805)}

Episode step 19430, time diff 2.116673469543457, total time dif 1766.3522474765778)
step: 19430 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(2.8754616), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0694093368947506), 'actor_loss': np.float64(-0.9771555364131927), 'hyper_actor_loss': np.float64(0.0003006660146638751), 'behavior_loss': np.float64(0.3939733237028122)}

Episode step 19440, time diff 1.9517247676849365, total time dif 1768.4689209461212)
step: 19440 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(1.2998717), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08727975636720657), 'actor_loss': np.float64(-1.0136656522750855), 'hyper_actor_loss': np.float64(0.00029294191335793585), 'behavior_loss': np.float64(0.3760148674249649)}

Episode step 19450, time diff 2.0469565391540527, total time dif 1770.4206457138062)
step: 19450 @ episode report: {'average_total_reward': np.float32(11.295556), 'reward_variance': np.float32(3.096918), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06492619514465332), 'actor_loss': np.float64(-1.0104085028171539), 'hyper_actor_loss': np.float64(0.0002751151449047029), 'behavior_loss': np.float64(0.3764541268348694)}

Episode step 19460, time diff 2.0801949501037598, total time dif 1772.4676022529602)
step: 19460 @ episode report: {'average_total_reward': np.float32(10.44889), 'reward_variance': np.float32(2.7294374), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07374620512127876), 'actor_loss': np.float64(-0.9692728102207184), 'hyper_actor_loss': np.float64(0.00029314020357560364), 'behavior_loss': np.float64(0.4025729805231094)}

Episode step 19470, time diff 1.931138038635254, total time dif 1774.547797203064)
step: 19470 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.4127419), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0647878360003233), 'actor_loss': np.float64(-0.9849174857139588), 'hyper_actor_loss': np.float64(0.00031593995518051086), 'behavior_loss': np.float64(0.3892251491546631)}

Episode step 19480, time diff 1.840261459350586, total time dif 1776.4789352416992)
step: 19480 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.7168338), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05119631327688694), 'actor_loss': np.float64(-0.9835948586463928), 'hyper_actor_loss': np.float64(0.00028959289484191685), 'behavior_loss': np.float64(0.3919691532850266)}

Episode step 19490, time diff 2.2813587188720703, total time dif 1778.3191967010498)
step: 19490 @ episode report: {'average_total_reward': np.float32(10.197778), 'reward_variance': np.float32(5.0675497), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06809592097997666), 'actor_loss': np.float64(-0.9560925602912903), 'hyper_actor_loss': np.float64(0.0002852266828995198), 'behavior_loss': np.float64(0.41349533200263977)}

Episode step 19500, time diff 1.8173878192901611, total time dif 1780.6005554199219)
step: 19500 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(3.0766003), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07865966446697711), 'actor_loss': np.float64(-0.9883630812168122), 'hyper_actor_loss': np.float64(0.0002693617469049059), 'behavior_loss': np.float64(0.4339110732078552)}

Episode step 19510, time diff 1.8179690837860107, total time dif 1782.417943239212)
step: 19510 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(4.510481), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07461933009326457), 'actor_loss': np.float64(-1.0060658693313598), 'hyper_actor_loss': np.float64(0.0002724237594520673), 'behavior_loss': np.float64(0.4064364552497864)}

Episode step 19520, time diff 1.7356343269348145, total time dif 1784.235912322998)
step: 19520 @ episode report: {'average_total_reward': np.float32(9.785556), 'reward_variance': np.float32(1.9330631), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06670108959078788), 'actor_loss': np.float64(-0.985379421710968), 'hyper_actor_loss': np.float64(0.0003035420639207587), 'behavior_loss': np.float64(0.40973901748657227)}

Episode step 19530, time diff 1.7953596115112305, total time dif 1785.9715466499329)
step: 19530 @ episode report: {'average_total_reward': np.float32(10.995556), 'reward_variance': np.float32(2.4025736), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08010048549622298), 'actor_loss': np.float64(-0.984192717075348), 'hyper_actor_loss': np.float64(0.00028643807745538653), 'behavior_loss': np.float64(0.4235007166862488)}

Episode step 19540, time diff 1.8196098804473877, total time dif 1787.766906261444)
step: 19540 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(1.3792212), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07243916988372803), 'actor_loss': np.float64(-1.022406768798828), 'hyper_actor_loss': np.float64(0.0002779575894237496), 'behavior_loss': np.float64(0.4097584456205368)}

Episode step 19550, time diff 1.806056022644043, total time dif 1789.5865161418915)
step: 19550 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(2.0836797), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0845188744366169), 'actor_loss': np.float64(-0.999104768037796), 'hyper_actor_loss': np.float64(0.00027676300087478014), 'behavior_loss': np.float64(0.4235711008310318)}

Episode step 19560, time diff 1.787893295288086, total time dif 1791.3925721645355)
step: 19560 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(4.5432544), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808334290981292), 'actor_loss': np.float64(-1.0047576606273652), 'hyper_actor_loss': np.float64(0.0002704270431422628), 'behavior_loss': np.float64(0.4196274966001511)}

Episode step 19570, time diff 1.8484320640563965, total time dif 1793.1804654598236)
step: 19570 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(2.1662726), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06572099840268493), 'actor_loss': np.float64(-0.9776781499385834), 'hyper_actor_loss': np.float64(0.00027496498951222746), 'behavior_loss': np.float64(0.4390681266784668)}

Episode step 19580, time diff 1.7539339065551758, total time dif 1795.02889752388)
step: 19580 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.2157598), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06570441238582134), 'actor_loss': np.float64(-0.9584977865219116), 'hyper_actor_loss': np.float64(0.00028146947588538753), 'behavior_loss': np.float64(0.4401722639799118)}

Episode step 19590, time diff 1.8548922538757324, total time dif 1796.7828314304352)
step: 19590 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(4.0130887), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07223417200148105), 'actor_loss': np.float64(-0.997108006477356), 'hyper_actor_loss': np.float64(0.0002920874539995566), 'behavior_loss': np.float64(0.46309750378131864)}

Episode step 19600, time diff 1.8020286560058594, total time dif 1798.637723684311)
step: 19600 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(4.443124), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0621887493878603), 'actor_loss': np.float64(-0.9693375945091247), 'hyper_actor_loss': np.float64(0.0002847704396117479), 'behavior_loss': np.float64(0.4437233120203018)}

Episode step 19610, time diff 1.7586238384246826, total time dif 1800.4397523403168)
step: 19610 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(5.8451123), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06947592310607434), 'actor_loss': np.float64(-0.9764163494110107), 'hyper_actor_loss': np.float64(0.00026410358404973524), 'behavior_loss': np.float64(0.4424784302711487)}

Episode step 19620, time diff 1.8340675830841064, total time dif 1802.1983761787415)
step: 19620 @ episode report: {'average_total_reward': np.float32(10.061112), 'reward_variance': np.float32(1.987784), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533335), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081948481500148), 'actor_loss': np.float64(-0.9942817330360413), 'hyper_actor_loss': np.float64(0.0002842126734321937), 'behavior_loss': np.float64(0.4251425415277481)}

Episode step 19630, time diff 1.7724876403808594, total time dif 1804.0324437618256)
step: 19630 @ episode report: {'average_total_reward': np.float32(9.414445), 'reward_variance': np.float32(2.7433343), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07932951711118222), 'actor_loss': np.float64(-0.9979780972003937), 'hyper_actor_loss': np.float64(0.0002583437948487699), 'behavior_loss': np.float64(0.4436300486326218)}

Episode step 19640, time diff 1.812159538269043, total time dif 1805.8049314022064)
step: 19640 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(3.4176548), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07083399184048175), 'actor_loss': np.float64(-1.0076226890087128), 'hyper_actor_loss': np.float64(0.0002734744921326637), 'behavior_loss': np.float64(0.43299393355846405)}

Episode step 19650, time diff 2.0966365337371826, total time dif 1807.6170909404755)
step: 19650 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(1.8713872), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07418711073696613), 'actor_loss': np.float64(-0.9856368243694306), 'hyper_actor_loss': np.float64(0.0002656029697391205), 'behavior_loss': np.float64(0.41757823824882506)}

Episode step 19660, time diff 1.791478157043457, total time dif 1809.7137274742126)
step: 19660 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(2.8697302), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0721471730619669), 'actor_loss': np.float64(-0.9938513100147247), 'hyper_actor_loss': np.float64(0.0002693528542295098), 'behavior_loss': np.float64(0.41637989282608034)}

Episode step 19670, time diff 1.9077847003936768, total time dif 1811.505205631256)
step: 19670 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(1.9025301), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06092393733561039), 'actor_loss': np.float64(-0.9983380019664765), 'hyper_actor_loss': np.float64(0.0002622221320052631), 'behavior_loss': np.float64(0.36956129372119906)}

Episode step 19680, time diff 1.9800567626953125, total time dif 1813.4129903316498)
step: 19680 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(0.92890894), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0673324815928936), 'actor_loss': np.float64(-0.9841017425060272), 'hyper_actor_loss': np.float64(0.00028630302258534355), 'behavior_loss': np.float64(0.4457059681415558)}

Episode step 19690, time diff 1.9070427417755127, total time dif 1815.393047094345)
step: 19690 @ episode report: {'average_total_reward': np.float32(9.800001), 'reward_variance': np.float32(1.0760496), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07016365677118301), 'actor_loss': np.float64(-0.9765772581100464), 'hyper_actor_loss': np.float64(0.0003615280147641897), 'behavior_loss': np.float64(0.49527632296085355)}

Episode step 19700, time diff 1.8812720775604248, total time dif 1817.3000898361206)
step: 19700 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(3.3507965), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07441238686442375), 'actor_loss': np.float64(-1.0077474534511566), 'hyper_actor_loss': np.float64(0.00035489695728756486), 'behavior_loss': np.float64(0.4192063957452774)}

Episode step 19710, time diff 2.0160727500915527, total time dif 1819.181361913681)
step: 19710 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(1.517832), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06311992034316063), 'actor_loss': np.float64(-1.007782870531082), 'hyper_actor_loss': np.float64(0.00030955729307606815), 'behavior_loss': np.float64(0.4301732450723648)}

Episode step 19720, time diff 1.9797406196594238, total time dif 1821.1974346637726)
step: 19720 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.6441743), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05383569784462452), 'actor_loss': np.float64(-0.9651086926460266), 'hyper_actor_loss': np.float64(0.00027899158012587575), 'behavior_loss': np.float64(0.4428567111492157)}

Episode step 19730, time diff 2.0178136825561523, total time dif 1823.177175283432)
step: 19730 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.8066921), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07832530327141285), 'actor_loss': np.float64(-0.9722858190536499), 'hyper_actor_loss': np.float64(0.00029332760750548913), 'behavior_loss': np.float64(0.4460433483123779)}

Episode step 19740, time diff 1.9103789329528809, total time dif 1825.1949889659882)
step: 19740 @ episode report: {'average_total_reward': np.float32(10.434446), 'reward_variance': np.float32(2.9149256), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05877708327025175), 'actor_loss': np.float64(-1.014442354440689), 'hyper_actor_loss': np.float64(0.00028460677567636593), 'behavior_loss': np.float64(0.4596655428409576)}

Episode step 19750, time diff 1.8956372737884521, total time dif 1827.105367898941)
step: 19750 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(5.135804), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07791519723832607), 'actor_loss': np.float64(-0.9709477543830871), 'hyper_actor_loss': np.float64(0.0002744673329289071), 'behavior_loss': np.float64(0.4609982132911682)}

Episode step 19760, time diff 1.9077520370483398, total time dif 1829.0010051727295)
step: 19760 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(1.7424263), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0718983095139265), 'actor_loss': np.float64(-0.9919225871562958), 'hyper_actor_loss': np.float64(0.00029210743377916515), 'behavior_loss': np.float64(0.46706363260746003)}

Episode step 19770, time diff 1.959937334060669, total time dif 1830.9087572097778)
step: 19770 @ episode report: {'average_total_reward': np.float32(9.6877775), 'reward_variance': np.float32(3.1136153), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06653563193976879), 'actor_loss': np.float64(-0.9865804612636566), 'hyper_actor_loss': np.float64(0.00029728553199674933), 'behavior_loss': np.float64(0.4799043834209442)}

Episode step 19780, time diff 1.9767708778381348, total time dif 1832.8686945438385)
step: 19780 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(0.78656316), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055722626298666), 'actor_loss': np.float64(-0.9690456807613372), 'hyper_actor_loss': np.float64(0.00030917320982553064), 'behavior_loss': np.float64(0.4761323481798172)}

Episode step 19790, time diff 1.9966528415679932, total time dif 1834.8454654216766)
step: 19790 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(3.31766), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06595545373857022), 'actor_loss': np.float64(-0.976554024219513), 'hyper_actor_loss': np.float64(0.0003801723592914641), 'behavior_loss': np.float64(0.4990919500589371)}

Episode step 19800, time diff 2.169796943664551, total time dif 1836.8421182632446)
step: 19800 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(3.3424714), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07084021270275116), 'actor_loss': np.float64(-0.9824909031391144), 'hyper_actor_loss': np.float64(0.00045264070213306694), 'behavior_loss': np.float64(0.4610689848661423)}

Episode step 19810, time diff 2.033550977706909, total time dif 1839.0119152069092)
step: 19810 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(2.483359), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0693012110888958), 'actor_loss': np.float64(-1.0017950356006622), 'hyper_actor_loss': np.float64(0.0004638371814507991), 'behavior_loss': np.float64(0.5221090495586396)}

Episode step 19820, time diff 2.0118656158447266, total time dif 1841.045466184616)
step: 19820 @ episode report: {'average_total_reward': np.float32(8.077779), 'reward_variance': np.float32(2.1516795), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08022875729948283), 'actor_loss': np.float64(-1.0083789706230164), 'hyper_actor_loss': np.float64(0.0003615153604187071), 'behavior_loss': np.float64(0.5032247304916382)}

Episode step 19830, time diff 2.195469617843628, total time dif 1843.0573318004608)
step: 19830 @ episode report: {'average_total_reward': np.float32(7.704445), 'reward_variance': np.float32(1.92082), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07702795676887035), 'actor_loss': np.float64(-0.9882756769657135), 'hyper_actor_loss': np.float64(0.0003468254551989958), 'behavior_loss': np.float64(0.48183626532554624)}

Episode step 19840, time diff 2.2864394187927246, total time dif 1845.2528014183044)
step: 19840 @ episode report: {'average_total_reward': np.float32(7.567778), 'reward_variance': np.float32(2.6718624), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06629897095263004), 'actor_loss': np.float64(-0.989314979314804), 'hyper_actor_loss': np.float64(0.00032074472110252825), 'behavior_loss': np.float64(0.5339331030845642)}

Episode step 19850, time diff 2.2643866539001465, total time dif 1847.5392408370972)
step: 19850 @ episode report: {'average_total_reward': np.float32(6.7822227), 'reward_variance': np.float32(1.4752153), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0706488024443388), 'actor_loss': np.float64(-0.9703191339969635), 'hyper_actor_loss': np.float64(0.00032876297482289376), 'behavior_loss': np.float64(0.47407768964767455)}

Episode step 19860, time diff 2.2282915115356445, total time dif 1849.8036274909973)
step: 19860 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(3.2825794), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07353380396962166), 'actor_loss': np.float64(-0.9854020357131958), 'hyper_actor_loss': np.float64(0.0003559917735401541), 'behavior_loss': np.float64(0.5254377901554108)}

Episode step 19870, time diff 2.110806465148926, total time dif 1852.031919002533)
step: 19870 @ episode report: {'average_total_reward': np.float32(7.231111), 'reward_variance': np.float32(3.0622914), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07274742163717747), 'actor_loss': np.float64(-0.9862793564796448), 'hyper_actor_loss': np.float64(0.00034281778789591044), 'behavior_loss': np.float64(0.5115410119295121)}

Episode step 19880, time diff 2.058269739151001, total time dif 1854.1427254676819)
step: 19880 @ episode report: {'average_total_reward': np.float32(8.041111), 'reward_variance': np.float32(3.7783463), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06299875751137733), 'actor_loss': np.float64(-0.9877107918262482), 'hyper_actor_loss': np.float64(0.00033577413705643266), 'behavior_loss': np.float64(0.48833266496658323)}

Episode step 19890, time diff 2.0439820289611816, total time dif 1856.2009952068329)
step: 19890 @ episode report: {'average_total_reward': np.float32(8.016667), 'reward_variance': np.float32(3.2848709), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06700668055564166), 'actor_loss': np.float64(-0.9606068909168244), 'hyper_actor_loss': np.float64(0.00031374066602438686), 'behavior_loss': np.float64(0.5298784017562866)}

Episode step 19900, time diff 2.1133840084075928, total time dif 1858.244977235794)
step: 19900 @ episode report: {'average_total_reward': np.float32(7.2188888), 'reward_variance': np.float32(0.99985313), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07592127919197082), 'actor_loss': np.float64(-0.9847700834274292), 'hyper_actor_loss': np.float64(0.0003389109013369307), 'behavior_loss': np.float64(0.5409891545772553)}

Episode step 19910, time diff 2.3403818607330322, total time dif 1860.3583612442017)
step: 19910 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(1.050505), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08219753913581371), 'actor_loss': np.float64(-1.0138402581214905), 'hyper_actor_loss': np.float64(0.00031138993508648125), 'behavior_loss': np.float64(0.5349167436361313)}

Episode step 19920, time diff 2.249685287475586, total time dif 1862.6987431049347)
step: 19920 @ episode report: {'average_total_reward': np.float32(6.6577783), 'reward_variance': np.float32(2.5713294), 'max_total_reward': np.float32(8.77778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06030992120504379), 'actor_loss': np.float64(-0.974814909696579), 'hyper_actor_loss': np.float64(0.0003390581579878926), 'behavior_loss': np.float64(0.5166552603244782)}

Episode step 19930, time diff 2.164524555206299, total time dif 1864.9484283924103)
step: 19930 @ episode report: {'average_total_reward': np.float32(7.304445), 'reward_variance': np.float32(1.0383257), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07664634250104427), 'actor_loss': np.float64(-0.9823640584945679), 'hyper_actor_loss': np.float64(0.0003705713228555396), 'behavior_loss': np.float64(0.5209053575992584)}

Episode step 19940, time diff 2.034224510192871, total time dif 1867.1129529476166)
step: 19940 @ episode report: {'average_total_reward': np.float32(7.167778), 'reward_variance': np.float32(1.2740611), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0635623037815094), 'actor_loss': np.float64(-1.0062592208385468), 'hyper_actor_loss': np.float64(0.0003702149202581495), 'behavior_loss': np.float64(0.5290576726198196)}

Episode step 19950, time diff 2.0328407287597656, total time dif 1869.1471774578094)
step: 19950 @ episode report: {'average_total_reward': np.float32(7.504445), 'reward_variance': np.float32(1.2320297), 'max_total_reward': np.float32(8.77778), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06449754014611245), 'actor_loss': np.float64(-0.9616634845733643), 'hyper_actor_loss': np.float64(0.00034303720167372376), 'behavior_loss': np.float64(0.516538193821907)}

Episode step 19960, time diff 2.1584951877593994, total time dif 1871.1800181865692)
step: 19960 @ episode report: {'average_total_reward': np.float32(7.6800003), 'reward_variance': np.float32(3.2967114), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0760733313858509), 'actor_loss': np.float64(-0.9842622816562653), 'hyper_actor_loss': np.float64(0.00034633660688996313), 'behavior_loss': np.float64(0.5173329532146453)}

Episode step 19970, time diff 2.18290114402771, total time dif 1873.3385133743286)
step: 19970 @ episode report: {'average_total_reward': np.float32(7.667778), 'reward_variance': np.float32(2.13253), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07423100247979164), 'actor_loss': np.float64(-1.0197688460350036), 'hyper_actor_loss': np.float64(0.00031131026335060594), 'behavior_loss': np.float64(0.4901959538459778)}

Episode step 19980, time diff 1.9749913215637207, total time dif 1875.5214145183563)
step: 19980 @ episode report: {'average_total_reward': np.float32(7.11889), 'reward_variance': np.float32(2.431483), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06975189745426177), 'actor_loss': np.float64(-0.9936460077762603), 'hyper_actor_loss': np.float64(0.00029160292760934683), 'behavior_loss': np.float64(0.5643780559301377)}

Early stop manually
Exit completely without evaluation? (y/n) (default n):