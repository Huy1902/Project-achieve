Namespace(seed=7, cuda=-1, env_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.env', reward_func='mean_with_cost', max_step_per_episode=20, initial_temper=20, urm_log_path='output/ml1m/env/log/ml1m_user_env_lr0.001_reg0.0003.model.log', temper_sweet_point=0.9, temper_prob_lag=100, sasrec_n_layer=2, sasrec_d_model=32, sasrec_d_forward=64, sasrec_n_head=4, sasrec_dropout=0.1, critic_hidden_dims=[256, 64], critic_dropout_rate=0.2, gamma=0.9, n_iter=[50000], train_every_n_step=1, initial_greedy_epsilon=0.0, final_greedy_epsilon=0.0, elbow_greedy=0.1, check_episode=10, with_eval=False, save_path='output/ml1m/agents/hac_SASRec_actor0.0001_critic0.001_behave0.0001_hacoef0.1_niter50000_reg0.00001_ep0_noise0.1_bs64_epbs32_step20_topk1_seed7/model', episode_batch_size=32, batch_size=64, actor_lr=0.0001, critic_lr=0.001, actor_decay=1e-05, critic_decay=1e-05, target_mitigate_coef=0.01, behavior_lr=0.0001, behavior_decay=1e-05, hyper_actor_coef=0.1, slate_size=9, buffer_size=100000, start_timestamp=2000, noise_var=0.1, q_laplace_smoothness=0.5, topk_rate=1.0, empty_start_rate=0.0, device='cpu')
step: 0 @ episode report: {'average_total_reward': np.float64(0.0), 'reward_variance': np.float64(0.0), 'max_total_reward': np.float64(0.0), 'min_total_reward': np.float64(0.0), 'average_n_step': np.float64(0.0), 'max_n_step': np.float64(0.0), 'min_n_step': np.float64(0.0), 'buffer_size': 2048} @ step loss: {'critic_loss': np.float64(0.2494354546070099), 'actor_loss': np.float64(0.09359893202781677), 'hyper_actor_loss': np.float64(0.0891341045498848), 'behavior_loss': np.float64(1.052399754524231)}
step: 10 @ episode report: {'average_total_reward': np.float32(0.2511111), 'reward_variance': np.float32(0.08030124), 'max_total_reward': np.float32(0.79999995), 'min_total_reward': np.float32(-0.07777779), 'average_n_step': np.float32(2.5), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2368} @ step loss: {'critic_loss': np.float64(0.169151970744133), 'actor_loss': np.float64(-0.06519124992191791), 'hyper_actor_loss': np.float64(0.09028372764587403), 'behavior_loss': np.float64(1.0949015498161316)}
step: 20 @ episode report: {'average_total_reward': np.float32(0.6533334), 'reward_variance': np.float32(0.1947852), 'max_total_reward': np.float32(1.6777778), 'min_total_reward': np.float32(0.044444438), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2688} @ step loss: {'critic_loss': np.float64(0.11660691723227501), 'actor_loss': np.float64(-0.1274143822491169), 'hyper_actor_loss': np.float64(0.09234869033098221), 'behavior_loss': np.float64(0.9957338631153106)}
