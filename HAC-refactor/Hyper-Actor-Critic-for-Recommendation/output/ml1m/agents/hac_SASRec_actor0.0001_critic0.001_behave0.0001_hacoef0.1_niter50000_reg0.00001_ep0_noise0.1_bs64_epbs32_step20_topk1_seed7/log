Namespace(env_class='ML1MEnvironment_GPU', policy_class='SASRec', critic_class='GeneralCritic', agent_class='HAC', facade_class='OneStageFacade_HyperAction')
Loading environment
Environment arguments: 
Namespace(seed=19, batch_size=128, lr=0.001, epoch=2, model_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.model', loss='bce', l2_coef=0.0003, feature_dim=16, attn_n_head=2, hidden_dims=[256], dropout_rate=0.2, train_file='dataset/ml1m/ml1m_b_train.csv', val_file='dataset/ml1m/ml1m_b_test.csv', test_file='', n_worker=0, data_separator='@', user_meta_file='dataset/ml1m/user_info.npy', item_meta_file='dataset/ml1m/item_info.npy', max_seq_len=50, meta_data_separator=' ')
Loading raw data
init ml1m reader
Loading data filesLoad item meta data
Loading user response model
{'length': 5078, 'n_item': 1682, 'item_vec_size': 19, 'user_portrait_len': 24, 'max_seq_len': 50}
Load (checkpoint) from output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.model.checkpoint
Setup policy:
SASRec(
  (item_map): Linear(in_features=19, out_features=32, bias=True)
  (pos_emb): Embedding(50, 32)
  (emb_dropout): Dropout(p=0.1, inplace=False)
  (emb_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  (transformer): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
Setup critic:
GeneralCritic(
  (net): DNN(
    (layers): Sequential(
      (0): Linear(in_features=64, out_features=256, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.2, inplace=False)
      (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (4): Linear(in_features=256, out_features=64, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.2, inplace=False)
      (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (8): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Setup agent with data-specific facade
Namespace(seed=7, cuda=-1, env_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.env', reward_func='mean_with_cost', max_step_per_episode=20, initial_temper=20, urm_log_path='output/ml1m/env/log/ml1m_user_env_lr0.001_reg0.0003.model.log', temper_sweet_point=0.9, temper_prob_lag=100, sasrec_n_layer=2, sasrec_d_model=32, sasrec_d_forward=64, sasrec_n_head=4, sasrec_dropout=0.1, critic_hidden_dims=[256, 64], critic_dropout_rate=0.2, gamma=0.9, n_iter=[50000], train_every_n_step=1, initial_greedy_epsilon=0.0, final_greedy_epsilon=0.0, elbow_greedy=0.1, check_episode=10, with_eval=False, save_path='output/ml1m/agents/hac_SASRec_actor0.0001_critic0.001_behave0.0001_hacoef0.1_niter50000_reg0.00001_ep0_noise0.1_bs64_epbs32_step20_topk1_seed7/model', episode_batch_size=32, batch_size=64, actor_lr=0.0001, critic_lr=0.001, actor_decay=1e-05, critic_decay=1e-05, target_mitigate_coef=0.01, behavior_lr=0.0001, behavior_decay=1e-05, hyper_actor_coef=0.1, slate_size=9, buffer_size=100000, start_timestamp=2000, noise_var=0.1, q_laplace_smoothness=0.5, topk_rate=1.0, empty_start_rate=0.0, device='cpu')
Run procedures before training
Total 63 prepare steps
Training:
Episode step 0, time diff 0.2067866325378418, total time dif 0.0)
step: 0 @ episode report: {'average_total_reward': np.float64(0.0), 'reward_variance': np.float64(0.0), 'max_total_reward': np.float64(0.0), 'min_total_reward': np.float64(0.0), 'average_n_step': np.float64(0.0), 'max_n_step': np.float64(0.0), 'min_n_step': np.float64(0.0), 'buffer_size': 2048} @ step loss: {'critic_loss': np.float64(0.2494354546070099), 'actor_loss': np.float64(0.09359893202781677), 'hyper_actor_loss': np.float64(0.0891341045498848), 'behavior_loss': np.float64(1.052399754524231)}

Episode step 10, time diff 0.7613623142242432, total time dif 0.2067866325378418)
step: 10 @ episode report: {'average_total_reward': np.float32(0.2511111), 'reward_variance': np.float32(0.08030124), 'max_total_reward': np.float32(0.79999995), 'min_total_reward': np.float32(-0.07777779), 'average_n_step': np.float32(2.5), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2368} @ step loss: {'critic_loss': np.float64(0.169151970744133), 'actor_loss': np.float64(-0.06519124992191791), 'hyper_actor_loss': np.float64(0.09028372764587403), 'behavior_loss': np.float64(1.0949015498161316)}

Episode step 20, time diff 0.7611479759216309, total time dif 0.968148946762085)
step: 20 @ episode report: {'average_total_reward': np.float32(0.6533334), 'reward_variance': np.float32(0.1947852), 'max_total_reward': np.float32(1.6777778), 'min_total_reward': np.float32(0.044444438), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2688} @ step loss: {'critic_loss': np.float64(0.11660691723227501), 'actor_loss': np.float64(-0.1274143822491169), 'hyper_actor_loss': np.float64(0.09234869033098221), 'behavior_loss': np.float64(0.9957338631153106)}

Early stop manually
Exit completely without evaluation? (y/n) (default n):