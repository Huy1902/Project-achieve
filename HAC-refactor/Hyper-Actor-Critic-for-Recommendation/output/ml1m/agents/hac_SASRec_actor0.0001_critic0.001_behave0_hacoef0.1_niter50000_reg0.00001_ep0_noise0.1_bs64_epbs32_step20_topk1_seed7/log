Namespace(env_class='ML1MEnvironment_GPU', policy_class='SASRec', critic_class='GeneralCritic', agent_class='HAC', facade_class='OneStageFacade_HyperAction')
Loading environment
Environment arguments: 
Namespace(seed=19, batch_size=128, lr=0.001, epoch=2, model_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.model', loss='bce', l2_coef=0.0003, feature_dim=16, attn_n_head=2, hidden_dims=[256], dropout_rate=0.2, train_file='dataset/ml1m/ml1m_b_train.csv', val_file='dataset/ml1m/ml1m_b_test.csv', test_file='', n_worker=0, data_separator='@', user_meta_file='dataset/ml1m/user_info.npy', item_meta_file='dataset/ml1m/item_info.npy', max_seq_len=50, meta_data_separator=' ')
Loading raw data
init ml1m reader
Loading data filesLoad item meta data
Loading user response model
{'length': 5078, 'n_item': 1682, 'item_vec_size': 19, 'user_portrait_len': 24, 'max_seq_len': 50}
Load (checkpoint) from output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.model.checkpoint
Setup policy:
SASRec(
  (item_map): Linear(in_features=19, out_features=32, bias=True)
  (pos_emb): Embedding(50, 32)
  (emb_dropout): Dropout(p=0.1, inplace=False)
  (emb_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  (transformer): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
Setup critic:
GeneralCritic(
  (net): DNN(
    (layers): Sequential(
      (0): Linear(in_features=64, out_features=256, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.2, inplace=False)
      (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (4): Linear(in_features=256, out_features=64, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.2, inplace=False)
      (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (8): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Setup agent with data-specific facade
Namespace(seed=7, cuda=-1, env_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003.env', reward_func='mean_with_cost', max_step_per_episode=20, initial_temper=20, urm_log_path='output/ml1m/env/log/ml1m_user_env_lr0.001_reg0.0003.model.log', temper_sweet_point=0.9, temper_prob_lag=100, sasrec_n_layer=2, sasrec_d_model=32, sasrec_d_forward=64, sasrec_n_head=4, sasrec_dropout=0.1, critic_hidden_dims=[256, 64], critic_dropout_rate=0.2, gamma=0.9, n_iter=[50000], train_every_n_step=1, initial_greedy_epsilon=0.0, final_greedy_epsilon=0.0, elbow_greedy=0.1, check_episode=10, with_eval=False, save_path='output/ml1m/agents/hac_SASRec_actor0.0001_critic0.001_behave0_hacoef0.1_niter50000_reg0.00001_ep0_noise0.1_bs64_epbs32_step20_topk1_seed7/model', episode_batch_size=32, batch_size=64, actor_lr=0.0001, critic_lr=0.001, actor_decay=1e-05, critic_decay=1e-05, target_mitigate_coef=0.01, behavior_lr=0.0, behavior_decay=1e-05, hyper_actor_coef=0.1, slate_size=9, buffer_size=100000, start_timestamp=2000, noise_var=0.1, q_laplace_smoothness=0.5, topk_rate=1.0, empty_start_rate=0.0, device='cpu')
Run procedures before training
Total 63 prepare steps
Training:
Episode step 0, time diff 0.2576572895050049, total time dif 0.0)
step: 0 @ episode report: {'average_total_reward': np.float64(0.0), 'reward_variance': np.float64(0.0), 'max_total_reward': np.float64(0.0), 'min_total_reward': np.float64(0.0), 'average_n_step': np.float64(0.0), 'max_n_step': np.float64(0.0), 'min_n_step': np.float64(0.0), 'buffer_size': 2048} @ step loss: {'critic_loss': np.float64(0.2494354546070099), 'actor_loss': np.float64(0.09359893202781677), 'hyper_actor_loss': np.float64(0.0891341045498848), 'behavior_loss': np.float64(1.052399754524231)}

Episode step 10, time diff 1.1157736778259277, total time dif 0.2576572895050049)
step: 10 @ episode report: {'average_total_reward': np.float32(0.26777774), 'reward_variance': np.float32(0.07282592), 'max_total_reward': np.float32(0.6777778), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.7), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2368} @ step loss: {'critic_loss': np.float64(0.1689274199306965), 'actor_loss': np.float64(-0.047470304742455484), 'hyper_actor_loss': np.float64(0.08913536816835403), 'behavior_loss': np.float64(1.1916433691978454)}

Episode step 20, time diff 1.2249300479888916, total time dif 1.3734309673309326)
step: 20 @ episode report: {'average_total_reward': np.float32(0.4288889), 'reward_variance': np.float32(0.12040001), 'max_total_reward': np.float32(1.1666667), 'min_total_reward': np.float32(-0.077777795), 'average_n_step': np.float32(2.8), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 2688} @ step loss: {'critic_loss': np.float64(0.11212041676044464), 'actor_loss': np.float64(-0.05804857835173607), 'hyper_actor_loss': np.float64(0.08919432088732719), 'behavior_loss': np.float64(1.2560992240905762)}

Episode step 30, time diff 1.1373531818389893, total time dif 2.598361015319824)
step: 30 @ episode report: {'average_total_reward': np.float32(0.6044444), 'reward_variance': np.float32(0.054375313), 'max_total_reward': np.float32(1.0444446), 'min_total_reward': np.float32(0.31111106), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(3.0), 'buffer_size': 3008} @ step loss: {'critic_loss': np.float64(0.07648441307246685), 'actor_loss': np.float64(-0.125979695469141), 'hyper_actor_loss': np.float64(0.08899214416742325), 'behavior_loss': np.float64(1.2775025486946106)}

Episode step 40, time diff 1.0334503650665283, total time dif 3.7357141971588135)
step: 40 @ episode report: {'average_total_reward': np.float32(0.9122222), 'reward_variance': np.float32(0.1277395), 'max_total_reward': np.float32(1.8), 'min_total_reward': np.float32(0.5555555), 'average_n_step': np.float32(3.1), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 3328} @ step loss: {'critic_loss': np.float64(0.06804028637707234), 'actor_loss': np.float64(-0.2131920874118805), 'hyper_actor_loss': np.float64(0.0884235106408596), 'behavior_loss': np.float64(1.2852623224258424)}

Episode step 50, time diff 1.0702955722808838, total time dif 4.769164562225342)
step: 50 @ episode report: {'average_total_reward': np.float32(1.4000001), 'reward_variance': np.float32(0.15362962), 'max_total_reward': np.float32(1.9222224), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 3648} @ step loss: {'critic_loss': np.float64(0.057102839648723605), 'actor_loss': np.float64(-0.37793426513671874), 'hyper_actor_loss': np.float64(0.08723484948277474), 'behavior_loss': np.float64(1.26385555267334)}

Episode step 60, time diff 1.1161174774169922, total time dif 5.839460134506226)
step: 60 @ episode report: {'average_total_reward': np.float32(0.9611112), 'reward_variance': np.float32(0.25299388), 'max_total_reward': np.float32(1.9222224), 'min_total_reward': np.float32(0.044444438), 'average_n_step': np.float32(3.1), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(2.0), 'buffer_size': 3968} @ step loss: {'critic_loss': np.float64(0.052608133479952815), 'actor_loss': np.float64(-0.5274596959352493), 'hyper_actor_loss': np.float64(0.08516937717795373), 'behavior_loss': np.float64(1.3574729561805725)}

Episode step 70, time diff 1.166884183883667, total time dif 6.955577611923218)
step: 70 @ episode report: {'average_total_reward': np.float32(0.91), 'reward_variance': np.float32(0.02823334), 'max_total_reward': np.float32(1.1666667), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.0), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(3.0), 'buffer_size': 4288} @ step loss: {'critic_loss': np.float64(0.05258113332092762), 'actor_loss': np.float64(-0.5706496715545655), 'hyper_actor_loss': np.float64(0.08303062543272972), 'behavior_loss': np.float64(1.4348718523979187)}

Episode step 80, time diff 1.112196922302246, total time dif 8.122461795806885)
step: 80 @ episode report: {'average_total_reward': np.float32(1.0611112), 'reward_variance': np.float32(0.11375926), 'max_total_reward': np.float32(1.8), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.2), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 4608} @ step loss: {'critic_loss': np.float64(0.04766046516597271), 'actor_loss': np.float64(-0.6402624905109405), 'hyper_actor_loss': np.float64(0.08156856298446655), 'behavior_loss': np.float64(1.5583608627319336)}

Episode step 90, time diff 1.2857599258422852, total time dif 9.23465871810913)
step: 90 @ episode report: {'average_total_reward': np.float32(1.0366666), 'reward_variance': np.float32(0.1533593), 'max_total_reward': np.float32(1.8000002), 'min_total_reward': np.float32(0.43333328), 'average_n_step': np.float32(3.2), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 4928} @ step loss: {'critic_loss': np.float64(0.04268734417855739), 'actor_loss': np.float64(-0.776069051027298), 'hyper_actor_loss': np.float64(0.0806501142680645), 'behavior_loss': np.float64(1.5360880255699159)}

Episode step 100, time diff 1.4037797451019287, total time dif 10.520418643951416)
step: 100 @ episode report: {'average_total_reward': np.float32(1.0755556), 'reward_variance': np.float32(0.30473584), 'max_total_reward': np.float32(2.1666667), 'min_total_reward': np.float32(0.31111106), 'average_n_step': np.float32(3.3), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 5248} @ step loss: {'critic_loss': np.float64(0.0445308905094862), 'actor_loss': np.float64(-0.9102891743183136), 'hyper_actor_loss': np.float64(0.08039052188396453), 'behavior_loss': np.float64(1.5544912934303283)}

Episode step 110, time diff 1.2134284973144531, total time dif 11.924198389053345)
step: 110 @ episode report: {'average_total_reward': np.float32(2.6911113), 'reward_variance': np.float32(0.51920503), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 5568} @ step loss: {'critic_loss': np.float64(0.053461996093392375), 'actor_loss': np.float64(-1.141605281829834), 'hyper_actor_loss': np.float64(0.08020080104470254), 'behavior_loss': np.float64(1.5782356262207031)}

Episode step 120, time diff 0.9926457405090332, total time dif 13.137626886367798)
step: 120 @ episode report: {'average_total_reward': np.float32(4.2255554), 'reward_variance': np.float32(0.96728504), 'max_total_reward': np.float32(6.5333333), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 5888} @ step loss: {'critic_loss': np.float64(0.058392129838466644), 'actor_loss': np.float64(-1.3757655262947082), 'hyper_actor_loss': np.float64(0.08108350858092309), 'behavior_loss': np.float64(1.39469233751297)}

Episode step 130, time diff 0.9905617237091064, total time dif 14.130272626876831)
step: 130 @ episode report: {'average_total_reward': np.float32(4.0622225), 'reward_variance': np.float32(0.7607951), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 6208} @ step loss: {'critic_loss': np.float64(0.06664327308535575), 'actor_loss': np.float64(-1.3661776781082153), 'hyper_actor_loss': np.float64(0.082480738312006), 'behavior_loss': np.float64(1.2989808082580567)}

Episode step 140, time diff 0.9684827327728271, total time dif 15.120834350585938)
step: 140 @ episode report: {'average_total_reward': np.float32(4.5988894), 'reward_variance': np.float32(0.9936658), 'max_total_reward': np.float32(6.411112), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 6528} @ step loss: {'critic_loss': np.float64(0.0763431940227747), 'actor_loss': np.float64(-1.3464761257171631), 'hyper_actor_loss': np.float64(0.08301562890410423), 'behavior_loss': np.float64(1.2011704325675965)}

Episode step 150, time diff 1.0048036575317383, total time dif 16.089317083358765)
step: 150 @ episode report: {'average_total_reward': np.float32(4.5111113), 'reward_variance': np.float32(2.231679), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 6848} @ step loss: {'critic_loss': np.float64(0.08048605285584927), 'actor_loss': np.float64(-1.2777073979377747), 'hyper_actor_loss': np.float64(0.08339112028479576), 'behavior_loss': np.float64(1.0982056736946106)}

Episode step 160, time diff 1.6444072723388672, total time dif 17.094120740890503)
step: 160 @ episode report: {'average_total_reward': np.float32(5.035556), 'reward_variance': np.float32(0.7377235), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 7168} @ step loss: {'critic_loss': np.float64(0.11455038413405419), 'actor_loss': np.float64(-1.3125850915908814), 'hyper_actor_loss': np.float64(0.08332511782646179), 'behavior_loss': np.float64(1.098269534111023)}

Episode step 170, time diff 1.91927170753479, total time dif 18.73852801322937)
step: 170 @ episode report: {'average_total_reward': np.float32(4.623333), 'reward_variance': np.float32(0.89062834), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 7488} @ step loss: {'critic_loss': np.float64(0.12710584327578545), 'actor_loss': np.float64(-1.260278618335724), 'hyper_actor_loss': np.float64(0.08339682444930077), 'behavior_loss': np.float64(1.025510746240616)}

Episode step 180, time diff 1.9611797332763672, total time dif 20.65779972076416)
step: 180 @ episode report: {'average_total_reward': np.float32(6.0577784), 'reward_variance': np.float32(0.96510637), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 7808} @ step loss: {'critic_loss': np.float64(0.15265740528702737), 'actor_loss': np.float64(-1.27854083776474), 'hyper_actor_loss': np.float64(0.08335785195231438), 'behavior_loss': np.float64(1.0515625)}

Episode step 190, time diff 0.9385011196136475, total time dif 22.618979454040527)
step: 190 @ episode report: {'average_total_reward': np.float32(5.794444), 'reward_variance': np.float32(1.0889446), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 8128} @ step loss: {'critic_loss': np.float64(0.17688452899456025), 'actor_loss': np.float64(-1.262713074684143), 'hyper_actor_loss': np.float64(0.08365678638219834), 'behavior_loss': np.float64(1.009584504365921)}

Episode step 200, time diff 0.8140707015991211, total time dif 23.557480573654175)
step: 200 @ episode report: {'average_total_reward': np.float32(7.3922224), 'reward_variance': np.float32(2.4775822), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 8448} @ step loss: {'critic_loss': np.float64(0.18773146718740463), 'actor_loss': np.float64(-1.3579696536064148), 'hyper_actor_loss': np.float64(0.08367806896567345), 'behavior_loss': np.float64(0.9416295289993286)}

Episode step 210, time diff 0.8133866786956787, total time dif 24.371551275253296)
step: 210 @ episode report: {'average_total_reward': np.float32(6.694445), 'reward_variance': np.float32(1.1578333), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 8768} @ step loss: {'critic_loss': np.float64(0.21280146688222884), 'actor_loss': np.float64(-1.3822871446609497), 'hyper_actor_loss': np.float64(0.08409732654690742), 'behavior_loss': np.float64(0.9082363545894623)}

Episode step 220, time diff 0.7475872039794922, total time dif 25.184937953948975)
step: 220 @ episode report: {'average_total_reward': np.float32(7.343334), 'reward_variance': np.float32(1.4738138), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 9088} @ step loss: {'critic_loss': np.float64(0.2178659811615944), 'actor_loss': np.float64(-1.2855851769447326), 'hyper_actor_loss': np.float64(0.08443307131528854), 'behavior_loss': np.float64(0.8475275039672852)}

Episode step 230, time diff 0.6905593872070312, total time dif 25.932525157928467)
step: 230 @ episode report: {'average_total_reward': np.float32(6.8066664), 'reward_variance': np.float32(0.9462026), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 9408} @ step loss: {'critic_loss': np.float64(0.20413783192634583), 'actor_loss': np.float64(-1.2068894028663635), 'hyper_actor_loss': np.float64(0.08471975773572922), 'behavior_loss': np.float64(0.7866889953613281)}

Episode step 240, time diff 0.6513674259185791, total time dif 26.623084545135498)
step: 240 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(1.4020939), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 9728} @ step loss: {'critic_loss': np.float64(0.23087764531373978), 'actor_loss': np.float64(-1.3387312293052673), 'hyper_actor_loss': np.float64(0.08519020304083824), 'behavior_loss': np.float64(0.7757530570030212)}

Episode step 250, time diff 0.6453990936279297, total time dif 27.274451971054077)
step: 250 @ episode report: {'average_total_reward': np.float32(7.916667), 'reward_variance': np.float32(1.8201296), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 10048} @ step loss: {'critic_loss': np.float64(0.25153522938489914), 'actor_loss': np.float64(-1.35955810546875), 'hyper_actor_loss': np.float64(0.0863642267882824), 'behavior_loss': np.float64(0.6863439857959748)}

Episode step 260, time diff 0.7827985286712646, total time dif 27.919851064682007)
step: 260 @ episode report: {'average_total_reward': np.float32(6.421111), 'reward_variance': np.float32(1.6121101), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 10368} @ step loss: {'critic_loss': np.float64(0.2370930552482605), 'actor_loss': np.float64(-1.4514095306396484), 'hyper_actor_loss': np.float64(0.0880852185189724), 'behavior_loss': np.float64(0.6548619210720062)}

Episode step 270, time diff 0.7081115245819092, total time dif 28.70264959335327)
step: 270 @ episode report: {'average_total_reward': np.float32(4.537778), 'reward_variance': np.float32(1.4243258), 'max_total_reward': np.float32(6.2888885), 'min_total_reward': np.float32(3.0444448), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 10688} @ step loss: {'critic_loss': np.float64(0.22540740966796874), 'actor_loss': np.float64(-1.3412393808364869), 'hyper_actor_loss': np.float64(0.08953101709485053), 'behavior_loss': np.float64(0.6100662171840667)}

Episode step 280, time diff 0.7516641616821289, total time dif 29.41076111793518)
step: 280 @ episode report: {'average_total_reward': np.float32(3.44), 'reward_variance': np.float32(1.1706474), 'max_total_reward': np.float32(6.28889), 'min_total_reward': np.float32(2.1666665), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 11008} @ step loss: {'critic_loss': np.float64(0.2265202984213829), 'actor_loss': np.float64(-1.31496741771698), 'hyper_actor_loss': np.float64(0.09038927927613258), 'behavior_loss': np.float64(0.5909643054008484)}

Episode step 290, time diff 0.6852879524230957, total time dif 30.16242527961731)
step: 290 @ episode report: {'average_total_reward': np.float32(3.3277779), 'reward_variance': np.float32(0.71790755), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 11328} @ step loss: {'critic_loss': np.float64(0.2464676856994629), 'actor_loss': np.float64(-1.3863922834396363), 'hyper_actor_loss': np.float64(0.09134609028697013), 'behavior_loss': np.float64(0.5934676826000214)}

Episode step 300, time diff 0.6794745922088623, total time dif 30.847713232040405)
step: 300 @ episode report: {'average_total_reward': np.float32(3.5011113), 'reward_variance': np.float32(1.066184), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 11648} @ step loss: {'critic_loss': np.float64(0.23303904235363007), 'actor_loss': np.float64(-1.2905492067337037), 'hyper_actor_loss': np.float64(0.09184363409876824), 'behavior_loss': np.float64(0.598151582479477)}

Episode step 310, time diff 0.6946959495544434, total time dif 31.527187824249268)
step: 310 @ episode report: {'average_total_reward': np.float32(3.337778), 'reward_variance': np.float32(0.75921506), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 11968} @ step loss: {'critic_loss': np.float64(0.22655128836631774), 'actor_loss': np.float64(-1.3292084336280823), 'hyper_actor_loss': np.float64(0.0933712862432003), 'behavior_loss': np.float64(0.6050553798675538)}

Episode step 320, time diff 0.6720349788665771, total time dif 32.22188377380371)
step: 320 @ episode report: {'average_total_reward': np.float32(3.3400002), 'reward_variance': np.float32(0.34872097), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 12288} @ step loss: {'critic_loss': np.float64(0.2673676639795303), 'actor_loss': np.float64(-1.4604859232902527), 'hyper_actor_loss': np.float64(0.09338300153613091), 'behavior_loss': np.float64(0.610232025384903)}

Episode step 330, time diff 0.6917314529418945, total time dif 32.89391875267029)
step: 330 @ episode report: {'average_total_reward': np.float32(3.5033333), 'reward_variance': np.float32(0.5781), 'max_total_reward': np.float32(5.166667), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 12608} @ step loss: {'critic_loss': np.float64(0.2523247092962265), 'actor_loss': np.float64(-1.3674840927124023), 'hyper_actor_loss': np.float64(0.093064945936203), 'behavior_loss': np.float64(0.6085296511650086)}

Episode step 340, time diff 0.670210599899292, total time dif 33.58565020561218)
step: 340 @ episode report: {'average_total_reward': np.float32(2.8666668), 'reward_variance': np.float32(0.57849383), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 12928} @ step loss: {'critic_loss': np.float64(0.25175644010305404), 'actor_loss': np.float64(-1.2990317821502686), 'hyper_actor_loss': np.float64(0.09343776628375053), 'behavior_loss': np.float64(0.594388610124588)}

Episode step 350, time diff 0.6885478496551514, total time dif 34.255860805511475)
step: 350 @ episode report: {'average_total_reward': np.float32(2.266667), 'reward_variance': np.float32(1.136889), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(0.8000001), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 13248} @ step loss: {'critic_loss': np.float64(0.2570711329579353), 'actor_loss': np.float64(-1.398704218864441), 'hyper_actor_loss': np.float64(0.09394646361470223), 'behavior_loss': np.float64(0.6192900657653808)}

Episode step 360, time diff 0.8440659046173096, total time dif 34.944408655166626)
step: 360 @ episode report: {'average_total_reward': np.float32(2.1200001), 'reward_variance': np.float32(0.65182227), 'max_total_reward': np.float32(3.166667), 'min_total_reward': np.float32(0.5555555), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 13568} @ step loss: {'critic_loss': np.float64(0.21789560616016387), 'actor_loss': np.float64(-1.272520935535431), 'hyper_actor_loss': np.float64(0.09406391084194184), 'behavior_loss': np.float64(0.6257544696331024)}

Episode step 370, time diff 0.8044259548187256, total time dif 35.788474559783936)
step: 370 @ episode report: {'average_total_reward': np.float32(1.3344445), 'reward_variance': np.float32(0.24297409), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.67777777), 'average_n_step': np.float32(3.4), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 13888} @ step loss: {'critic_loss': np.float64(0.26292961537837983), 'actor_loss': np.float64(-1.3930282235145568), 'hyper_actor_loss': np.float64(0.09440144747495652), 'behavior_loss': np.float64(0.6221587181091308)}

Episode step 380, time diff 0.7560951709747314, total time dif 36.59290051460266)
step: 380 @ episode report: {'average_total_reward': np.float32(1.3222224), 'reward_variance': np.float32(0.27476546), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.55555546), 'average_n_step': np.float32(3.4), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 14208} @ step loss: {'critic_loss': np.float64(0.22321737706661224), 'actor_loss': np.float64(-1.3687384247779846), 'hyper_actor_loss': np.float64(0.09427611827850342), 'behavior_loss': np.float64(0.6513435184955597)}

Episode step 390, time diff 0.7192659378051758, total time dif 37.34899568557739)
step: 390 @ episode report: {'average_total_reward': np.float32(1.158889), 'reward_variance': np.float32(0.19029756), 'max_total_reward': np.float32(1.9222224), 'min_total_reward': np.float32(0.4333334), 'average_n_step': np.float32(3.2), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 14528} @ step loss: {'critic_loss': np.float64(0.25791487395763396), 'actor_loss': np.float64(-1.412390375137329), 'hyper_actor_loss': np.float64(0.09373768791556358), 'behavior_loss': np.float64(0.6667476058006286)}

Episode step 400, time diff 0.7690963745117188, total time dif 38.06826162338257)
step: 400 @ episode report: {'average_total_reward': np.float32(1.1200001), 'reward_variance': np.float32(0.1176494), 'max_total_reward': np.float32(1.9222224), 'min_total_reward': np.float32(0.43333334), 'average_n_step': np.float32(3.1), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 14848} @ step loss: {'critic_loss': np.float64(0.2520531013607979), 'actor_loss': np.float64(-1.2538461089134216), 'hyper_actor_loss': np.float64(0.09391436576843262), 'behavior_loss': np.float64(0.7541713237762451)}

Episode step 410, time diff 0.6482584476470947, total time dif 38.83735799789429)
step: 410 @ episode report: {'average_total_reward': np.float32(1.148889), 'reward_variance': np.float32(0.25296792), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.3), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 15168} @ step loss: {'critic_loss': np.float64(0.23350560963153838), 'actor_loss': np.float64(-1.1315145134925841), 'hyper_actor_loss': np.float64(0.09453714415431022), 'behavior_loss': np.float64(0.7466788947582245)}

Episode step 420, time diff 0.8450102806091309, total time dif 39.48561644554138)
step: 420 @ episode report: {'average_total_reward': np.float32(1.238889), 'reward_variance': np.float32(0.28687042), 'max_total_reward': np.float32(1.8000002), 'min_total_reward': np.float32(0.43333328), 'average_n_step': np.float32(3.5), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 15488} @ step loss: {'critic_loss': np.float64(0.2536542400717735), 'actor_loss': np.float64(-1.137174391746521), 'hyper_actor_loss': np.float64(0.09511291906237603), 'behavior_loss': np.float64(0.7335300385951996)}

Episode step 430, time diff 0.6872570514678955, total time dif 40.33062672615051)
step: 430 @ episode report: {'average_total_reward': np.float32(1.0488889), 'reward_variance': np.float32(0.10057285), 'max_total_reward': np.float32(1.6777779), 'min_total_reward': np.float32(0.67777777), 'average_n_step': np.float32(3.2), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 15808} @ step loss: {'critic_loss': np.float64(0.24701859205961227), 'actor_loss': np.float64(-1.1275581955909728), 'hyper_actor_loss': np.float64(0.09574747830629349), 'behavior_loss': np.float64(0.7332765400409699)}

Episode step 440, time diff 0.710536003112793, total time dif 41.01788377761841)
step: 440 @ episode report: {'average_total_reward': np.float32(1.5955558), 'reward_variance': np.float32(0.53810376), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 16128} @ step loss: {'critic_loss': np.float64(0.23712107241153718), 'actor_loss': np.float64(-1.0158765137195587), 'hyper_actor_loss': np.float64(0.09561916664242745), 'behavior_loss': np.float64(0.7469576835632324)}

Episode step 450, time diff 0.7160327434539795, total time dif 41.7284197807312)
step: 450 @ episode report: {'average_total_reward': np.float32(1.0466667), 'reward_variance': np.float32(0.10189631), 'max_total_reward': np.float32(1.8000002), 'min_total_reward': np.float32(0.5555556), 'average_n_step': np.float32(3.1), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 16448} @ step loss: {'critic_loss': np.float64(0.25250510275363924), 'actor_loss': np.float64(-1.0305347204208375), 'hyper_actor_loss': np.float64(0.09535670652985573), 'behavior_loss': np.float64(0.7559075891971588)}

Episode step 460, time diff 0.6861662864685059, total time dif 42.44445252418518)
step: 460 @ episode report: {'average_total_reward': np.float32(1.4100001), 'reward_variance': np.float32(0.46845555), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(0.43333334), 'average_n_step': np.float32(3.5), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 16768} @ step loss: {'critic_loss': np.float64(0.24453813433647156), 'actor_loss': np.float64(-1.0168282628059386), 'hyper_actor_loss': np.float64(0.09492968171834945), 'behavior_loss': np.float64(0.7623172402381897)}

Episode step 470, time diff 0.6233007907867432, total time dif 43.13061881065369)
step: 470 @ episode report: {'average_total_reward': np.float32(1.8955557), 'reward_variance': np.float32(0.5386718), 'max_total_reward': np.float32(3.044445), 'min_total_reward': np.float32(0.8000001), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 17088} @ step loss: {'critic_loss': np.float64(0.2193642795085907), 'actor_loss': np.float64(-0.9734496891498565), 'hyper_actor_loss': np.float64(0.09428907260298729), 'behavior_loss': np.float64(0.7183244168758393)}

Episode step 480, time diff 0.7187104225158691, total time dif 43.75391960144043)
step: 480 @ episode report: {'average_total_reward': np.float32(2.0688891), 'reward_variance': np.float32(0.37740254), 'max_total_reward': np.float32(3.2888892), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 17408} @ step loss: {'critic_loss': np.float64(0.2108977034687996), 'actor_loss': np.float64(-1.0368433952331544), 'hyper_actor_loss': np.float64(0.09372212290763855), 'behavior_loss': np.float64(0.7010450005531311)}

Episode step 490, time diff 0.6775367259979248, total time dif 44.4726300239563)
step: 490 @ episode report: {'average_total_reward': np.float32(3.1566668), 'reward_variance': np.float32(1.1821346), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(1.6777779), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 17728} @ step loss: {'critic_loss': np.float64(0.20140235722064972), 'actor_loss': np.float64(-1.0232245802879334), 'hyper_actor_loss': np.float64(0.09376093819737434), 'behavior_loss': np.float64(0.7338774144649506)}

Episode step 500, time diff 0.709625244140625, total time dif 45.150166749954224)
step: 500 @ episode report: {'average_total_reward': np.float32(4.4744444), 'reward_variance': np.float32(0.8361), 'max_total_reward': np.float32(6.288889), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 18048} @ step loss: {'critic_loss': np.float64(0.1744542881846428), 'actor_loss': np.float64(-1.1009945392608642), 'hyper_actor_loss': np.float64(0.09395186826586724), 'behavior_loss': np.float64(0.6900108873844146)}

Episode step 510, time diff 0.6803324222564697, total time dif 45.85979199409485)
step: 510 @ episode report: {'average_total_reward': np.float32(5.6844444), 'reward_variance': np.float32(0.34022728), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 18368} @ step loss: {'critic_loss': np.float64(0.21262765377759935), 'actor_loss': np.float64(-1.139256477355957), 'hyper_actor_loss': np.float64(0.0940332904458046), 'behavior_loss': np.float64(0.6733166575431824)}

Episode step 520, time diff 0.6712884902954102, total time dif 46.54012441635132)
step: 520 @ episode report: {'average_total_reward': np.float32(7.1555567), 'reward_variance': np.float32(2.6954327), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 18688} @ step loss: {'critic_loss': np.float64(0.18466853946447373), 'actor_loss': np.float64(-1.1166212797164916), 'hyper_actor_loss': np.float64(0.09397744536399841), 'behavior_loss': np.float64(0.6338123559951783)}

Episode step 530, time diff 0.6671056747436523, total time dif 47.21141290664673)
step: 530 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(1.7772957), 'max_total_reward': np.float32(9.655557), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 19008} @ step loss: {'critic_loss': np.float64(0.16904562935233117), 'actor_loss': np.float64(-1.1525265216827392), 'hyper_actor_loss': np.float64(0.09422419369220733), 'behavior_loss': np.float64(0.6321616768836975)}

Episode step 540, time diff 0.6952452659606934, total time dif 47.87851858139038)
step: 540 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(4.784753), 'max_total_reward': np.float32(13.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 19328} @ step loss: {'critic_loss': np.float64(0.159596186876297), 'actor_loss': np.float64(-1.2397468090057373), 'hyper_actor_loss': np.float64(0.09486456587910652), 'behavior_loss': np.float64(0.608165043592453)}

Episode step 550, time diff 0.6758358478546143, total time dif 48.573763847351074)
step: 550 @ episode report: {'average_total_reward': np.float32(7.867778), 'reward_variance': np.float32(3.201889), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 19648} @ step loss: {'critic_loss': np.float64(0.18490172177553177), 'actor_loss': np.float64(-1.2661630868911744), 'hyper_actor_loss': np.float64(0.09528746381402016), 'behavior_loss': np.float64(0.5843883454799652)}

Episode step 560, time diff 0.7007505893707275, total time dif 49.24959969520569)
step: 560 @ episode report: {'average_total_reward': np.float32(7.9044447), 'reward_variance': np.float32(2.021881), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 19968} @ step loss: {'critic_loss': np.float64(0.18610266745090484), 'actor_loss': np.float64(-1.264577615261078), 'hyper_actor_loss': np.float64(0.09555526822805405), 'behavior_loss': np.float64(0.5617686152458191)}

Episode step 570, time diff 0.6745386123657227, total time dif 49.950350284576416)
step: 570 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(2.1766963), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 20288} @ step loss: {'critic_loss': np.float64(0.20693184137344361), 'actor_loss': np.float64(-1.3429540514945983), 'hyper_actor_loss': np.float64(0.09548692032694817), 'behavior_loss': np.float64(0.5550110578536988)}

Episode step 580, time diff 0.8457202911376953, total time dif 50.62488889694214)
step: 580 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(2.8626487), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 20608} @ step loss: {'critic_loss': np.float64(0.17990729659795762), 'actor_loss': np.float64(-1.2498815178871154), 'hyper_actor_loss': np.float64(0.09484638795256614), 'behavior_loss': np.float64(0.524843281507492)}

Episode step 590, time diff 0.664682149887085, total time dif 51.470609188079834)
step: 590 @ episode report: {'average_total_reward': np.float32(8.414444), 'reward_variance': np.float32(5.551384), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 20928} @ step loss: {'critic_loss': np.float64(0.18026578053832054), 'actor_loss': np.float64(-1.2971485376358032), 'hyper_actor_loss': np.float64(0.09397749006748199), 'behavior_loss': np.float64(0.526651120185852)}

Episode step 600, time diff 0.6864101886749268, total time dif 52.13529133796692)
step: 600 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(1.970993), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 21248} @ step loss: {'critic_loss': np.float64(0.2021704360842705), 'actor_loss': np.float64(-1.2915201306343078), 'hyper_actor_loss': np.float64(0.09394703060388565), 'behavior_loss': np.float64(0.5199869722127914)}

Episode step 610, time diff 0.6807107925415039, total time dif 52.821701526641846)
step: 610 @ episode report: {'average_total_reward': np.float32(8.663333), 'reward_variance': np.float32(2.6946685), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 21568} @ step loss: {'critic_loss': np.float64(0.20413959473371507), 'actor_loss': np.float64(-1.356496560573578), 'hyper_actor_loss': np.float64(0.09395844116806984), 'behavior_loss': np.float64(0.5124448657035827)}

Episode step 620, time diff 0.6860067844390869, total time dif 53.50241231918335)
step: 620 @ episode report: {'average_total_reward': np.float32(8.626667), 'reward_variance': np.float32(1.4180791), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 21888} @ step loss: {'critic_loss': np.float64(0.20534664392471313), 'actor_loss': np.float64(-1.333957064151764), 'hyper_actor_loss': np.float64(0.09371783435344697), 'behavior_loss': np.float64(0.5179833739995956)}

Episode step 630, time diff 0.6779279708862305, total time dif 54.18841910362244)
step: 630 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(1.8228695), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 22208} @ step loss: {'critic_loss': np.float64(0.20938764065504073), 'actor_loss': np.float64(-1.2768661260604859), 'hyper_actor_loss': np.float64(0.09319781586527824), 'behavior_loss': np.float64(0.5112640053033829)}

Episode step 640, time diff 0.6938903331756592, total time dif 54.86634707450867)
step: 640 @ episode report: {'average_total_reward': np.float32(9.436667), 'reward_variance': np.float32(3.5153096), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 22528} @ step loss: {'critic_loss': np.float64(0.19354897737503052), 'actor_loss': np.float64(-1.2889858484268188), 'hyper_actor_loss': np.float64(0.09238527491688728), 'behavior_loss': np.float64(0.5097373872995377)}

Episode step 650, time diff 0.681710958480835, total time dif 55.560237407684326)
step: 650 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(4.390715), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 22848} @ step loss: {'critic_loss': np.float64(0.20487200915813447), 'actor_loss': np.float64(-1.2941322565078734), 'hyper_actor_loss': np.float64(0.09155286997556686), 'behavior_loss': np.float64(0.5076209813356399)}

Episode step 660, time diff 0.712904691696167, total time dif 56.24194836616516)
step: 660 @ episode report: {'average_total_reward': np.float32(9.675556), 'reward_variance': np.float32(0.72318023), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 23168} @ step loss: {'critic_loss': np.float64(0.20469361245632173), 'actor_loss': np.float64(-1.323642361164093), 'hyper_actor_loss': np.float64(0.09100234434008599), 'behavior_loss': np.float64(0.49921904802322387)}

Episode step 670, time diff 0.652099609375, total time dif 56.95485305786133)
step: 670 @ episode report: {'average_total_reward': np.float32(9.887778), 'reward_variance': np.float32(1.0297642), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 23488} @ step loss: {'critic_loss': np.float64(0.23061905354261397), 'actor_loss': np.float64(-1.2896487712860107), 'hyper_actor_loss': np.float64(0.09070098027586937), 'behavior_loss': np.float64(0.5083281844854355)}

Episode step 680, time diff 0.7082564830780029, total time dif 57.60695266723633)
step: 680 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(2.2092462), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 23808} @ step loss: {'critic_loss': np.float64(0.22654718160629272), 'actor_loss': np.float64(-1.2315376043319701), 'hyper_actor_loss': np.float64(0.09014256149530411), 'behavior_loss': np.float64(0.5210551023483276)}

Episode step 690, time diff 0.6640660762786865, total time dif 58.31520915031433)
step: 690 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(2.7119265), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 24128} @ step loss: {'critic_loss': np.float64(0.20857212841510772), 'actor_loss': np.float64(-1.2434746742248535), 'hyper_actor_loss': np.float64(0.08974525779485702), 'behavior_loss': np.float64(0.49828066527843473)}

Episode step 700, time diff 0.6670758724212646, total time dif 58.97927522659302)
step: 700 @ episode report: {'average_total_reward': np.float32(9.636668), 'reward_variance': np.float32(1.4945202), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 24448} @ step loss: {'critic_loss': np.float64(0.22071928530931473), 'actor_loss': np.float64(-1.2419723749160767), 'hyper_actor_loss': np.float64(0.08843220248818398), 'behavior_loss': np.float64(0.5023892462253571)}

Episode step 710, time diff 0.6300697326660156, total time dif 59.64635109901428)
step: 710 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(1.1431906), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 24768} @ step loss: {'critic_loss': np.float64(0.18612681180238724), 'actor_loss': np.float64(-1.209662425518036), 'hyper_actor_loss': np.float64(0.0875724770128727), 'behavior_loss': np.float64(0.4930341809988022)}

Episode step 720, time diff 0.667243242263794, total time dif 60.2764208316803)
step: 720 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(1.6469437), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 25088} @ step loss: {'critic_loss': np.float64(0.21435724347829818), 'actor_loss': np.float64(-1.2430836796760558), 'hyper_actor_loss': np.float64(0.08740559592843056), 'behavior_loss': np.float64(0.5038051038980484)}

Episode step 730, time diff 0.6701991558074951, total time dif 60.94366407394409)
step: 730 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(2.100989), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.2888894), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 25408} @ step loss: {'critic_loss': np.float64(0.20154957622289657), 'actor_loss': np.float64(-1.2340540051460267), 'hyper_actor_loss': np.float64(0.08687403872609138), 'behavior_loss': np.float64(0.5178677320480347)}

Episode step 740, time diff 0.8160417079925537, total time dif 61.61386322975159)
step: 740 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(2.7487862), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 25728} @ step loss: {'critic_loss': np.float64(0.2168823778629303), 'actor_loss': np.float64(-1.248787558078766), 'hyper_actor_loss': np.float64(0.08656174093484878), 'behavior_loss': np.float64(0.49093486070632936)}

Episode step 750, time diff 0.6339645385742188, total time dif 62.42990493774414)
step: 750 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(1.2619952), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 26048} @ step loss: {'critic_loss': np.float64(0.2105729103088379), 'actor_loss': np.float64(-1.205911386013031), 'hyper_actor_loss': np.float64(0.08586809933185577), 'behavior_loss': np.float64(0.48975775241851804)}

Episode step 760, time diff 0.6857349872589111, total time dif 63.06386947631836)
step: 760 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.4534187), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.411112), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 26368} @ step loss: {'critic_loss': np.float64(0.21536863893270491), 'actor_loss': np.float64(-1.2801024436950683), 'hyper_actor_loss': np.float64(0.08511680141091346), 'behavior_loss': np.float64(0.49278652369976045)}

Episode step 770, time diff 0.6628298759460449, total time dif 63.74960446357727)
step: 770 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(1.7428265), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 26688} @ step loss: {'critic_loss': np.float64(0.18759987726807595), 'actor_loss': np.float64(-1.2143898129463195), 'hyper_actor_loss': np.float64(0.08336295187473297), 'behavior_loss': np.float64(0.5122836589813232)}

Episode step 780, time diff 0.6478312015533447, total time dif 64.41243433952332)
step: 780 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(2.1285546), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 27008} @ step loss: {'critic_loss': np.float64(0.20076586306095123), 'actor_loss': np.float64(-1.1814913153648376), 'hyper_actor_loss': np.float64(0.08337421342730522), 'behavior_loss': np.float64(0.5293615072965622)}

Episode step 790, time diff 0.7201218605041504, total time dif 65.06026554107666)
step: 790 @ episode report: {'average_total_reward': np.float32(9.775556), 'reward_variance': np.float32(2.787798), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 27328} @ step loss: {'critic_loss': np.float64(0.19021523147821426), 'actor_loss': np.float64(-1.231832218170166), 'hyper_actor_loss': np.float64(0.0826135367155075), 'behavior_loss': np.float64(0.5460339665412903)}

Episode step 800, time diff 0.6902637481689453, total time dif 65.78038740158081)
step: 800 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(3.3081734), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.2888894), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 27648} @ step loss: {'critic_loss': np.float64(0.165228021889925), 'actor_loss': np.float64(-1.2454231023788451), 'hyper_actor_loss': np.float64(0.08205713331699371), 'behavior_loss': np.float64(0.5241966873407364)}

Episode step 810, time diff 0.6894633769989014, total time dif 66.47065114974976)
step: 810 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(1.3383007), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 27968} @ step loss: {'critic_loss': np.float64(0.1686060257256031), 'actor_loss': np.float64(-1.3218080043792724), 'hyper_actor_loss': np.float64(0.08138210996985436), 'behavior_loss': np.float64(0.516085296869278)}

Episode step 820, time diff 0.6722128391265869, total time dif 67.16011452674866)
step: 820 @ episode report: {'average_total_reward': np.float32(9.238889), 'reward_variance': np.float32(2.8557594), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 28288} @ step loss: {'critic_loss': np.float64(0.19853901714086533), 'actor_loss': np.float64(-1.1849592685699464), 'hyper_actor_loss': np.float64(0.08069443479180335), 'behavior_loss': np.float64(0.5389792591333389)}

Episode step 830, time diff 0.6991019248962402, total time dif 67.83232736587524)
step: 830 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(1.0924001), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 28608} @ step loss: {'critic_loss': np.float64(0.20310538560152053), 'actor_loss': np.float64(-1.2324755549430848), 'hyper_actor_loss': np.float64(0.07983251363039016), 'behavior_loss': np.float64(0.5620769500732422)}

Episode step 840, time diff 0.6752912998199463, total time dif 68.53142929077148)
step: 840 @ episode report: {'average_total_reward': np.float32(8.077777), 'reward_variance': np.float32(1.247926), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 28928} @ step loss: {'critic_loss': np.float64(0.21363545656204225), 'actor_loss': np.float64(-1.219751763343811), 'hyper_actor_loss': np.float64(0.07926938608288765), 'behavior_loss': np.float64(0.5526759684085846)}

Episode step 850, time diff 0.6976041793823242, total time dif 69.20672059059143)
step: 850 @ episode report: {'average_total_reward': np.float32(7.8922224), 'reward_variance': np.float32(2.6901488), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 29248} @ step loss: {'critic_loss': np.float64(0.16594985574483873), 'actor_loss': np.float64(-1.202601969242096), 'hyper_actor_loss': np.float64(0.07819094508886337), 'behavior_loss': np.float64(0.5668884515762329)}

Episode step 860, time diff 0.6801600456237793, total time dif 69.90432476997375)
step: 860 @ episode report: {'average_total_reward': np.float32(6.1944447), 'reward_variance': np.float32(5.2618084), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(4.0), 'buffer_size': 29568} @ step loss: {'critic_loss': np.float64(0.17613017857074736), 'actor_loss': np.float64(-1.2434297919273376), 'hyper_actor_loss': np.float64(0.07658076286315918), 'behavior_loss': np.float64(0.6069604456424713)}

Episode step 870, time diff 0.6645722389221191, total time dif 70.58448481559753)
step: 870 @ episode report: {'average_total_reward': np.float32(6.421111), 'reward_variance': np.float32(1.7711973), 'max_total_reward': np.float32(9.533335), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 29888} @ step loss: {'critic_loss': np.float64(0.1663380205631256), 'actor_loss': np.float64(-1.3795182943344115), 'hyper_actor_loss': np.float64(0.07450410723686218), 'behavior_loss': np.float64(0.6164368093013763)}

Episode step 880, time diff 0.680124044418335, total time dif 71.24905705451965)
step: 880 @ episode report: {'average_total_reward': np.float32(5.3477774), 'reward_variance': np.float32(0.9074336), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 30208} @ step loss: {'critic_loss': np.float64(0.16804626435041428), 'actor_loss': np.float64(-1.2962445378303529), 'hyper_actor_loss': np.float64(0.0722695991396904), 'behavior_loss': np.float64(0.6382261216640472)}

Episode step 890, time diff 0.6619279384613037, total time dif 71.92918109893799)
step: 890 @ episode report: {'average_total_reward': np.float32(5.635556), 'reward_variance': np.float32(1.5216496), 'max_total_reward': np.float32(8.28889), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 30528} @ step loss: {'critic_loss': np.float64(0.16938701421022415), 'actor_loss': np.float64(-1.17035630941391), 'hyper_actor_loss': np.float64(0.07146859616041183), 'behavior_loss': np.float64(0.6520071506500245)}

Episode step 900, time diff 0.7134771347045898, total time dif 72.59110903739929)
step: 900 @ episode report: {'average_total_reward': np.float32(5.15), 'reward_variance': np.float32(1.0609686), 'max_total_reward': np.float32(7.411111), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 30848} @ step loss: {'critic_loss': np.float64(0.16690268516540527), 'actor_loss': np.float64(-1.1352463841438294), 'hyper_actor_loss': np.float64(0.07087796032428742), 'behavior_loss': np.float64(0.6240255057811737)}

Episode step 910, time diff 0.8536500930786133, total time dif 73.30458617210388)
step: 910 @ episode report: {'average_total_reward': np.float32(4.486667), 'reward_variance': np.float32(0.6648593), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.1666665), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 31168} @ step loss: {'critic_loss': np.float64(0.16625023782253265), 'actor_loss': np.float64(-1.1750429391860961), 'hyper_actor_loss': np.float64(0.07075565010309219), 'behavior_loss': np.float64(0.6655834257602692)}

Episode step 920, time diff 0.6697866916656494, total time dif 74.1582362651825)
step: 920 @ episode report: {'average_total_reward': np.float32(5.2477784), 'reward_variance': np.float32(1.7269148), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 31488} @ step loss: {'critic_loss': np.float64(0.16801079213619233), 'actor_loss': np.float64(-1.0688977360725402), 'hyper_actor_loss': np.float64(0.07080165967345238), 'behavior_loss': np.float64(0.6827394068241119)}

Episode step 930, time diff 0.703697919845581, total time dif 74.82802295684814)
step: 930 @ episode report: {'average_total_reward': np.float32(5.184445), 'reward_variance': np.float32(2.1503758), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 31808} @ step loss: {'critic_loss': np.float64(0.15646397769451142), 'actor_loss': np.float64(-1.10289124250412), 'hyper_actor_loss': np.float64(0.0713766224682331), 'behavior_loss': np.float64(0.7090236485004425)}

Episode step 940, time diff 0.7057232856750488, total time dif 75.53172087669373)
step: 940 @ episode report: {'average_total_reward': np.float32(5.086667), 'reward_variance': np.float32(1.2994769), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 32128} @ step loss: {'critic_loss': np.float64(0.1558794878423214), 'actor_loss': np.float64(-1.048254382610321), 'hyper_actor_loss': np.float64(0.07127085030078888), 'behavior_loss': np.float64(0.7249988317489624)}

Episode step 950, time diff 0.6669714450836182, total time dif 76.23744416236877)
step: 950 @ episode report: {'average_total_reward': np.float32(4.574445), 'reward_variance': np.float32(0.6660754), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 32448} @ step loss: {'critic_loss': np.float64(0.15414412394165994), 'actor_loss': np.float64(-1.0019470155239105), 'hyper_actor_loss': np.float64(0.07139643728733062), 'behavior_loss': np.float64(0.7532895624637603)}

Episode step 960, time diff 0.6895654201507568, total time dif 76.90441560745239)
step: 960 @ episode report: {'average_total_reward': np.float32(4.4622226), 'reward_variance': np.float32(1.8874127), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 32768} @ step loss: {'critic_loss': np.float64(0.1319755084812641), 'actor_loss': np.float64(-1.0169621229171752), 'hyper_actor_loss': np.float64(0.07097997516393661), 'behavior_loss': np.float64(0.7325096428394318)}

Episode step 970, time diff 0.6673991680145264, total time dif 77.59398102760315)
step: 970 @ episode report: {'average_total_reward': np.float32(3.9011111), 'reward_variance': np.float32(0.83364075), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 33088} @ step loss: {'critic_loss': np.float64(0.14782730266451835), 'actor_loss': np.float64(-0.9997876822948456), 'hyper_actor_loss': np.float64(0.07010368779301643), 'behavior_loss': np.float64(0.7193393409252167)}

Episode step 980, time diff 0.6634838581085205, total time dif 78.26138019561768)
step: 980 @ episode report: {'average_total_reward': np.float32(4.5744443), 'reward_variance': np.float32(0.67802584), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(2.9222224), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 33408} @ step loss: {'critic_loss': np.float64(0.14075680896639825), 'actor_loss': np.float64(-1.055476152896881), 'hyper_actor_loss': np.float64(0.06984917372465134), 'behavior_loss': np.float64(0.7255763292312623)}

Episode step 990, time diff 0.6655104160308838, total time dif 78.9248640537262)
step: 990 @ episode report: {'average_total_reward': np.float32(5.1111116), 'reward_variance': np.float32(1.4414078), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 33728} @ step loss: {'critic_loss': np.float64(0.1554380849003792), 'actor_loss': np.float64(-1.0512795567512512), 'hyper_actor_loss': np.float64(0.06915764585137367), 'behavior_loss': np.float64(0.7767767250537873)}

Episode step 1000, time diff 0.6861622333526611, total time dif 79.59037446975708)
step: 1000 @ episode report: {'average_total_reward': np.float32(4.711111), 'reward_variance': np.float32(2.5102966), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 34048} @ step loss: {'critic_loss': np.float64(0.14098287373781204), 'actor_loss': np.float64(-1.015036302804947), 'hyper_actor_loss': np.float64(0.06906408816576004), 'behavior_loss': np.float64(0.8132682383060456)}

Episode step 1010, time diff 0.6986732482910156, total time dif 80.27653670310974)
step: 1010 @ episode report: {'average_total_reward': np.float32(4.537778), 'reward_variance': np.float32(1.0507704), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 34368} @ step loss: {'critic_loss': np.float64(0.12663911208510398), 'actor_loss': np.float64(-0.9811846017837524), 'hyper_actor_loss': np.float64(0.06889500170946121), 'behavior_loss': np.float64(0.7819716691970825)}

Episode step 1020, time diff 0.6780483722686768, total time dif 80.97520995140076)
step: 1020 @ episode report: {'average_total_reward': np.float32(4.847778), 'reward_variance': np.float32(0.73847026), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 34688} @ step loss: {'critic_loss': np.float64(0.12267014160752296), 'actor_loss': np.float64(-0.9695284903049469), 'hyper_actor_loss': np.float64(0.06899110227823257), 'behavior_loss': np.float64(0.8130794405937195)}

Episode step 1030, time diff 0.675015926361084, total time dif 81.65325832366943)
step: 1030 @ episode report: {'average_total_reward': np.float32(5.2355556), 'reward_variance': np.float32(2.5168347), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35008} @ step loss: {'critic_loss': np.float64(0.1323098510503769), 'actor_loss': np.float64(-0.9781351685523987), 'hyper_actor_loss': np.float64(0.06863049119710922), 'behavior_loss': np.float64(0.854165780544281)}

Episode step 1040, time diff 0.6997642517089844, total time dif 82.32827425003052)
step: 1040 @ episode report: {'average_total_reward': np.float32(4.974445), 'reward_variance': np.float32(2.5524955), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35328} @ step loss: {'critic_loss': np.float64(0.11410757303237914), 'actor_loss': np.float64(-0.9739168822765351), 'hyper_actor_loss': np.float64(0.06812736392021179), 'behavior_loss': np.float64(0.8170341849327087)}

Episode step 1050, time diff 0.6555964946746826, total time dif 83.0280385017395)
step: 1050 @ episode report: {'average_total_reward': np.float32(4.847778), 'reward_variance': np.float32(1.7584708), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35648} @ step loss: {'critic_loss': np.float64(0.107595806568861), 'actor_loss': np.float64(-0.9669306755065918), 'hyper_actor_loss': np.float64(0.06759499609470368), 'behavior_loss': np.float64(0.8953451216220856)}

Episode step 1060, time diff 0.8297653198242188, total time dif 83.68363499641418)
step: 1060 @ episode report: {'average_total_reward': np.float32(4.55), 'reward_variance': np.float32(0.833537), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.2888894), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 35968} @ step loss: {'critic_loss': np.float64(0.13124669343233109), 'actor_loss': np.float64(-1.027079552412033), 'hyper_actor_loss': np.float64(0.0673169769346714), 'behavior_loss': np.float64(0.8663932681083679)}

Episode step 1070, time diff 0.712209939956665, total time dif 84.5134003162384)
step: 1070 @ episode report: {'average_total_reward': np.float32(3.6744447), 'reward_variance': np.float32(1.3339026), 'max_total_reward': np.float32(5.6555552), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 36288} @ step loss: {'critic_loss': np.float64(0.12513499855995178), 'actor_loss': np.float64(-0.9782060563564301), 'hyper_actor_loss': np.float64(0.06717842817306519), 'behavior_loss': np.float64(0.8915860056877136)}

Episode step 1080, time diff 0.6998276710510254, total time dif 85.22561025619507)
step: 1080 @ episode report: {'average_total_reward': np.float32(5.323333), 'reward_variance': np.float32(0.8184555), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 36608} @ step loss: {'critic_loss': np.float64(0.13356610536575317), 'actor_loss': np.float64(-0.9893280148506165), 'hyper_actor_loss': np.float64(0.06691515371203423), 'behavior_loss': np.float64(0.9244099617004394)}

Episode step 1090, time diff 0.7187347412109375, total time dif 85.9254379272461)
step: 1090 @ episode report: {'average_total_reward': np.float32(4.6622224), 'reward_variance': np.float32(0.4912642), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 36928} @ step loss: {'critic_loss': np.float64(0.11011976450681686), 'actor_loss': np.float64(-0.9839908480644226), 'hyper_actor_loss': np.float64(0.06654450595378876), 'behavior_loss': np.float64(0.8764580726623535)}

Episode step 1100, time diff 0.6770896911621094, total time dif 86.64417266845703)
step: 1100 @ episode report: {'average_total_reward': np.float32(4.325556), 'reward_variance': np.float32(1.2071124), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 37248} @ step loss: {'critic_loss': np.float64(0.1114541970193386), 'actor_loss': np.float64(-0.938367348909378), 'hyper_actor_loss': np.float64(0.0660997211933136), 'behavior_loss': np.float64(0.8666357576847077)}

Episode step 1110, time diff 0.6592059135437012, total time dif 87.32126235961914)
step: 1110 @ episode report: {'average_total_reward': np.float32(4.9355555), 'reward_variance': np.float32(1.219971), 'max_total_reward': np.float32(6.5333343), 'min_total_reward': np.float32(3.4111109), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 37568} @ step loss: {'critic_loss': np.float64(0.11972124949097633), 'actor_loss': np.float64(-0.9933450877666473), 'hyper_actor_loss': np.float64(0.06586159840226173), 'behavior_loss': np.float64(0.8991557896137238)}

Episode step 1120, time diff 0.7024037837982178, total time dif 87.98046827316284)
step: 1120 @ episode report: {'average_total_reward': np.float32(4.4255557), 'reward_variance': np.float32(1.4728405), 'max_total_reward': np.float32(6.5333333), 'min_total_reward': np.float32(2.9222226), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 37888} @ step loss: {'critic_loss': np.float64(0.11606948971748351), 'actor_loss': np.float64(-0.910701596736908), 'hyper_actor_loss': np.float64(0.06549419537186622), 'behavior_loss': np.float64(0.9294665455818176)}

Episode step 1130, time diff 0.7102437019348145, total time dif 88.68287205696106)
step: 1130 @ episode report: {'average_total_reward': np.float32(4.9477777), 'reward_variance': np.float32(1.3840506), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 38208} @ step loss: {'critic_loss': np.float64(0.12088796272873878), 'actor_loss': np.float64(-0.9725320160388946), 'hyper_actor_loss': np.float64(0.06489522308111191), 'behavior_loss': np.float64(0.9563309192657471)}

Episode step 1140, time diff 0.7040278911590576, total time dif 89.39311575889587)
step: 1140 @ episode report: {'average_total_reward': np.float32(4.2111115), 'reward_variance': np.float32(0.7061237), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 38528} @ step loss: {'critic_loss': np.float64(0.11577133350074291), 'actor_loss': np.float64(-0.9669896960258484), 'hyper_actor_loss': np.float64(0.06441944241523742), 'behavior_loss': np.float64(1.0057286143302917)}

Episode step 1150, time diff 0.6890213489532471, total time dif 90.09714365005493)
step: 1150 @ episode report: {'average_total_reward': np.float32(4.237778), 'reward_variance': np.float32(0.9133137), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 38848} @ step loss: {'critic_loss': np.float64(0.12377025783061982), 'actor_loss': np.float64(-0.9785391628742218), 'hyper_actor_loss': np.float64(0.06493040770292283), 'behavior_loss': np.float64(0.9803772985935211)}

Episode step 1160, time diff 0.6940271854400635, total time dif 90.78616499900818)
step: 1160 @ episode report: {'average_total_reward': np.float32(4.8988886), 'reward_variance': np.float32(0.985246), 'max_total_reward': np.float32(6.28889), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 39168} @ step loss: {'critic_loss': np.float64(0.11833812072873115), 'actor_loss': np.float64(-1.030969500541687), 'hyper_actor_loss': np.float64(0.06604327410459518), 'behavior_loss': np.float64(0.9327623248100281)}

Episode step 1170, time diff 0.6765732765197754, total time dif 91.48019218444824)
step: 1170 @ episode report: {'average_total_reward': np.float32(3.8744445), 'reward_variance': np.float32(1.3002977), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 39488} @ step loss: {'critic_loss': np.float64(0.13263960108160971), 'actor_loss': np.float64(-1.0356267809867858), 'hyper_actor_loss': np.float64(0.06733664572238922), 'behavior_loss': np.float64(0.9698779344558716)}

Episode step 1180, time diff 0.6993763446807861, total time dif 92.15676546096802)
step: 1180 @ episode report: {'average_total_reward': np.float32(4.425555), 'reward_variance': np.float32(1.653384), 'max_total_reward': np.float32(6.5333343), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 39808} @ step loss: {'critic_loss': np.float64(0.12309681251645088), 'actor_loss': np.float64(-1.04296715259552), 'hyper_actor_loss': np.float64(0.06831766963005066), 'behavior_loss': np.float64(0.896690171957016)}

Episode step 1190, time diff 0.6971948146820068, total time dif 92.8561418056488)
step: 1190 @ episode report: {'average_total_reward': np.float32(4.411111), 'reward_variance': np.float32(0.25730872), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 40128} @ step loss: {'critic_loss': np.float64(0.12503007799386978), 'actor_loss': np.float64(-0.9977471947669982), 'hyper_actor_loss': np.float64(0.06972871497273445), 'behavior_loss': np.float64(0.9540404796600341)}

Episode step 1200, time diff 0.6755764484405518, total time dif 93.55333662033081)
step: 1200 @ episode report: {'average_total_reward': np.float32(4.847778), 'reward_variance': np.float32(1.6761739), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 40448} @ step loss: {'critic_loss': np.float64(0.11062205657362938), 'actor_loss': np.float64(-1.0680237412452698), 'hyper_actor_loss': np.float64(0.07268455103039742), 'behavior_loss': np.float64(0.8517389893531799)}

Episode step 1210, time diff 0.6760907173156738, total time dif 94.22891306877136)
step: 1210 @ episode report: {'average_total_reward': np.float32(3.976667), 'reward_variance': np.float32(0.58951724), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(2.8000002), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 40768} @ step loss: {'critic_loss': np.float64(0.1096595399081707), 'actor_loss': np.float64(-1.1212966918945313), 'hyper_actor_loss': np.float64(0.07629546001553536), 'behavior_loss': np.float64(0.8503999710083008)}

Episode step 1220, time diff 0.8200533390045166, total time dif 94.90500378608704)
step: 1220 @ episode report: {'average_total_reward': np.float32(4.125556), 'reward_variance': np.float32(2.1174831), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(1.9222221), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 41088} @ step loss: {'critic_loss': np.float64(0.12509054318070412), 'actor_loss': np.float64(-1.1977334141731262), 'hyper_actor_loss': np.float64(0.07859672084450722), 'behavior_loss': np.float64(0.7492506742477417)}

Episode step 1230, time diff 0.695493221282959, total time dif 95.72505712509155)
step: 1230 @ episode report: {'average_total_reward': np.float32(3.4033332), 'reward_variance': np.float32(0.9280013), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.0444448), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 41408} @ step loss: {'critic_loss': np.float64(0.1287866435945034), 'actor_loss': np.float64(-1.2237908244132996), 'hyper_actor_loss': np.float64(0.07917045652866364), 'behavior_loss': np.float64(0.7787992656230927)}

Episode step 1240, time diff 0.6827621459960938, total time dif 96.42055034637451)
step: 1240 @ episode report: {'average_total_reward': np.float32(3.0544446), 'reward_variance': np.float32(0.6189988), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(1.5555557), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 41728} @ step loss: {'critic_loss': np.float64(0.11387440413236619), 'actor_loss': np.float64(-1.2108721137046814), 'hyper_actor_loss': np.float64(0.0804570235311985), 'behavior_loss': np.float64(0.7310675323009491)}

Episode step 1250, time diff 0.720531702041626, total time dif 97.1033124923706)
step: 1250 @ episode report: {'average_total_reward': np.float32(3.2644448), 'reward_variance': np.float32(0.4387853), 'max_total_reward': np.float32(4.2888894), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 42048} @ step loss: {'critic_loss': np.float64(0.11080055981874466), 'actor_loss': np.float64(-1.213252353668213), 'hyper_actor_loss': np.float64(0.08142694383859635), 'behavior_loss': np.float64(0.6976691126823426)}

Episode step 1260, time diff 0.7363243103027344, total time dif 97.82384419441223)
step: 1260 @ episode report: {'average_total_reward': np.float32(2.5688891), 'reward_variance': np.float32(0.625946), 'max_total_reward': np.float32(4.411112), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 42368} @ step loss: {'critic_loss': np.float64(0.14913978949189186), 'actor_loss': np.float64(-1.1397842645645142), 'hyper_actor_loss': np.float64(0.08163893073797227), 'behavior_loss': np.float64(0.7271451056003571)}

Episode step 1270, time diff 0.7240281105041504, total time dif 98.56016850471497)
step: 1270 @ episode report: {'average_total_reward': np.float32(2.73), 'reward_variance': np.float32(1.3674337), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 42688} @ step loss: {'critic_loss': np.float64(0.1367718443274498), 'actor_loss': np.float64(-1.0921186327934265), 'hyper_actor_loss': np.float64(0.0807230144739151), 'behavior_loss': np.float64(0.7415789484977722)}

Episode step 1280, time diff 0.6940186023712158, total time dif 99.28419661521912)
step: 1280 @ episode report: {'average_total_reward': np.float32(2.3200002), 'reward_variance': np.float32(0.19853835), 'max_total_reward': np.float32(3.166667), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.3), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 43008} @ step loss: {'critic_loss': np.float64(0.13606307357549668), 'actor_loss': np.float64(-1.0335086703300476), 'hyper_actor_loss': np.float64(0.07987193539738655), 'behavior_loss': np.float64(0.7029682278633118)}

Episode step 1290, time diff 0.6893918514251709, total time dif 99.97821521759033)
step: 1290 @ episode report: {'average_total_reward': np.float32(2.2077777), 'reward_variance': np.float32(0.45059386), 'max_total_reward': np.float32(3.2888892), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 43328} @ step loss: {'critic_loss': np.float64(0.13873443454504014), 'actor_loss': np.float64(-1.0025601029396056), 'hyper_actor_loss': np.float64(0.07906539365649223), 'behavior_loss': np.float64(0.7222739636898041)}

Episode step 1300, time diff 0.7068095207214355, total time dif 100.6676070690155)
step: 1300 @ episode report: {'average_total_reward': np.float32(2.22), 'reward_variance': np.float32(0.28634077), 'max_total_reward': np.float32(2.9222226), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 43648} @ step loss: {'critic_loss': np.float64(0.14346789121627807), 'actor_loss': np.float64(-0.9958913326263428), 'hyper_actor_loss': np.float64(0.07802585586905479), 'behavior_loss': np.float64(0.7128236174583436)}

Episode step 1310, time diff 0.705848217010498, total time dif 101.37441658973694)
step: 1310 @ episode report: {'average_total_reward': np.float32(2.0711112), 'reward_variance': np.float32(0.32862222), 'max_total_reward': np.float32(3.1666667), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 43968} @ step loss: {'critic_loss': np.float64(0.1250439241528511), 'actor_loss': np.float64(-1.0189863324165345), 'hyper_actor_loss': np.float64(0.07675193846225739), 'behavior_loss': np.float64(0.6997154951095581)}

Episode step 1320, time diff 0.7218575477600098, total time dif 102.08026480674744)
step: 1320 @ episode report: {'average_total_reward': np.float32(2.0566669), 'reward_variance': np.float32(0.040183943), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(1.6777779), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(4.0), 'buffer_size': 44288} @ step loss: {'critic_loss': np.float64(0.1328078493475914), 'actor_loss': np.float64(-1.1296070218086243), 'hyper_actor_loss': np.float64(0.07599717602133751), 'behavior_loss': np.float64(0.729014390707016)}

Episode step 1330, time diff 0.7833261489868164, total time dif 102.80212235450745)
step: 1330 @ episode report: {'average_total_reward': np.float32(1.72), 'reward_variance': np.float32(0.36078516), 'max_total_reward': np.float32(2.9222221), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 44608} @ step loss: {'critic_loss': np.float64(0.1348717153072357), 'actor_loss': np.float64(-1.0479527831077575), 'hyper_actor_loss': np.float64(0.07490034773945808), 'behavior_loss': np.float64(0.7280463933944702)}

Episode step 1340, time diff 0.7673943042755127, total time dif 103.58544850349426)
step: 1340 @ episode report: {'average_total_reward': np.float32(1.7344444), 'reward_variance': np.float32(0.13771483), 'max_total_reward': np.float32(2.166667), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 44928} @ step loss: {'critic_loss': np.float64(0.1391017183661461), 'actor_loss': np.float64(-0.9680399894714355), 'hyper_actor_loss': np.float64(0.07302556410431862), 'behavior_loss': np.float64(0.7366185903549194)}

Episode step 1350, time diff 0.777061939239502, total time dif 104.35284280776978)
step: 1350 @ episode report: {'average_total_reward': np.float32(1.8466667), 'reward_variance': np.float32(0.24485926), 'max_total_reward': np.float32(2.8000002), 'min_total_reward': np.float32(0.92222244), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 45248} @ step loss: {'critic_loss': np.float64(0.13082632496953012), 'actor_loss': np.float64(-0.9384183824062348), 'hyper_actor_loss': np.float64(0.07092626690864563), 'behavior_loss': np.float64(0.8002150774002075)}

Episode step 1360, time diff 0.7594590187072754, total time dif 105.12990474700928)
step: 1360 @ episode report: {'average_total_reward': np.float32(2.007778), 'reward_variance': np.float32(0.3143222), 'max_total_reward': np.float32(2.9222221), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 45568} @ step loss: {'critic_loss': np.float64(0.11923222243785858), 'actor_loss': np.float64(-0.9385389089584351), 'hyper_actor_loss': np.float64(0.06990714147686958), 'behavior_loss': np.float64(0.801354855298996)}

Episode step 1370, time diff 0.770256519317627, total time dif 105.88936376571655)
step: 1370 @ episode report: {'average_total_reward': np.float32(1.8344446), 'reward_variance': np.float32(0.22887535), 'max_total_reward': np.float32(2.9222224), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 45888} @ step loss: {'critic_loss': np.float64(0.13281626775860786), 'actor_loss': np.float64(-0.9626036584377289), 'hyper_actor_loss': np.float64(0.06926820948719978), 'behavior_loss': np.float64(0.7978390693664551)}

Episode step 1380, time diff 0.9253218173980713, total time dif 106.65962028503418)
step: 1380 @ episode report: {'average_total_reward': np.float32(2.1444447), 'reward_variance': np.float32(0.58271617), 'max_total_reward': np.float32(3.9222226), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 46208} @ step loss: {'critic_loss': np.float64(0.13180470168590547), 'actor_loss': np.float64(-0.9742282152175903), 'hyper_actor_loss': np.float64(0.06856980845332146), 'behavior_loss': np.float64(0.8642444014549255)}

Episode step 1390, time diff 0.7714402675628662, total time dif 107.58494210243225)
step: 1390 @ episode report: {'average_total_reward': np.float32(2.1055555), 'reward_variance': np.float32(0.360821), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 46528} @ step loss: {'critic_loss': np.float64(0.12622208818793296), 'actor_loss': np.float64(-0.9030826032161713), 'hyper_actor_loss': np.float64(0.06778382733464242), 'behavior_loss': np.float64(0.7942880988121033)}

Episode step 1400, time diff 0.766101598739624, total time dif 108.35638236999512)
step: 1400 @ episode report: {'average_total_reward': np.float32(2.0955558), 'reward_variance': np.float32(0.9222767), 'max_total_reward': np.float32(3.2888892), 'min_total_reward': np.float32(0.43333334), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 46848} @ step loss: {'critic_loss': np.float64(0.12897201254963875), 'actor_loss': np.float64(-0.9633419632911682), 'hyper_actor_loss': np.float64(0.06673037111759186), 'behavior_loss': np.float64(0.8898908436298371)}

Episode step 1410, time diff 0.7248780727386475, total time dif 109.12248396873474)
step: 1410 @ episode report: {'average_total_reward': np.float32(2.43), 'reward_variance': np.float32(0.5532853), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.3), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 47168} @ step loss: {'critic_loss': np.float64(0.12160411775112152), 'actor_loss': np.float64(-0.8876013219356537), 'hyper_actor_loss': np.float64(0.06551730111241341), 'behavior_loss': np.float64(0.9367165505886078)}

Episode step 1420, time diff 0.7964756488800049, total time dif 109.84736204147339)
step: 1420 @ episode report: {'average_total_reward': np.float32(3.2788892), 'reward_variance': np.float32(0.76169026), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 47488} @ step loss: {'critic_loss': np.float64(0.1126009076833725), 'actor_loss': np.float64(-0.9211863100528717), 'hyper_actor_loss': np.float64(0.06434339806437492), 'behavior_loss': np.float64(0.9130949318408966)}

Episode step 1430, time diff 0.759286642074585, total time dif 110.6438376903534)
step: 1430 @ episode report: {'average_total_reward': np.float32(2.5422225), 'reward_variance': np.float32(0.36814326), 'max_total_reward': np.float32(3.411111), 'min_total_reward': np.float32(1.8000002), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 47808} @ step loss: {'critic_loss': np.float64(0.10193514972925186), 'actor_loss': np.float64(-0.8794182896614074), 'hyper_actor_loss': np.float64(0.06298324391245842), 'behavior_loss': np.float64(0.8860108196735382)}

Episode step 1440, time diff 0.7269008159637451, total time dif 111.40312433242798)
step: 1440 @ episode report: {'average_total_reward': np.float32(2.94), 'reward_variance': np.float32(0.21168406), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 48128} @ step loss: {'critic_loss': np.float64(0.1286507323384285), 'actor_loss': np.float64(-0.9503657042980194), 'hyper_actor_loss': np.float64(0.06184379607439041), 'behavior_loss': np.float64(0.9499940991401672)}

Episode step 1450, time diff 0.748539924621582, total time dif 112.13002514839172)
step: 1450 @ episode report: {'average_total_reward': np.float32(3.4277782), 'reward_variance': np.float32(1.349241), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 48448} @ step loss: {'critic_loss': np.float64(0.11503329873085022), 'actor_loss': np.float64(-0.9355811059474946), 'hyper_actor_loss': np.float64(0.06070826165378094), 'behavior_loss': np.float64(0.9857049465179444)}

Episode step 1460, time diff 0.7872824668884277, total time dif 112.8785650730133)
step: 1460 @ episode report: {'average_total_reward': np.float32(3.464445), 'reward_variance': np.float32(0.40557545), 'max_total_reward': np.float32(4.411112), 'min_total_reward': np.float32(2.2888892), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 48768} @ step loss: {'critic_loss': np.float64(0.10811228305101395), 'actor_loss': np.float64(-0.9545298397541047), 'hyper_actor_loss': np.float64(0.05969817526638508), 'behavior_loss': np.float64(0.9722123384475708)}

Episode step 1470, time diff 0.736438512802124, total time dif 113.66584753990173)
step: 1470 @ episode report: {'average_total_reward': np.float32(3.44), 'reward_variance': np.float32(0.5646717), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 49088} @ step loss: {'critic_loss': np.float64(0.12760774418711662), 'actor_loss': np.float64(-0.9675633192062378), 'hyper_actor_loss': np.float64(0.05860304720699787), 'behavior_loss': np.float64(1.1135696470737457)}

Episode step 1480, time diff 0.8016781806945801, total time dif 114.40228605270386)
step: 1480 @ episode report: {'average_total_reward': np.float32(3.8622222), 'reward_variance': np.float32(0.26254815), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 49408} @ step loss: {'critic_loss': np.float64(0.10551571547985077), 'actor_loss': np.float64(-0.894896250963211), 'hyper_actor_loss': np.float64(0.05771789401769638), 'behavior_loss': np.float64(1.066887879371643)}

Episode step 1490, time diff 0.7483053207397461, total time dif 115.20396423339844)
step: 1490 @ episode report: {'average_total_reward': np.float32(3.8011112), 'reward_variance': np.float32(0.45406055), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 49728} @ step loss: {'critic_loss': np.float64(0.10416319295763969), 'actor_loss': np.float64(-0.937953794002533), 'hyper_actor_loss': np.float64(0.05696183554828167), 'behavior_loss': np.float64(1.1207578301429748)}

Episode step 1500, time diff 0.7653319835662842, total time dif 115.95226955413818)
step: 1500 @ episode report: {'average_total_reward': np.float32(4.1744447), 'reward_variance': np.float32(0.87965566), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.1666672), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 50048} @ step loss: {'critic_loss': np.float64(0.11078966781497002), 'actor_loss': np.float64(-0.9650566339492798), 'hyper_actor_loss': np.float64(0.05627352520823479), 'behavior_loss': np.float64(1.1588809847831727)}

Episode step 1510, time diff 0.7240009307861328, total time dif 116.71760153770447)
step: 1510 @ episode report: {'average_total_reward': np.float32(4.6111116), 'reward_variance': np.float32(0.7544693), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 50368} @ step loss: {'critic_loss': np.float64(0.10853536203503608), 'actor_loss': np.float64(-0.9300818562507629), 'hyper_actor_loss': np.float64(0.055577218905091284), 'behavior_loss': np.float64(1.1149213910102844)}

Episode step 1520, time diff 0.7255666255950928, total time dif 117.4416024684906)
step: 1520 @ episode report: {'average_total_reward': np.float32(5.1600003), 'reward_variance': np.float32(1.6987703), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 50688} @ step loss: {'critic_loss': np.float64(0.11110626608133316), 'actor_loss': np.float64(-0.9511412739753723), 'hyper_actor_loss': np.float64(0.05482283495366573), 'behavior_loss': np.float64(1.1900216937065125)}

Episode step 1530, time diff 0.7192568778991699, total time dif 118.1671690940857)
step: 1530 @ episode report: {'average_total_reward': np.float32(5.0233335), 'reward_variance': np.float32(1.4646038), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 51008} @ step loss: {'critic_loss': np.float64(0.11519902646541595), 'actor_loss': np.float64(-0.9483957409858703), 'hyper_actor_loss': np.float64(0.053978117555379866), 'behavior_loss': np.float64(1.229656207561493)}

Episode step 1540, time diff 0.8478519916534424, total time dif 118.88642597198486)
step: 1540 @ episode report: {'average_total_reward': np.float32(5.2722225), 'reward_variance': np.float32(2.5757592), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 51328} @ step loss: {'critic_loss': np.float64(0.11854840442538261), 'actor_loss': np.float64(-0.9580475807189941), 'hyper_actor_loss': np.float64(0.05313357375562191), 'behavior_loss': np.float64(1.19404456615448)}

Episode step 1550, time diff 0.7813332080841064, total time dif 119.7342779636383)
step: 1550 @ episode report: {'average_total_reward': np.float32(5.757778), 'reward_variance': np.float32(2.5267112), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 51648} @ step loss: {'critic_loss': np.float64(0.11542479991912842), 'actor_loss': np.float64(-0.9962638735771179), 'hyper_actor_loss': np.float64(0.05259600467979908), 'behavior_loss': np.float64(1.2374780416488647)}

Episode step 1560, time diff 0.7196292877197266, total time dif 120.51561117172241)
step: 1560 @ episode report: {'average_total_reward': np.float32(5.645556), 'reward_variance': np.float32(0.99265337), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 51968} @ step loss: {'critic_loss': np.float64(0.10753931552171707), 'actor_loss': np.float64(-0.9666141331195831), 'hyper_actor_loss': np.float64(0.052055930718779564), 'behavior_loss': np.float64(1.2474859833717347)}

Episode step 1570, time diff 0.7548017501831055, total time dif 121.23524045944214)
step: 1570 @ episode report: {'average_total_reward': np.float32(5.472223), 'reward_variance': np.float32(2.4661057), 'max_total_reward': np.float32(7.5333343), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 52288} @ step loss: {'critic_loss': np.float64(0.11489430516958236), 'actor_loss': np.float64(-0.9476454317569732), 'hyper_actor_loss': np.float64(0.05140382498502731), 'behavior_loss': np.float64(1.3539144039154052)}

Episode step 1580, time diff 0.7479400634765625, total time dif 121.99004220962524)
step: 1580 @ episode report: {'average_total_reward': np.float32(6.045556), 'reward_variance': np.float32(1.0615667), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 52608} @ step loss: {'critic_loss': np.float64(0.11160879507660866), 'actor_loss': np.float64(-0.985341089963913), 'hyper_actor_loss': np.float64(0.05082423612475395), 'behavior_loss': np.float64(1.2630208015441895)}

Episode step 1590, time diff 0.7987222671508789, total time dif 122.7379822731018)
step: 1590 @ episode report: {'average_total_reward': np.float32(6.17), 'reward_variance': np.float32(0.6814334), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 52928} @ step loss: {'critic_loss': np.float64(0.1038546696305275), 'actor_loss': np.float64(-0.9717732846736908), 'hyper_actor_loss': np.float64(0.05047558471560478), 'behavior_loss': np.float64(1.27862046957016)}

Episode step 1600, time diff 0.7366225719451904, total time dif 123.53670454025269)
step: 1600 @ episode report: {'average_total_reward': np.float32(6.9800005), 'reward_variance': np.float32(1.9992545), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 53248} @ step loss: {'critic_loss': np.float64(0.12186235263943672), 'actor_loss': np.float64(-0.9806372821331024), 'hyper_actor_loss': np.float64(0.05015006102621555), 'behavior_loss': np.float64(1.3458241820335388)}

Episode step 1610, time diff 0.7476143836975098, total time dif 124.27332711219788)
step: 1610 @ episode report: {'average_total_reward': np.float32(6.357778), 'reward_variance': np.float32(1.4220446), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 53568} @ step loss: {'critic_loss': np.float64(0.12218636944890023), 'actor_loss': np.float64(-0.959151703119278), 'hyper_actor_loss': np.float64(0.04996102005243301), 'behavior_loss': np.float64(1.3462090015411377)}

Episode step 1620, time diff 0.7760326862335205, total time dif 125.02094149589539)
step: 1620 @ episode report: {'average_total_reward': np.float32(6.582222), 'reward_variance': np.float32(1.7622029), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 53888} @ step loss: {'critic_loss': np.float64(0.12105313390493393), 'actor_loss': np.float64(-0.993389230966568), 'hyper_actor_loss': np.float64(0.04991474486887455), 'behavior_loss': np.float64(1.3364817261695863)}

Episode step 1630, time diff 0.7842633724212646, total time dif 125.7969741821289)
step: 1630 @ episode report: {'average_total_reward': np.float32(6.7188897), 'reward_variance': np.float32(1.9962479), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 54208} @ step loss: {'critic_loss': np.float64(0.11615084782242775), 'actor_loss': np.float64(-0.9743160545825958), 'hyper_actor_loss': np.float64(0.04971761144697666), 'behavior_loss': np.float64(1.2745653986930847)}

Episode step 1640, time diff 0.7580525875091553, total time dif 126.58123755455017)
step: 1640 @ episode report: {'average_total_reward': np.float32(7.6555557), 'reward_variance': np.float32(1.9780737), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 54528} @ step loss: {'critic_loss': np.float64(0.11069212555885315), 'actor_loss': np.float64(-0.9447858572006226), 'hyper_actor_loss': np.float64(0.049504606798291204), 'behavior_loss': np.float64(1.324631142616272)}

Episode step 1650, time diff 0.7729852199554443, total time dif 127.33929014205933)
step: 1650 @ episode report: {'average_total_reward': np.float32(6.1700006), 'reward_variance': np.float32(3.4431121), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 54848} @ step loss: {'critic_loss': np.float64(0.11755430549383164), 'actor_loss': np.float64(-0.9689173877239228), 'hyper_actor_loss': np.float64(0.049243923649191855), 'behavior_loss': np.float64(1.3840027093887328)}

Episode step 1660, time diff 0.745689868927002, total time dif 128.11227536201477)
step: 1660 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(4.1274614), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 55168} @ step loss: {'critic_loss': np.float64(0.11338675022125244), 'actor_loss': np.float64(-0.984330701828003), 'hyper_actor_loss': np.float64(0.048666490241885185), 'behavior_loss': np.float64(1.3166261434555053)}

Episode step 1670, time diff 0.67000412940979, total time dif 128.85796523094177)
step: 1670 @ episode report: {'average_total_reward': np.float32(6.345556), 'reward_variance': np.float32(1.9109007), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 55488} @ step loss: {'critic_loss': np.float64(0.1111357681453228), 'actor_loss': np.float64(-0.9570188462734223), 'hyper_actor_loss': np.float64(0.04823398664593696), 'behavior_loss': np.float64(1.3382938146591186)}

Episode step 1680, time diff 0.7140367031097412, total time dif 129.52796936035156)
step: 1680 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(1.5084798), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 55808} @ step loss: {'critic_loss': np.float64(0.12034132406115532), 'actor_loss': np.float64(-1.0029366612434387), 'hyper_actor_loss': np.float64(0.04797567576169968), 'behavior_loss': np.float64(1.3471880435943604)}

Episode step 1690, time diff 0.7500998973846436, total time dif 130.2420060634613)
step: 1690 @ episode report: {'average_total_reward': np.float32(8.941112), 'reward_variance': np.float32(4.0118036), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 56128} @ step loss: {'critic_loss': np.float64(0.1274153098464012), 'actor_loss': np.float64(-1.013675570487976), 'hyper_actor_loss': np.float64(0.04780353493988514), 'behavior_loss': np.float64(1.3370258212089539)}

Episode step 1700, time diff 0.9304454326629639, total time dif 130.99210596084595)
step: 1700 @ episode report: {'average_total_reward': np.float32(7.543334), 'reward_variance': np.float32(2.4580114), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 56448} @ step loss: {'critic_loss': np.float64(0.11886650621891022), 'actor_loss': np.float64(-1.0204230785369872), 'hyper_actor_loss': np.float64(0.04754940159618855), 'behavior_loss': np.float64(1.2983819127082825)}

Episode step 1710, time diff 0.7789268493652344, total time dif 131.9225513935089)
step: 1710 @ episode report: {'average_total_reward': np.float32(7.131111), 'reward_variance': np.float32(0.813995), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 56768} @ step loss: {'critic_loss': np.float64(0.09827543422579765), 'actor_loss': np.float64(-0.980120187997818), 'hyper_actor_loss': np.float64(0.047356630116701125), 'behavior_loss': np.float64(1.289714777469635)}

Episode step 1720, time diff 0.8021132946014404, total time dif 132.70147824287415)
step: 1720 @ episode report: {'average_total_reward': np.float32(8.32889), 'reward_variance': np.float32(1.9735851), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 57088} @ step loss: {'critic_loss': np.float64(0.10722055435180664), 'actor_loss': np.float64(-0.9717311680316925), 'hyper_actor_loss': np.float64(0.047362295165658), 'behavior_loss': np.float64(1.3653778433799744)}

Episode step 1730, time diff 0.8479223251342773, total time dif 133.5035915374756)
step: 1730 @ episode report: {'average_total_reward': np.float32(6.8966665), 'reward_variance': np.float32(2.6824956), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 57408} @ step loss: {'critic_loss': np.float64(0.11303300485014915), 'actor_loss': np.float64(-1.0209696471691132), 'hyper_actor_loss': np.float64(0.04728223532438278), 'behavior_loss': np.float64(1.300368320941925)}

Episode step 1740, time diff 0.8213791847229004, total time dif 134.35151386260986)
step: 1740 @ episode report: {'average_total_reward': np.float32(6.9455557), 'reward_variance': np.float32(1.0265795), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 57728} @ step loss: {'critic_loss': np.float64(0.12170513719320297), 'actor_loss': np.float64(-1.0250415623188018), 'hyper_actor_loss': np.float64(0.04759491980075836), 'behavior_loss': np.float64(1.3192745208740235)}

Episode step 1750, time diff 0.7965884208679199, total time dif 135.17289304733276)
step: 1750 @ episode report: {'average_total_reward': np.float32(7.506667), 'reward_variance': np.float32(4.558203), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(5.0), 'buffer_size': 58048} @ step loss: {'critic_loss': np.float64(0.09243343695998192), 'actor_loss': np.float64(-0.9522409737110138), 'hyper_actor_loss': np.float64(0.0476486649364233), 'behavior_loss': np.float64(1.3083633184432983)}

Episode step 1760, time diff 0.7920970916748047, total time dif 135.96948146820068)
step: 1760 @ episode report: {'average_total_reward': np.float32(6.745556), 'reward_variance': np.float32(2.1769), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 58368} @ step loss: {'critic_loss': np.float64(0.11649470254778863), 'actor_loss': np.float64(-1.0180832684040069), 'hyper_actor_loss': np.float64(0.04788400046527386), 'behavior_loss': np.float64(1.2543550729751587)}

Episode step 1770, time diff 0.7718038558959961, total time dif 136.7615785598755)
step: 1770 @ episode report: {'average_total_reward': np.float32(8.028891), 'reward_variance': np.float32(2.3187711), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 58688} @ step loss: {'critic_loss': np.float64(0.10993855148553848), 'actor_loss': np.float64(-1.006287169456482), 'hyper_actor_loss': np.float64(0.048224248737096784), 'behavior_loss': np.float64(1.2928189873695373)}

Episode step 1780, time diff 0.7494547367095947, total time dif 137.53338241577148)
step: 1780 @ episode report: {'average_total_reward': np.float32(6.6066666), 'reward_variance': np.float32(1.8051159), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 59008} @ step loss: {'critic_loss': np.float64(0.10822305157780647), 'actor_loss': np.float64(-1.0145496487617494), 'hyper_actor_loss': np.float64(0.04887923002243042), 'behavior_loss': np.float64(1.309666383266449)}

Episode step 1790, time diff 0.7611157894134521, total time dif 138.28283715248108)
step: 1790 @ episode report: {'average_total_reward': np.float32(6.8555555), 'reward_variance': np.float32(2.7171602), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 59328} @ step loss: {'critic_loss': np.float64(0.11712869852781296), 'actor_loss': np.float64(-1.0579236984252929), 'hyper_actor_loss': np.float64(0.04952235370874405), 'behavior_loss': np.float64(1.188195788860321)}

Episode step 1800, time diff 0.7713565826416016, total time dif 139.04395294189453)
step: 1800 @ episode report: {'average_total_reward': np.float32(7.28), 'reward_variance': np.float32(2.2006373), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 59648} @ step loss: {'critic_loss': np.float64(0.10367708951234818), 'actor_loss': np.float64(-1.0123109936714172), 'hyper_actor_loss': np.float64(0.05034000091254711), 'behavior_loss': np.float64(1.1707927763462067)}

Episode step 1810, time diff 0.7510955333709717, total time dif 139.81530952453613)
step: 1810 @ episode report: {'average_total_reward': np.float32(6.7699995), 'reward_variance': np.float32(3.6043963), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 59968} @ step loss: {'critic_loss': np.float64(0.12012107148766518), 'actor_loss': np.float64(-1.0370196223258972), 'hyper_actor_loss': np.float64(0.05095882937312126), 'behavior_loss': np.float64(1.2226513028144836)}

Episode step 1820, time diff 0.7257421016693115, total time dif 140.5664050579071)
step: 1820 @ episode report: {'average_total_reward': np.float32(7.1433334), 'reward_variance': np.float32(1.1867768), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 60288} @ step loss: {'critic_loss': np.float64(0.11654313057661056), 'actor_loss': np.float64(-1.0077272176742553), 'hyper_actor_loss': np.float64(0.050957823544740675), 'behavior_loss': np.float64(1.1610945165157318)}

Episode step 1830, time diff 0.7614035606384277, total time dif 141.29214715957642)
step: 1830 @ episode report: {'average_total_reward': np.float32(6.357778), 'reward_variance': np.float32(3.7103906), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 60608} @ step loss: {'critic_loss': np.float64(0.11925607472658158), 'actor_loss': np.float64(-1.0176679909229278), 'hyper_actor_loss': np.float64(0.05093005299568176), 'behavior_loss': np.float64(1.1994096279144286)}

Episode step 1840, time diff 0.7300097942352295, total time dif 142.05355072021484)
step: 1840 @ episode report: {'average_total_reward': np.float32(6.345556), 'reward_variance': np.float32(1.3751965), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 60928} @ step loss: {'critic_loss': np.float64(0.11975532099604606), 'actor_loss': np.float64(-1.0251500010490417), 'hyper_actor_loss': np.float64(0.05099946223199368), 'behavior_loss': np.float64(1.2677883625030517)}

Episode step 1850, time diff 0.7760665416717529, total time dif 142.78356051445007)
step: 1850 @ episode report: {'average_total_reward': np.float32(6.418889), 'reward_variance': np.float32(2.128545), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 61248} @ step loss: {'critic_loss': np.float64(0.11146457567811012), 'actor_loss': np.float64(-1.0170122027397155), 'hyper_actor_loss': np.float64(0.0506585244089365), 'behavior_loss': np.float64(1.2885296404361726)}

Episode step 1860, time diff 0.9190049171447754, total time dif 143.55962705612183)
step: 1860 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(2.3153942), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 61568} @ step loss: {'critic_loss': np.float64(0.11933288052678108), 'actor_loss': np.float64(-1.0041349232196808), 'hyper_actor_loss': np.float64(0.04981903620064258), 'behavior_loss': np.float64(1.2089506030082702)}

Episode step 1870, time diff 0.7321767807006836, total time dif 144.4786319732666)
step: 1870 @ episode report: {'average_total_reward': np.float32(6.418889), 'reward_variance': np.float32(2.5056312), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 61888} @ step loss: {'critic_loss': np.float64(0.1094228096306324), 'actor_loss': np.float64(-0.9950012922286987), 'hyper_actor_loss': np.float64(0.04906948246061802), 'behavior_loss': np.float64(1.2287967801094055)}

Episode step 1880, time diff 0.701819896697998, total time dif 145.21080875396729)
step: 1880 @ episode report: {'average_total_reward': np.float32(6.5944443), 'reward_variance': np.float32(1.2536108), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 62208} @ step loss: {'critic_loss': np.float64(0.10917377695441247), 'actor_loss': np.float64(-0.9876925528049469), 'hyper_actor_loss': np.float64(0.04785190112888813), 'behavior_loss': np.float64(1.2152784585952758)}

Episode step 1890, time diff 0.7037520408630371, total time dif 145.91262865066528)
step: 1890 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(3.1758378), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 62528} @ step loss: {'critic_loss': np.float64(0.12435985952615738), 'actor_loss': np.float64(-1.0233857750892639), 'hyper_actor_loss': np.float64(0.04717329777777195), 'behavior_loss': np.float64(1.272083854675293)}

Episode step 1900, time diff 0.6949241161346436, total time dif 146.61638069152832)
step: 1900 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(1.0837977), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 62848} @ step loss: {'critic_loss': np.float64(0.11550751850008964), 'actor_loss': np.float64(-0.9595653533935546), 'hyper_actor_loss': np.float64(0.046634423360228536), 'behavior_loss': np.float64(1.27567218542099)}

Episode step 1910, time diff 0.7647602558135986, total time dif 147.31130480766296)
step: 1910 @ episode report: {'average_total_reward': np.float32(7.004445), 'reward_variance': np.float32(2.393289), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 63168} @ step loss: {'critic_loss': np.float64(0.13625120744109154), 'actor_loss': np.float64(-1.0506009697914123), 'hyper_actor_loss': np.float64(0.04623185992240906), 'behavior_loss': np.float64(1.31901695728302)}

Episode step 1920, time diff 0.7071695327758789, total time dif 148.07606506347656)
step: 1920 @ episode report: {'average_total_reward': np.float32(7.082223), 'reward_variance': np.float32(1.5583261), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 63488} @ step loss: {'critic_loss': np.float64(0.11856252625584603), 'actor_loss': np.float64(-0.9871920347213745), 'hyper_actor_loss': np.float64(0.04632185250520706), 'behavior_loss': np.float64(1.3176316380500794)}

Episode step 1930, time diff 0.7388122081756592, total time dif 148.78323459625244)
step: 1930 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(1.8044885), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 63808} @ step loss: {'critic_loss': np.float64(0.11833198070526123), 'actor_loss': np.float64(-1.0796878337860107), 'hyper_actor_loss': np.float64(0.046414048224687574), 'behavior_loss': np.float64(1.2500450015068054)}

Episode step 1940, time diff 0.7342398166656494, total time dif 149.5220468044281)
step: 1940 @ episode report: {'average_total_reward': np.float32(6.631111), 'reward_variance': np.float32(1.9685137), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 64128} @ step loss: {'critic_loss': np.float64(0.12367523014545441), 'actor_loss': np.float64(-1.0090980887413026), 'hyper_actor_loss': np.float64(0.04616823196411133), 'behavior_loss': np.float64(1.3267227292060852)}

Episode step 1950, time diff 0.7550702095031738, total time dif 150.25628662109375)
step: 1950 @ episode report: {'average_total_reward': np.float32(6.8433332), 'reward_variance': np.float32(2.1767278), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 64448} @ step loss: {'critic_loss': np.float64(0.12075354978442192), 'actor_loss': np.float64(-1.0485455870628357), 'hyper_actor_loss': np.float64(0.0458456490188837), 'behavior_loss': np.float64(1.2502330422401429)}

Episode step 1960, time diff 0.7835855484008789, total time dif 151.01135683059692)
step: 1960 @ episode report: {'average_total_reward': np.float32(7.2188897), 'reward_variance': np.float32(1.4118038), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 64768} @ step loss: {'critic_loss': np.float64(0.11759807541966438), 'actor_loss': np.float64(-1.0307699382305144), 'hyper_actor_loss': np.float64(0.0453175600618124), 'behavior_loss': np.float64(1.274149239063263)}

Episode step 1970, time diff 0.7621746063232422, total time dif 151.7949423789978)
step: 1970 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(1.6740538), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 65088} @ step loss: {'critic_loss': np.float64(0.1269519731402397), 'actor_loss': np.float64(-1.0084849357604981), 'hyper_actor_loss': np.float64(0.04460539259016514), 'behavior_loss': np.float64(1.3314635515213014)}

Episode step 1980, time diff 0.7832398414611816, total time dif 152.55711698532104)
step: 1980 @ episode report: {'average_total_reward': np.float32(7.267778), 'reward_variance': np.float32(3.7314925), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 65408} @ step loss: {'critic_loss': np.float64(0.10290576368570328), 'actor_loss': np.float64(-1.038928258419037), 'hyper_actor_loss': np.float64(0.044378069788217546), 'behavior_loss': np.float64(1.320078229904175)}

Episode step 1990, time diff 0.7171626091003418, total time dif 153.34035682678223)
step: 1990 @ episode report: {'average_total_reward': np.float32(7.6044455), 'reward_variance': np.float32(1.8265731), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 65728} @ step loss: {'critic_loss': np.float64(0.11639048159122467), 'actor_loss': np.float64(-0.9768588006496429), 'hyper_actor_loss': np.float64(0.04432363025844097), 'behavior_loss': np.float64(1.2895431756973266)}

Episode step 2000, time diff 0.702653169631958, total time dif 154.05751943588257)
step: 2000 @ episode report: {'average_total_reward': np.float32(7.304445), 'reward_variance': np.float32(3.566054), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 66048} @ step loss: {'critic_loss': np.float64(0.1058586809784174), 'actor_loss': np.float64(-0.9969770431518554), 'hyper_actor_loss': np.float64(0.04404070489108562), 'behavior_loss': np.float64(1.301922821998596)}

Episode step 2010, time diff 0.7248201370239258, total time dif 154.76017260551453)
step: 2010 @ episode report: {'average_total_reward': np.float32(6.8188896), 'reward_variance': np.float32(2.9164953), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 66368} @ step loss: {'critic_loss': np.float64(0.12694284319877625), 'actor_loss': np.float64(-0.9943149328231812), 'hyper_actor_loss': np.float64(0.0436446163803339), 'behavior_loss': np.float64(1.389179253578186)}

Episode step 2020, time diff 0.9082322120666504, total time dif 155.48499274253845)
step: 2020 @ episode report: {'average_total_reward': np.float32(6.943334), 'reward_variance': np.float32(1.8536165), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 66688} @ step loss: {'critic_loss': np.float64(0.11201549172401429), 'actor_loss': np.float64(-0.9847703576087952), 'hyper_actor_loss': np.float64(0.043016687780618665), 'behavior_loss': np.float64(1.402850294113159)}

Episode step 2030, time diff 0.7533667087554932, total time dif 156.3932249546051)
step: 2030 @ episode report: {'average_total_reward': np.float32(7.1922226), 'reward_variance': np.float32(2.0040507), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 67008} @ step loss: {'critic_loss': np.float64(0.10673360228538513), 'actor_loss': np.float64(-1.0262299239635468), 'hyper_actor_loss': np.float64(0.0422431543469429), 'behavior_loss': np.float64(1.320102834701538)}

Episode step 2040, time diff 0.7003920078277588, total time dif 157.1465916633606)
step: 2040 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(1.0196059), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 67328} @ step loss: {'critic_loss': np.float64(0.11130680963397026), 'actor_loss': np.float64(-0.9644656181335449), 'hyper_actor_loss': np.float64(0.04188183695077896), 'behavior_loss': np.float64(1.3247822403907776)}

Episode step 2050, time diff 0.7700533866882324, total time dif 157.84698367118835)
step: 2050 @ episode report: {'average_total_reward': np.float32(7.541112), 'reward_variance': np.float32(2.7545197), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 67648} @ step loss: {'critic_loss': np.float64(0.11469360589981079), 'actor_loss': np.float64(-1.023523223400116), 'hyper_actor_loss': np.float64(0.04133644178509712), 'behavior_loss': np.float64(1.3855679035186768)}

Episode step 2060, time diff 0.7539269924163818, total time dif 158.6170370578766)
step: 2060 @ episode report: {'average_total_reward': np.float32(7.6433334), 'reward_variance': np.float32(1.5669255), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 67968} @ step loss: {'critic_loss': np.float64(0.12154230698943139), 'actor_loss': np.float64(-1.0186332702636718), 'hyper_actor_loss': np.float64(0.04055334143340587), 'behavior_loss': np.float64(1.3345598220825194)}

Episode step 2070, time diff 0.7433972358703613, total time dif 159.37096405029297)
step: 2070 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(3.0230324), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 68288} @ step loss: {'critic_loss': np.float64(0.11183159649372101), 'actor_loss': np.float64(-1.028331607580185), 'hyper_actor_loss': np.float64(0.040601049363613126), 'behavior_loss': np.float64(1.358474850654602)}

Episode step 2080, time diff 0.7663698196411133, total time dif 160.11436128616333)
step: 2080 @ episode report: {'average_total_reward': np.float32(7.5288897), 'reward_variance': np.float32(2.1974368), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 68608} @ step loss: {'critic_loss': np.float64(0.11515644118189812), 'actor_loss': np.float64(-0.9957017600536346), 'hyper_actor_loss': np.float64(0.040733468160033225), 'behavior_loss': np.float64(1.3638302445411683)}

Episode step 2090, time diff 0.7665820121765137, total time dif 160.88073110580444)
step: 2090 @ episode report: {'average_total_reward': np.float32(7.38), 'reward_variance': np.float32(1.1063659), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 68928} @ step loss: {'critic_loss': np.float64(0.11597138121724129), 'actor_loss': np.float64(-0.9862345635890961), 'hyper_actor_loss': np.float64(0.04026381522417068), 'behavior_loss': np.float64(1.3868791103363036)}

Episode step 2100, time diff 0.7633316516876221, total time dif 161.64731311798096)
step: 2100 @ episode report: {'average_total_reward': np.float32(7.965556), 'reward_variance': np.float32(2.5812454), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 69248} @ step loss: {'critic_loss': np.float64(0.121047742664814), 'actor_loss': np.float64(-1.0124184489250183), 'hyper_actor_loss': np.float64(0.03953295275568962), 'behavior_loss': np.float64(1.3402502298355103)}

Episode step 2110, time diff 0.740699052810669, total time dif 162.41064476966858)
step: 2110 @ episode report: {'average_total_reward': np.float32(7.78), 'reward_variance': np.float32(1.4077232), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 69568} @ step loss: {'critic_loss': np.float64(0.1121116004884243), 'actor_loss': np.float64(-0.9881934642791748), 'hyper_actor_loss': np.float64(0.03864070400595665), 'behavior_loss': np.float64(1.4083525896072389)}

Episode step 2120, time diff 0.7541232109069824, total time dif 163.15134382247925)
step: 2120 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(5.305452), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 69888} @ step loss: {'critic_loss': np.float64(0.11074802055954933), 'actor_loss': np.float64(-1.0093259692192078), 'hyper_actor_loss': np.float64(0.03826549202203751), 'behavior_loss': np.float64(1.4618234992027284)}

Episode step 2130, time diff 0.8241109848022461, total time dif 163.90546703338623)
step: 2130 @ episode report: {'average_total_reward': np.float32(8.116667), 'reward_variance': np.float32(2.7734623), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 70208} @ step loss: {'critic_loss': np.float64(0.10348428562283515), 'actor_loss': np.float64(-0.9970335543155671), 'hyper_actor_loss': np.float64(0.03783759512007236), 'behavior_loss': np.float64(1.4105356097221375)}

Episode step 2140, time diff 0.8143918514251709, total time dif 164.72957801818848)
step: 2140 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(1.0516064), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 70528} @ step loss: {'critic_loss': np.float64(0.12384062111377717), 'actor_loss': np.float64(-1.047297763824463), 'hyper_actor_loss': np.float64(0.03796697370707989), 'behavior_loss': np.float64(1.4165499091148377)}

Episode step 2150, time diff 0.7715423107147217, total time dif 165.54396986961365)
step: 2150 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(1.9562981), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 70848} @ step loss: {'critic_loss': np.float64(0.10290592014789582), 'actor_loss': np.float64(-1.0037052869796752), 'hyper_actor_loss': np.float64(0.037723909318447116), 'behavior_loss': np.float64(1.492886745929718)}

Episode step 2160, time diff 0.7525954246520996, total time dif 166.31551218032837)
step: 2160 @ episode report: {'average_total_reward': np.float32(7.5800004), 'reward_variance': np.float32(0.63145196), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 71168} @ step loss: {'critic_loss': np.float64(0.10298421829938889), 'actor_loss': np.float64(-1.004108303785324), 'hyper_actor_loss': np.float64(0.037237852439284326), 'behavior_loss': np.float64(1.400544321537018)}

Episode step 2170, time diff 0.8047354221343994, total time dif 167.06810760498047)
step: 2170 @ episode report: {'average_total_reward': np.float32(8.614446), 'reward_variance': np.float32(2.2232363), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 71488} @ step loss: {'critic_loss': np.float64(0.11311334297060967), 'actor_loss': np.float64(-1.0001915454864503), 'hyper_actor_loss': np.float64(0.036558642610907556), 'behavior_loss': np.float64(1.4925950646400452)}

Episode step 2180, time diff 0.865523099899292, total time dif 167.87284302711487)
step: 2180 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(2.0439525), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 71808} @ step loss: {'critic_loss': np.float64(0.1121688287705183), 'actor_loss': np.float64(-0.986411190032959), 'hyper_actor_loss': np.float64(0.03565347716212273), 'behavior_loss': np.float64(1.5956715583801269)}

Episode step 2190, time diff 0.8202378749847412, total time dif 168.73836612701416)
step: 2190 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(3.613067), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 72128} @ step loss: {'critic_loss': np.float64(0.11123836562037467), 'actor_loss': np.float64(-1.0178224563598632), 'hyper_actor_loss': np.float64(0.03475873582065105), 'behavior_loss': np.float64(1.457285988330841)}

Episode step 2200, time diff 0.8108458518981934, total time dif 169.5586040019989)
step: 2200 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(4.8427224), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 72448} @ step loss: {'critic_loss': np.float64(0.10941485688090324), 'actor_loss': np.float64(-0.9834131062030792), 'hyper_actor_loss': np.float64(0.03398597799241543), 'behavior_loss': np.float64(1.5990278363227843)}

Episode step 2210, time diff 0.8394012451171875, total time dif 170.3694498538971)
step: 2210 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(2.7941527), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 72768} @ step loss: {'critic_loss': np.float64(0.11183357313275337), 'actor_loss': np.float64(-1.0238723278045654), 'hyper_actor_loss': np.float64(0.03315535597503185), 'behavior_loss': np.float64(1.4657910823822022)}

Episode step 2220, time diff 0.7674956321716309, total time dif 171.20885109901428)
step: 2220 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(2.1133842), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 73088} @ step loss: {'critic_loss': np.float64(0.11299489364027977), 'actor_loss': np.float64(-1.0130269050598144), 'hyper_actor_loss': np.float64(0.03275415971875191), 'behavior_loss': np.float64(1.5605488657951354)}

Episode step 2230, time diff 0.8131370544433594, total time dif 171.9763467311859)
step: 2230 @ episode report: {'average_total_reward': np.float32(8.128889), 'reward_variance': np.float32(0.7799312), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 73408} @ step loss: {'critic_loss': np.float64(0.10971997380256653), 'actor_loss': np.float64(-1.0118458032608033), 'hyper_actor_loss': np.float64(0.03262826092541218), 'behavior_loss': np.float64(1.5701842546463012)}

Episode step 2240, time diff 0.7488422393798828, total time dif 172.78948378562927)
step: 2240 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(0.9326927), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 73728} @ step loss: {'critic_loss': np.float64(0.12106564044952392), 'actor_loss': np.float64(-0.9970648288726807), 'hyper_actor_loss': np.float64(0.03256493620574474), 'behavior_loss': np.float64(1.5932129740715026)}

Episode step 2250, time diff 0.7978863716125488, total time dif 173.53832602500916)
step: 2250 @ episode report: {'average_total_reward': np.float32(7.641112), 'reward_variance': np.float32(2.2978284), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 74048} @ step loss: {'critic_loss': np.float64(0.10900070369243622), 'actor_loss': np.float64(-1.0282027304172516), 'hyper_actor_loss': np.float64(0.03255934864282608), 'behavior_loss': np.float64(1.5932572960853577)}

Episode step 2260, time diff 0.801917552947998, total time dif 174.3362123966217)
step: 2260 @ episode report: {'average_total_reward': np.float32(8.465556), 'reward_variance': np.float32(0.6020111), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 74368} @ step loss: {'critic_loss': np.float64(0.10966797098517418), 'actor_loss': np.float64(-0.9915262758731842), 'hyper_actor_loss': np.float64(0.03234337754547596), 'behavior_loss': np.float64(1.5925676822662354)}

Episode step 2270, time diff 0.7449252605438232, total time dif 175.1381299495697)
step: 2270 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(4.4781303), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 74688} @ step loss: {'critic_loss': np.float64(0.11651238277554513), 'actor_loss': np.float64(-1.0088784754276277), 'hyper_actor_loss': np.float64(0.032115993276238444), 'behavior_loss': np.float64(1.6483169794082642)}

Episode step 2280, time diff 0.8213446140289307, total time dif 175.88305521011353)
step: 2280 @ episode report: {'average_total_reward': np.float32(7.8166666), 'reward_variance': np.float32(3.5120068), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 75008} @ step loss: {'critic_loss': np.float64(0.12379805222153664), 'actor_loss': np.float64(-1.0515796542167664), 'hyper_actor_loss': np.float64(0.031724394485354426), 'behavior_loss': np.float64(1.5387079834938049)}

Episode step 2290, time diff 0.8110208511352539, total time dif 176.70439982414246)
step: 2290 @ episode report: {'average_total_reward': np.float32(8.055555), 'reward_variance': np.float32(1.5906429), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 75328} @ step loss: {'critic_loss': np.float64(0.11828534081578254), 'actor_loss': np.float64(-1.0084619879722596), 'hyper_actor_loss': np.float64(0.03185459524393082), 'behavior_loss': np.float64(1.596393358707428)}

Episode step 2300, time diff 0.8014402389526367, total time dif 177.5154206752777)
step: 2300 @ episode report: {'average_total_reward': np.float32(8.377779), 'reward_variance': np.float32(4.776766), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 75648} @ step loss: {'critic_loss': np.float64(0.12326731383800507), 'actor_loss': np.float64(-1.0464819073677063), 'hyper_actor_loss': np.float64(0.031510601937770846), 'behavior_loss': np.float64(1.5829829931259156)}

Episode step 2310, time diff 0.7388949394226074, total time dif 178.31686091423035)
step: 2310 @ episode report: {'average_total_reward': np.float32(8.277778), 'reward_variance': np.float32(2.191679), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 75968} @ step loss: {'critic_loss': np.float64(0.10551448240876198), 'actor_loss': np.float64(-1.0193168759346007), 'hyper_actor_loss': np.float64(0.031083414703607558), 'behavior_loss': np.float64(1.5149922251701355)}

Episode step 2320, time diff 0.7841827869415283, total time dif 179.05575585365295)
step: 2320 @ episode report: {'average_total_reward': np.float32(7.504445), 'reward_variance': np.float32(3.3263507), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 76288} @ step loss: {'critic_loss': np.float64(0.12287994846701622), 'actor_loss': np.float64(-1.0123477101325988), 'hyper_actor_loss': np.float64(0.031066294573247433), 'behavior_loss': np.float64(1.6147733449935913)}

Episode step 2330, time diff 0.751366376876831, total time dif 179.83993864059448)
step: 2330 @ episode report: {'average_total_reward': np.float32(6.8188887), 'reward_variance': np.float32(1.0875324), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 76608} @ step loss: {'critic_loss': np.float64(0.10856089442968368), 'actor_loss': np.float64(-1.0266043543815613), 'hyper_actor_loss': np.float64(0.030633234418928623), 'behavior_loss': np.float64(1.5946685194969177)}

Episode step 2340, time diff 0.750236988067627, total time dif 180.5913050174713)
step: 2340 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(1.8041182), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 76928} @ step loss: {'critic_loss': np.float64(0.12433860152959823), 'actor_loss': np.float64(-0.9933227241039276), 'hyper_actor_loss': np.float64(0.029956088960170747), 'behavior_loss': np.float64(1.6599686861038208)}

Episode step 2350, time diff 0.9034004211425781, total time dif 181.34154200553894)
step: 2350 @ episode report: {'average_total_reward': np.float32(8.1044445), 'reward_variance': np.float32(1.5187205), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 77248} @ step loss: {'critic_loss': np.float64(0.10310451164841652), 'actor_loss': np.float64(-1.0130783498287201), 'hyper_actor_loss': np.float64(0.02952268738299608), 'behavior_loss': np.float64(1.616925549507141)}

Episode step 2360, time diff 0.7691588401794434, total time dif 182.24494242668152)
step: 2360 @ episode report: {'average_total_reward': np.float32(8.302223), 'reward_variance': np.float32(2.4888597), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 77568} @ step loss: {'critic_loss': np.float64(0.11413622424006462), 'actor_loss': np.float64(-1.0070394098758697), 'hyper_actor_loss': np.float64(0.029736616648733616), 'behavior_loss': np.float64(1.65365971326828)}

Episode step 2370, time diff 0.8275086879730225, total time dif 183.01410126686096)
step: 2370 @ episode report: {'average_total_reward': np.float32(7.3433332), 'reward_variance': np.float32(1.0892953), 'max_total_reward': np.float32(8.77778), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 77888} @ step loss: {'critic_loss': np.float64(0.11157910153269768), 'actor_loss': np.float64(-1.0438018441200256), 'hyper_actor_loss': np.float64(0.029871560633182526), 'behavior_loss': np.float64(1.601932430267334)}

Episode step 2380, time diff 0.8293797969818115, total time dif 183.84160995483398)
step: 2380 @ episode report: {'average_total_reward': np.float32(8.32889), 'reward_variance': np.float32(1.1112149), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 78208} @ step loss: {'critic_loss': np.float64(0.10945875719189643), 'actor_loss': np.float64(-0.9961275398731232), 'hyper_actor_loss': np.float64(0.02993982471525669), 'behavior_loss': np.float64(1.6077385425567627)}

Episode step 2390, time diff 0.9464857578277588, total time dif 184.6709897518158)
step: 2390 @ episode report: {'average_total_reward': np.float32(8.004445), 'reward_variance': np.float32(2.2052398), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 78528} @ step loss: {'critic_loss': np.float64(0.11907154619693756), 'actor_loss': np.float64(-1.0133660912513733), 'hyper_actor_loss': np.float64(0.029671471752226352), 'behavior_loss': np.float64(1.6341739654541017)}

Episode step 2400, time diff 0.988548755645752, total time dif 185.61747550964355)
step: 2400 @ episode report: {'average_total_reward': np.float32(8.653334), 'reward_variance': np.float32(3.3923163), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 78848} @ step loss: {'critic_loss': np.float64(0.11896654851734638), 'actor_loss': np.float64(-1.010931622982025), 'hyper_actor_loss': np.float64(0.0286734938621521), 'behavior_loss': np.float64(1.6983028173446655)}

Episode step 2410, time diff 1.097219705581665, total time dif 186.6060242652893)
step: 2410 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(1.3307953), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 79168} @ step loss: {'critic_loss': np.float64(0.11905698031187058), 'actor_loss': np.float64(-0.9669981300830841), 'hyper_actor_loss': np.float64(0.026840034313499926), 'behavior_loss': np.float64(1.7351481556892394)}

Episode step 2420, time diff 0.7944183349609375, total time dif 187.70324397087097)
step: 2420 @ episode report: {'average_total_reward': np.float32(7.653334), 'reward_variance': np.float32(2.7758467), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 79488} @ step loss: {'critic_loss': np.float64(0.11143273487687111), 'actor_loss': np.float64(-0.9998045206069947), 'hyper_actor_loss': np.float64(0.024865846335887908), 'behavior_loss': np.float64(1.7995145440101623)}

Episode step 2430, time diff 0.8318266868591309, total time dif 188.4976623058319)
step: 2430 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(2.8904705), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 79808} @ step loss: {'critic_loss': np.float64(0.09885520599782467), 'actor_loss': np.float64(-0.9995241224765777), 'hyper_actor_loss': np.float64(0.02405350673943758), 'behavior_loss': np.float64(1.7318041324615479)}

Episode step 2440, time diff 0.7787728309631348, total time dif 189.32948899269104)
step: 2440 @ episode report: {'average_total_reward': np.float32(7.716667), 'reward_variance': np.float32(1.7728459), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 80128} @ step loss: {'critic_loss': np.float64(0.11304725334048271), 'actor_loss': np.float64(-0.9772308647632599), 'hyper_actor_loss': np.float64(0.024093636497855186), 'behavior_loss': np.float64(1.7080894112586975)}

Episode step 2450, time diff 0.8063502311706543, total time dif 190.10826182365417)
step: 2450 @ episode report: {'average_total_reward': np.float32(7.767778), 'reward_variance': np.float32(1.2016164), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 80448} @ step loss: {'critic_loss': np.float64(0.11032391265034676), 'actor_loss': np.float64(-1.0053644597530365), 'hyper_actor_loss': np.float64(0.0244126433506608), 'behavior_loss': np.float64(1.7878965735435486)}

Episode step 2460, time diff 0.7655484676361084, total time dif 190.91461205482483)
step: 2460 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(2.2874122), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 80768} @ step loss: {'critic_loss': np.float64(0.11377781853079796), 'actor_loss': np.float64(-1.0271430730819702), 'hyper_actor_loss': np.float64(0.024786829575896265), 'behavior_loss': np.float64(1.656955349445343)}

Episode step 2470, time diff 0.7781758308410645, total time dif 191.68016052246094)
step: 2470 @ episode report: {'average_total_reward': np.float32(7.965556), 'reward_variance': np.float32(2.3538141), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 81088} @ step loss: {'critic_loss': np.float64(0.11582142785191536), 'actor_loss': np.float64(-1.0275584578514099), 'hyper_actor_loss': np.float64(0.024944275990128516), 'behavior_loss': np.float64(1.7670029640197753)}

Episode step 2480, time diff 0.7162001132965088, total time dif 192.458336353302)
step: 2480 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(1.035452), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 81408} @ step loss: {'critic_loss': np.float64(0.09369831308722495), 'actor_loss': np.float64(-1.0138086795806884), 'hyper_actor_loss': np.float64(0.025406530313193797), 'behavior_loss': np.float64(1.7286381125450134)}

Episode step 2490, time diff 0.8206169605255127, total time dif 193.1745364665985)
step: 2490 @ episode report: {'average_total_reward': np.float32(7.1066675), 'reward_variance': np.float32(0.735882), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 81728} @ step loss: {'critic_loss': np.float64(0.11363463774323464), 'actor_loss': np.float64(-1.0102098286151886), 'hyper_actor_loss': np.float64(0.026053671538829804), 'behavior_loss': np.float64(1.8295234441757202)}

Episode step 2500, time diff 0.8340373039245605, total time dif 193.99515342712402)
step: 2500 @ episode report: {'average_total_reward': np.float32(7.7922225), 'reward_variance': np.float32(1.5511616), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 82048} @ step loss: {'critic_loss': np.float64(0.10686174258589745), 'actor_loss': np.float64(-1.0112503826618195), 'hyper_actor_loss': np.float64(0.026205211691558362), 'behavior_loss': np.float64(1.7103455543518067)}

Episode step 2510, time diff 0.9986462593078613, total time dif 194.82919073104858)
step: 2510 @ episode report: {'average_total_reward': np.float32(6.7433333), 'reward_variance': np.float32(3.464431), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 82368} @ step loss: {'critic_loss': np.float64(0.10936737358570099), 'actor_loss': np.float64(-1.013712042570114), 'hyper_actor_loss': np.float64(0.025345265679061414), 'behavior_loss': np.float64(1.7202247262001038)}

Episode step 2520, time diff 0.825340986251831, total time dif 195.82783699035645)
step: 2520 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(1.6873287), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 82688} @ step loss: {'critic_loss': np.float64(0.10204936265945434), 'actor_loss': np.float64(-0.9796269357204437), 'hyper_actor_loss': np.float64(0.024886340461671354), 'behavior_loss': np.float64(1.832618522644043)}

Episode step 2530, time diff 0.7855818271636963, total time dif 196.65317797660828)
step: 2530 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(1.9603573), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 83008} @ step loss: {'critic_loss': np.float64(0.11095490977168083), 'actor_loss': np.float64(-0.9929758727550506), 'hyper_actor_loss': np.float64(0.024456128105521203), 'behavior_loss': np.float64(1.663303256034851)}

Episode step 2540, time diff 0.817540168762207, total time dif 197.43875980377197)
step: 2540 @ episode report: {'average_total_reward': np.float32(6.7433333), 'reward_variance': np.float32(2.7327027), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 83328} @ step loss: {'critic_loss': np.float64(0.1221076987683773), 'actor_loss': np.float64(-0.9991407454013824), 'hyper_actor_loss': np.float64(0.0242377944290638), 'behavior_loss': np.float64(1.8365662336349486)}

Episode step 2550, time diff 0.7713541984558105, total time dif 198.25629997253418)
step: 2550 @ episode report: {'average_total_reward': np.float32(5.996667), 'reward_variance': np.float32(3.2973104), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 83648} @ step loss: {'critic_loss': np.float64(0.10298558846116065), 'actor_loss': np.float64(-0.9627889752388), 'hyper_actor_loss': np.float64(0.02369783017784357), 'behavior_loss': np.float64(1.8091654300689697)}

Episode step 2560, time diff 0.8039991855621338, total time dif 199.02765417099)
step: 2560 @ episode report: {'average_total_reward': np.float32(6.6066675), 'reward_variance': np.float32(1.2035853), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 83968} @ step loss: {'critic_loss': np.float64(0.09775320030748844), 'actor_loss': np.float64(-0.9674175143241882), 'hyper_actor_loss': np.float64(0.022924664430320263), 'behavior_loss': np.float64(1.7592190623283386)}

Episode step 2570, time diff 0.8124215602874756, total time dif 199.83165335655212)
step: 2570 @ episode report: {'average_total_reward': np.float32(6.7066665), 'reward_variance': np.float32(3.8465729), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 84288} @ step loss: {'critic_loss': np.float64(0.1179964080452919), 'actor_loss': np.float64(-0.9878452599048615), 'hyper_actor_loss': np.float64(0.022391508147120476), 'behavior_loss': np.float64(1.8317068457603454)}

Episode step 2580, time diff 0.7785124778747559, total time dif 200.6440749168396)
step: 2580 @ episode report: {'average_total_reward': np.float32(7.1433344), 'reward_variance': np.float32(1.6985054), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 84608} @ step loss: {'critic_loss': np.float64(0.12063587680459023), 'actor_loss': np.float64(-0.9890891134738922), 'hyper_actor_loss': np.float64(0.022272677533328532), 'behavior_loss': np.float64(1.7991895914077758)}

Episode step 2590, time diff 0.8025951385498047, total time dif 201.42258739471436)
step: 2590 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(2.7563078), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 84928} @ step loss: {'critic_loss': np.float64(0.11960513815283776), 'actor_loss': np.float64(-1.0261061012744903), 'hyper_actor_loss': np.float64(0.022311502508819102), 'behavior_loss': np.float64(1.8666693210601806)}

Episode step 2600, time diff 0.7987298965454102, total time dif 202.22518253326416)
step: 2600 @ episode report: {'average_total_reward': np.float32(6.667778), 'reward_variance': np.float32(2.958506), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 85248} @ step loss: {'critic_loss': np.float64(0.10907223299145699), 'actor_loss': np.float64(-0.9622354984283448), 'hyper_actor_loss': np.float64(0.02215562015771866), 'behavior_loss': np.float64(1.9186829209327698)}

Episode step 2610, time diff 0.8499650955200195, total time dif 203.02391242980957)
step: 2610 @ episode report: {'average_total_reward': np.float32(6.182222), 'reward_variance': np.float32(0.7375604), 'max_total_reward': np.float32(7.6555552), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 85568} @ step loss: {'critic_loss': np.float64(0.10664769299328328), 'actor_loss': np.float64(-0.9993791103363037), 'hyper_actor_loss': np.float64(0.022164331190288065), 'behavior_loss': np.float64(1.7366423606872559)}

Episode step 2620, time diff 0.8094887733459473, total time dif 203.8738775253296)
step: 2620 @ episode report: {'average_total_reward': np.float32(6.667778), 'reward_variance': np.float32(3.1964319), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 85888} @ step loss: {'critic_loss': np.float64(0.10411152616143227), 'actor_loss': np.float64(-0.993946886062622), 'hyper_actor_loss': np.float64(0.02169199027121067), 'behavior_loss': np.float64(1.7914970159530639)}

Episode step 2630, time diff 0.7945811748504639, total time dif 204.68336629867554)
step: 2630 @ episode report: {'average_total_reward': np.float32(7.416667), 'reward_variance': np.float32(3.4868712), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 86208} @ step loss: {'critic_loss': np.float64(0.11067176274955273), 'actor_loss': np.float64(-0.9214415073394775), 'hyper_actor_loss': np.float64(0.021335308998823167), 'behavior_loss': np.float64(1.915002703666687)}

Episode step 2640, time diff 0.8353641033172607, total time dif 205.477947473526)
step: 2640 @ episode report: {'average_total_reward': np.float32(7.11889), 'reward_variance': np.float32(1.375088), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 86528} @ step loss: {'critic_loss': np.float64(0.11050672307610512), 'actor_loss': np.float64(-1.034651553630829), 'hyper_actor_loss': np.float64(0.021885026060044766), 'behavior_loss': np.float64(1.9490029335021972)}

Episode step 2650, time diff 0.8274800777435303, total time dif 206.31331157684326)
step: 2650 @ episode report: {'average_total_reward': np.float32(6.757778), 'reward_variance': np.float32(2.332686), 'max_total_reward': np.float32(8.655555), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 86848} @ step loss: {'critic_loss': np.float64(0.12302737534046174), 'actor_loss': np.float64(-1.0128323435783386), 'hyper_actor_loss': np.float64(0.02313118353486061), 'behavior_loss': np.float64(1.8579190611839294)}

Episode step 2660, time diff 0.8124122619628906, total time dif 207.1407916545868)
step: 2660 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(1.7552297), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 87168} @ step loss: {'critic_loss': np.float64(0.10795009434223175), 'actor_loss': np.float64(-1.0128133654594422), 'hyper_actor_loss': np.float64(0.024726892076432704), 'behavior_loss': np.float64(1.8344725370407104)}

Episode step 2670, time diff 0.9815921783447266, total time dif 207.95320391654968)
step: 2670 @ episode report: {'average_total_reward': np.float32(6.231111), 'reward_variance': np.float32(1.1999209), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 87488} @ step loss: {'critic_loss': np.float64(0.10854872688651085), 'actor_loss': np.float64(-1.0391573667526246), 'hyper_actor_loss': np.float64(0.02560294922441244), 'behavior_loss': np.float64(1.6924804091453551)}

Episode step 2680, time diff 0.883751392364502, total time dif 208.9347960948944)
step: 2680 @ episode report: {'average_total_reward': np.float32(6.7066674), 'reward_variance': np.float32(0.9033631), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 87808} @ step loss: {'critic_loss': np.float64(0.10707125216722488), 'actor_loss': np.float64(-1.0497970223426818), 'hyper_actor_loss': np.float64(0.029114150255918504), 'behavior_loss': np.float64(1.876251459121704)}

Episode step 2690, time diff 0.8523905277252197, total time dif 209.8185474872589)
step: 2690 @ episode report: {'average_total_reward': np.float32(6.618889), 'reward_variance': np.float32(1.2196062), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 88128} @ step loss: {'critic_loss': np.float64(0.10026885271072387), 'actor_loss': np.float64(-1.0782087087631225), 'hyper_actor_loss': np.float64(0.034694654494524), 'behavior_loss': np.float64(1.6198278546333313)}

Episode step 2700, time diff 0.8507077693939209, total time dif 210.67093801498413)
step: 2700 @ episode report: {'average_total_reward': np.float32(5.9700003), 'reward_variance': np.float32(3.2747173), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 88448} @ step loss: {'critic_loss': np.float64(0.1225724071264267), 'actor_loss': np.float64(-1.1245670080184937), 'hyper_actor_loss': np.float64(0.039709684625267985), 'behavior_loss': np.float64(1.6055073857307434)}

Episode step 2710, time diff 0.8265559673309326, total time dif 211.52164578437805)
step: 2710 @ episode report: {'average_total_reward': np.float32(6.3333335), 'reward_variance': np.float32(2.2468143), 'max_total_reward': np.float32(8.777777), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 88768} @ step loss: {'critic_loss': np.float64(0.11323810741305351), 'actor_loss': np.float64(-1.1705986380577087), 'hyper_actor_loss': np.float64(0.043914413824677465), 'behavior_loss': np.float64(1.423643457889557)}

Episode step 2720, time diff 0.8569962978363037, total time dif 212.34820175170898)
step: 2720 @ episode report: {'average_total_reward': np.float32(5.735555), 'reward_variance': np.float32(1.0525876), 'max_total_reward': np.float32(7.4111114), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 89088} @ step loss: {'critic_loss': np.float64(0.10350460857152939), 'actor_loss': np.float64(-1.215076732635498), 'hyper_actor_loss': np.float64(0.04955766573548317), 'behavior_loss': np.float64(1.4192189812660216)}

Episode step 2730, time diff 0.8273177146911621, total time dif 213.2051980495453)
step: 2730 @ episode report: {'average_total_reward': np.float32(4.3011107), 'reward_variance': np.float32(0.9140606), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 89408} @ step loss: {'critic_loss': np.float64(0.10904627516865731), 'actor_loss': np.float64(-1.2642245769500733), 'hyper_actor_loss': np.float64(0.05410596542060375), 'behavior_loss': np.float64(1.2862829923629762)}

Episode step 2740, time diff 0.8036031723022461, total time dif 214.03251576423645)
step: 2740 @ episode report: {'average_total_reward': np.float32(2.8300002), 'reward_variance': np.float32(0.21802595), 'max_total_reward': np.float32(3.411111), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 89728} @ step loss: {'critic_loss': np.float64(0.11278348341584206), 'actor_loss': np.float64(-1.3774120688438416), 'hyper_actor_loss': np.float64(0.05731053538620472), 'behavior_loss': np.float64(1.0937914848327637)}

Episode step 2750, time diff 0.8376598358154297, total time dif 214.8361189365387)
step: 2750 @ episode report: {'average_total_reward': np.float32(2.478889), 'reward_variance': np.float32(0.489295), 'max_total_reward': np.float32(3.411111), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.3), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 90048} @ step loss: {'critic_loss': np.float64(0.13269557282328606), 'actor_loss': np.float64(-1.4113619208335877), 'hyper_actor_loss': np.float64(0.06074423640966416), 'behavior_loss': np.float64(1.0976138472557069)}

Episode step 2760, time diff 0.8317005634307861, total time dif 215.67377877235413)
step: 2760 @ episode report: {'average_total_reward': np.float32(1.9955555), 'reward_variance': np.float32(0.15415308), 'max_total_reward': np.float32(2.8), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 90368} @ step loss: {'critic_loss': np.float64(0.11888378635048866), 'actor_loss': np.float64(-1.3985694885253905), 'hyper_actor_loss': np.float64(0.06510333381593228), 'behavior_loss': np.float64(0.9569256961345672)}

Episode step 2770, time diff 0.8332793712615967, total time dif 216.5054793357849)
step: 2770 @ episode report: {'average_total_reward': np.float32(2.0711112), 'reward_variance': np.float32(0.3017334), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(1.0444443), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 90688} @ step loss: {'critic_loss': np.float64(0.11122132763266564), 'actor_loss': np.float64(-1.4022432446479798), 'hyper_actor_loss': np.float64(0.06943667382001877), 'behavior_loss': np.float64(0.8391249537467956)}

Episode step 2780, time diff 0.8366987705230713, total time dif 217.3387587070465)
step: 2780 @ episode report: {'average_total_reward': np.float32(1.6855557), 'reward_variance': np.float32(0.34439626), 'max_total_reward': np.float32(2.9222221), 'min_total_reward': np.float32(0.67777777), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 91008} @ step loss: {'critic_loss': np.float64(0.11554654315114021), 'actor_loss': np.float64(-1.241142785549164), 'hyper_actor_loss': np.float64(0.07234231829643249), 'behavior_loss': np.float64(0.8421193420886993)}

Episode step 2790, time diff 0.8279924392700195, total time dif 218.17545747756958)
step: 2790 @ episode report: {'average_total_reward': np.float32(1.61), 'reward_variance': np.float32(0.13312224), 'max_total_reward': np.float32(2.0444446), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 91328} @ step loss: {'critic_loss': np.float64(0.13937193155288696), 'actor_loss': np.float64(-1.0609550833702088), 'hyper_actor_loss': np.float64(0.07377429828047752), 'behavior_loss': np.float64(0.7798631846904754)}

Episode step 2800, time diff 0.8436861038208008, total time dif 219.0034499168396)
step: 2800 @ episode report: {'average_total_reward': np.float32(1.5344446), 'reward_variance': np.float32(0.22260375), 'max_total_reward': np.float32(2.166667), 'min_total_reward': np.float32(0.9222222), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 91648} @ step loss: {'critic_loss': np.float64(0.11559839099645615), 'actor_loss': np.float64(-0.8488138437271118), 'hyper_actor_loss': np.float64(0.07373476848006248), 'behavior_loss': np.float64(0.828532463312149)}

Episode step 2810, time diff 0.8234403133392334, total time dif 219.8471360206604)
step: 2810 @ episode report: {'average_total_reward': np.float32(1.8955555), 'reward_variance': np.float32(0.54257286), 'max_total_reward': np.float32(3.677778), 'min_total_reward': np.float32(1.0444444), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 91968} @ step loss: {'critic_loss': np.float64(0.09835666790604591), 'actor_loss': np.float64(-0.8485833704471588), 'hyper_actor_loss': np.float64(0.07382087707519532), 'behavior_loss': np.float64(0.8322216868400574)}

Episode step 2820, time diff 0.7980706691741943, total time dif 220.67057633399963)
step: 2820 @ episode report: {'average_total_reward': np.float32(2.2933335), 'reward_variance': np.float32(0.5295359), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(1.6777778), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 92288} @ step loss: {'critic_loss': np.float64(0.11892132088541985), 'actor_loss': np.float64(-0.9353216707706451), 'hyper_actor_loss': np.float64(0.07258765622973443), 'behavior_loss': np.float64(0.9414299488067627)}

Episode step 2830, time diff 0.9763948917388916, total time dif 221.46864700317383)
step: 2830 @ episode report: {'average_total_reward': np.float32(3.3033338), 'reward_variance': np.float32(0.7251865), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 92608} @ step loss: {'critic_loss': np.float64(0.10956933349370956), 'actor_loss': np.float64(-0.9610810697078704), 'hyper_actor_loss': np.float64(0.06819887608289718), 'behavior_loss': np.float64(1.0652876317501068)}

Episode step 2840, time diff 0.8220038414001465, total time dif 222.44504189491272)
step: 2840 @ episode report: {'average_total_reward': np.float32(4.037778), 'reward_variance': np.float32(0.6852888), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.0444448), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 92928} @ step loss: {'critic_loss': np.float64(0.10718299075961113), 'actor_loss': np.float64(-0.9093662619590759), 'hyper_actor_loss': np.float64(0.06222869902849197), 'behavior_loss': np.float64(1.3182599782943725)}

Episode step 2850, time diff 0.8369166851043701, total time dif 223.26704573631287)
step: 2850 @ episode report: {'average_total_reward': np.float32(4.4233336), 'reward_variance': np.float32(0.73551726), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 93248} @ step loss: {'critic_loss': np.float64(0.09338398948311806), 'actor_loss': np.float64(-0.9539956510066986), 'hyper_actor_loss': np.float64(0.05535142309963703), 'behavior_loss': np.float64(1.3765250444412231)}

Episode step 2860, time diff 0.8122532367706299, total time dif 224.10396242141724)
step: 2860 @ episode report: {'average_total_reward': np.float32(5.245556), 'reward_variance': np.float32(0.2490975), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(6.0), 'buffer_size': 93568} @ step loss: {'critic_loss': np.float64(0.1064186193048954), 'actor_loss': np.float64(-0.9153487622737885), 'hyper_actor_loss': np.float64(0.04875972904264927), 'behavior_loss': np.float64(1.5465042471885682)}

Episode step 2870, time diff 0.8396365642547607, total time dif 224.91621565818787)
step: 2870 @ episode report: {'average_total_reward': np.float32(6.0311112), 'reward_variance': np.float32(0.8375015), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 93888} @ step loss: {'critic_loss': np.float64(0.10185857713222504), 'actor_loss': np.float64(-0.9271839201450348), 'hyper_actor_loss': np.float64(0.042575033754110335), 'behavior_loss': np.float64(1.686493194103241)}

Episode step 2880, time diff 0.8303112983703613, total time dif 225.75585222244263)
step: 2880 @ episode report: {'average_total_reward': np.float32(7.5922227), 'reward_variance': np.float32(4.648198), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 94208} @ step loss: {'critic_loss': np.float64(0.0918625257909298), 'actor_loss': np.float64(-0.9149102449417115), 'hyper_actor_loss': np.float64(0.03671428635716438), 'behavior_loss': np.float64(1.7446340203285218)}

Episode step 2890, time diff 0.832935094833374, total time dif 226.586163520813)
step: 2890 @ episode report: {'average_total_reward': np.float32(6.682223), 'reward_variance': np.float32(1.3002769), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 94528} @ step loss: {'critic_loss': np.float64(0.10338700264692306), 'actor_loss': np.float64(-0.9374386310577393), 'hyper_actor_loss': np.float64(0.03272868264466524), 'behavior_loss': np.float64(1.8417239904403686)}

Episode step 2900, time diff 0.8023707866668701, total time dif 227.41909861564636)
step: 2900 @ episode report: {'average_total_reward': np.float32(7.1066666), 'reward_variance': np.float32(1.5817833), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 94848} @ step loss: {'critic_loss': np.float64(0.09155080020427704), 'actor_loss': np.float64(-0.9543500125408173), 'hyper_actor_loss': np.float64(0.02986499797552824), 'behavior_loss': np.float64(1.908332097530365)}

Episode step 2910, time diff 0.8060801029205322, total time dif 228.22146940231323)
step: 2910 @ episode report: {'average_total_reward': np.float32(7.5288887), 'reward_variance': np.float32(2.3041778), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 95168} @ step loss: {'critic_loss': np.float64(0.11080694943666458), 'actor_loss': np.float64(-0.9322956621646881), 'hyper_actor_loss': np.float64(0.02722796369343996), 'behavior_loss': np.float64(2.0164326548576357)}

Episode step 2920, time diff 0.8207392692565918, total time dif 229.02754950523376)
step: 2920 @ episode report: {'average_total_reward': np.float32(6.1333337), 'reward_variance': np.float32(1.0624446), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 95488} @ step loss: {'critic_loss': np.float64(0.09514155238866806), 'actor_loss': np.float64(-0.9388464987277985), 'hyper_actor_loss': np.float64(0.0253520829603076), 'behavior_loss': np.float64(2.051215875148773)}

Episode step 2930, time diff 0.8239893913269043, total time dif 229.84828877449036)
step: 2930 @ episode report: {'average_total_reward': np.float32(6.318889), 'reward_variance': np.float32(1.3437794), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 95808} @ step loss: {'critic_loss': np.float64(0.08766695410013199), 'actor_loss': np.float64(-0.9212292909622193), 'hyper_actor_loss': np.float64(0.023219670355319976), 'behavior_loss': np.float64(2.0609872341156006)}

Episode step 2940, time diff 0.805983304977417, total time dif 230.67227816581726)
step: 2940 @ episode report: {'average_total_reward': np.float32(6.667778), 'reward_variance': np.float32(2.0058634), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 96128} @ step loss: {'critic_loss': np.float64(0.08388292342424393), 'actor_loss': np.float64(-0.9345563411712646), 'hyper_actor_loss': np.float64(0.02175239473581314), 'behavior_loss': np.float64(2.0432831168174745)}

Episode step 2950, time diff 0.7523305416107178, total time dif 231.47826147079468)
step: 2950 @ episode report: {'average_total_reward': np.float32(6.9188895), 'reward_variance': np.float32(1.8181989), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 96448} @ step loss: {'critic_loss': np.float64(0.1008559674024582), 'actor_loss': np.float64(-0.9538050413131713), 'hyper_actor_loss': np.float64(0.020343844406306745), 'behavior_loss': np.float64(1.963476026058197)}

Episode step 2960, time diff 0.8064069747924805, total time dif 232.2305920124054)
step: 2960 @ episode report: {'average_total_reward': np.float32(7.604445), 'reward_variance': np.float32(2.0635116), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 96768} @ step loss: {'critic_loss': np.float64(0.09097402095794678), 'actor_loss': np.float64(-0.9473958373069763), 'hyper_actor_loss': np.float64(0.019541187770664693), 'behavior_loss': np.float64(2.0038341879844666)}

Episode step 2970, time diff 0.7364275455474854, total time dif 233.03699898719788)
step: 2970 @ episode report: {'average_total_reward': np.float32(7.3188896), 'reward_variance': np.float32(1.4185693), 'max_total_reward': np.float32(9.533334), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 97088} @ step loss: {'critic_loss': np.float64(0.09623829945921898), 'actor_loss': np.float64(-0.9180714964866639), 'hyper_actor_loss': np.float64(0.01859539970755577), 'behavior_loss': np.float64(2.177179229259491)}

Episode step 2980, time diff 0.9132158756256104, total time dif 233.77342653274536)
step: 2980 @ episode report: {'average_total_reward': np.float32(7.928889), 'reward_variance': np.float32(1.33877), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 97408} @ step loss: {'critic_loss': np.float64(0.10363265499472618), 'actor_loss': np.float64(-0.9607303500175476), 'hyper_actor_loss': np.float64(0.017879623174667358), 'behavior_loss': np.float64(2.083832883834839)}

Episode step 2990, time diff 0.8288154602050781, total time dif 234.68664240837097)
step: 2990 @ episode report: {'average_total_reward': np.float32(7.8900003), 'reward_variance': np.float32(2.2794685), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 97728} @ step loss: {'critic_loss': np.float64(0.09930402114987373), 'actor_loss': np.float64(-0.93528254032135), 'hyper_actor_loss': np.float64(0.01729999706149101), 'behavior_loss': np.float64(2.141941261291504)}

Episode step 3000, time diff 0.7584338188171387, total time dif 235.51545786857605)
step: 3000 @ episode report: {'average_total_reward': np.float32(7.167778), 'reward_variance': np.float32(2.9364316), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 98048} @ step loss: {'critic_loss': np.float64(0.105242071300745), 'actor_loss': np.float64(-0.9704135179519653), 'hyper_actor_loss': np.float64(0.01685463897883892), 'behavior_loss': np.float64(2.28024183511734)}

Episode step 3010, time diff 0.7510509490966797, total time dif 236.2738916873932)
step: 3010 @ episode report: {'average_total_reward': np.float32(8.077778), 'reward_variance': np.float32(1.6449387), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 98368} @ step loss: {'critic_loss': np.float64(0.0905759498476982), 'actor_loss': np.float64(-0.9440903961658478), 'hyper_actor_loss': np.float64(0.016815736517310144), 'behavior_loss': np.float64(2.1883732080459595)}

Episode step 3020, time diff 0.7762606143951416, total time dif 237.02494263648987)
step: 3020 @ episode report: {'average_total_reward': np.float32(6.9944444), 'reward_variance': np.float32(0.983883), 'max_total_reward': np.float32(8.533335), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 98688} @ step loss: {'critic_loss': np.float64(0.09296096116304398), 'actor_loss': np.float64(-0.9389859557151794), 'hyper_actor_loss': np.float64(0.01650808211416006), 'behavior_loss': np.float64(2.2279306411743165)}

Episode step 3030, time diff 0.7672080993652344, total time dif 237.801203250885)
step: 3030 @ episode report: {'average_total_reward': np.float32(8.116667), 'reward_variance': np.float32(1.563957), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 99008} @ step loss: {'critic_loss': np.float64(0.10783408731222152), 'actor_loss': np.float64(-0.9925101161003113), 'hyper_actor_loss': np.float64(0.016407561674714088), 'behavior_loss': np.float64(2.218533420562744)}

Episode step 3040, time diff 0.7690231800079346, total time dif 238.56841135025024)
step: 3040 @ episode report: {'average_total_reward': np.float32(7.916667), 'reward_variance': np.float32(1.8231176), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 99328} @ step loss: {'critic_loss': np.float64(0.10494250729680062), 'actor_loss': np.float64(-0.9416328132152557), 'hyper_actor_loss': np.float64(0.016476314701139926), 'behavior_loss': np.float64(2.2028268694877626)}

Episode step 3050, time diff 0.7451033592224121, total time dif 239.33743453025818)
step: 3050 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(2.8581595), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 99648} @ step loss: {'critic_loss': np.float64(0.09794717654585838), 'actor_loss': np.float64(-1.0110706746578217), 'hyper_actor_loss': np.float64(0.017029283195734025), 'behavior_loss': np.float64(2.2360228538513183)}

Episode step 3060, time diff 0.7695860862731934, total time dif 240.0825378894806)
step: 3060 @ episode report: {'average_total_reward': np.float32(7.4044447), 'reward_variance': np.float32(1.6474371), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 99968} @ step loss: {'critic_loss': np.float64(0.09903762228786946), 'actor_loss': np.float64(-0.9317169427871704), 'hyper_actor_loss': np.float64(0.01739903651177883), 'behavior_loss': np.float64(2.1446088910102845)}

Episode step 3070, time diff 0.7822654247283936, total time dif 240.85212397575378)
step: 3070 @ episode report: {'average_total_reward': np.float32(8.290001), 'reward_variance': np.float32(2.675591), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09020791128277779), 'actor_loss': np.float64(-0.987071305513382), 'hyper_actor_loss': np.float64(0.017347681522369384), 'behavior_loss': np.float64(2.1524736404418947)}

Episode step 3080, time diff 0.7356889247894287, total time dif 241.63438940048218)
step: 3080 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(1.6525971), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08906621113419533), 'actor_loss': np.float64(-0.9620241403579712), 'hyper_actor_loss': np.float64(0.01769982110708952), 'behavior_loss': np.float64(2.102106285095215)}

Episode step 3090, time diff 0.7580549716949463, total time dif 242.3700783252716)
step: 3090 @ episode report: {'average_total_reward': np.float32(8.953334), 'reward_variance': np.float32(2.142268), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09853834211826325), 'actor_loss': np.float64(-0.9937666237354279), 'hyper_actor_loss': np.float64(0.01817487720400095), 'behavior_loss': np.float64(2.1655187129974367)}

Episode step 3100, time diff 0.7148199081420898, total time dif 243.12813329696655)
step: 3100 @ episode report: {'average_total_reward': np.float32(8.241112), 'reward_variance': np.float32(2.371631), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10040691569447517), 'actor_loss': np.float64(-0.9600062489509582), 'hyper_actor_loss': np.float64(0.018328260630369186), 'behavior_loss': np.float64(2.2411023378372192)}

Episode step 3110, time diff 0.859224796295166, total time dif 243.84295320510864)
step: 3110 @ episode report: {'average_total_reward': np.float32(7.4044447), 'reward_variance': np.float32(4.6320305), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10326489210128784), 'actor_loss': np.float64(-0.9862509489059448), 'hyper_actor_loss': np.float64(0.019094863906502724), 'behavior_loss': np.float64(2.0296719789505007)}

Episode step 3120, time diff 0.757220983505249, total time dif 244.7021780014038)
step: 3120 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(1.5772903), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09992845207452775), 'actor_loss': np.float64(-0.9951010882854462), 'hyper_actor_loss': np.float64(0.01931301411241293), 'behavior_loss': np.float64(2.20730082988739)}

Episode step 3130, time diff 0.745652437210083, total time dif 245.45939898490906)
step: 3130 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(1.233062), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08910884335637093), 'actor_loss': np.float64(-0.9655148267745972), 'hyper_actor_loss': np.float64(0.02008939515799284), 'behavior_loss': np.float64(2.0541872382164)}

Episode step 3140, time diff 0.7398157119750977, total time dif 246.20505142211914)
step: 3140 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(1.6559947), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09901250675320625), 'actor_loss': np.float64(-0.9763483524322509), 'hyper_actor_loss': np.float64(0.020550431683659554), 'behavior_loss': np.float64(2.127460265159607)}

Episode step 3150, time diff 1.0171740055084229, total time dif 246.94486713409424)
step: 3150 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(2.1906471), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10171889141201973), 'actor_loss': np.float64(-1.0036542177200318), 'hyper_actor_loss': np.float64(0.020897759683430196), 'behavior_loss': np.float64(2.040937304496765)}

Episode step 3160, time diff 0.773428201675415, total time dif 247.96204113960266)
step: 3160 @ episode report: {'average_total_reward': np.float32(8.092222), 'reward_variance': np.float32(1.694643), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.533333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07953973561525345), 'actor_loss': np.float64(-0.9302122712135314), 'hyper_actor_loss': np.float64(0.021088095754384993), 'behavior_loss': np.float64(1.9353306531906127)}

Episode step 3170, time diff 0.7878105640411377, total time dif 248.73546934127808)
step: 3170 @ episode report: {'average_total_reward': np.float32(7.194444), 'reward_variance': np.float32(1.7445004), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11671726033091545), 'actor_loss': np.float64(-0.9885148108005524), 'hyper_actor_loss': np.float64(0.02120197396725416), 'behavior_loss': np.float64(1.9793269991874696)}

Episode step 3180, time diff 0.7597272396087646, total time dif 249.5232799053192)
step: 3180 @ episode report: {'average_total_reward': np.float32(7.3555555), 'reward_variance': np.float32(2.1361978), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09019947871565819), 'actor_loss': np.float64(-0.9602572083473205), 'hyper_actor_loss': np.float64(0.021532473899424077), 'behavior_loss': np.float64(1.974721086025238)}

Episode step 3190, time diff 0.748884916305542, total time dif 250.28300714492798)
step: 3190 @ episode report: {'average_total_reward': np.float32(7.480001), 'reward_variance': np.float32(3.145799), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10454977154731751), 'actor_loss': np.float64(-0.9863070547580719), 'hyper_actor_loss': np.float64(0.02205881830304861), 'behavior_loss': np.float64(1.87737877368927)}

Episode step 3200, time diff 0.7541728019714355, total time dif 251.03189206123352)
step: 3200 @ episode report: {'average_total_reward': np.float32(7.131111), 'reward_variance': np.float32(1.3561432), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08407003842294217), 'actor_loss': np.float64(-0.9762022197246552), 'hyper_actor_loss': np.float64(0.02233734615147114), 'behavior_loss': np.float64(1.93331698179245)}

Episode step 3210, time diff 0.7279953956604004, total time dif 251.78606486320496)
step: 3210 @ episode report: {'average_total_reward': np.float32(7.316667), 'reward_variance': np.float32(1.672698), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09739024266600609), 'actor_loss': np.float64(-0.9948015332221984), 'hyper_actor_loss': np.float64(0.022870827838778494), 'behavior_loss': np.float64(1.8882285952568054)}

Episode step 3220, time diff 0.7102665901184082, total time dif 252.51406025886536)
step: 3220 @ episode report: {'average_total_reward': np.float32(8.441112), 'reward_variance': np.float32(1.9268408), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0879993423819542), 'actor_loss': np.float64(-0.9956125736236572), 'hyper_actor_loss': np.float64(0.023585462383925913), 'behavior_loss': np.float64(1.798177468776703)}

Episode step 3230, time diff 0.7408366203308105, total time dif 253.22432684898376)
step: 3230 @ episode report: {'average_total_reward': np.float32(7.5800004), 'reward_variance': np.float32(1.9292297), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07944496013224125), 'actor_loss': np.float64(-0.9536903381347657), 'hyper_actor_loss': np.float64(0.024102382734417914), 'behavior_loss': np.float64(1.8495434641838073)}

Episode step 3240, time diff 0.7898566722869873, total time dif 253.96516346931458)
step: 3240 @ episode report: {'average_total_reward': np.float32(6.757778), 'reward_variance': np.float32(1.013452), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10440249517560005), 'actor_loss': np.float64(-1.0071667551994323), 'hyper_actor_loss': np.float64(0.024020100384950636), 'behavior_loss': np.float64(1.8576557397842408)}

Episode step 3250, time diff 0.747699499130249, total time dif 254.75502014160156)
step: 3250 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(2.6911464), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0982170507311821), 'actor_loss': np.float64(-0.9780418395996093), 'hyper_actor_loss': np.float64(0.023785082809627055), 'behavior_loss': np.float64(1.77926025390625)}

Episode step 3260, time diff 0.7729918956756592, total time dif 255.5027196407318)
step: 3260 @ episode report: {'average_total_reward': np.float32(7.3166666), 'reward_variance': np.float32(3.05576), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09088349714875221), 'actor_loss': np.float64(-0.9914901554584503), 'hyper_actor_loss': np.float64(0.023637747019529344), 'behavior_loss': np.float64(1.846497118473053)}

Episode step 3270, time diff 0.7563767433166504, total time dif 256.27571153640747)
step: 3270 @ episode report: {'average_total_reward': np.float32(7.541112), 'reward_variance': np.float32(2.612372), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09345945119857788), 'actor_loss': np.float64(-0.9493613362312316), 'hyper_actor_loss': np.float64(0.023913836851716042), 'behavior_loss': np.float64(1.7297484040260316)}

Episode step 3280, time diff 0.7273802757263184, total time dif 257.0320882797241)
step: 3280 @ episode report: {'average_total_reward': np.float32(8.465555), 'reward_variance': np.float32(2.4584062), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09595918729901314), 'actor_loss': np.float64(-1.0027860701084137), 'hyper_actor_loss': np.float64(0.023815271444618702), 'behavior_loss': np.float64(1.8231581449508667)}

Episode step 3290, time diff 0.7348225116729736, total time dif 257.75946855545044)
step: 3290 @ episode report: {'average_total_reward': np.float32(7.4044447), 'reward_variance': np.float32(3.355708), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0983108066022396), 'actor_loss': np.float64(-0.9627645492553711), 'hyper_actor_loss': np.float64(0.023798569105565548), 'behavior_loss': np.float64(1.8505735635757445)}

Episode step 3300, time diff 0.7412970066070557, total time dif 258.4942910671234)
step: 3300 @ episode report: {'average_total_reward': np.float32(6.5188894), 'reward_variance': np.float32(1.7376808), 'max_total_reward': np.float32(8.533335), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10177548974752426), 'actor_loss': np.float64(-1.0096810400485992), 'hyper_actor_loss': np.float64(0.023804382979869844), 'behavior_loss': np.float64(1.8817986130714417)}

Episode step 3310, time diff 0.9302051067352295, total time dif 259.23558807373047)
step: 3310 @ episode report: {'average_total_reward': np.float32(7.604445), 'reward_variance': np.float32(2.5702522), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09713578969240189), 'actor_loss': np.float64(-0.9924333930015564), 'hyper_actor_loss': np.float64(0.024252107366919518), 'behavior_loss': np.float64(1.7441531658172607)}

Episode step 3320, time diff 0.7259676456451416, total time dif 260.1657931804657)
step: 3320 @ episode report: {'average_total_reward': np.float32(7.055556), 'reward_variance': np.float32(2.0884204), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09015476368367673), 'actor_loss': np.float64(-0.9744141519069671), 'hyper_actor_loss': np.float64(0.024989608488976956), 'behavior_loss': np.float64(1.8224195480346679)}

Episode step 3330, time diff 0.7298135757446289, total time dif 260.89176082611084)
step: 3330 @ episode report: {'average_total_reward': np.float32(7.2433343), 'reward_variance': np.float32(0.77470267), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0972887136042118), 'actor_loss': np.float64(-1.0018937230110168), 'hyper_actor_loss': np.float64(0.026146698370575906), 'behavior_loss': np.float64(1.83372905254364)}

Episode step 3340, time diff 0.7315335273742676, total time dif 261.62157440185547)
step: 3340 @ episode report: {'average_total_reward': np.float32(6.3311114), 'reward_variance': np.float32(2.4371803), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09152193889021873), 'actor_loss': np.float64(-0.9956483542919159), 'hyper_actor_loss': np.float64(0.027509227953851224), 'behavior_loss': np.float64(1.6817256808280945)}

Episode step 3350, time diff 0.812659740447998, total time dif 262.35310792922974)
step: 3350 @ episode report: {'average_total_reward': np.float32(6.682223), 'reward_variance': np.float32(0.53568417), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09332354962825776), 'actor_loss': np.float64(-1.0035407841205597), 'hyper_actor_loss': np.float64(0.027979827672243118), 'behavior_loss': np.float64(1.728496527671814)}

Episode step 3360, time diff 0.7379641532897949, total time dif 263.16576766967773)
step: 3360 @ episode report: {'average_total_reward': np.float32(6.794445), 'reward_variance': np.float32(1.3438092), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0943900678306818), 'actor_loss': np.float64(-0.9753538548946381), 'hyper_actor_loss': np.float64(0.028001399338245393), 'behavior_loss': np.float64(1.7418959856033325)}

Episode step 3370, time diff 0.7519416809082031, total time dif 263.90373182296753)
step: 3370 @ episode report: {'average_total_reward': np.float32(7.482222), 'reward_variance': np.float32(1.5776837), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09705576747655868), 'actor_loss': np.float64(-0.9830261290073394), 'hyper_actor_loss': np.float64(0.028032567538321018), 'behavior_loss': np.float64(1.7711216688156128)}

Episode step 3380, time diff 0.8008599281311035, total time dif 264.65567350387573)
step: 3380 @ episode report: {'average_total_reward': np.float32(7.131111), 'reward_variance': np.float32(1.6813536), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09691674858331681), 'actor_loss': np.float64(-0.9815261721611023), 'hyper_actor_loss': np.float64(0.027433878928422927), 'behavior_loss': np.float64(1.7235981583595277)}

Episode step 3390, time diff 0.7507412433624268, total time dif 265.45653343200684)
step: 3390 @ episode report: {'average_total_reward': np.float32(6.582223), 'reward_variance': np.float32(1.0524739), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09577673859894276), 'actor_loss': np.float64(-0.9699324011802674), 'hyper_actor_loss': np.float64(0.026968786492943764), 'behavior_loss': np.float64(1.618039619922638)}

Episode step 3400, time diff 0.7409570217132568, total time dif 266.20727467536926)
step: 3400 @ episode report: {'average_total_reward': np.float32(7.504444), 'reward_variance': np.float32(1.7846706), 'max_total_reward': np.float32(10.899999), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09620614275336266), 'actor_loss': np.float64(-0.9947533905506134), 'hyper_actor_loss': np.float64(0.026492834277451038), 'behavior_loss': np.float64(1.8412924528121948)}

Episode step 3410, time diff 0.6860227584838867, total time dif 266.9482316970825)
step: 3410 @ episode report: {'average_total_reward': np.float32(6.2822223), 'reward_variance': np.float32(2.5571659), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09417448043823243), 'actor_loss': np.float64(-0.9817576766014099), 'hyper_actor_loss': np.float64(0.026365750655531883), 'behavior_loss': np.float64(1.730799102783203)}

Episode step 3420, time diff 0.7392303943634033, total time dif 267.6342544555664)
step: 3420 @ episode report: {'average_total_reward': np.float32(6.8433332), 'reward_variance': np.float32(3.8027031), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09687733724713325), 'actor_loss': np.float64(-0.9996885597705841), 'hyper_actor_loss': np.float64(0.025671530142426492), 'behavior_loss': np.float64(1.8097052812576293)}

Episode step 3430, time diff 0.6719522476196289, total time dif 268.3734848499298)
step: 3430 @ episode report: {'average_total_reward': np.float32(7.992223), 'reward_variance': np.float32(1.3083713), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08955069854855538), 'actor_loss': np.float64(-0.9695397078990936), 'hyper_actor_loss': np.float64(0.025785909779369832), 'behavior_loss': np.float64(1.7896041989326477)}

Episode step 3440, time diff 0.677509069442749, total time dif 269.04543709754944)
step: 3440 @ episode report: {'average_total_reward': np.float32(7.204445), 'reward_variance': np.float32(3.0322022), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09381408616900444), 'actor_loss': np.float64(-0.9528916299343109), 'hyper_actor_loss': np.float64(0.025665423646569253), 'behavior_loss': np.float64(1.8006591320037841)}

Episode step 3450, time diff 0.6830840110778809, total time dif 269.7229461669922)
step: 3450 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(3.717279), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08748603239655495), 'actor_loss': np.float64(-1.0058560729026795), 'hyper_actor_loss': np.float64(0.025583166815340518), 'behavior_loss': np.float64(1.6290504574775695)}

Episode step 3460, time diff 0.6752400398254395, total time dif 270.40603017807007)
step: 3460 @ episode report: {'average_total_reward': np.float32(7.853334), 'reward_variance': np.float32(4.6370325), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1026070300489664), 'actor_loss': np.float64(-0.9735572576522827), 'hyper_actor_loss': np.float64(0.025314841605722904), 'behavior_loss': np.float64(1.8105496883392334)}

Episode step 3470, time diff 0.8363721370697021, total time dif 271.0812702178955)
step: 3470 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(3.106193), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11194198057055474), 'actor_loss': np.float64(-1.0018798768520356), 'hyper_actor_loss': np.float64(0.02498023770749569), 'behavior_loss': np.float64(1.7358612418174744)}

Episode step 3480, time diff 0.6665639877319336, total time dif 271.9176423549652)
step: 3480 @ episode report: {'average_total_reward': np.float32(7.067778), 'reward_variance': np.float32(1.8360856), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10896389335393905), 'actor_loss': np.float64(-1.0355031967163086), 'hyper_actor_loss': np.float64(0.024674000032246114), 'behavior_loss': np.float64(1.7468135237693787)}

Episode step 3490, time diff 0.6920883655548096, total time dif 272.58420634269714)
step: 3490 @ episode report: {'average_total_reward': np.float32(7.543334), 'reward_variance': np.float32(2.079467), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08530690297484397), 'actor_loss': np.float64(-0.9783530950546264), 'hyper_actor_loss': np.float64(0.024527240172028543), 'behavior_loss': np.float64(1.728567087650299)}

Episode step 3500, time diff 0.6659419536590576, total time dif 273.27629470825195)
step: 3500 @ episode report: {'average_total_reward': np.float32(7.704445), 'reward_variance': np.float32(3.7408192), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09961104914546012), 'actor_loss': np.float64(-1.0252604842185975), 'hyper_actor_loss': np.float64(0.02508575711399317), 'behavior_loss': np.float64(1.793152630329132)}

Episode step 3510, time diff 0.6941425800323486, total time dif 273.942236661911)
step: 3510 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(2.973699), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09544055238366127), 'actor_loss': np.float64(-0.9649577260017395), 'hyper_actor_loss': np.float64(0.024871865287423134), 'behavior_loss': np.float64(1.8639794826507567)}

Episode step 3520, time diff 0.6834311485290527, total time dif 274.63637924194336)
step: 3520 @ episode report: {'average_total_reward': np.float32(6.9188895), 'reward_variance': np.float32(0.69298935), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08690410628914833), 'actor_loss': np.float64(-0.9832737624645234), 'hyper_actor_loss': np.float64(0.024914909340441226), 'behavior_loss': np.float64(1.774599301815033)}

Episode step 3530, time diff 0.6577363014221191, total time dif 275.3198103904724)
step: 3530 @ episode report: {'average_total_reward': np.float32(6.9066668), 'reward_variance': np.float32(2.5327208), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08859839886426926), 'actor_loss': np.float64(-0.9740824699401855), 'hyper_actor_loss': np.float64(0.02477595079690218), 'behavior_loss': np.float64(1.7397856831550598)}

Episode step 3540, time diff 0.6642637252807617, total time dif 275.97754669189453)
step: 3540 @ episode report: {'average_total_reward': np.float32(6.4433336), 'reward_variance': np.float32(1.9090979), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09319663271307946), 'actor_loss': np.float64(-1.016464227437973), 'hyper_actor_loss': np.float64(0.02491942886263132), 'behavior_loss': np.float64(1.8597888231277466)}

Episode step 3550, time diff 0.6905577182769775, total time dif 276.6418104171753)
step: 3550 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(1.3552544), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0787696111947298), 'actor_loss': np.float64(-0.9893622517585754), 'hyper_actor_loss': np.float64(0.02483154498040676), 'behavior_loss': np.float64(1.8212908029556274)}

Episode step 3560, time diff 0.6465928554534912, total time dif 277.33236813545227)
step: 3560 @ episode report: {'average_total_reward': np.float32(6.457778), 'reward_variance': np.float32(0.9795755), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0794171966612339), 'actor_loss': np.float64(-0.9579521536827087), 'hyper_actor_loss': np.float64(0.025242846831679343), 'behavior_loss': np.float64(1.8371086835861206)}

Episode step 3570, time diff 0.6573410034179688, total time dif 277.97896099090576)
step: 3570 @ episode report: {'average_total_reward': np.float32(6.9944444), 'reward_variance': np.float32(1.8726976), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08609585836529732), 'actor_loss': np.float64(-0.9865042924880981), 'hyper_actor_loss': np.float64(0.025396423228085042), 'behavior_loss': np.float64(1.845355260372162)}

Episode step 3580, time diff 0.6926562786102295, total time dif 278.63630199432373)
step: 3580 @ episode report: {'average_total_reward': np.float32(6.767778), 'reward_variance': np.float32(2.197172), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08819693177938462), 'actor_loss': np.float64(-0.9857405722141266), 'hyper_actor_loss': np.float64(0.02553149703890085), 'behavior_loss': np.float64(1.816999363899231)}

Episode step 3590, time diff 0.6746318340301514, total time dif 279.32895827293396)
step: 3590 @ episode report: {'average_total_reward': np.float32(7.753334), 'reward_variance': np.float32(4.246364), 'max_total_reward': np.float32(10.899999), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1034846380352974), 'actor_loss': np.float64(-1.004955679178238), 'hyper_actor_loss': np.float64(0.024794014729559423), 'behavior_loss': np.float64(1.84731365442276)}

Episode step 3600, time diff 0.6649842262268066, total time dif 280.0035901069641)
step: 3600 @ episode report: {'average_total_reward': np.float32(6.906667), 'reward_variance': np.float32(2.4120297), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08708004876971245), 'actor_loss': np.float64(-0.9980583667755127), 'hyper_actor_loss': np.float64(0.024125759862363337), 'behavior_loss': np.float64(1.8211767196655273)}

Episode step 3610, time diff 0.720585823059082, total time dif 280.6685743331909)
step: 3610 @ episode report: {'average_total_reward': np.float32(7.118889), 'reward_variance': np.float32(1.7965443), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10906712785363197), 'actor_loss': np.float64(-0.9891034483909606), 'hyper_actor_loss': np.float64(0.02450749147683382), 'behavior_loss': np.float64(1.8572027802467346)}

Episode step 3620, time diff 0.6631536483764648, total time dif 281.38916015625)
step: 3620 @ episode report: {'average_total_reward': np.float32(6.0700006), 'reward_variance': np.float32(1.8758532), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09139237329363822), 'actor_loss': np.float64(-1.008076947927475), 'hyper_actor_loss': np.float64(0.025119188241660595), 'behavior_loss': np.float64(1.834528386592865)}

Episode step 3630, time diff 0.7852833271026611, total time dif 282.05231380462646)
step: 3630 @ episode report: {'average_total_reward': np.float32(6.5188894), 'reward_variance': np.float32(0.7621244), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10050913989543915), 'actor_loss': np.float64(-0.9843947052955627), 'hyper_actor_loss': np.float64(0.025526359491050244), 'behavior_loss': np.float64(1.719117248058319)}

Episode step 3640, time diff 0.7099971771240234, total time dif 282.8375971317291)
step: 3640 @ episode report: {'average_total_reward': np.float32(6.8066673), 'reward_variance': np.float32(2.8255863), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08809918239712715), 'actor_loss': np.float64(-1.0118850886821746), 'hyper_actor_loss': np.float64(0.02496310882270336), 'behavior_loss': np.float64(1.791725718975067)}

Episode step 3650, time diff 0.6745076179504395, total time dif 283.54759430885315)
step: 3650 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(1.0789088), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10602028630673885), 'actor_loss': np.float64(-0.9738080859184265), 'hyper_actor_loss': np.float64(0.02400482799857855), 'behavior_loss': np.float64(1.8879202008247375)}

Episode step 3660, time diff 0.6725091934204102, total time dif 284.2221019268036)
step: 3660 @ episode report: {'average_total_reward': np.float32(7.006667), 'reward_variance': np.float32(1.7375603), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09662592634558678), 'actor_loss': np.float64(-1.0098170280456542), 'hyper_actor_loss': np.float64(0.02336171194911003), 'behavior_loss': np.float64(1.8429053664207458)}

Episode step 3670, time diff 0.6631572246551514, total time dif 284.894611120224)
step: 3670 @ episode report: {'average_total_reward': np.float32(6.506667), 'reward_variance': np.float32(2.49882), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0780483465641737), 'actor_loss': np.float64(-0.9350666522979736), 'hyper_actor_loss': np.float64(0.022212915867567063), 'behavior_loss': np.float64(1.8823270797729492)}

Episode step 3680, time diff 0.685046911239624, total time dif 285.55776834487915)
step: 3680 @ episode report: {'average_total_reward': np.float32(6.455556), 'reward_variance': np.float32(1.1186175), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09967766180634499), 'actor_loss': np.float64(-0.9934062600135803), 'hyper_actor_loss': np.float64(0.02196783721446991), 'behavior_loss': np.float64(1.995107352733612)}

Episode step 3690, time diff 0.6684844493865967, total time dif 286.2428152561188)
step: 3690 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(2.0011408), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0909100167453289), 'actor_loss': np.float64(-1.0231061339378358), 'hyper_actor_loss': np.float64(0.02225746847689152), 'behavior_loss': np.float64(1.900412368774414)}

Episode step 3700, time diff 0.6461193561553955, total time dif 286.91129970550537)
step: 3700 @ episode report: {'average_total_reward': np.float32(6.955556), 'reward_variance': np.float32(1.1167411), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10158986002206802), 'actor_loss': np.float64(-1.0209401845932007), 'hyper_actor_loss': np.float64(0.02288690712302923), 'behavior_loss': np.float64(1.937025237083435)}

Episode step 3710, time diff 0.6837782859802246, total time dif 287.55741906166077)
step: 3710 @ episode report: {'average_total_reward': np.float32(7.753334), 'reward_variance': np.float32(4.3531075), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10030753947794438), 'actor_loss': np.float64(-1.0136228561401368), 'hyper_actor_loss': np.float64(0.023117810487747192), 'behavior_loss': np.float64(1.9709778666496276)}

Episode step 3720, time diff 0.7010912895202637, total time dif 288.241197347641)
step: 3720 @ episode report: {'average_total_reward': np.float32(6.631111), 'reward_variance': np.float32(2.841847), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08471538610756398), 'actor_loss': np.float64(-0.9652004420757294), 'hyper_actor_loss': np.float64(0.023415803164243697), 'behavior_loss': np.float64(1.844417428970337)}

Episode step 3730, time diff 0.6868176460266113, total time dif 288.94228863716125)
step: 3730 @ episode report: {'average_total_reward': np.float32(7.5288897), 'reward_variance': np.float32(3.3605723), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09994547143578529), 'actor_loss': np.float64(-0.9784124612808227), 'hyper_actor_loss': np.float64(0.023401495441794394), 'behavior_loss': np.float64(1.9214072585105897)}

Episode step 3740, time diff 0.665762186050415, total time dif 289.62910628318787)
step: 3740 @ episode report: {'average_total_reward': np.float32(7.006667), 'reward_variance': np.float32(2.389437), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08480312637984752), 'actor_loss': np.float64(-0.9526207149028778), 'hyper_actor_loss': np.float64(0.02295736875385046), 'behavior_loss': np.float64(1.8920457363128662)}

Episode step 3750, time diff 0.6650574207305908, total time dif 290.2948684692383)
step: 3750 @ episode report: {'average_total_reward': np.float32(7.406667), 'reward_variance': np.float32(2.4338326), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07756268754601478), 'actor_loss': np.float64(-0.955036997795105), 'hyper_actor_loss': np.float64(0.022719265706837178), 'behavior_loss': np.float64(1.920268189907074)}

Episode step 3760, time diff 0.6523206233978271, total time dif 290.9599258899689)
step: 3760 @ episode report: {'average_total_reward': np.float32(7.5166674), 'reward_variance': np.float32(3.1373646), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08971403986215591), 'actor_loss': np.float64(-0.9568959891796112), 'hyper_actor_loss': np.float64(0.0222117405384779), 'behavior_loss': np.float64(2.0749706983566285)}

Episode step 3770, time diff 0.6555473804473877, total time dif 291.6122465133667)
step: 3770 @ episode report: {'average_total_reward': np.float32(7.267779), 'reward_variance': np.float32(1.57932), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10066211149096489), 'actor_loss': np.float64(-0.9538085520267486), 'hyper_actor_loss': np.float64(0.02189448941498995), 'behavior_loss': np.float64(1.9432912468910217)}

Episode step 3780, time diff 0.6870675086975098, total time dif 292.2677938938141)
step: 3780 @ episode report: {'average_total_reward': np.float32(7.2433333), 'reward_variance': np.float32(1.8146282), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08186500370502472), 'actor_loss': np.float64(-0.9602037310600281), 'hyper_actor_loss': np.float64(0.02171130310744047), 'behavior_loss': np.float64(1.9465235352516175)}

Episode step 3790, time diff 0.7846975326538086, total time dif 292.9548614025116)
step: 3790 @ episode report: {'average_total_reward': np.float32(7.118889), 'reward_variance': np.float32(2.6235075), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09226912260055542), 'actor_loss': np.float64(-0.9805683374404908), 'hyper_actor_loss': np.float64(0.021091791987419128), 'behavior_loss': np.float64(1.9516512274742126)}

Episode step 3800, time diff 0.6841814517974854, total time dif 293.7395589351654)
step: 3800 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(2.825715), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09734922498464585), 'actor_loss': np.float64(-0.9872494697570801), 'hyper_actor_loss': np.float64(0.021299088932573797), 'behavior_loss': np.float64(1.9524888038635253)}

Episode step 3810, time diff 0.6888382434844971, total time dif 294.4237403869629)
step: 3810 @ episode report: {'average_total_reward': np.float32(6.645556), 'reward_variance': np.float32(3.421616), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10048749670386314), 'actor_loss': np.float64(-0.9723457753658294), 'hyper_actor_loss': np.float64(0.021416904963552953), 'behavior_loss': np.float64(1.994642162322998)}

Episode step 3820, time diff 0.6582789421081543, total time dif 295.1125786304474)
step: 3820 @ episode report: {'average_total_reward': np.float32(6.955556), 'reward_variance': np.float32(1.76563), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08563006706535817), 'actor_loss': np.float64(-0.976455682516098), 'hyper_actor_loss': np.float64(0.02163369543850422), 'behavior_loss': np.float64(1.9008987188339233)}

Episode step 3830, time diff 0.674124002456665, total time dif 295.77085757255554)
step: 3830 @ episode report: {'average_total_reward': np.float32(5.87), 'reward_variance': np.float32(1.4809396), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1068459302186966), 'actor_loss': np.float64(-1.0229375958442688), 'hyper_actor_loss': np.float64(0.02174578458070755), 'behavior_loss': np.float64(1.894126832485199)}

Episode step 3840, time diff 0.6723771095275879, total time dif 296.4449815750122)
step: 3840 @ episode report: {'average_total_reward': np.float32(7.455556), 'reward_variance': np.float32(1.5579998), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08358118087053298), 'actor_loss': np.float64(-0.9728225290775299), 'hyper_actor_loss': np.float64(0.022108210436999797), 'behavior_loss': np.float64(1.9801056385040283)}

Episode step 3850, time diff 0.7190859317779541, total time dif 297.1173586845398)
step: 3850 @ episode report: {'average_total_reward': np.float32(5.7822227), 'reward_variance': np.float32(1.5545238), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08775850683450699), 'actor_loss': np.float64(-0.9622775912284851), 'hyper_actor_loss': np.float64(0.02151530347764492), 'behavior_loss': np.float64(1.9494320273399353)}

Episode step 3860, time diff 0.6720187664031982, total time dif 297.83644461631775)
step: 3860 @ episode report: {'average_total_reward': np.float32(5.7333336), 'reward_variance': np.float32(1.5400745), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08940231688320636), 'actor_loss': np.float64(-0.9740028560161591), 'hyper_actor_loss': np.float64(0.021122227236628533), 'behavior_loss': np.float64(1.9980346441268921)}

Episode step 3870, time diff 0.6981852054595947, total time dif 298.50846338272095)
step: 3870 @ episode report: {'average_total_reward': np.float32(5.77), 'reward_variance': np.float32(2.6537547), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07805982604622841), 'actor_loss': np.float64(-0.9456946194171906), 'hyper_actor_loss': np.float64(0.02082349415868521), 'behavior_loss': np.float64(2.0190991878509523)}

Episode step 3880, time diff 0.6631438732147217, total time dif 299.20664858818054)
step: 3880 @ episode report: {'average_total_reward': np.float32(6.3700004), 'reward_variance': np.float32(1.5179522), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08707142844796181), 'actor_loss': np.float64(-0.9925669014453888), 'hyper_actor_loss': np.float64(0.021002424694597722), 'behavior_loss': np.float64(1.9215244889259337)}

Episode step 3890, time diff 0.6703922748565674, total time dif 299.86979246139526)
step: 3890 @ episode report: {'average_total_reward': np.float32(5.784445), 'reward_variance': np.float32(2.8314862), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09146501794457436), 'actor_loss': np.float64(-0.9526813089847564), 'hyper_actor_loss': np.float64(0.020933833345770834), 'behavior_loss': np.float64(2.06930912733078)}

Episode step 3900, time diff 0.6836051940917969, total time dif 300.54018473625183)
step: 3900 @ episode report: {'average_total_reward': np.float32(5.7211113), 'reward_variance': np.float32(3.42416), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0946394756436348), 'actor_loss': np.float64(-0.9784048497676849), 'hyper_actor_loss': np.float64(0.020998112671077252), 'behavior_loss': np.float64(1.9932201266288758)}

Episode step 3910, time diff 0.6824145317077637, total time dif 301.2237899303436)
step: 3910 @ episode report: {'average_total_reward': np.float32(6.0211115), 'reward_variance': np.float32(1.0096903), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09363120570778846), 'actor_loss': np.float64(-0.9873935580253601), 'hyper_actor_loss': np.float64(0.020645926520228387), 'behavior_loss': np.float64(1.995719575881958)}

Episode step 3920, time diff 0.6620440483093262, total time dif 301.9062044620514)
step: 3920 @ episode report: {'average_total_reward': np.float32(5.545556), 'reward_variance': np.float32(1.2941345), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0849829412996769), 'actor_loss': np.float64(-0.9396523416042328), 'hyper_actor_loss': np.float64(0.020108948461711407), 'behavior_loss': np.float64(1.9818496227264404)}

Episode step 3930, time diff 0.6550343036651611, total time dif 302.5682485103607)
step: 3930 @ episode report: {'average_total_reward': np.float32(6.2311115), 'reward_variance': np.float32(1.9036748), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0915108636021614), 'actor_loss': np.float64(-0.9965101182460785), 'hyper_actor_loss': np.float64(0.019355432689189912), 'behavior_loss': np.float64(2.0607841491699217)}

Episode step 3940, time diff 0.6616494655609131, total time dif 303.2232828140259)
step: 3940 @ episode report: {'average_total_reward': np.float32(4.747778), 'reward_variance': np.float32(0.503853), 'max_total_reward': np.float32(5.6555552), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08005224987864494), 'actor_loss': np.float64(-0.9686034202575684), 'hyper_actor_loss': np.float64(0.018681043945252895), 'behavior_loss': np.float64(2.295827317237854)}

Episode step 3950, time diff 0.784949541091919, total time dif 303.8849322795868)
step: 3950 @ episode report: {'average_total_reward': np.float32(4.684445), 'reward_variance': np.float32(0.21356058), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08735749945044517), 'actor_loss': np.float64(-0.9528245627880096), 'hyper_actor_loss': np.float64(0.018150604888796808), 'behavior_loss': np.float64(2.1820197820663454)}

Episode step 3960, time diff 0.7019646167755127, total time dif 304.6698818206787)
step: 3960 @ episode report: {'average_total_reward': np.float32(5.9455557), 'reward_variance': np.float32(2.7124062), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09764897972345352), 'actor_loss': np.float64(-0.9721150934696198), 'hyper_actor_loss': np.float64(0.017786716111004353), 'behavior_loss': np.float64(2.139441454410553)}

Episode step 3970, time diff 0.6856465339660645, total time dif 305.3718464374542)
step: 3970 @ episode report: {'average_total_reward': np.float32(4.711111), 'reward_variance': np.float32(1.3237039), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09050087593495845), 'actor_loss': np.float64(-0.9738725543022155), 'hyper_actor_loss': np.float64(0.01732485890388489), 'behavior_loss': np.float64(2.2672046303749083)}

Episode step 3980, time diff 0.6728427410125732, total time dif 306.0574929714203)
step: 3980 @ episode report: {'average_total_reward': np.float32(4.66), 'reward_variance': np.float32(0.8173631), 'max_total_reward': np.float32(6.5333333), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07756400071084499), 'actor_loss': np.float64(-0.9334118127822876), 'hyper_actor_loss': np.float64(0.01726409625262022), 'behavior_loss': np.float64(2.426063907146454)}

Episode step 3990, time diff 0.697695255279541, total time dif 306.73033571243286)
step: 3990 @ episode report: {'average_total_reward': np.float32(4.1988893), 'reward_variance': np.float32(0.7988258), 'max_total_reward': np.float32(6.411111), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09769490957260132), 'actor_loss': np.float64(-0.9887035489082336), 'hyper_actor_loss': np.float64(0.017140136659145357), 'behavior_loss': np.float64(2.2491963505744934)}

Episode step 4000, time diff 0.6748321056365967, total time dif 307.4280309677124)
step: 4000 @ episode report: {'average_total_reward': np.float32(4.001111), 'reward_variance': np.float32(1.0610482), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09038161039352417), 'actor_loss': np.float64(-0.9869913339614869), 'hyper_actor_loss': np.float64(0.017299578152596952), 'behavior_loss': np.float64(2.2933404445648193)}

Episode step 4010, time diff 0.6833109855651855, total time dif 308.102863073349)
step: 4010 @ episode report: {'average_total_reward': np.float32(4.086667), 'reward_variance': np.float32(1.3847605), 'max_total_reward': np.float32(5.655556), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08673069290816784), 'actor_loss': np.float64(-0.955044686794281), 'hyper_actor_loss': np.float64(0.017644474841654302), 'behavior_loss': np.float64(2.1684070110321043)}

Episode step 4020, time diff 0.6689190864562988, total time dif 308.7861740589142)
step: 4020 @ episode report: {'average_total_reward': np.float32(4.362222), 'reward_variance': np.float32(1.0644249), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0827870711684227), 'actor_loss': np.float64(-0.9514571547508239), 'hyper_actor_loss': np.float64(0.018212834000587465), 'behavior_loss': np.float64(2.181413769721985)}

Episode step 4030, time diff 0.6676278114318848, total time dif 309.4550931453705)
step: 4030 @ episode report: {'average_total_reward': np.float32(4.2133336), 'reward_variance': np.float32(0.4583407), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08959441259503365), 'actor_loss': np.float64(-0.9926869332790375), 'hyper_actor_loss': np.float64(0.018554291874170303), 'behavior_loss': np.float64(2.1868090987205506)}

Episode step 4040, time diff 0.6626529693603516, total time dif 310.12272095680237)
step: 4040 @ episode report: {'average_total_reward': np.float32(4.498889), 'reward_variance': np.float32(0.91623324), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08709818720817566), 'actor_loss': np.float64(-0.9573958516120911), 'hyper_actor_loss': np.float64(0.018622591719031335), 'behavior_loss': np.float64(2.12818443775177)}

Episode step 4050, time diff 0.6688046455383301, total time dif 310.7853739261627)
step: 4050 @ episode report: {'average_total_reward': np.float32(4.535556), 'reward_variance': np.float32(1.3498719), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09802045449614524), 'actor_loss': np.float64(-0.9651705265045166), 'hyper_actor_loss': np.float64(0.018857094272971154), 'behavior_loss': np.float64(2.4046642422676086)}

Episode step 4060, time diff 0.6633927822113037, total time dif 311.45417857170105)
step: 4060 @ episode report: {'average_total_reward': np.float32(4.411111), 'reward_variance': np.float32(1.6379261), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08917007893323899), 'actor_loss': np.float64(-0.9781666815280914), 'hyper_actor_loss': np.float64(0.018690168857574463), 'behavior_loss': np.float64(2.1872377157211305)}

Episode step 4070, time diff 0.6565229892730713, total time dif 312.11757135391235)
step: 4070 @ episode report: {'average_total_reward': np.float32(4.45), 'reward_variance': np.float32(1.2705495), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09328043311834336), 'actor_loss': np.float64(-0.9243314802646637), 'hyper_actor_loss': np.float64(0.018413518741726875), 'behavior_loss': np.float64(2.3214610815048218)}

Episode step 4080, time diff 0.6774938106536865, total time dif 312.7740943431854)
step: 4080 @ episode report: {'average_total_reward': np.float32(4.1722226), 'reward_variance': np.float32(1.260895), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.2888892), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10227463170886039), 'actor_loss': np.float64(-0.9953334093093872), 'hyper_actor_loss': np.float64(0.018116528168320656), 'behavior_loss': np.float64(2.316305232048035)}

Episode step 4090, time diff 0.6718635559082031, total time dif 313.4515881538391)
step: 4090 @ episode report: {'average_total_reward': np.float32(4.7111106), 'reward_variance': np.float32(0.8294569), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09025196246802807), 'actor_loss': np.float64(-0.9137911081314087), 'hyper_actor_loss': np.float64(0.01790346037596464), 'behavior_loss': np.float64(2.3368479013442993)}

Episode step 4100, time diff 0.6758427619934082, total time dif 314.1234517097473)
step: 4100 @ episode report: {'average_total_reward': np.float32(4.8500004), 'reward_variance': np.float32(3.7109196), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(2.166667), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0865815319120884), 'actor_loss': np.float64(-0.9816567599773407), 'hyper_actor_loss': np.float64(0.017568570375442506), 'behavior_loss': np.float64(2.147778272628784)}

Episode step 4110, time diff 0.7853326797485352, total time dif 314.7992944717407)
step: 4110 @ episode report: {'average_total_reward': np.float32(4.8233333), 'reward_variance': np.float32(1.814629), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09430209062993526), 'actor_loss': np.float64(-0.9380071640014649), 'hyper_actor_loss': np.float64(0.017135386168956757), 'behavior_loss': np.float64(2.3575930118560793)}

Episode step 4120, time diff 0.696882963180542, total time dif 315.58462715148926)
step: 4120 @ episode report: {'average_total_reward': np.float32(4.635556), 'reward_variance': np.float32(2.2424147), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08769085146486759), 'actor_loss': np.float64(-0.9661495625972748), 'hyper_actor_loss': np.float64(0.01652650386095047), 'behavior_loss': np.float64(2.3935914516448973)}

Episode step 4130, time diff 0.67482590675354, total time dif 316.2815101146698)
step: 4130 @ episode report: {'average_total_reward': np.float32(4.6844444), 'reward_variance': np.float32(0.88689387), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08357177786529064), 'actor_loss': np.float64(-0.9317925810813904), 'hyper_actor_loss': np.float64(0.015841415990144013), 'behavior_loss': np.float64(2.4715492486953736)}

Episode step 4140, time diff 0.6864440441131592, total time dif 316.95633602142334)
step: 4140 @ episode report: {'average_total_reward': np.float32(5.6333337), 'reward_variance': np.float32(3.1020246), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08280150145292282), 'actor_loss': np.float64(-0.9673392474651337), 'hyper_actor_loss': np.float64(0.015286625362932682), 'behavior_loss': np.float64(2.4839278936386107)}

Episode step 4150, time diff 0.6977818012237549, total time dif 317.6427800655365)
step: 4150 @ episode report: {'average_total_reward': np.float32(5.496667), 'reward_variance': np.float32(2.530866), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0921092040836811), 'actor_loss': np.float64(-0.940749216079712), 'hyper_actor_loss': np.float64(0.0149068558588624), 'behavior_loss': np.float64(2.5990707874298096)}

Episode step 4160, time diff 0.6983675956726074, total time dif 318.34056186676025)
step: 4160 @ episode report: {'average_total_reward': np.float32(6.172223), 'reward_variance': np.float32(1.4329202), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0826793685555458), 'actor_loss': np.float64(-0.9686523318290711), 'hyper_actor_loss': np.float64(0.014822936151176691), 'behavior_loss': np.float64(2.5225716114044188)}

Episode step 4170, time diff 0.6737344264984131, total time dif 319.03892946243286)
step: 4170 @ episode report: {'average_total_reward': np.float32(4.7988887), 'reward_variance': np.float32(1.1009994), 'max_total_reward': np.float32(6.6555567), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09324638918042183), 'actor_loss': np.float64(-0.9574082911014556), 'hyper_actor_loss': np.float64(0.014763041865080595), 'behavior_loss': np.float64(2.4673895597457887)}

Episode step 4180, time diff 0.7245025634765625, total time dif 319.7126638889313)
step: 4180 @ episode report: {'average_total_reward': np.float32(5.46), 'reward_variance': np.float32(1.22603), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09278296008706093), 'actor_loss': np.float64(-0.9512708187103271), 'hyper_actor_loss': np.float64(0.014278894942253827), 'behavior_loss': np.float64(2.5336666822433473)}

Episode step 4190, time diff 0.6919436454772949, total time dif 320.43716645240784)
step: 4190 @ episode report: {'average_total_reward': np.float32(6.208889), 'reward_variance': np.float32(1.1104395), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09149175137281418), 'actor_loss': np.float64(-0.9690827667713166), 'hyper_actor_loss': np.float64(0.014006668329238891), 'behavior_loss': np.float64(2.5814308404922484)}

Episode step 4200, time diff 0.6659946441650391, total time dif 321.12911009788513)
step: 4200 @ episode report: {'average_total_reward': np.float32(5.808889), 'reward_variance': np.float32(1.5582424), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09755969420075417), 'actor_loss': np.float64(-0.9504936754703521), 'hyper_actor_loss': np.float64(0.013647172879427672), 'behavior_loss': np.float64(2.639087677001953)}

Episode step 4210, time diff 0.6692807674407959, total time dif 321.79510474205017)
step: 4210 @ episode report: {'average_total_reward': np.float32(5.56), 'reward_variance': np.float32(1.4913136), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08392644263803958), 'actor_loss': np.float64(-0.941212922334671), 'hyper_actor_loss': np.float64(0.013193394429981709), 'behavior_loss': np.float64(2.559622120857239)}

Episode step 4220, time diff 0.6703357696533203, total time dif 322.46438550949097)
step: 4220 @ episode report: {'average_total_reward': np.float32(6.0211115), 'reward_variance': np.float32(0.5303818), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09349221102893353), 'actor_loss': np.float64(-0.9755151450634003), 'hyper_actor_loss': np.float64(0.012408591993153095), 'behavior_loss': np.float64(2.477055275440216)}

Episode step 4230, time diff 0.6745281219482422, total time dif 323.1347212791443)
step: 4230 @ episode report: {'average_total_reward': np.float32(5.882222), 'reward_variance': np.float32(1.4436347), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09957053810358048), 'actor_loss': np.float64(-0.9608951210975647), 'hyper_actor_loss': np.float64(0.011761464271694421), 'behavior_loss': np.float64(2.5945544719696043)}

Episode step 4240, time diff 0.6778252124786377, total time dif 323.80924940109253)
step: 4240 @ episode report: {'average_total_reward': np.float32(5.645556), 'reward_variance': np.float32(0.49487543), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08740345537662506), 'actor_loss': np.float64(-0.9285837113857269), 'hyper_actor_loss': np.float64(0.011008429434150457), 'behavior_loss': np.float64(2.6141822576522826)}

Episode step 4250, time diff 0.6610729694366455, total time dif 324.48707461357117)
step: 4250 @ episode report: {'average_total_reward': np.float32(6.1333337), 'reward_variance': np.float32(1.3831359), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08651392832398415), 'actor_loss': np.float64(-0.920174914598465), 'hyper_actor_loss': np.float64(0.010285231936722993), 'behavior_loss': np.float64(2.5624236822128297)}

Episode step 4260, time diff 0.6534478664398193, total time dif 325.1481475830078)
step: 4260 @ episode report: {'average_total_reward': np.float32(5.882222), 'reward_variance': np.float32(0.91244954), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09837085828185081), 'actor_loss': np.float64(-0.9635895371437073), 'hyper_actor_loss': np.float64(0.009983737114816904), 'behavior_loss': np.float64(2.729329228401184)}

Episode step 4270, time diff 0.8263041973114014, total time dif 325.80159544944763)
step: 4270 @ episode report: {'average_total_reward': np.float32(6.3944445), 'reward_variance': np.float32(2.1408217), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09374525919556617), 'actor_loss': np.float64(-0.9291970610618592), 'hyper_actor_loss': np.float64(0.009458506479859352), 'behavior_loss': np.float64(2.7818068742752073)}

Episode step 4280, time diff 0.6878454685211182, total time dif 326.62789964675903)
step: 4280 @ episode report: {'average_total_reward': np.float32(6.131111), 'reward_variance': np.float32(2.9926126), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09268731027841567), 'actor_loss': np.float64(-0.9630556225776672), 'hyper_actor_loss': np.float64(0.009119350556284189), 'behavior_loss': np.float64(2.7131497383117678)}

Episode step 4290, time diff 0.6585290431976318, total time dif 327.31574511528015)
step: 4290 @ episode report: {'average_total_reward': np.float32(7.480001), 'reward_variance': np.float32(2.3353038), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09142653532326221), 'actor_loss': np.float64(-0.9702761173248291), 'hyper_actor_loss': np.float64(0.009124552737921477), 'behavior_loss': np.float64(2.5573861598968506)}

Episode step 4300, time diff 0.6903316974639893, total time dif 327.9742741584778)
step: 4300 @ episode report: {'average_total_reward': np.float32(6.308889), 'reward_variance': np.float32(1.3446374), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08742149919271469), 'actor_loss': np.float64(-0.9299969613552094), 'hyper_actor_loss': np.float64(0.009265299886465073), 'behavior_loss': np.float64(2.869450545310974)}

Episode step 4310, time diff 0.6929421424865723, total time dif 328.6646058559418)
step: 4310 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(1.3293087), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08988911807537078), 'actor_loss': np.float64(-0.9645898044109344), 'hyper_actor_loss': np.float64(0.009297101572155953), 'behavior_loss': np.float64(2.7883556842803956)}

Episode step 4320, time diff 0.6925983428955078, total time dif 329.35754799842834)
step: 4320 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(2.1767275), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0974576637148857), 'actor_loss': np.float64(-0.9596649169921875), 'hyper_actor_loss': np.float64(0.009491810481995345), 'behavior_loss': np.float64(2.6649046659469606)}

Episode step 4330, time diff 0.6915056705474854, total time dif 330.05014634132385)
step: 4330 @ episode report: {'average_total_reward': np.float32(5.9700003), 'reward_variance': np.float32(0.91755664), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0812185388058424), 'actor_loss': np.float64(-0.9703009128570557), 'hyper_actor_loss': np.float64(0.009464624524116515), 'behavior_loss': np.float64(2.497530686855316)}

Episode step 4340, time diff 0.6616990566253662, total time dif 330.74165201187134)
step: 4340 @ episode report: {'average_total_reward': np.float32(5.9822226), 'reward_variance': np.float32(1.96761), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08736084178090095), 'actor_loss': np.float64(-0.943998646736145), 'hyper_actor_loss': np.float64(0.009279458690434695), 'behavior_loss': np.float64(2.564249634742737)}

Episode step 4350, time diff 0.6671252250671387, total time dif 331.4033510684967)
step: 4350 @ episode report: {'average_total_reward': np.float32(6.4066668), 'reward_variance': np.float32(1.2966712), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09046581983566285), 'actor_loss': np.float64(-0.9848213136196137), 'hyper_actor_loss': np.float64(0.008932832442224026), 'behavior_loss': np.float64(2.5106287240982055)}

Episode step 4360, time diff 0.6697874069213867, total time dif 332.07047629356384)
step: 4360 @ episode report: {'average_total_reward': np.float32(7.543334), 'reward_variance': np.float32(0.649962), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10130584985017776), 'actor_loss': np.float64(-0.9584000647068024), 'hyper_actor_loss': np.float64(0.008795357402414083), 'behavior_loss': np.float64(2.750164258480072)}

Episode step 4370, time diff 0.6948564052581787, total time dif 332.74026370048523)
step: 4370 @ episode report: {'average_total_reward': np.float32(6.318889), 'reward_variance': np.float32(4.8780265), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07968315295875072), 'actor_loss': np.float64(-0.9561435759067536), 'hyper_actor_loss': np.float64(0.008544068224728108), 'behavior_loss': np.float64(2.4791601419448854)}

Episode step 4380, time diff 0.6994528770446777, total time dif 333.4351201057434)
step: 4380 @ episode report: {'average_total_reward': np.float32(6.994445), 'reward_variance': np.float32(2.6018827), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08607068806886672), 'actor_loss': np.float64(-0.9139054596424103), 'hyper_actor_loss': np.float64(0.008651926089078188), 'behavior_loss': np.float64(2.723531413078308)}

Episode step 4390, time diff 0.659888505935669, total time dif 334.1345729827881)
step: 4390 @ episode report: {'average_total_reward': np.float32(7.231112), 'reward_variance': np.float32(3.564045), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08389671221375465), 'actor_loss': np.float64(-0.9809791505336761), 'hyper_actor_loss': np.float64(0.008913112711161375), 'behavior_loss': np.float64(2.4467039585113524)}

Episode step 4400, time diff 0.6323015689849854, total time dif 334.79446148872375)
step: 4400 @ episode report: {'average_total_reward': np.float32(5.7211113), 'reward_variance': np.float32(1.084925), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0909457579255104), 'actor_loss': np.float64(-0.9555562973022461), 'hyper_actor_loss': np.float64(0.008888700697571039), 'behavior_loss': np.float64(2.592727530002594)}

Episode step 4410, time diff 0.6738965511322021, total time dif 335.42676305770874)
step: 4410 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(2.3153536), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08662853427231312), 'actor_loss': np.float64(-0.9674665093421936), 'hyper_actor_loss': np.float64(0.009037649445235729), 'behavior_loss': np.float64(2.656788444519043)}

Episode step 4420, time diff 0.6674330234527588, total time dif 336.10065960884094)
step: 4420 @ episode report: {'average_total_reward': np.float32(6.8066664), 'reward_variance': np.float32(1.7662023), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09749600738286972), 'actor_loss': np.float64(-0.9770054161548615), 'hyper_actor_loss': np.float64(0.009420698788017035), 'behavior_loss': np.float64(2.6543732643127442)}

Episode step 4430, time diff 0.7991156578063965, total time dif 336.7680926322937)
step: 4430 @ episode report: {'average_total_reward': np.float32(6.245556), 'reward_variance': np.float32(1.5413446), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.9222221), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09497832283377647), 'actor_loss': np.float64(-0.9507866144180298), 'hyper_actor_loss': np.float64(0.009913691412657499), 'behavior_loss': np.float64(2.755914044380188)}

Episode step 4440, time diff 0.6798334121704102, total time dif 337.5672082901001)
step: 4440 @ episode report: {'average_total_reward': np.float32(6.4333334), 'reward_variance': np.float32(1.5613341), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08455389961600304), 'actor_loss': np.float64(-0.981521588563919), 'hyper_actor_loss': np.float64(0.010585951339453458), 'behavior_loss': np.float64(2.6441562175750732)}

Episode step 4450, time diff 0.6730918884277344, total time dif 338.2470417022705)
step: 4450 @ episode report: {'average_total_reward': np.float32(7.2188888), 'reward_variance': np.float32(2.4288163), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0855888407677412), 'actor_loss': np.float64(-0.941784119606018), 'hyper_actor_loss': np.float64(0.010491082258522511), 'behavior_loss': np.float64(2.6023301601409914)}

Episode step 4460, time diff 0.6615397930145264, total time dif 338.92013359069824)
step: 4460 @ episode report: {'average_total_reward': np.float32(6.7188888), 'reward_variance': np.float32(1.7354085), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09220009818673133), 'actor_loss': np.float64(-0.9952078878879547), 'hyper_actor_loss': np.float64(0.010634672548621892), 'behavior_loss': np.float64(2.565497708320618)}

Episode step 4470, time diff 0.6879940032958984, total time dif 339.58167338371277)
step: 4470 @ episode report: {'average_total_reward': np.float32(6.367778), 'reward_variance': np.float32(3.3833451), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09107470139861107), 'actor_loss': np.float64(-0.9447318375110626), 'hyper_actor_loss': np.float64(0.01042284034192562), 'behavior_loss': np.float64(2.509755039215088)}

Episode step 4480, time diff 0.6873481273651123, total time dif 340.26966738700867)
step: 4480 @ episode report: {'average_total_reward': np.float32(6.6066666), 'reward_variance': np.float32(1.5836593), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0933829627931118), 'actor_loss': np.float64(-0.969319611787796), 'hyper_actor_loss': np.float64(0.010317175649106503), 'behavior_loss': np.float64(2.5203842163085937)}

Episode step 4490, time diff 0.668806791305542, total time dif 340.9570155143738)
step: 4490 @ episode report: {'average_total_reward': np.float32(5.9700003), 'reward_variance': np.float32(3.1355572), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08919797390699387), 'actor_loss': np.float64(-0.9680493891239166), 'hyper_actor_loss': np.float64(0.010175363719463348), 'behavior_loss': np.float64(2.63518750667572)}

Episode step 4500, time diff 0.6628592014312744, total time dif 341.6258223056793)
step: 4500 @ episode report: {'average_total_reward': np.float32(6.3944445), 'reward_variance': np.float32(1.7632653), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08785193488001823), 'actor_loss': np.float64(-0.93867347240448), 'hyper_actor_loss': np.float64(0.010022158827632665), 'behavior_loss': np.float64(2.6933417081832887)}

Episode step 4510, time diff 0.7128310203552246, total time dif 342.2886815071106)
step: 4510 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(1.5452294), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08664030395448208), 'actor_loss': np.float64(-0.9790421605110169), 'hyper_actor_loss': np.float64(0.01009715609252453), 'behavior_loss': np.float64(2.513766384124756)}

Episode step 4520, time diff 0.6793863773345947, total time dif 343.0015125274658)
step: 4520 @ episode report: {'average_total_reward': np.float32(6.494445), 'reward_variance': np.float32(2.33492), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09500100910663604), 'actor_loss': np.float64(-0.9565811038017273), 'hyper_actor_loss': np.float64(0.010071961954236031), 'behavior_loss': np.float64(2.4715235948562624)}

Episode step 4530, time diff 0.6764726638793945, total time dif 343.6808989048004)
step: 4530 @ episode report: {'average_total_reward': np.float32(6.7333336), 'reward_variance': np.float32(0.9794567), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07942745797336101), 'actor_loss': np.float64(-0.9457688689231872), 'hyper_actor_loss': np.float64(0.010020723566412926), 'behavior_loss': np.float64(2.7067898869514466)}

Episode step 4540, time diff 0.6808586120605469, total time dif 344.3573715686798)
step: 4540 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(0.9442425), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07104440107941627), 'actor_loss': np.float64(-0.9428631007671356), 'hyper_actor_loss': np.float64(0.009975811373442412), 'behavior_loss': np.float64(2.68971426486969)}

Episode step 4550, time diff 0.6731834411621094, total time dif 345.03823018074036)
step: 4550 @ episode report: {'average_total_reward': np.float32(6.382222), 'reward_variance': np.float32(3.4011414), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0926834311336279), 'actor_loss': np.float64(-0.9541140794754028), 'hyper_actor_loss': np.float64(0.009940725564956666), 'behavior_loss': np.float64(2.5549834728240968)}

Episode step 4560, time diff 0.6937775611877441, total time dif 345.71141362190247)
step: 4560 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(2.7118468), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08537535816431045), 'actor_loss': np.float64(-0.9644717276096344), 'hyper_actor_loss': np.float64(0.009870603401213884), 'behavior_loss': np.float64(2.6959035634994506)}

Episode step 4570, time diff 0.7120339870452881, total time dif 346.4051911830902)
step: 4570 @ episode report: {'average_total_reward': np.float32(6.2066665), 'reward_variance': np.float32(2.0194864), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09437032602727413), 'actor_loss': np.float64(-0.9484601616859436), 'hyper_actor_loss': np.float64(0.009524304885417223), 'behavior_loss': np.float64(2.756020522117615)}

Episode step 4580, time diff 0.6474690437316895, total time dif 347.1172251701355)
step: 4580 @ episode report: {'average_total_reward': np.float32(6.9433336), 'reward_variance': np.float32(1.8566034), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08514158166944981), 'actor_loss': np.float64(-0.9717979490756988), 'hyper_actor_loss': np.float64(0.00947912484407425), 'behavior_loss': np.float64(2.3309367179870604)}

Episode step 4590, time diff 0.8397431373596191, total time dif 347.7646942138672)
step: 4590 @ episode report: {'average_total_reward': np.float32(6.9944444), 'reward_variance': np.float32(2.2856362), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0876644305884838), 'actor_loss': np.float64(-0.9567878901958465), 'hyper_actor_loss': np.float64(0.009186941757798195), 'behavior_loss': np.float64(2.619776177406311)}

Episode step 4600, time diff 0.654313325881958, total time dif 348.6044373512268)
step: 4600 @ episode report: {'average_total_reward': np.float32(6.8944445), 'reward_variance': np.float32(3.1247222), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09273500591516495), 'actor_loss': np.float64(-0.9481404781341553), 'hyper_actor_loss': np.float64(0.008853510953485966), 'behavior_loss': np.float64(2.7809030771255494)}

Episode step 4610, time diff 0.6500914096832275, total time dif 349.25875067710876)
step: 4610 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(1.6863903), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09480995908379555), 'actor_loss': np.float64(-0.9883207857608796), 'hyper_actor_loss': np.float64(0.008387654274702071), 'behavior_loss': np.float64(2.533186197280884)}

Episode step 4620, time diff 0.6495766639709473, total time dif 349.908842086792)
step: 4620 @ episode report: {'average_total_reward': np.float32(6.082223), 'reward_variance': np.float32(0.78178275), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0802028939127922), 'actor_loss': np.float64(-0.9658791363239289), 'hyper_actor_loss': np.float64(0.008230975363403559), 'behavior_loss': np.float64(2.481260633468628)}

Episode step 4630, time diff 0.6780574321746826, total time dif 350.55841875076294)
step: 4630 @ episode report: {'average_total_reward': np.float32(6.406667), 'reward_variance': np.float32(1.3759803), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08421744350343943), 'actor_loss': np.float64(-0.9502776563167572), 'hyper_actor_loss': np.float64(0.008132832078263163), 'behavior_loss': np.float64(2.638726496696472)}

Episode step 4640, time diff 0.6622869968414307, total time dif 351.2364761829376)
step: 4640 @ episode report: {'average_total_reward': np.float32(5.996667), 'reward_variance': np.float32(2.3746188), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08683216199278831), 'actor_loss': np.float64(-0.9315359413623809), 'hyper_actor_loss': np.float64(0.008039358956739306), 'behavior_loss': np.float64(2.8079812049865724)}

Episode step 4650, time diff 0.674597978591919, total time dif 351.89876317977905)
step: 4650 @ episode report: {'average_total_reward': np.float32(6.5066667), 'reward_variance': np.float32(0.5386715), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0833078682422638), 'actor_loss': np.float64(-0.9745783269405365), 'hyper_actor_loss': np.float64(0.008211608231067657), 'behavior_loss': np.float64(2.732823204994202)}

Episode step 4660, time diff 0.650815486907959, total time dif 352.573361158371)
step: 4660 @ episode report: {'average_total_reward': np.float32(7.0066667), 'reward_variance': np.float32(2.7919555), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06281126514077187), 'actor_loss': np.float64(-0.920962929725647), 'hyper_actor_loss': np.float64(0.008305760938674212), 'behavior_loss': np.float64(2.339362144470215)}

Episode step 4670, time diff 0.6621840000152588, total time dif 353.22417664527893)
step: 4670 @ episode report: {'average_total_reward': np.float32(6.0211115), 'reward_variance': np.float32(1.345863), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08815269842743874), 'actor_loss': np.float64(-0.9819069862365722), 'hyper_actor_loss': np.float64(0.008275870233774185), 'behavior_loss': np.float64(2.3505823731422426)}

Episode step 4680, time diff 0.6590583324432373, total time dif 353.8863606452942)
step: 4680 @ episode report: {'average_total_reward': np.float32(6.3966665), 'reward_variance': np.float32(1.62894), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08150167241692544), 'actor_loss': np.float64(-0.9194734394550323), 'hyper_actor_loss': np.float64(0.008474378660321236), 'behavior_loss': np.float64(2.6922338962554933)}

Episode step 4690, time diff 0.6540849208831787, total time dif 354.5454189777374)
step: 4690 @ episode report: {'average_total_reward': np.float32(5.845556), 'reward_variance': np.float32(5.026061), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08796731606125832), 'actor_loss': np.float64(-0.9701262593269349), 'hyper_actor_loss': np.float64(0.008606343250721692), 'behavior_loss': np.float64(2.532039260864258)}

Episode step 4700, time diff 0.6552152633666992, total time dif 355.1995038986206)
step: 4700 @ episode report: {'average_total_reward': np.float32(6.794445), 'reward_variance': np.float32(1.158747), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08207987211644649), 'actor_loss': np.float64(-0.9497822463512421), 'hyper_actor_loss': np.float64(0.008822136092931032), 'behavior_loss': np.float64(2.64240620136261)}

Episode step 4710, time diff 0.6861803531646729, total time dif 355.8547191619873)
step: 4710 @ episode report: {'average_total_reward': np.float32(6.333334), 'reward_variance': np.float32(2.189951), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07634094320237636), 'actor_loss': np.float64(-0.9559743881225586), 'hyper_actor_loss': np.float64(0.008894525561481714), 'behavior_loss': np.float64(2.4844478368759155)}

Episode step 4720, time diff 0.6705331802368164, total time dif 356.540899515152)
step: 4720 @ episode report: {'average_total_reward': np.float32(6.096667), 'reward_variance': np.float32(1.0547171), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07805265784263611), 'actor_loss': np.float64(-0.9348349332809448), 'hyper_actor_loss': np.float64(0.009043999575078487), 'behavior_loss': np.float64(2.5616297721862793)}

Episode step 4730, time diff 0.6529233455657959, total time dif 357.2114326953888)
step: 4730 @ episode report: {'average_total_reward': np.float32(5.996667), 'reward_variance': np.float32(1.917236), 'max_total_reward': np.float32(7.5333343), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08150190822780132), 'actor_loss': np.float64(-0.968353945016861), 'hyper_actor_loss': np.float64(0.008991771191358567), 'behavior_loss': np.float64(2.514495813846588)}

Episode step 4740, time diff 0.6326847076416016, total time dif 357.8643560409546)
step: 4740 @ episode report: {'average_total_reward': np.float32(6.518889), 'reward_variance': np.float32(1.2933104), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07904560305178165), 'actor_loss': np.float64(-0.9525149405002594), 'hyper_actor_loss': np.float64(0.008879020437598228), 'behavior_loss': np.float64(2.5189203023910522)}

Episode step 4750, time diff 0.8608100414276123, total time dif 358.4970407485962)
step: 4750 @ episode report: {'average_total_reward': np.float32(6.1211114), 'reward_variance': np.float32(2.2006783), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08466165326535702), 'actor_loss': np.float64(-0.9339904129505158), 'hyper_actor_loss': np.float64(0.008631643094122409), 'behavior_loss': np.float64(2.5502600193023683)}

Episode step 4760, time diff 0.7488136291503906, total time dif 359.3578507900238)
step: 4760 @ episode report: {'average_total_reward': np.float32(6.2700005), 'reward_variance': np.float32(1.4880016), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07843200117349625), 'actor_loss': np.float64(-0.9662885427474975), 'hyper_actor_loss': np.float64(0.008311999682337045), 'behavior_loss': np.float64(2.563180422782898)}

Episode step 4770, time diff 0.6899609565734863, total time dif 360.1066644191742)
step: 4770 @ episode report: {'average_total_reward': np.float32(6.133334), 'reward_variance': np.float32(0.9497288), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08451100029051303), 'actor_loss': np.float64(-0.9532696664333343), 'hyper_actor_loss': np.float64(0.00830571362748742), 'behavior_loss': np.float64(2.399948072433472)}

Episode step 4780, time diff 0.7206635475158691, total time dif 360.7966253757477)
step: 4780 @ episode report: {'average_total_reward': np.float32(5.508889), 'reward_variance': np.float32(1.106143), 'max_total_reward': np.float32(7.6555552), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09055367410182953), 'actor_loss': np.float64(-0.9647832810878754), 'hyper_actor_loss': np.float64(0.00811127652414143), 'behavior_loss': np.float64(2.5454620718955994)}

Episode step 4790, time diff 0.7484495639801025, total time dif 361.51728892326355)
step: 4790 @ episode report: {'average_total_reward': np.float32(6.333333), 'reward_variance': np.float32(0.7779258), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08267182428389788), 'actor_loss': np.float64(-0.9813225209712982), 'hyper_actor_loss': np.float64(0.00855358587577939), 'behavior_loss': np.float64(2.557866072654724)}

Episode step 4800, time diff 0.7052633762359619, total time dif 362.26573848724365)
step: 4800 @ episode report: {'average_total_reward': np.float32(6.6699996), 'reward_variance': np.float32(2.0688655), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09444906786084176), 'actor_loss': np.float64(-0.957311499118805), 'hyper_actor_loss': np.float64(0.009029390383511781), 'behavior_loss': np.float64(2.5796977519989013)}

Episode step 4810, time diff 0.7125284671783447, total time dif 362.9710018634796)
step: 4810 @ episode report: {'average_total_reward': np.float32(6.533334), 'reward_variance': np.float32(1.668346), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08377289101481437), 'actor_loss': np.float64(-0.963353842496872), 'hyper_actor_loss': np.float64(0.009359145816415549), 'behavior_loss': np.float64(2.5239101886749267)}

Episode step 4820, time diff 0.680199146270752, total time dif 363.68353033065796)
step: 4820 @ episode report: {'average_total_reward': np.float32(5.9211116), 'reward_variance': np.float32(2.4780853), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09239128120243549), 'actor_loss': np.float64(-0.9689709901809692), 'hyper_actor_loss': np.float64(0.009581526182591915), 'behavior_loss': np.float64(2.4876793384552003)}

Episode step 4830, time diff 0.7015378475189209, total time dif 364.3637294769287)
step: 4830 @ episode report: {'average_total_reward': np.float32(6.308889), 'reward_variance': np.float32(2.133675), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07617778703570366), 'actor_loss': np.float64(-0.9566648483276368), 'hyper_actor_loss': np.float64(0.009448815137147903), 'behavior_loss': np.float64(2.621857237815857)}

Episode step 4840, time diff 0.6630067825317383, total time dif 365.06526732444763)
step: 4840 @ episode report: {'average_total_reward': np.float32(6.0822225), 'reward_variance': np.float32(0.8655359), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07785954885184765), 'actor_loss': np.float64(-0.9360426425933838), 'hyper_actor_loss': np.float64(0.009393989574164152), 'behavior_loss': np.float64(2.613721489906311)}

Episode step 4850, time diff 0.6697092056274414, total time dif 365.72827410697937)
step: 4850 @ episode report: {'average_total_reward': np.float32(6.1577783), 'reward_variance': np.float32(0.9442417), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07416789755225181), 'actor_loss': np.float64(-0.9644322037696839), 'hyper_actor_loss': np.float64(0.009290726482868194), 'behavior_loss': np.float64(2.4489771842956545)}

Episode step 4860, time diff 0.6771111488342285, total time dif 366.3979833126068)
step: 4860 @ episode report: {'average_total_reward': np.float32(6.5311112), 'reward_variance': np.float32(2.3773036), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09791049808263778), 'actor_loss': np.float64(-0.9812963902950287), 'hyper_actor_loss': np.float64(0.009343929961323738), 'behavior_loss': np.float64(2.5074621438980103)}

Episode step 4870, time diff 0.7587668895721436, total time dif 367.07509446144104)
step: 4870 @ episode report: {'average_total_reward': np.float32(5.908889), 'reward_variance': np.float32(1.2629336), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0951564446091652), 'actor_loss': np.float64(-0.9511071562767028), 'hyper_actor_loss': np.float64(0.009232307504862546), 'behavior_loss': np.float64(2.636232829093933)}

Episode step 4880, time diff 0.711414098739624, total time dif 367.8338613510132)
step: 4880 @ episode report: {'average_total_reward': np.float32(6.6211114), 'reward_variance': np.float32(1.7850479), 'max_total_reward': np.float32(8.411111), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08354853391647339), 'actor_loss': np.float64(-0.9551653921604156), 'hyper_actor_loss': np.float64(0.009334366023540496), 'behavior_loss': np.float64(2.476003646850586)}

Episode step 4890, time diff 0.6829397678375244, total time dif 368.5452754497528)
step: 4890 @ episode report: {'average_total_reward': np.float32(6.582223), 'reward_variance': np.float32(1.5407457), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09405677989125252), 'actor_loss': np.float64(-0.9516678154468536), 'hyper_actor_loss': np.float64(0.009132614638656378), 'behavior_loss': np.float64(2.670414161682129)}

Episode step 4900, time diff 0.7323942184448242, total time dif 369.22821521759033)
step: 4900 @ episode report: {'average_total_reward': np.float32(6.418889), 'reward_variance': np.float32(6.193507), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0764441441744566), 'actor_loss': np.float64(-0.9593944847583771), 'hyper_actor_loss': np.float64(0.009129455126821995), 'behavior_loss': np.float64(2.559879755973816)}

Episode step 4910, time diff 0.8191111087799072, total time dif 369.96060943603516)
step: 4910 @ episode report: {'average_total_reward': np.float32(6.2577777), 'reward_variance': np.float32(2.110884), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07725356742739678), 'actor_loss': np.float64(-0.9519834280014038), 'hyper_actor_loss': np.float64(0.009672341775149107), 'behavior_loss': np.float64(2.6754085779190064)}

Episode step 4920, time diff 0.6860482692718506, total time dif 370.77972054481506)
step: 4920 @ episode report: {'average_total_reward': np.float32(5.7333336), 'reward_variance': np.float32(0.8422964), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07485899776220321), 'actor_loss': np.float64(-0.9438516736030579), 'hyper_actor_loss': np.float64(0.010169660579413176), 'behavior_loss': np.float64(2.56226851940155)}

Episode step 4930, time diff 0.7132532596588135, total time dif 371.4657688140869)
step: 4930 @ episode report: {'average_total_reward': np.float32(5.147778), 'reward_variance': np.float32(1.3722979), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07607887461781501), 'actor_loss': np.float64(-0.9585186004638672), 'hyper_actor_loss': np.float64(0.010801945999264716), 'behavior_loss': np.float64(2.3948435544967652)}

Episode step 4940, time diff 0.7484440803527832, total time dif 372.1790220737457)
step: 4940 @ episode report: {'average_total_reward': np.float32(5.9455557), 'reward_variance': np.float32(1.748801), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09646705761551858), 'actor_loss': np.float64(-0.9468766212463379), 'hyper_actor_loss': np.float64(0.010951405763626099), 'behavior_loss': np.float64(2.5766344547271727)}

Episode step 4950, time diff 0.7237856388092041, total time dif 372.9274661540985)
step: 4950 @ episode report: {'average_total_reward': np.float32(4.6988893), 'reward_variance': np.float32(1.627641), 'max_total_reward': np.float32(7.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06914758086204528), 'actor_loss': np.float64(-0.9729950249195098), 'hyper_actor_loss': np.float64(0.011386497225612403), 'behavior_loss': np.float64(2.5872629046440125)}

Episode step 4960, time diff 0.7373840808868408, total time dif 373.6512517929077)
step: 4960 @ episode report: {'average_total_reward': np.float32(4.674445), 'reward_variance': np.float32(1.2241743), 'max_total_reward': np.float32(6.411112), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07730970047414303), 'actor_loss': np.float64(-0.928706020116806), 'hyper_actor_loss': np.float64(0.011864732205867767), 'behavior_loss': np.float64(2.5686936378479004)}

Episode step 4970, time diff 0.700319766998291, total time dif 374.38863587379456)
step: 4970 @ episode report: {'average_total_reward': np.float32(3.676667), 'reward_variance': np.float32(0.75134444), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.2888892), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08800526633858681), 'actor_loss': np.float64(-0.9977214336395264), 'hyper_actor_loss': np.float64(0.013059902656823396), 'behavior_loss': np.float64(2.4109305143356323)}

Episode step 4980, time diff 0.7574648857116699, total time dif 375.08895564079285)
step: 4980 @ episode report: {'average_total_reward': np.float32(3.6155555), 'reward_variance': np.float32(0.27128884), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.9222224), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08042961284518242), 'actor_loss': np.float64(-0.966750967502594), 'hyper_actor_loss': np.float64(0.014172462280839682), 'behavior_loss': np.float64(2.3364278793334963)}

Episode step 4990, time diff 0.7005364894866943, total time dif 375.8464205265045)
step: 4990 @ episode report: {'average_total_reward': np.float32(3.0888891), 'reward_variance': np.float32(0.23577783), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07391671277582645), 'actor_loss': np.float64(-0.9652596592903138), 'hyper_actor_loss': np.float64(0.016140530444681646), 'behavior_loss': np.float64(2.2714440703392027)}

Episode step 5000, time diff 0.6783161163330078, total time dif 376.5469570159912)
step: 5000 @ episode report: {'average_total_reward': np.float32(3.1766667), 'reward_variance': np.float32(0.6165545), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0858660914003849), 'actor_loss': np.float64(-0.9898643314838409), 'hyper_actor_loss': np.float64(0.018378661014139652), 'behavior_loss': np.float64(2.3803629159927366)}

Episode step 5010, time diff 0.7710802555084229, total time dif 377.2252731323242)
step: 5010 @ episode report: {'average_total_reward': np.float32(2.8300002), 'reward_variance': np.float32(0.32775438), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.6777778), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0863530982285738), 'actor_loss': np.float64(-1.007081037759781), 'hyper_actor_loss': np.float64(0.01927921213209629), 'behavior_loss': np.float64(2.4547481536865234)}

Episode step 5020, time diff 0.6969053745269775, total time dif 377.99635338783264)
step: 5020 @ episode report: {'average_total_reward': np.float32(3.0544446), 'reward_variance': np.float32(0.74322116), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09643558785319328), 'actor_loss': np.float64(-1.0284723579883575), 'hyper_actor_loss': np.float64(0.020929066091775896), 'behavior_loss': np.float64(2.1261263847351075)}

Episode step 5030, time diff 0.6993381977081299, total time dif 378.6932587623596)
step: 5030 @ episode report: {'average_total_reward': np.float32(2.7933335), 'reward_variance': np.float32(0.5002272), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08950541764497758), 'actor_loss': np.float64(-0.9830267667770386), 'hyper_actor_loss': np.float64(0.022294855676591396), 'behavior_loss': np.float64(2.301687276363373)}

Episode step 5040, time diff 0.7249712944030762, total time dif 379.39259696006775)
step: 5040 @ episode report: {'average_total_reward': np.float32(3.115556), 'reward_variance': np.float32(0.6435112), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08013188764452935), 'actor_loss': np.float64(-0.951455956697464), 'hyper_actor_loss': np.float64(0.02193677965551615), 'behavior_loss': np.float64(2.1540091395378114)}

Episode step 5050, time diff 0.7149453163146973, total time dif 380.1175682544708)
step: 5050 @ episode report: {'average_total_reward': np.float32(3.0033336), 'reward_variance': np.float32(0.39572975), 'max_total_reward': np.float32(4.2888894), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08467267528176307), 'actor_loss': np.float64(-0.9685815274715424), 'hyper_actor_loss': np.float64(0.02175367418676615), 'behavior_loss': np.float64(2.023822546005249)}

Episode step 5060, time diff 0.7261731624603271, total time dif 380.8325135707855)
step: 5060 @ episode report: {'average_total_reward': np.float32(2.9277778), 'reward_variance': np.float32(0.56610495), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08964334353804589), 'actor_loss': np.float64(-0.9323935449123383), 'hyper_actor_loss': np.float64(0.021168907545506953), 'behavior_loss': np.float64(2.337489402294159)}

Episode step 5070, time diff 0.8297266960144043, total time dif 381.55868673324585)
step: 5070 @ episode report: {'average_total_reward': np.float32(3.2155557), 'reward_variance': np.float32(0.46741238), 'max_total_reward': np.float32(4.2888894), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0782623428851366), 'actor_loss': np.float64(-0.9336115062236786), 'hyper_actor_loss': np.float64(0.02010541409254074), 'behavior_loss': np.float64(2.1919742107391356)}

Episode step 5080, time diff 0.6745760440826416, total time dif 382.38841342926025)
step: 5080 @ episode report: {'average_total_reward': np.float32(3.376667), 'reward_variance': np.float32(0.45087534), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09285438731312752), 'actor_loss': np.float64(-0.9660757601261138), 'hyper_actor_loss': np.float64(0.01850371528416872), 'behavior_loss': np.float64(2.3041776537895204)}

Episode step 5090, time diff 0.7294280529022217, total time dif 383.0629894733429)
step: 5090 @ episode report: {'average_total_reward': np.float32(3.1522224), 'reward_variance': np.float32(0.35261863), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09062382802367211), 'actor_loss': np.float64(-0.954015564918518), 'hyper_actor_loss': np.float64(0.016665406711399555), 'behavior_loss': np.float64(2.442918586730957)}

Episode step 5100, time diff 0.672635555267334, total time dif 383.7924175262451)
step: 5100 @ episode report: {'average_total_reward': np.float32(3.4033337), 'reward_variance': np.float32(0.28210005), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.8000002), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08016051538288593), 'actor_loss': np.float64(-0.9377223491668701), 'hyper_actor_loss': np.float64(0.014817404188215732), 'behavior_loss': np.float64(2.389435338973999)}

Episode step 5110, time diff 0.7025275230407715, total time dif 384.46505308151245)
step: 5110 @ episode report: {'average_total_reward': np.float32(3.1644444), 'reward_variance': np.float32(0.354316), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09363864026963711), 'actor_loss': np.float64(-0.9387614250183105), 'hyper_actor_loss': np.float64(0.01326692970469594), 'behavior_loss': np.float64(2.5868183970451355)}

Episode step 5120, time diff 0.653303861618042, total time dif 385.1675806045532)
step: 5120 @ episode report: {'average_total_reward': np.float32(3.4522223), 'reward_variance': np.float32(0.7742729), 'max_total_reward': np.float32(5.166667), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0869388148188591), 'actor_loss': np.float64(-0.9516427993774415), 'hyper_actor_loss': np.float64(0.01164997909218073), 'behavior_loss': np.float64(2.626655411720276)}

Episode step 5130, time diff 0.6930935382843018, total time dif 385.82088446617126)
step: 5130 @ episode report: {'average_total_reward': np.float32(4.4255557), 'reward_variance': np.float32(0.44089), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.9222226), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08165598176419735), 'actor_loss': np.float64(-0.8985252201557159), 'hyper_actor_loss': np.float64(0.010348264127969742), 'behavior_loss': np.float64(2.641572380065918)}

Episode step 5140, time diff 0.6614053249359131, total time dif 386.51397800445557)
step: 5140 @ episode report: {'average_total_reward': np.float32(3.6622224), 'reward_variance': np.float32(0.8349435), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08152846060693264), 'actor_loss': np.float64(-0.9225585103034973), 'hyper_actor_loss': np.float64(0.009222837723791599), 'behavior_loss': np.float64(2.7891646146774294)}

Episode step 5150, time diff 0.6815690994262695, total time dif 387.1753833293915)
step: 5150 @ episode report: {'average_total_reward': np.float32(4.15), 'reward_variance': np.float32(0.8180556), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08532383143901826), 'actor_loss': np.float64(-0.9166506290435791), 'hyper_actor_loss': np.float64(0.007899201242253184), 'behavior_loss': np.float64(2.7280243396759034)}

Episode step 5160, time diff 0.6674089431762695, total time dif 387.85695242881775)
step: 5160 @ episode report: {'average_total_reward': np.float32(3.7155557), 'reward_variance': np.float32(0.5663012), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.0444443), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07642953656613827), 'actor_loss': np.float64(-0.9286925971508027), 'hyper_actor_loss': np.float64(0.007240628777071834), 'behavior_loss': np.float64(2.644688582420349)}

Episode step 5170, time diff 0.6943378448486328, total time dif 388.524361371994)
step: 5170 @ episode report: {'average_total_reward': np.float32(4.037778), 'reward_variance': np.float32(1.3496596), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09606363698840141), 'actor_loss': np.float64(-0.9126517295837402), 'hyper_actor_loss': np.float64(0.006844386458396912), 'behavior_loss': np.float64(2.888319158554077)}

Episode step 5180, time diff 0.6951124668121338, total time dif 389.21869921684265)
step: 5180 @ episode report: {'average_total_reward': np.float32(5.335555), 'reward_variance': np.float32(1.2797974), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08471632562577724), 'actor_loss': np.float64(-0.9548004925251007), 'hyper_actor_loss': np.float64(0.00669839265756309), 'behavior_loss': np.float64(2.715836215019226)}

Episode step 5190, time diff 0.7050673961639404, total time dif 389.9138116836548)
step: 5190 @ episode report: {'average_total_reward': np.float32(5.7577777), 'reward_variance': np.float32(1.3356742), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08299881592392921), 'actor_loss': np.float64(-0.9097407579421997), 'hyper_actor_loss': np.float64(0.006536563858389854), 'behavior_loss': np.float64(2.6502246856689453)}

Episode step 5200, time diff 0.6690914630889893, total time dif 390.6188790798187)
step: 5200 @ episode report: {'average_total_reward': np.float32(6.218889), 'reward_variance': np.float32(2.3161), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07912445031106471), 'actor_loss': np.float64(-0.9414795517921448), 'hyper_actor_loss': np.float64(0.006478177197277546), 'behavior_loss': np.float64(2.741234040260315)}

Episode step 5210, time diff 0.7662103176116943, total time dif 391.2879705429077)
step: 5210 @ episode report: {'average_total_reward': np.float32(6.3944445), 'reward_variance': np.float32(1.7467966), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.082476070150733), 'actor_loss': np.float64(-0.9237278401851654), 'hyper_actor_loss': np.float64(0.006378838373348117), 'behavior_loss': np.float64(2.7539822340011595)}

Episode step 5220, time diff 0.7233154773712158, total time dif 392.0541808605194)
step: 5220 @ episode report: {'average_total_reward': np.float32(6.245556), 'reward_variance': np.float32(2.180283), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08856492154300213), 'actor_loss': np.float64(-0.9151151120662689), 'hyper_actor_loss': np.float64(0.006182901421561837), 'behavior_loss': np.float64(2.533641719818115)}

Episode step 5230, time diff 0.8887395858764648, total time dif 392.7774963378906)
step: 5230 @ episode report: {'average_total_reward': np.float32(6.645555), 'reward_variance': np.float32(1.7861345), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08323871046304702), 'actor_loss': np.float64(-0.9451432883739471), 'hyper_actor_loss': np.float64(0.006276580970734358), 'behavior_loss': np.float64(2.522301471233368)}

Episode step 5240, time diff 0.684119462966919, total time dif 393.6662359237671)
step: 5240 @ episode report: {'average_total_reward': np.float32(6.906667), 'reward_variance': np.float32(1.8045242), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08047371134161949), 'actor_loss': np.float64(-0.8926401555538177), 'hyper_actor_loss': np.float64(0.006413443898782134), 'behavior_loss': np.float64(2.4950077295303346)}

Episode step 5250, time diff 0.7353878021240234, total time dif 394.350355386734)
step: 5250 @ episode report: {'average_total_reward': np.float32(7.5311112), 'reward_variance': np.float32(1.2226622), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.044444), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07945795096457005), 'actor_loss': np.float64(-0.9462238788604737), 'hyper_actor_loss': np.float64(0.006713643856346607), 'behavior_loss': np.float64(2.3716503143310548)}

Episode step 5260, time diff 0.7266623973846436, total time dif 395.08574318885803)
step: 5260 @ episode report: {'average_total_reward': np.float32(7.7655563), 'reward_variance': np.float32(2.531073), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07981167361140251), 'actor_loss': np.float64(-0.9256361246109008), 'hyper_actor_loss': np.float64(0.007033305615186692), 'behavior_loss': np.float64(2.274127411842346)}

Episode step 5270, time diff 0.682898759841919, total time dif 395.8124055862427)
step: 5270 @ episode report: {'average_total_reward': np.float32(7.6288896), 'reward_variance': np.float32(3.1232653), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09351272322237492), 'actor_loss': np.float64(-0.9298188865184784), 'hyper_actor_loss': np.float64(0.0073058872949332), 'behavior_loss': np.float64(2.2341902375221254)}

Episode step 5280, time diff 0.7487151622772217, total time dif 396.4953043460846)
step: 5280 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(3.6415856), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09775957241654396), 'actor_loss': np.float64(-0.9724320948123932), 'hyper_actor_loss': np.float64(0.007461880100890994), 'behavior_loss': np.float64(2.3413464069366454)}

Episode step 5290, time diff 0.7446660995483398, total time dif 397.2440195083618)
step: 5290 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(3.222524), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08483733870089054), 'actor_loss': np.float64(-0.9564356803894043), 'hyper_actor_loss': np.float64(0.0075079122558236126), 'behavior_loss': np.float64(2.2191948294639587)}

Episode step 5300, time diff 0.8858029842376709, total time dif 397.98868560791016)
step: 5300 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(3.6664898), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08064808063209057), 'actor_loss': np.float64(-0.9616591572761536), 'hyper_actor_loss': np.float64(0.0074454642366617915), 'behavior_loss': np.float64(2.1112327575683594)}

Episode step 5310, time diff 0.7512414455413818, total time dif 398.8744885921478)
step: 5310 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(2.0371861), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0942458115518093), 'actor_loss': np.float64(-0.9409964680671692), 'hyper_actor_loss': np.float64(0.007595130801200866), 'behavior_loss': np.float64(2.260492777824402)}

Episode step 5320, time diff 0.7103672027587891, total time dif 399.6257300376892)
step: 5320 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(4.4710174), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08339467607438564), 'actor_loss': np.float64(-0.9368911266326905), 'hyper_actor_loss': np.float64(0.007639719592407346), 'behavior_loss': np.float64(2.094686710834503)}

Episode step 5330, time diff 0.7433640956878662, total time dif 400.336097240448)
step: 5330 @ episode report: {'average_total_reward': np.float32(7.6655564), 'reward_variance': np.float32(4.2475924), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08804689794778824), 'actor_loss': np.float64(-0.949524313211441), 'hyper_actor_loss': np.float64(0.007666798867285251), 'behavior_loss': np.float64(2.2618084192276)}

Episode step 5340, time diff 0.7144300937652588, total time dif 401.07946133613586)
step: 5340 @ episode report: {'average_total_reward': np.float32(7.28), 'reward_variance': np.float32(3.1043901), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08265312798321248), 'actor_loss': np.float64(-0.930397766828537), 'hyper_actor_loss': np.float64(0.007669092807918787), 'behavior_loss': np.float64(2.1530322790145875)}

Episode step 5350, time diff 0.7033891677856445, total time dif 401.7938914299011)
step: 5350 @ episode report: {'average_total_reward': np.float32(7.953334), 'reward_variance': np.float32(2.5487857), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08874778896570205), 'actor_loss': np.float64(-0.9691412448883057), 'hyper_actor_loss': np.float64(0.007457315130159259), 'behavior_loss': np.float64(2.2328860759735107)}

Episode step 5360, time diff 0.6896493434906006, total time dif 402.49728059768677)
step: 5360 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(0.99375427), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09315412305295467), 'actor_loss': np.float64(-0.9383549153804779), 'hyper_actor_loss': np.float64(0.007060927944257856), 'behavior_loss': np.float64(2.221500563621521)}

Episode step 5370, time diff 0.6810076236724854, total time dif 403.18692994117737)
step: 5370 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(3.8322277), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0885156337171793), 'actor_loss': np.float64(-0.9714068233966827), 'hyper_actor_loss': np.float64(0.00669576721265912), 'behavior_loss': np.float64(2.216234529018402)}

Episode step 5380, time diff 0.6986594200134277, total time dif 403.86793756484985)
step: 5380 @ episode report: {'average_total_reward': np.float32(7.965556), 'reward_variance': np.float32(2.9124317), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09195090308785439), 'actor_loss': np.float64(-0.9328168630599976), 'hyper_actor_loss': np.float64(0.006491287471726536), 'behavior_loss': np.float64(2.1947132110595704)}

Episode step 5390, time diff 0.8219318389892578, total time dif 404.5665969848633)
step: 5390 @ episode report: {'average_total_reward': np.float32(7.8777785), 'reward_variance': np.float32(2.9843216), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09931511282920838), 'actor_loss': np.float64(-0.9800931811332703), 'hyper_actor_loss': np.float64(0.006237301276996732), 'behavior_loss': np.float64(2.2276235938072206)}

Episode step 5400, time diff 0.737786054611206, total time dif 405.38852882385254)
step: 5400 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(3.3818538), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09168796911835671), 'actor_loss': np.float64(-0.9527220606803894), 'hyper_actor_loss': np.float64(0.0060965330339968204), 'behavior_loss': np.float64(2.2738809466361998)}

Episode step 5410, time diff 0.6882541179656982, total time dif 406.12631487846375)
step: 5410 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(0.9124445), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09352345317602158), 'actor_loss': np.float64(-0.9560420095920563), 'hyper_actor_loss': np.float64(0.005998387606814504), 'behavior_loss': np.float64(2.246180009841919)}

Episode step 5420, time diff 0.714587926864624, total time dif 406.81456899642944)
step: 5420 @ episode report: {'average_total_reward': np.float32(8.041112), 'reward_variance': np.float32(1.2181994), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09611052945256233), 'actor_loss': np.float64(-0.9590476036071778), 'hyper_actor_loss': np.float64(0.005825772788375616), 'behavior_loss': np.float64(2.3231011390686036)}

Episode step 5430, time diff 0.7022366523742676, total time dif 407.52915692329407)
step: 5430 @ episode report: {'average_total_reward': np.float32(8.316668), 'reward_variance': np.float32(2.3824267), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08656208254396916), 'actor_loss': np.float64(-0.946620762348175), 'hyper_actor_loss': np.float64(0.005808350769802928), 'behavior_loss': np.float64(2.1697977423667907)}

Episode step 5440, time diff 0.7898077964782715, total time dif 408.23139357566833)
step: 5440 @ episode report: {'average_total_reward': np.float32(8.041112), 'reward_variance': np.float32(2.2945201), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09372071772813798), 'actor_loss': np.float64(-0.9595501065254212), 'hyper_actor_loss': np.float64(0.00593921602703631), 'behavior_loss': np.float64(2.2589736461639403)}

Episode step 5450, time diff 0.9704864025115967, total time dif 409.0212013721466)
step: 5450 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(2.3992705), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09391091130673886), 'actor_loss': np.float64(-0.9389463186264038), 'hyper_actor_loss': np.float64(0.006411117548123002), 'behavior_loss': np.float64(2.2948023319244384)}

Episode step 5460, time diff 0.8684799671173096, total time dif 409.9916877746582)
step: 5460 @ episode report: {'average_total_reward': np.float32(7.7411118), 'reward_variance': np.float32(0.7343228), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08046662732958794), 'actor_loss': np.float64(-0.9418234467506409), 'hyper_actor_loss': np.float64(0.006559250922873616), 'behavior_loss': np.float64(2.1917943716049195)}

Episode step 5470, time diff 0.8857498168945312, total time dif 410.8601677417755)
step: 5470 @ episode report: {'average_total_reward': np.float32(7.78), 'reward_variance': np.float32(1.1588346), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07719428185373545), 'actor_loss': np.float64(-0.9274671375751495), 'hyper_actor_loss': np.float64(0.006794874463230372), 'behavior_loss': np.float64(2.1331434726715086)}

Episode step 5480, time diff 0.7898507118225098, total time dif 411.74591755867004)
step: 5480 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(1.7941538), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08788969963788987), 'actor_loss': np.float64(-0.9497659027576446), 'hyper_actor_loss': np.float64(0.007066962588578462), 'behavior_loss': np.float64(2.1564979910850526)}

Episode step 5490, time diff 0.9352250099182129, total time dif 412.53576827049255)
step: 5490 @ episode report: {'average_total_reward': np.float32(7.492223), 'reward_variance': np.float32(4.1120505), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0802947849035263), 'actor_loss': np.float64(-0.9657844662666321), 'hyper_actor_loss': np.float64(0.007722061639651656), 'behavior_loss': np.float64(2.0664077043533324)}

Episode step 5500, time diff 0.9532625675201416, total time dif 413.47099328041077)
step: 5500 @ episode report: {'average_total_reward': np.float32(8.1044445), 'reward_variance': np.float32(3.5641532), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08560067117214203), 'actor_loss': np.float64(-0.9405609369277954), 'hyper_actor_loss': np.float64(0.007935473741963506), 'behavior_loss': np.float64(2.0735345125198363)}

Episode step 5510, time diff 0.7083079814910889, total time dif 414.4242558479309)
step: 5510 @ episode report: {'average_total_reward': np.float32(7.6288896), 'reward_variance': np.float32(1.6853383), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09118409231305122), 'actor_loss': np.float64(-0.9679103851318359), 'hyper_actor_loss': np.float64(0.007816345244646073), 'behavior_loss': np.float64(2.165725076198578)}

Episode step 5520, time diff 0.7595078945159912, total time dif 415.132563829422)
step: 5520 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(1.1866473), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09156287238001823), 'actor_loss': np.float64(-0.9377163112163543), 'hyper_actor_loss': np.float64(0.007755220402032137), 'behavior_loss': np.float64(2.305436944961548)}

Episode step 5530, time diff 0.7277705669403076, total time dif 415.892071723938)
step: 5530 @ episode report: {'average_total_reward': np.float32(8.016667), 'reward_variance': np.float32(2.1352167), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07588448822498321), 'actor_loss': np.float64(-0.9644604921340942), 'hyper_actor_loss': np.float64(0.008076021447777748), 'behavior_loss': np.float64(2.106430244445801)}

Episode step 5540, time diff 0.6655182838439941, total time dif 416.6198422908783)
step: 5540 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(3.071264), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08645167127251625), 'actor_loss': np.float64(-0.935290539264679), 'hyper_actor_loss': np.float64(0.008279427140951156), 'behavior_loss': np.float64(2.1901984930038454)}

Episode step 5550, time diff 0.6750690937042236, total time dif 417.2853605747223)
step: 5550 @ episode report: {'average_total_reward': np.float32(8.041112), 'reward_variance': np.float32(2.0915322), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08767753019928932), 'actor_loss': np.float64(-0.9677800834178925), 'hyper_actor_loss': np.float64(0.008142125140875579), 'behavior_loss': np.float64(2.2026294350624083)}

Episode step 5560, time diff 0.9880692958831787, total time dif 417.9604296684265)
step: 5560 @ episode report: {'average_total_reward': np.float32(8.39), 'reward_variance': np.float32(2.359394), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07697485722601413), 'actor_loss': np.float64(-0.9327342569828033), 'hyper_actor_loss': np.float64(0.007850190019235015), 'behavior_loss': np.float64(2.141364598274231)}

Episode step 5570, time diff 0.7961101531982422, total time dif 418.9484989643097)
step: 5570 @ episode report: {'average_total_reward': np.float32(8.465556), 'reward_variance': np.float32(0.7656161), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07823671996593476), 'actor_loss': np.float64(-0.945904940366745), 'hyper_actor_loss': np.float64(0.007994973612949253), 'behavior_loss': np.float64(2.1168187141418455)}

Episode step 5580, time diff 0.7643022537231445, total time dif 419.74460911750793)
step: 5580 @ episode report: {'average_total_reward': np.float32(8.49), 'reward_variance': np.float32(1.5907758), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0800604160875082), 'actor_loss': np.float64(-0.9667938470840454), 'hyper_actor_loss': np.float64(0.007869074074551462), 'behavior_loss': np.float64(2.0578585624694825)}

Episode step 5590, time diff 0.7989575862884521, total time dif 420.5089113712311)
step: 5590 @ episode report: {'average_total_reward': np.float32(7.953334), 'reward_variance': np.float32(1.3277979), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08855095133185387), 'actor_loss': np.float64(-0.9624099671840668), 'hyper_actor_loss': np.float64(0.008189920429140329), 'behavior_loss': np.float64(2.157582402229309)}

Episode step 5600, time diff 0.7646167278289795, total time dif 421.30786895751953)
step: 5600 @ episode report: {'average_total_reward': np.float32(7.7555556), 'reward_variance': np.float32(1.1857778), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08803798444569111), 'actor_loss': np.float64(-0.9527400374412537), 'hyper_actor_loss': np.float64(0.008393602538853883), 'behavior_loss': np.float64(2.2876357078552245)}

Episode step 5610, time diff 0.8278365135192871, total time dif 422.0724856853485)
step: 5610 @ episode report: {'average_total_reward': np.float32(7.8288894), 'reward_variance': np.float32(3.8465724), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08401716761291027), 'actor_loss': np.float64(-0.9880632698535919), 'hyper_actor_loss': np.float64(0.008904189988970756), 'behavior_loss': np.float64(2.220008397102356)}

Episode step 5620, time diff 0.9014079570770264, total time dif 422.9003221988678)
step: 5620 @ episode report: {'average_total_reward': np.float32(7.641112), 'reward_variance': np.float32(1.5666434), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08278204463422298), 'actor_loss': np.float64(-0.9557627081871033), 'hyper_actor_loss': np.float64(0.0102001934312284), 'behavior_loss': np.float64(2.1012448906898498)}

Episode step 5630, time diff 0.7721717357635498, total time dif 423.8017301559448)
step: 5630 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(1.2385234), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09452829882502556), 'actor_loss': np.float64(-1.0325716495513917), 'hyper_actor_loss': np.float64(0.011766143888235093), 'behavior_loss': np.float64(2.080831742286682)}

Episode step 5640, time diff 0.7330574989318848, total time dif 424.5739018917084)
step: 5640 @ episode report: {'average_total_reward': np.float32(7.992223), 'reward_variance': np.float32(2.0839279), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09543267413973808), 'actor_loss': np.float64(-0.9742768228054046), 'hyper_actor_loss': np.float64(0.013504122383892536), 'behavior_loss': np.float64(2.0158872604370117)}

Episode step 5650, time diff 0.7980775833129883, total time dif 425.30695939064026)
step: 5650 @ episode report: {'average_total_reward': np.float32(6.88), 'reward_variance': np.float32(1.1082422), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09207158982753753), 'actor_loss': np.float64(-1.0039489209651946), 'hyper_actor_loss': np.float64(0.0157174838706851), 'behavior_loss': np.float64(2.1757858395576477)}

Episode step 5660, time diff 0.7378652095794678, total time dif 426.10503697395325)
step: 5660 @ episode report: {'average_total_reward': np.float32(7.4044447), 'reward_variance': np.float32(0.7741035), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08087562136352063), 'actor_loss': np.float64(-0.9663762629032135), 'hyper_actor_loss': np.float64(0.017313285358250143), 'behavior_loss': np.float64(2.0537951827049254)}

Episode step 5670, time diff 0.8030884265899658, total time dif 426.8429021835327)
step: 5670 @ episode report: {'average_total_reward': np.float32(6.8555565), 'reward_variance': np.float32(1.1490374), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09107360132038593), 'actor_loss': np.float64(-1.0059503138065338), 'hyper_actor_loss': np.float64(0.018151690997183324), 'behavior_loss': np.float64(2.005287456512451)}

Episode step 5680, time diff 0.8959999084472656, total time dif 427.6459906101227)
step: 5680 @ episode report: {'average_total_reward': np.float32(8.192223), 'reward_variance': np.float32(1.9491861), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09324818588793278), 'actor_loss': np.float64(-1.011092907190323), 'hyper_actor_loss': np.float64(0.018919306062161922), 'behavior_loss': np.float64(1.9851966738700866)}

Episode step 5690, time diff 0.7424535751342773, total time dif 428.54199051856995)
step: 5690 @ episode report: {'average_total_reward': np.float32(8.504445), 'reward_variance': np.float32(1.4514863), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08460203781723977), 'actor_loss': np.float64(-0.9760313808918), 'hyper_actor_loss': np.float64(0.02016017585992813), 'behavior_loss': np.float64(2.0095441699028016)}

Episode step 5700, time diff 0.7298495769500732, total time dif 429.2844440937042)
step: 5700 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(1.0490974), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08907998502254486), 'actor_loss': np.float64(-1.0245898246765137), 'hyper_actor_loss': np.float64(0.021912177093327045), 'behavior_loss': np.float64(1.9784130811691285)}

Episode step 5710, time diff 0.8099241256713867, total time dif 430.0142936706543)
step: 5710 @ episode report: {'average_total_reward': np.float32(7.231111), 'reward_variance': np.float32(1.2482669), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.166667), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09200325272977353), 'actor_loss': np.float64(-1.0286857604980468), 'hyper_actor_loss': np.float64(0.024071041494607925), 'behavior_loss': np.float64(1.942997896671295)}

Episode step 5720, time diff 0.9397244453430176, total time dif 430.8242177963257)
step: 5720 @ episode report: {'average_total_reward': np.float32(8.241112), 'reward_variance': np.float32(3.0255075), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09293148964643479), 'actor_loss': np.float64(-1.0029832899570466), 'hyper_actor_loss': np.float64(0.027199641801416874), 'behavior_loss': np.float64(1.827654767036438)}

Episode step 5730, time diff 0.7212603092193604, total time dif 431.7639422416687)
step: 5730 @ episode report: {'average_total_reward': np.float32(6.7433333), 'reward_variance': np.float32(1.3531711), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09090095311403275), 'actor_loss': np.float64(-1.0538074493408203), 'hyper_actor_loss': np.float64(0.03003667704761028), 'behavior_loss': np.float64(1.7609947800636292)}

Episode step 5740, time diff 0.8571197986602783, total time dif 432.48520255088806)
step: 5740 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(1.8509827), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09048869013786316), 'actor_loss': np.float64(-1.0204709351062775), 'hyper_actor_loss': np.float64(0.032023075595498086), 'behavior_loss': np.float64(1.7395411014556885)}

Episode step 5750, time diff 0.9830014705657959, total time dif 433.34232234954834)
step: 5750 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(2.218703), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08308864943683147), 'actor_loss': np.float64(-1.0123191237449647), 'hyper_actor_loss': np.float64(0.033562348783016206), 'behavior_loss': np.float64(1.7684362530708313)}

Episode step 5760, time diff 1.2252182960510254, total time dif 434.32532382011414)
step: 5760 @ episode report: {'average_total_reward': np.float32(6.5066667), 'reward_variance': np.float32(2.515758), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09044242128729821), 'actor_loss': np.float64(-1.0420825004577636), 'hyper_actor_loss': np.float64(0.035286845266819), 'behavior_loss': np.float64(1.6948914527893066)}

Episode step 5770, time diff 0.9818525314331055, total time dif 435.55054211616516)
step: 5770 @ episode report: {'average_total_reward': np.float32(7.1700006), 'reward_variance': np.float32(1.1392857), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10268153622746468), 'actor_loss': np.float64(-1.100735855102539), 'hyper_actor_loss': np.float64(0.03675187639892101), 'behavior_loss': np.float64(1.622166895866394)}

Episode step 5780, time diff 1.4864552021026611, total time dif 436.53239464759827)
step: 5780 @ episode report: {'average_total_reward': np.float32(5.5211115), 'reward_variance': np.float32(2.5096157), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10461403988301754), 'actor_loss': np.float64(-1.132697546482086), 'hyper_actor_loss': np.float64(0.03819136805832386), 'behavior_loss': np.float64(1.6228466272354125)}

Episode step 5790, time diff 1.0489871501922607, total time dif 438.0188498497009)
step: 5790 @ episode report: {'average_total_reward': np.float32(5.072222), 'reward_variance': np.float32(1.0267963), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0866896841675043), 'actor_loss': np.float64(-1.1204010248184204), 'hyper_actor_loss': np.float64(0.04078694060444832), 'behavior_loss': np.float64(1.4777991890907287)}

Episode step 5800, time diff 0.9010155200958252, total time dif 439.0678369998932)
step: 5800 @ episode report: {'average_total_reward': np.float32(4.2377787), 'reward_variance': np.float32(0.6664247), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1073916420340538), 'actor_loss': np.float64(-1.2727252006530763), 'hyper_actor_loss': np.float64(0.04947819374501705), 'behavior_loss': np.float64(1.26719788312912)}

Episode step 5810, time diff 1.118182897567749, total time dif 439.968852519989)
step: 5810 @ episode report: {'average_total_reward': np.float32(3.4033337), 'reward_variance': np.float32(0.5234828), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11300717815756797), 'actor_loss': np.float64(-1.2879631996154786), 'hyper_actor_loss': np.float64(0.053501073643565177), 'behavior_loss': np.float64(1.2484328150749207)}

Episode step 5820, time diff 0.8118293285369873, total time dif 441.08703541755676)
step: 5820 @ episode report: {'average_total_reward': np.float32(3.1644447), 'reward_variance': np.float32(0.84611857), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10730921477079391), 'actor_loss': np.float64(-1.36899391412735), 'hyper_actor_loss': np.float64(0.05693349242210388), 'behavior_loss': np.float64(1.1945104479789734)}

Episode step 5830, time diff 0.7304370403289795, total time dif 441.89886474609375)
step: 5830 @ episode report: {'average_total_reward': np.float32(2.8300004), 'reward_variance': np.float32(0.47886539), 'max_total_reward': np.float32(4.0444446), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10246111825108528), 'actor_loss': np.float64(-1.342203950881958), 'hyper_actor_loss': np.float64(0.059675386548042296), 'behavior_loss': np.float64(1.1570061564445495)}

Episode step 5840, time diff 0.7519350051879883, total time dif 442.62930178642273)
step: 5840 @ episode report: {'average_total_reward': np.float32(2.9544444), 'reward_variance': np.float32(0.7085543), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(1.9222224), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1242484413087368), 'actor_loss': np.float64(-1.364246666431427), 'hyper_actor_loss': np.float64(0.05744298957288265), 'behavior_loss': np.float64(1.2335256814956665)}

Episode step 5850, time diff 0.7237880229949951, total time dif 443.3812367916107)
step: 5850 @ episode report: {'average_total_reward': np.float32(2.9300003), 'reward_variance': np.float32(0.5304457), 'max_total_reward': np.float32(4.0444446), 'min_total_reward': np.float32(1.677778), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09055925160646439), 'actor_loss': np.float64(-1.3202507734298705), 'hyper_actor_loss': np.float64(0.056241641938686374), 'behavior_loss': np.float64(1.1822937369346618)}

Episode step 5860, time diff 0.724510908126831, total time dif 444.1050248146057)
step: 5860 @ episode report: {'average_total_reward': np.float32(3.1155558), 'reward_variance': np.float32(0.5028199), 'max_total_reward': np.float32(4.166667), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11124702244997024), 'actor_loss': np.float64(-1.314687764644623), 'hyper_actor_loss': np.float64(0.05624113231897354), 'behavior_loss': np.float64(1.1754672408103943)}

Episode step 5870, time diff 0.6910076141357422, total time dif 444.82953572273254)
step: 5870 @ episode report: {'average_total_reward': np.float32(3.0300002), 'reward_variance': np.float32(0.6887913), 'max_total_reward': np.float32(4.166667), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10271483287215233), 'actor_loss': np.float64(-1.202400016784668), 'hyper_actor_loss': np.float64(0.05450829789042473), 'behavior_loss': np.float64(1.3336742162704467)}

Episode step 5880, time diff 0.9195470809936523, total time dif 445.5205433368683)
step: 5880 @ episode report: {'average_total_reward': np.float32(2.7544446), 'reward_variance': np.float32(0.31033212), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11379856988787651), 'actor_loss': np.float64(-1.227099311351776), 'hyper_actor_loss': np.float64(0.05824769586324692), 'behavior_loss': np.float64(1.147939419746399)}

Episode step 5890, time diff 0.7925782203674316, total time dif 446.44009041786194)
step: 5890 @ episode report: {'average_total_reward': np.float32(2.5544448), 'reward_variance': np.float32(0.863073), 'max_total_reward': np.float32(4.0444446), 'min_total_reward': np.float32(1.1666666), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1151530034840107), 'actor_loss': np.float64(-1.152002763748169), 'hyper_actor_loss': np.float64(0.06100076660513878), 'behavior_loss': np.float64(1.083149218559265)}

Episode step 5900, time diff 0.8044993877410889, total time dif 447.23266863822937)
step: 5900 @ episode report: {'average_total_reward': np.float32(2.4811113), 'reward_variance': np.float32(0.27111238), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10574075430631638), 'actor_loss': np.float64(-1.1055371165275574), 'hyper_actor_loss': np.float64(0.06260848566889762), 'behavior_loss': np.float64(1.0793245017528534)}

Episode step 5910, time diff 0.7463843822479248, total time dif 448.03716802597046)
step: 5910 @ episode report: {'average_total_reward': np.float32(2.493333), 'reward_variance': np.float32(0.25879508), 'max_total_reward': np.float32(3.288889), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11896394938230515), 'actor_loss': np.float64(-1.0700583457946777), 'hyper_actor_loss': np.float64(0.06164025440812111), 'behavior_loss': np.float64(1.0396958112716674)}

Episode step 5920, time diff 0.7255961894989014, total time dif 448.7835524082184)
step: 5920 @ episode report: {'average_total_reward': np.float32(2.4322224), 'reward_variance': np.float32(0.5114185), 'max_total_reward': np.float32(3.2888892), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.4), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11107827723026276), 'actor_loss': np.float64(-1.059605097770691), 'hyper_actor_loss': np.float64(0.06069959439337254), 'behavior_loss': np.float64(1.0753048419952393)}

Episode step 5930, time diff 0.7254879474639893, total time dif 449.5091485977173)
step: 5930 @ episode report: {'average_total_reward': np.float32(2.32), 'reward_variance': np.float32(0.28735313), 'max_total_reward': np.float32(3.2888892), 'min_total_reward': np.float32(1.8), 'average_n_step': np.float32(4.3), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.12943350225687028), 'actor_loss': np.float64(-1.0781820178031922), 'hyper_actor_loss': np.float64(0.06000582613050938), 'behavior_loss': np.float64(1.0462308347225189)}

Episode step 5940, time diff 0.7637197971343994, total time dif 450.2346365451813)
step: 5940 @ episode report: {'average_total_reward': np.float32(2.7177777), 'reward_variance': np.float32(0.38101736), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(1.6777778), 'average_n_step': np.float32(4.6), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10844970270991325), 'actor_loss': np.float64(-1.1053561925888062), 'hyper_actor_loss': np.float64(0.05828666500747204), 'behavior_loss': np.float64(1.0974575698375701)}

Episode step 5950, time diff 0.7466881275177002, total time dif 450.9983563423157)
step: 5950 @ episode report: {'average_total_reward': np.float32(2.1955557), 'reward_variance': np.float32(0.28183213), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.2), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.12417796403169631), 'actor_loss': np.float64(-1.072913658618927), 'hyper_actor_loss': np.float64(0.05845220200717449), 'behavior_loss': np.float64(1.1750479102134705)}

Episode step 5960, time diff 0.8212282657623291, total time dif 451.7450444698334)
step: 5960 @ episode report: {'average_total_reward': np.float32(2.1322224), 'reward_variance': np.float32(0.3271963), 'max_total_reward': np.float32(3.1666667), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09475538656115531), 'actor_loss': np.float64(-1.0261896967887878), 'hyper_actor_loss': np.float64(0.05963718667626381), 'behavior_loss': np.float64(1.0757806479930878)}

Episode step 5970, time diff 0.7724208831787109, total time dif 452.5662727355957)
step: 5970 @ episode report: {'average_total_reward': np.float32(1.7833334), 'reward_variance': np.float32(0.34173465), 'max_total_reward': np.float32(2.9222226), 'min_total_reward': np.float32(1.0444443), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11895538717508317), 'actor_loss': np.float64(-1.0231182098388671), 'hyper_actor_loss': np.float64(0.05973439514636993), 'behavior_loss': np.float64(1.0963543891906737)}

Episode step 5980, time diff 0.7781145572662354, total time dif 453.3386936187744)
step: 5980 @ episode report: {'average_total_reward': np.float32(1.6711111), 'reward_variance': np.float32(0.34160987), 'max_total_reward': np.float32(3.0444443), 'min_total_reward': np.float32(1.0444443), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09897554144263268), 'actor_loss': np.float64(-1.012985223531723), 'hyper_actor_loss': np.float64(0.0583430927246809), 'behavior_loss': np.float64(1.1200096428394317)}

Episode step 5990, time diff 0.7555637359619141, total time dif 454.11680817604065)
step: 5990 @ episode report: {'average_total_reward': np.float32(1.8199999), 'reward_variance': np.float32(0.20095804), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09736545234918595), 'actor_loss': np.float64(-1.0126101911067962), 'hyper_actor_loss': np.float64(0.05771207325160503), 'behavior_loss': np.float64(1.1734915494918823)}

Episode step 6000, time diff 0.7824575901031494, total time dif 454.87237191200256)
step: 6000 @ episode report: {'average_total_reward': np.float32(2.0833335), 'reward_variance': np.float32(0.27798146), 'max_total_reward': np.float32(3.0444446), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(4.1), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09471429251134396), 'actor_loss': np.float64(-1.0324463605880738), 'hyper_actor_loss': np.float64(0.05776982717216015), 'behavior_loss': np.float64(1.0533998608589172)}

Episode step 6010, time diff 0.7460229396820068, total time dif 455.6548295021057)
step: 6010 @ episode report: {'average_total_reward': np.float32(1.6077778), 'reward_variance': np.float32(0.42874202), 'max_total_reward': np.float32(2.9222224), 'min_total_reward': np.float32(0.8000001), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0982657827436924), 'actor_loss': np.float64(-1.0519167900085449), 'hyper_actor_loss': np.float64(0.05770581066608429), 'behavior_loss': np.float64(1.055002999305725)}

Episode step 6020, time diff 0.738715648651123, total time dif 456.4008524417877)
step: 6020 @ episode report: {'average_total_reward': np.float32(1.817778), 'reward_variance': np.float32(0.2784988), 'max_total_reward': np.float32(2.288889), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(3.7), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11407087296247483), 'actor_loss': np.float64(-1.0581492900848388), 'hyper_actor_loss': np.float64(0.05604372844099999), 'behavior_loss': np.float64(1.072888469696045)}

Episode step 6030, time diff 0.72890305519104, total time dif 457.13956809043884)
step: 6030 @ episode report: {'average_total_reward': np.float32(1.807778), 'reward_variance': np.float32(0.39627284), 'max_total_reward': np.float32(3.1666667), 'min_total_reward': np.float32(0.92222226), 'average_n_step': np.float32(3.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09487522728741168), 'actor_loss': np.float64(-1.0077052295207978), 'hyper_actor_loss': np.float64(0.053736595436930656), 'behavior_loss': np.float64(1.0715997874736787)}

Episode step 6040, time diff 0.9399051666259766, total time dif 457.8684711456299)
step: 6040 @ episode report: {'average_total_reward': np.float32(1.1977779), 'reward_variance': np.float32(0.10374812), 'max_total_reward': np.float32(1.8), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(3.3), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10402291938662529), 'actor_loss': np.float64(-1.0502339363098145), 'hyper_actor_loss': np.float64(0.05118570774793625), 'behavior_loss': np.float64(1.0829759955406189)}

Episode step 6050, time diff 0.733522891998291, total time dif 458.80837631225586)
step: 6050 @ episode report: {'average_total_reward': np.float32(1.4855556), 'reward_variance': np.float32(0.20076668), 'max_total_reward': np.float32(2.1666667), 'min_total_reward': np.float32(0.8000001), 'average_n_step': np.float32(3.6), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11532679349184036), 'actor_loss': np.float64(-1.0318431675434112), 'hyper_actor_loss': np.float64(0.048958360031247136), 'behavior_loss': np.float64(1.2062394857406615)}

Episode step 6060, time diff 0.8162403106689453, total time dif 459.54189920425415)
step: 6060 @ episode report: {'average_total_reward': np.float32(1.8833334), 'reward_variance': np.float32(0.31437653), 'max_total_reward': np.float32(2.9222221), 'min_total_reward': np.float32(0.92222214), 'average_n_step': np.float32(3.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10690260455012321), 'actor_loss': np.float64(-1.0147006154060363), 'hyper_actor_loss': np.float64(0.045243586599826816), 'behavior_loss': np.float64(1.2470649182796478)}

Episode step 6070, time diff 0.7230446338653564, total time dif 460.3581395149231)
step: 6070 @ episode report: {'average_total_reward': np.float32(2.8422227), 'reward_variance': np.float32(1.2046865), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09774893745779992), 'actor_loss': np.float64(-0.9727677226066589), 'hyper_actor_loss': np.float64(0.04218630567193031), 'behavior_loss': np.float64(1.4408451914787292)}

Episode step 6080, time diff 0.7060558795928955, total time dif 461.08118414878845)
step: 6080 @ episode report: {'average_total_reward': np.float32(3.7644448), 'reward_variance': np.float32(0.6154519), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10133175924420357), 'actor_loss': np.float64(-0.9817503035068512), 'hyper_actor_loss': np.float64(0.03907945342361927), 'behavior_loss': np.float64(1.4840297460556031)}

Episode step 6090, time diff 0.776864767074585, total time dif 461.78724002838135)
step: 6090 @ episode report: {'average_total_reward': np.float32(4.323333), 'reward_variance': np.float32(0.5760852), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10128214284777641), 'actor_loss': np.float64(-0.9708375811576844), 'hyper_actor_loss': np.float64(0.03625611700117588), 'behavior_loss': np.float64(1.600743055343628)}

Episode step 6100, time diff 0.773216962814331, total time dif 462.56410479545593)
step: 6100 @ episode report: {'average_total_reward': np.float32(4.423333), 'reward_variance': np.float32(1.5873939), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09705644622445106), 'actor_loss': np.float64(-0.9553571879863739), 'hyper_actor_loss': np.float64(0.033368897065520285), 'behavior_loss': np.float64(1.6542401790618897)}

Episode step 6110, time diff 0.6820077896118164, total time dif 463.33732175827026)
step: 6110 @ episode report: {'average_total_reward': np.float32(5.345556), 'reward_variance': np.float32(0.9063818), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09020723477005958), 'actor_loss': np.float64(-0.9414517164230347), 'hyper_actor_loss': np.float64(0.030620836094021796), 'behavior_loss': np.float64(1.6289344787597657)}

Episode step 6120, time diff 0.7536568641662598, total time dif 464.0193295478821)
step: 6120 @ episode report: {'average_total_reward': np.float32(6.7555556), 'reward_variance': np.float32(1.773827), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09978889748454094), 'actor_loss': np.float64(-0.9634916067123414), 'hyper_actor_loss': np.float64(0.02851269766688347), 'behavior_loss': np.float64(1.6832295536994935)}

Episode step 6130, time diff 0.787360429763794, total time dif 464.77298641204834)
step: 6130 @ episode report: {'average_total_reward': np.float32(7.5066667), 'reward_variance': np.float32(0.9661037), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1031248152256012), 'actor_loss': np.float64(-0.991965401172638), 'hyper_actor_loss': np.float64(0.02736079916357994), 'behavior_loss': np.float64(1.6084569215774536)}

Episode step 6140, time diff 0.7469878196716309, total time dif 465.56034684181213)
step: 6140 @ episode report: {'average_total_reward': np.float32(6.533334), 'reward_variance': np.float32(2.6898034), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09693012982606888), 'actor_loss': np.float64(-0.9913019120693207), 'hyper_actor_loss': np.float64(0.025845722295343876), 'behavior_loss': np.float64(1.558212113380432)}

Episode step 6150, time diff 0.7689213752746582, total time dif 466.30733466148376)
step: 6150 @ episode report: {'average_total_reward': np.float32(6.682223), 'reward_variance': np.float32(2.4833384), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10530863329768181), 'actor_loss': np.float64(-0.9967102646827698), 'hyper_actor_loss': np.float64(0.024658918380737305), 'behavior_loss': np.float64(1.7514200329780578)}

Episode step 6160, time diff 0.8190503120422363, total time dif 467.0762560367584)
step: 6160 @ episode report: {'average_total_reward': np.float32(7.6166673), 'reward_variance': np.float32(3.1149933), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09931868836283683), 'actor_loss': np.float64(-0.9850071907043457), 'hyper_actor_loss': np.float64(0.02405489105731249), 'behavior_loss': np.float64(1.7414458990097046)}

Episode step 6170, time diff 0.8277833461761475, total time dif 467.89530634880066)
step: 6170 @ episode report: {'average_total_reward': np.float32(6.867778), 'reward_variance': np.float32(3.5605056), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10021023005247116), 'actor_loss': np.float64(-0.9777915477752686), 'hyper_actor_loss': np.float64(0.024120974726974963), 'behavior_loss': np.float64(1.7987194895744323)}

Episode step 6180, time diff 0.7859957218170166, total time dif 468.7230896949768)
step: 6180 @ episode report: {'average_total_reward': np.float32(7.504445), 'reward_variance': np.float32(2.349264), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08976244404911995), 'actor_loss': np.float64(-1.0029220640659333), 'hyper_actor_loss': np.float64(0.024692802131175993), 'behavior_loss': np.float64(1.5134112119674683)}

Episode step 6190, time diff 0.7763915061950684, total time dif 469.5090854167938)
step: 6190 @ episode report: {'average_total_reward': np.float32(7.294445), 'reward_variance': np.float32(2.409019), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09124516248703003), 'actor_loss': np.float64(-0.9800979793071747), 'hyper_actor_loss': np.float64(0.02619418315589428), 'behavior_loss': np.float64(1.5173778414726258)}

Episode step 6200, time diff 1.0246455669403076, total time dif 470.2854769229889)
step: 6200 @ episode report: {'average_total_reward': np.float32(6.5944443), 'reward_variance': np.float32(2.2277102), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09850075468420982), 'actor_loss': np.float64(-1.0005141973495484), 'hyper_actor_loss': np.float64(0.027587611600756644), 'behavior_loss': np.float64(1.511550986766815)}

Episode step 6210, time diff 0.7813057899475098, total time dif 471.3101224899292)
step: 6210 @ episode report: {'average_total_reward': np.float32(6.7822227), 'reward_variance': np.float32(2.189462), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08879143372178078), 'actor_loss': np.float64(-0.9857297480106354), 'hyper_actor_loss': np.float64(0.02825162187218666), 'behavior_loss': np.float64(1.382786011695862)}

Episode step 6220, time diff 0.8256573677062988, total time dif 472.0914282798767)
step: 6220 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(1.3548753), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09797847270965576), 'actor_loss': np.float64(-1.0088474869728088), 'hyper_actor_loss': np.float64(0.02836254108697176), 'behavior_loss': np.float64(1.3885074615478517)}

Episode step 6230, time diff 0.8477275371551514, total time dif 472.917085647583)
step: 6230 @ episode report: {'average_total_reward': np.float32(7.094445), 'reward_variance': np.float32(2.1283278), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11201076954603195), 'actor_loss': np.float64(-1.0082900464534759), 'hyper_actor_loss': np.float64(0.02842929232865572), 'behavior_loss': np.float64(1.385452687740326)}

Episode step 6240, time diff 0.8023204803466797, total time dif 473.76481318473816)
step: 6240 @ episode report: {'average_total_reward': np.float32(7.5922227), 'reward_variance': np.float32(1.0590879), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09130638800561428), 'actor_loss': np.float64(-1.0091718137264252), 'hyper_actor_loss': np.float64(0.028719438053667546), 'behavior_loss': np.float64(1.402622437477112)}

Episode step 6250, time diff 0.8439946174621582, total time dif 474.56713366508484)
step: 6250 @ episode report: {'average_total_reward': np.float32(7.8433332), 'reward_variance': np.float32(2.9991713), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09658120647072792), 'actor_loss': np.float64(-1.0020031571388244), 'hyper_actor_loss': np.float64(0.028675529919564725), 'behavior_loss': np.float64(1.359608542919159)}

Episode step 6260, time diff 0.7379679679870605, total time dif 475.411128282547)
step: 6260 @ episode report: {'average_total_reward': np.float32(5.8700004), 'reward_variance': np.float32(2.403162), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09485207572579384), 'actor_loss': np.float64(-1.0521239161491394), 'hyper_actor_loss': np.float64(0.02917072717100382), 'behavior_loss': np.float64(1.377161157131195)}

Episode step 6270, time diff 0.7723379135131836, total time dif 476.14909625053406)
step: 6270 @ episode report: {'average_total_reward': np.float32(6.057778), 'reward_variance': np.float32(3.0354521), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09307863041758538), 'actor_loss': np.float64(-0.9822019398212433), 'hyper_actor_loss': np.float64(0.02903984021395445), 'behavior_loss': np.float64(1.3881927013397217)}

Episode step 6280, time diff 0.8429393768310547, total time dif 476.92143416404724)
step: 6280 @ episode report: {'average_total_reward': np.float32(6.0577784), 'reward_variance': np.float32(1.0169826), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0985727146267891), 'actor_loss': np.float64(-1.0370460748672485), 'hyper_actor_loss': np.float64(0.029419330321252345), 'behavior_loss': np.float64(1.374839186668396)}

Episode step 6290, time diff 0.7657241821289062, total time dif 477.7643735408783)
step: 6290 @ episode report: {'average_total_reward': np.float32(7.0433335), 'reward_variance': np.float32(1.782307), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08934944421052933), 'actor_loss': np.float64(-0.9847155630588531), 'hyper_actor_loss': np.float64(0.029372915998101233), 'behavior_loss': np.float64(1.3904945731163025)}

Episode step 6300, time diff 0.835209846496582, total time dif 478.5300977230072)
step: 6300 @ episode report: {'average_total_reward': np.float32(6.582223), 'reward_variance': np.float32(1.6904004), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09618102237582207), 'actor_loss': np.float64(-1.0251040399074554), 'hyper_actor_loss': np.float64(0.029073774442076684), 'behavior_loss': np.float64(1.4003873229026795)}

Episode step 6310, time diff 0.7807114124298096, total time dif 479.3653075695038)
step: 6310 @ episode report: {'average_total_reward': np.float32(6.594445), 'reward_variance': np.float32(1.5045002), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0926552653312683), 'actor_loss': np.float64(-1.0154551267623901), 'hyper_actor_loss': np.float64(0.02913053911179304), 'behavior_loss': np.float64(1.3491680860519408)}

Episode step 6320, time diff 0.8258230686187744, total time dif 480.1460189819336)
step: 6320 @ episode report: {'average_total_reward': np.float32(6.3822227), 'reward_variance': np.float32(3.2011409), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0869408167898655), 'actor_loss': np.float64(-0.9857992768287659), 'hyper_actor_loss': np.float64(0.02892182841897011), 'behavior_loss': np.float64(1.3421444058418275)}

Episode step 6330, time diff 0.8094923496246338, total time dif 480.97184205055237)
step: 6330 @ episode report: {'average_total_reward': np.float32(6.533333), 'reward_variance': np.float32(3.518765), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09765475764870643), 'actor_loss': np.float64(-1.0182678341865539), 'hyper_actor_loss': np.float64(0.028849282674491404), 'behavior_loss': np.float64(1.4145010471343995)}

Episode step 6340, time diff 0.9272603988647461, total time dif 481.781334400177)
step: 6340 @ episode report: {'average_total_reward': np.float32(6.4188895), 'reward_variance': np.float32(2.0352857), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10915393158793449), 'actor_loss': np.float64(-0.9745987713336944), 'hyper_actor_loss': np.float64(0.028855451941490175), 'behavior_loss': np.float64(1.4077608942985536)}

Episode step 6350, time diff 0.7580790519714355, total time dif 482.70859479904175)
step: 6350 @ episode report: {'average_total_reward': np.float32(4.808889), 'reward_variance': np.float32(0.3123407), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10194792151451111), 'actor_loss': np.float64(-1.03575239777565), 'hyper_actor_loss': np.float64(0.028665537759661675), 'behavior_loss': np.float64(1.3517407774925232)}

Episode step 6360, time diff 0.9424045085906982, total time dif 483.4666738510132)
step: 6360 @ episode report: {'average_total_reward': np.float32(5.36), 'reward_variance': np.float32(0.78864706), 'max_total_reward': np.float32(6.6555552), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09998028874397277), 'actor_loss': np.float64(-0.939694207906723), 'hyper_actor_loss': np.float64(0.02889099232852459), 'behavior_loss': np.float64(1.365365481376648)}

Episode step 6370, time diff 0.7837753295898438, total time dif 484.4090783596039)
step: 6370 @ episode report: {'average_total_reward': np.float32(5.7966666), 'reward_variance': np.float32(1.5448649), 'max_total_reward': np.float32(8.655555), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09202226176857949), 'actor_loss': np.float64(-1.0225937247276307), 'hyper_actor_loss': np.float64(0.0287043871358037), 'behavior_loss': np.float64(1.3504983186721802)}

Episode step 6380, time diff 0.7847886085510254, total time dif 485.1928536891937)
step: 6380 @ episode report: {'average_total_reward': np.float32(6.07), 'reward_variance': np.float32(2.2040513), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09424689635634423), 'actor_loss': np.float64(-0.9689353883266449), 'hyper_actor_loss': np.float64(0.0284584516659379), 'behavior_loss': np.float64(1.2673059940338134)}

Episode step 6390, time diff 0.7027130126953125, total time dif 485.97764229774475)
step: 6390 @ episode report: {'average_total_reward': np.float32(5.26), 'reward_variance': np.float32(1.5123259), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08802123442292213), 'actor_loss': np.float64(-1.0130035817623138), 'hyper_actor_loss': np.float64(0.027621846832334995), 'behavior_loss': np.float64(1.4015381336212158)}

Episode step 6400, time diff 0.808499813079834, total time dif 486.68035531044006)
step: 6400 @ episode report: {'average_total_reward': np.float32(6.2577777), 'reward_variance': np.float32(1.4405383), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09380483403801917), 'actor_loss': np.float64(-0.952203768491745), 'hyper_actor_loss': np.float64(0.027766395546495914), 'behavior_loss': np.float64(1.320004940032959)}

Episode step 6410, time diff 0.787874698638916, total time dif 487.4888551235199)
step: 6410 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(2.0679462), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10070917010307312), 'actor_loss': np.float64(-1.0257131814956666), 'hyper_actor_loss': np.float64(0.027794277854263784), 'behavior_loss': np.float64(1.367299783229828)}

Episode step 6420, time diff 0.7627849578857422, total time dif 488.2767298221588)
step: 6420 @ episode report: {'average_total_reward': np.float32(6.045556), 'reward_variance': np.float32(4.386851), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10567142441868782), 'actor_loss': np.float64(-0.9879796743392945), 'hyper_actor_loss': np.float64(0.02729018181562424), 'behavior_loss': np.float64(1.3512433648109436)}

Episode step 6430, time diff 0.8181266784667969, total time dif 489.03951478004456)
step: 6430 @ episode report: {'average_total_reward': np.float32(5.994445), 'reward_variance': np.float32(1.6786726), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09830958619713784), 'actor_loss': np.float64(-1.0051912307739257), 'hyper_actor_loss': np.float64(0.026745929196476937), 'behavior_loss': np.float64(1.3322896838188172)}

Episode step 6440, time diff 0.7436060905456543, total time dif 489.85764145851135)
step: 6440 @ episode report: {'average_total_reward': np.float32(5.5844445), 'reward_variance': np.float32(0.7073382), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.0444446), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09831554256379604), 'actor_loss': np.float64(-0.9757092714309692), 'hyper_actor_loss': np.float64(0.02606253568083048), 'behavior_loss': np.float64(1.3664742231369018)}

Episode step 6450, time diff 0.7787578105926514, total time dif 490.601247549057)
step: 6450 @ episode report: {'average_total_reward': np.float32(5.447778), 'reward_variance': np.float32(1.5647421), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1074105404317379), 'actor_loss': np.float64(-1.023704034090042), 'hyper_actor_loss': np.float64(0.025555875152349472), 'behavior_loss': np.float64(1.4610294461250306)}

Episode step 6460, time diff 0.74880051612854, total time dif 491.38000535964966)
step: 6460 @ episode report: {'average_total_reward': np.float32(5.6966667), 'reward_variance': np.float32(1.0112112), 'max_total_reward': np.float32(7.533334), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08252680599689484), 'actor_loss': np.float64(-0.9727285921573638), 'hyper_actor_loss': np.float64(0.02532102484256029), 'behavior_loss': np.float64(1.4398418545722962)}

Episode step 6470, time diff 0.8062999248504639, total time dif 492.1288058757782)
step: 6470 @ episode report: {'average_total_reward': np.float32(5.008889), 'reward_variance': np.float32(0.70426667), 'max_total_reward': np.float32(6.5333333), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09205875024199486), 'actor_loss': np.float64(-0.9620001971721649), 'hyper_actor_loss': np.float64(0.025258703902363778), 'behavior_loss': np.float64(1.5085641264915466)}

Episode step 6480, time diff 0.8378479480743408, total time dif 492.93510580062866)
step: 6480 @ episode report: {'average_total_reward': np.float32(5.135556), 'reward_variance': np.float32(0.45747662), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09915639199316502), 'actor_loss': np.float64(-1.0316542387008667), 'hyper_actor_loss': np.float64(0.025171357952058316), 'behavior_loss': np.float64(1.531973135471344)}

Episode step 6490, time diff 0.7856345176696777, total time dif 493.772953748703)
step: 6490 @ episode report: {'average_total_reward': np.float32(4.7377787), 'reward_variance': np.float32(0.8927458), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08811848349869252), 'actor_loss': np.float64(-1.0068613171577454), 'hyper_actor_loss': np.float64(0.025065602734684944), 'behavior_loss': np.float64(1.5254652738571166)}

Episode step 6500, time diff 0.8256535530090332, total time dif 494.5585882663727)
step: 6500 @ episode report: {'average_total_reward': np.float32(4.6355553), 'reward_variance': np.float32(1.1142173), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10498348139226436), 'actor_loss': np.float64(-1.035033929347992), 'hyper_actor_loss': np.float64(0.024499188363552093), 'behavior_loss': np.float64(1.5378313541412354)}

Episode step 6510, time diff 0.8687515258789062, total time dif 495.3842418193817)
step: 6510 @ episode report: {'average_total_reward': np.float32(3.9255555), 'reward_variance': np.float32(0.9303222), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09572318494319916), 'actor_loss': np.float64(-0.9999556839466095), 'hyper_actor_loss': np.float64(0.024382015503942967), 'behavior_loss': np.float64(1.4846522212028503)}

Episode step 6520, time diff 0.8269128799438477, total time dif 496.2529933452606)
step: 6520 @ episode report: {'average_total_reward': np.float32(4.025556), 'reward_variance': np.float32(0.97827274), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.2888892), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10621435381472111), 'actor_loss': np.float64(-1.016403079032898), 'hyper_actor_loss': np.float64(0.024667884409427642), 'behavior_loss': np.float64(1.5896533608436585)}

Episode step 6530, time diff 0.7817897796630859, total time dif 497.07990622520447)
step: 6530 @ episode report: {'average_total_reward': np.float32(4.862222), 'reward_variance': np.float32(0.41963476), 'max_total_reward': np.float32(6.1666675), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(6.5), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09275750294327736), 'actor_loss': np.float64(-0.9603185296058655), 'hyper_actor_loss': np.float64(0.024791403487324715), 'behavior_loss': np.float64(1.6457233667373656)}

Episode step 6540, time diff 0.7329223155975342, total time dif 497.86169600486755)
step: 6540 @ episode report: {'average_total_reward': np.float32(4.8233333), 'reward_variance': np.float32(0.7622086), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10583351999521255), 'actor_loss': np.float64(-0.9681689500808716), 'hyper_actor_loss': np.float64(0.024916760437190533), 'behavior_loss': np.float64(1.6027135014533997)}

Episode step 6550, time diff 0.8272745609283447, total time dif 498.5946183204651)
step: 6550 @ episode report: {'average_total_reward': np.float32(5.0477777), 'reward_variance': np.float32(1.4370632), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0984336145222187), 'actor_loss': np.float64(-1.019053202867508), 'hyper_actor_loss': np.float64(0.024834180995821954), 'behavior_loss': np.float64(1.4477034330368042)}

Episode step 6560, time diff 0.7425835132598877, total time dif 499.42189288139343)
step: 6560 @ episode report: {'average_total_reward': np.float32(4.1744447), 'reward_variance': np.float32(1.0886185), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08629149496555329), 'actor_loss': np.float64(-0.9503144443035125), 'hyper_actor_loss': np.float64(0.025017546862363814), 'behavior_loss': np.float64(1.5748483896255494)}

Episode step 6570, time diff 0.7892453670501709, total time dif 500.1644763946533)
step: 6570 @ episode report: {'average_total_reward': np.float32(4.6988893), 'reward_variance': np.float32(1.1832703), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11032054647803306), 'actor_loss': np.float64(-1.0266182005405426), 'hyper_actor_loss': np.float64(0.02517212275415659), 'behavior_loss': np.float64(1.5123786330223083)}

Episode step 6580, time diff 0.7432639598846436, total time dif 500.9537217617035)
step: 6580 @ episode report: {'average_total_reward': np.float32(3.986667), 'reward_variance': np.float32(0.96298283), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10719134248793125), 'actor_loss': np.float64(-0.9809966027736664), 'hyper_actor_loss': np.float64(0.024934789724648), 'behavior_loss': np.float64(1.5347727298736573)}

Episode step 6590, time diff 0.7071471214294434, total time dif 501.69698572158813)
step: 6590 @ episode report: {'average_total_reward': np.float32(5.1111116), 'reward_variance': np.float32(1.9332101), 'max_total_reward': np.float32(7.5333343), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09429391548037529), 'actor_loss': np.float64(-0.9762566149234772), 'hyper_actor_loss': np.float64(0.02554915174841881), 'behavior_loss': np.float64(1.5553425073623657)}

Episode step 6600, time diff 0.7460589408874512, total time dif 502.4041328430176)
step: 6600 @ episode report: {'average_total_reward': np.float32(4.686667), 'reward_variance': np.float32(0.6539459), 'max_total_reward': np.float32(6.2888894), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09796033427119255), 'actor_loss': np.float64(-0.997823441028595), 'hyper_actor_loss': np.float64(0.025422445498406887), 'behavior_loss': np.float64(1.4542717933654785)}

Episode step 6610, time diff 0.7660055160522461, total time dif 503.15019178390503)
step: 6610 @ episode report: {'average_total_reward': np.float32(4.625556), 'reward_variance': np.float32(1.1048409), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(2.9222221), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07935334853827954), 'actor_loss': np.float64(-0.9887466430664062), 'hyper_actor_loss': np.float64(0.02561971787363291), 'behavior_loss': np.float64(1.431099259853363)}

Episode step 6620, time diff 0.7453892230987549, total time dif 503.9161972999573)
step: 6620 @ episode report: {'average_total_reward': np.float32(4.111111), 'reward_variance': np.float32(1.6234815), 'max_total_reward': np.float32(5.6555552), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10866898894309998), 'actor_loss': np.float64(-0.9985593318939209), 'hyper_actor_loss': np.float64(0.025884563475847243), 'behavior_loss': np.float64(1.4573636293411254)}

Episode step 6630, time diff 0.7719104290008545, total time dif 504.66158652305603)
step: 6630 @ episode report: {'average_total_reward': np.float32(4.101111), 'reward_variance': np.float32(0.57618403), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.9222224), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09370395913720131), 'actor_loss': np.float64(-1.0128259301185607), 'hyper_actor_loss': np.float64(0.026295429654419424), 'behavior_loss': np.float64(1.4668569803237914)}

Episode step 6640, time diff 0.7729787826538086, total time dif 505.4334969520569)
step: 6640 @ episode report: {'average_total_reward': np.float32(4.0111113), 'reward_variance': np.float32(1.201605), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1064334750175476), 'actor_loss': np.float64(-1.0034823656082152), 'hyper_actor_loss': np.float64(0.026533539220690728), 'behavior_loss': np.float64(1.5155914425849915)}

Episode step 6650, time diff 0.7228395938873291, total time dif 506.2064757347107)
step: 6650 @ episode report: {'average_total_reward': np.float32(4.401111), 'reward_variance': np.float32(0.40406045), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09715128391981125), 'actor_loss': np.float64(-1.0127451777458192), 'hyper_actor_loss': np.float64(0.02663299720734358), 'behavior_loss': np.float64(1.3868760585784912)}

Episode step 6660, time diff 0.7326016426086426, total time dif 506.929315328598)
step: 6660 @ episode report: {'average_total_reward': np.float32(3.3888888), 'reward_variance': np.float32(0.4959754), 'max_total_reward': np.float32(4.2888894), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09251115135848523), 'actor_loss': np.float64(-0.9619973838329315), 'hyper_actor_loss': np.float64(0.026511510647833348), 'behavior_loss': np.float64(1.475931739807129)}

Episode step 6670, time diff 0.8537187576293945, total time dif 507.66191697120667)
step: 6670 @ episode report: {'average_total_reward': np.float32(3.7622223), 'reward_variance': np.float32(0.5554865), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10054086819291115), 'actor_loss': np.float64(-0.9947558760643005), 'hyper_actor_loss': np.float64(0.026069326885044576), 'behavior_loss': np.float64(1.4397767543792725)}

Episode step 6680, time diff 0.7164614200592041, total time dif 508.51563572883606)
step: 6680 @ episode report: {'average_total_reward': np.float32(4.2477775), 'reward_variance': np.float32(0.46010002), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10667312815785408), 'actor_loss': np.float64(-0.9875275671482087), 'hyper_actor_loss': np.float64(0.025834178552031517), 'behavior_loss': np.float64(1.4096305131912232)}

Episode step 6690, time diff 0.7187528610229492, total time dif 509.23209714889526)
step: 6690 @ episode report: {'average_total_reward': np.float32(4.15), 'reward_variance': np.float32(0.50985825), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10147809460759163), 'actor_loss': np.float64(-0.9325951933860779), 'hyper_actor_loss': np.float64(0.02508602514863014), 'behavior_loss': np.float64(1.4834413051605224)}

Episode step 6700, time diff 0.699986457824707, total time dif 509.9508500099182)
step: 6700 @ episode report: {'average_total_reward': np.float32(3.95), 'reward_variance': np.float32(0.67714214), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09151796959340572), 'actor_loss': np.float64(-0.942780864238739), 'hyper_actor_loss': np.float64(0.023993059806525708), 'behavior_loss': np.float64(1.4849400401115418)}

Episode step 6710, time diff 0.898463249206543, total time dif 510.6508364677429)
step: 6710 @ episode report: {'average_total_reward': np.float32(4.223334), 'reward_variance': np.float32(1.44853), 'max_total_reward': np.float32(6.2888894), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10318613424897194), 'actor_loss': np.float64(-0.9106106460094452), 'hyper_actor_loss': np.float64(0.022805093601346017), 'behavior_loss': np.float64(1.5124796271324157)}

Episode step 6720, time diff 0.8817782402038574, total time dif 511.54929971694946)
step: 6720 @ episode report: {'average_total_reward': np.float32(6.394445), 'reward_variance': np.float32(3.271539), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10192912966012954), 'actor_loss': np.float64(-0.9804015576839447), 'hyper_actor_loss': np.float64(0.021635138802230357), 'behavior_loss': np.float64(1.4206568360328675)}

Episode step 6730, time diff 0.711707353591919, total time dif 512.4310779571533)
step: 6730 @ episode report: {'average_total_reward': np.float32(6.7066674), 'reward_variance': np.float32(5.015685), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10547288581728935), 'actor_loss': np.float64(-0.9886857450008393), 'hyper_actor_loss': np.float64(0.021301454119384288), 'behavior_loss': np.float64(1.499486792087555)}

Episode step 6740, time diff 0.7257463932037354, total time dif 513.1427853107452)
step: 6740 @ episode report: {'average_total_reward': np.float32(8.377779), 'reward_variance': np.float32(2.11884), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11285163611173629), 'actor_loss': np.float64(-1.0111547231674194), 'hyper_actor_loss': np.float64(0.020845326781272887), 'behavior_loss': np.float64(1.5660528540611267)}

Episode step 6750, time diff 0.7001867294311523, total time dif 513.868531703949)
step: 6750 @ episode report: {'average_total_reward': np.float32(7.7777786), 'reward_variance': np.float32(2.6668894), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1054088182747364), 'actor_loss': np.float64(-0.9753567934036255), 'hyper_actor_loss': np.float64(0.02057616747915745), 'behavior_loss': np.float64(1.5304561853408813)}

Episode step 6760, time diff 0.7067880630493164, total time dif 514.5687184333801)
step: 6760 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(3.3613846), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09454375505447388), 'actor_loss': np.float64(-0.9813502311706543), 'hyper_actor_loss': np.float64(0.020352056436240672), 'behavior_loss': np.float64(1.5158167123794555)}

Episode step 6770, time diff 0.6894102096557617, total time dif 515.2755064964294)
step: 6770 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(2.6466682), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08866326734423638), 'actor_loss': np.float64(-1.0013812601566314), 'hyper_actor_loss': np.float64(0.019984229281544686), 'behavior_loss': np.float64(1.4862764000892639)}

Episode step 6780, time diff 0.7022943496704102, total time dif 515.9649167060852)
step: 6780 @ episode report: {'average_total_reward': np.float32(8.863333), 'reward_variance': np.float32(3.2501006), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10641708225011826), 'actor_loss': np.float64(-0.9883408963680267), 'hyper_actor_loss': np.float64(0.019594017788767813), 'behavior_loss': np.float64(1.4668399453163148)}

Episode step 6790, time diff 0.6773607730865479, total time dif 516.6672110557556)
step: 6790 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(1.2890493), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10295548364520073), 'actor_loss': np.float64(-0.981017118692398), 'hyper_actor_loss': np.float64(0.019335583969950675), 'behavior_loss': np.float64(1.5845252513885497)}

Episode step 6800, time diff 0.7979083061218262, total time dif 517.3445718288422)
step: 6800 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(5.8866677), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08576754257082939), 'actor_loss': np.float64(-0.998473709821701), 'hyper_actor_loss': np.float64(0.018974727392196654), 'behavior_loss': np.float64(1.459071660041809)}

Episode step 6810, time diff 0.836205005645752, total time dif 518.142480134964)
step: 6810 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(2.5514195), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10166156142950059), 'actor_loss': np.float64(-0.9938642263412476), 'hyper_actor_loss': np.float64(0.018899635598063468), 'behavior_loss': np.float64(1.5431594252586365)}

Episode step 6820, time diff 0.8392307758331299, total time dif 518.9786851406097)
step: 6820 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(6.760785), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09907706007361412), 'actor_loss': np.float64(-0.9838320493698121), 'hyper_actor_loss': np.float64(0.018863405846059323), 'behavior_loss': np.float64(1.5221761345863343)}

Episode step 6830, time diff 0.7152364253997803, total time dif 519.8179159164429)
step: 6830 @ episode report: {'average_total_reward': np.float32(9.387779), 'reward_variance': np.float32(4.606308), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10080650299787522), 'actor_loss': np.float64(-0.9694406449794769), 'hyper_actor_loss': np.float64(0.01922161541879177), 'behavior_loss': np.float64(1.451837146282196)}

Episode step 6840, time diff 0.8936939239501953, total time dif 520.5331523418427)
step: 6840 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.7129996), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10127563625574112), 'actor_loss': np.float64(-0.9636137962341309), 'hyper_actor_loss': np.float64(0.019341323152184488), 'behavior_loss': np.float64(1.443335509300232)}

Episode step 6850, time diff 0.7321369647979736, total time dif 521.4268462657928)
step: 6850 @ episode report: {'average_total_reward': np.float32(10.35889), 'reward_variance': np.float32(4.5463724), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0968714140355587), 'actor_loss': np.float64(-1.00216543674469), 'hyper_actor_loss': np.float64(0.01904644649475813), 'behavior_loss': np.float64(1.392434561252594)}

Episode step 6860, time diff 0.7676858901977539, total time dif 522.1589832305908)
step: 6860 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.0534815), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09987199194729328), 'actor_loss': np.float64(-0.9829736411571502), 'hyper_actor_loss': np.float64(0.018699103407561778), 'behavior_loss': np.float64(1.3921035766601562)}

Episode step 6870, time diff 0.799323558807373, total time dif 522.9266691207886)
step: 6870 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(2.0446782), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10065609887242317), 'actor_loss': np.float64(-0.9973982155323029), 'hyper_actor_loss': np.float64(0.018664728663861752), 'behavior_loss': np.float64(1.4506343841552733)}

Episode step 6880, time diff 0.8466091156005859, total time dif 523.725992679596)
step: 6880 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(2.951433), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10251911804080009), 'actor_loss': np.float64(-1.0105593979358674), 'hyper_actor_loss': np.float64(0.018468642607331276), 'behavior_loss': np.float64(1.507506537437439)}

Episode step 6890, time diff 0.7670543193817139, total time dif 524.5726017951965)
step: 6890 @ episode report: {'average_total_reward': np.float32(10.461111), 'reward_variance': np.float32(2.5936847), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10142255760729313), 'actor_loss': np.float64(-0.9926022529602051), 'hyper_actor_loss': np.float64(0.018708568625152112), 'behavior_loss': np.float64(1.5724045515060425)}

Episode step 6900, time diff 0.7843532562255859, total time dif 525.3396561145782)
step: 6900 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(2.2579122), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09035561755299568), 'actor_loss': np.float64(-1.0236721277236938), 'hyper_actor_loss': np.float64(0.01823557447642088), 'behavior_loss': np.float64(1.446191656589508)}

Episode step 6910, time diff 0.7394199371337891, total time dif 526.1240093708038)
step: 6910 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.072001), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0932516947388649), 'actor_loss': np.float64(-0.9829880833625794), 'hyper_actor_loss': np.float64(0.01807046514004469), 'behavior_loss': np.float64(1.5376241326332092)}

Episode step 6920, time diff 0.7889456748962402, total time dif 526.8634293079376)
step: 6920 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.7424555), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09850881174206734), 'actor_loss': np.float64(-1.022639125585556), 'hyper_actor_loss': np.float64(0.01773423533886671), 'behavior_loss': np.float64(1.507127594947815)}

Episode step 6930, time diff 0.7294309139251709, total time dif 527.6523749828339)
step: 6930 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.6660354), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08751373291015625), 'actor_loss': np.float64(-0.9899333238601684), 'hyper_actor_loss': np.float64(0.01832838822156191), 'behavior_loss': np.float64(1.5187975287437439)}

Episode step 6940, time diff 0.7149052619934082, total time dif 528.381805896759)
step: 6940 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.04418), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10772788375616074), 'actor_loss': np.float64(-1.0405959606170654), 'hyper_actor_loss': np.float64(0.01846171449869871), 'behavior_loss': np.float64(1.5757355570793152)}

Episode step 6950, time diff 0.7186357975006104, total time dif 529.0967111587524)
step: 6950 @ episode report: {'average_total_reward': np.float32(8.377779), 'reward_variance': np.float32(2.2699506), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10799138993024826), 'actor_loss': np.float64(-1.0087736427783967), 'hyper_actor_loss': np.float64(0.01860755905508995), 'behavior_loss': np.float64(1.5448850035667419)}

Episode step 6960, time diff 0.7120716571807861, total time dif 529.815346956253)
step: 6960 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(2.969858), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1012090153992176), 'actor_loss': np.float64(-1.063357377052307), 'hyper_actor_loss': np.float64(0.01863207221031189), 'behavior_loss': np.float64(1.5740665197372437)}

Episode step 6970, time diff 0.7483689785003662, total time dif 530.5274186134338)
step: 6970 @ episode report: {'average_total_reward': np.float32(7.555556), 'reward_variance': np.float32(1.8650866), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10193920284509658), 'actor_loss': np.float64(-1.0316001534461976), 'hyper_actor_loss': np.float64(0.019078875705599786), 'behavior_loss': np.float64(1.633533811569214)}

Episode step 6980, time diff 0.7005634307861328, total time dif 531.2757875919342)
step: 6980 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(1.9398025), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11592370495200158), 'actor_loss': np.float64(-1.0485919952392577), 'hyper_actor_loss': np.float64(0.01973824892193079), 'behavior_loss': np.float64(1.496417725086212)}

Episode step 6990, time diff 0.7499332427978516, total time dif 531.9763510227203)
step: 6990 @ episode report: {'average_total_reward': np.float32(6.382222), 'reward_variance': np.float32(0.66543704), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10909646600484849), 'actor_loss': np.float64(-1.0519976258277892), 'hyper_actor_loss': np.float64(0.020109987072646617), 'behavior_loss': np.float64(1.5438205480575562)}

Episode step 7000, time diff 0.7427339553833008, total time dif 532.7262842655182)
step: 7000 @ episode report: {'average_total_reward': np.float32(6.045556), 'reward_variance': np.float32(1.7259376), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10487274751067162), 'actor_loss': np.float64(-1.0046710252761841), 'hyper_actor_loss': np.float64(0.019729122705757617), 'behavior_loss': np.float64(1.461106538772583)}

Episode step 7010, time diff 0.8973443508148193, total time dif 533.4690182209015)
step: 7010 @ episode report: {'average_total_reward': np.float32(4.635556), 'reward_variance': np.float32(0.6623408), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.1666665), 'average_n_step': np.float32(6.2), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1045866146683693), 'actor_loss': np.float64(-1.0549049377441406), 'hyper_actor_loss': np.float64(0.01931620016694069), 'behavior_loss': np.float64(1.5139020681381226)}

Episode step 7020, time diff 0.7048318386077881, total time dif 534.3663625717163)
step: 7020 @ episode report: {'average_total_reward': np.float32(4.798889), 'reward_variance': np.float32(0.70598656), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(6.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09836254566907883), 'actor_loss': np.float64(-1.0007275462150573), 'hyper_actor_loss': np.float64(0.019488439708948136), 'behavior_loss': np.float64(1.4561073422431945)}

Episode step 7030, time diff 0.7388710975646973, total time dif 535.0711944103241)
step: 7030 @ episode report: {'average_total_reward': np.float32(5.6333337), 'reward_variance': np.float32(1.7034817), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09782725349068641), 'actor_loss': np.float64(-1.0498409271240234), 'hyper_actor_loss': np.float64(0.01946326792240143), 'behavior_loss': np.float64(1.4760178446769714)}

Episode step 7040, time diff 0.8621940612792969, total time dif 535.8100655078888)
step: 7040 @ episode report: {'average_total_reward': np.float32(5.6844444), 'reward_variance': np.float32(1.4624497), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11118682324886323), 'actor_loss': np.float64(-1.032879614830017), 'hyper_actor_loss': np.float64(0.02022499665617943), 'behavior_loss': np.float64(1.498950147628784)}

Episode step 7050, time diff 0.8269445896148682, total time dif 536.6722595691681)
step: 7050 @ episode report: {'average_total_reward': np.float32(5.6600003), 'reward_variance': np.float32(2.7415614), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0903943046927452), 'actor_loss': np.float64(-0.993431681394577), 'hyper_actor_loss': np.float64(0.02071730699390173), 'behavior_loss': np.float64(1.4768684148788451)}

Episode step 7060, time diff 0.7226839065551758, total time dif 537.499204158783)
step: 7060 @ episode report: {'average_total_reward': np.float32(6.0211115), 'reward_variance': np.float32(1.1578138), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0957581140100956), 'actor_loss': np.float64(-1.0174443483352662), 'hyper_actor_loss': np.float64(0.021016505546867847), 'behavior_loss': np.float64(1.4320897936820984)}

Episode step 7070, time diff 0.7499980926513672, total time dif 538.2218880653381)
step: 7070 @ episode report: {'average_total_reward': np.float32(5.608889), 'reward_variance': np.float32(0.44340253), 'max_total_reward': np.float32(6.655556), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0906107097864151), 'actor_loss': np.float64(-1.0196983158588409), 'hyper_actor_loss': np.float64(0.02139941118657589), 'behavior_loss': np.float64(1.4218591690063476)}

Episode step 7080, time diff 0.722214937210083, total time dif 538.9718861579895)
step: 7080 @ episode report: {'average_total_reward': np.float32(5.4600005), 'reward_variance': np.float32(2.1926224), 'max_total_reward': np.float32(7.655556), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.098340630158782), 'actor_loss': np.float64(-0.9809156835079194), 'hyper_actor_loss': np.float64(0.022236739099025727), 'behavior_loss': np.float64(1.4356114983558654)}

Episode step 7090, time diff 0.6821460723876953, total time dif 539.6941010951996)
step: 7090 @ episode report: {'average_total_reward': np.float32(6.867778), 'reward_variance': np.float32(2.9270976), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09535949304699898), 'actor_loss': np.float64(-1.0019958734512329), 'hyper_actor_loss': np.float64(0.022975381650030612), 'behavior_loss': np.float64(1.390914511680603)}

Episode step 7100, time diff 0.6931028366088867, total time dif 540.3762471675873)
step: 7100 @ episode report: {'average_total_reward': np.float32(6.8555555), 'reward_variance': np.float32(1.6587658), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11292416602373123), 'actor_loss': np.float64(-0.9794649183750153), 'hyper_actor_loss': np.float64(0.022259797900915146), 'behavior_loss': np.float64(1.4867500901222228)}

Episode step 7110, time diff 0.7269251346588135, total time dif 541.0693500041962)
step: 7110 @ episode report: {'average_total_reward': np.float32(6.057778), 'reward_variance': np.float32(1.8519217), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11008980274200439), 'actor_loss': np.float64(-0.9848005890846252), 'hyper_actor_loss': np.float64(0.02034672684967518), 'behavior_loss': np.float64(1.457634484767914)}

Episode step 7120, time diff 0.7029716968536377, total time dif 541.796275138855)
step: 7120 @ episode report: {'average_total_reward': np.float32(7.082223), 'reward_variance': np.float32(3.112968), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10669952258467674), 'actor_loss': np.float64(-0.9884853482246398), 'hyper_actor_loss': np.float64(0.018793433904647827), 'behavior_loss': np.float64(1.591977560520172)}

Episode step 7130, time diff 0.7499825954437256, total time dif 542.4992468357086)
step: 7130 @ episode report: {'average_total_reward': np.float32(6.894445), 'reward_variance': np.float32(3.6439564), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09494590796530247), 'actor_loss': np.float64(-0.9605076789855957), 'hyper_actor_loss': np.float64(0.017084184288978576), 'behavior_loss': np.float64(1.5285572767257691)}

Episode step 7140, time diff 0.718555212020874, total time dif 543.2492294311523)
step: 7140 @ episode report: {'average_total_reward': np.float32(7.3555555), 'reward_variance': np.float32(2.296815), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11134904026985168), 'actor_loss': np.float64(-0.9930939733982086), 'hyper_actor_loss': np.float64(0.015970620233565568), 'behavior_loss': np.float64(1.6868866086006165)}

Episode step 7150, time diff 0.664374828338623, total time dif 543.9677846431732)
step: 7150 @ episode report: {'average_total_reward': np.float32(6.6333337), 'reward_variance': np.float32(1.2241733), 'max_total_reward': np.float32(8.533334), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10715608596801758), 'actor_loss': np.float64(-1.0414013624191285), 'hyper_actor_loss': np.float64(0.015168702136725188), 'behavior_loss': np.float64(1.5919777154922485)}

Episode step 7160, time diff 0.6934926509857178, total time dif 544.6321594715118)
step: 7160 @ episode report: {'average_total_reward': np.float32(5.77), 'reward_variance': np.float32(1.462717), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08709335289895534), 'actor_loss': np.float64(-0.9728714823722839), 'hyper_actor_loss': np.float64(0.0151862608268857), 'behavior_loss': np.float64(1.595369851589203)}

Episode step 7170, time diff 0.8605453968048096, total time dif 545.3256521224976)
step: 7170 @ episode report: {'average_total_reward': np.float32(7.28), 'reward_variance': np.float32(1.2235509), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10820419043302536), 'actor_loss': np.float64(-1.0040384709835053), 'hyper_actor_loss': np.float64(0.015249376650899649), 'behavior_loss': np.float64(1.50501868724823)}

Episode step 7180, time diff 0.6849758625030518, total time dif 546.1861975193024)
step: 7180 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(1.3502663), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10129237025976182), 'actor_loss': np.float64(-0.9956240952014923), 'hyper_actor_loss': np.float64(0.014916588831692935), 'behavior_loss': np.float64(1.5393039107322692)}

Episode step 7190, time diff 0.7052798271179199, total time dif 546.8711733818054)
step: 7190 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(1.2009138), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11255493983626366), 'actor_loss': np.float64(-0.9770642638206481), 'hyper_actor_loss': np.float64(0.014441265352070331), 'behavior_loss': np.float64(1.4941646218299867)}

Episode step 7200, time diff 0.7969377040863037, total time dif 547.5764532089233)
step: 7200 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(1.4152113), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09689472913742066), 'actor_loss': np.float64(-1.0296070516109466), 'hyper_actor_loss': np.float64(0.013610512390732766), 'behavior_loss': np.float64(1.5256749510765075)}

Episode step 7210, time diff 0.7033767700195312, total time dif 548.3733909130096)
step: 7210 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.7373648), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09775863066315651), 'actor_loss': np.float64(-0.9432776749134064), 'hyper_actor_loss': np.float64(0.012701034266501664), 'behavior_loss': np.float64(1.5997546553611754)}

Episode step 7220, time diff 0.7593917846679688, total time dif 549.0767676830292)
step: 7220 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(1.1422828), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1151145339012146), 'actor_loss': np.float64(-1.0393553674221039), 'hyper_actor_loss': np.float64(0.01215662956237793), 'behavior_loss': np.float64(1.6061494946479797)}

Episode step 7230, time diff 0.6880617141723633, total time dif 549.8361594676971)
step: 7230 @ episode report: {'average_total_reward': np.float32(7.9533334), 'reward_variance': np.float32(3.3567605), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10640694126486779), 'actor_loss': np.float64(-1.015474945306778), 'hyper_actor_loss': np.float64(0.011833220534026623), 'behavior_loss': np.float64(1.6051538109779357)}

Episode step 7240, time diff 0.731987714767456, total time dif 550.5242211818695)
step: 7240 @ episode report: {'average_total_reward': np.float32(8.341112), 'reward_variance': np.float32(3.7430634), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0925277378410101), 'actor_loss': np.float64(-0.9760087311267853), 'hyper_actor_loss': np.float64(0.011640041787177324), 'behavior_loss': np.float64(1.6216570734977722)}

Episode step 7250, time diff 0.7334480285644531, total time dif 551.256208896637)
step: 7250 @ episode report: {'average_total_reward': np.float32(8.228889), 'reward_variance': np.float32(0.59672135), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09344237968325615), 'actor_loss': np.float64(-0.988457202911377), 'hyper_actor_loss': np.float64(0.010959362145513296), 'behavior_loss': np.float64(1.6482291340827941)}

Episode step 7260, time diff 0.7225518226623535, total time dif 551.9896569252014)
step: 7260 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.4879556), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11114927902817726), 'actor_loss': np.float64(-1.0113593101501466), 'hyper_actor_loss': np.float64(0.01059655686840415), 'behavior_loss': np.float64(1.5653889298439025)}

Episode step 7270, time diff 0.7458517551422119, total time dif 552.7122087478638)
step: 7270 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.0319556), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09969093576073647), 'actor_loss': np.float64(-0.9922797083854675), 'hyper_actor_loss': np.float64(0.010200292337685823), 'behavior_loss': np.float64(1.5454270124435425)}

Episode step 7280, time diff 0.6622812747955322, total time dif 553.458060503006)
step: 7280 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.1781282), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09813942462205887), 'actor_loss': np.float64(-0.9936736345291137), 'hyper_actor_loss': np.float64(0.009655879251658917), 'behavior_loss': np.float64(1.6449544310569764)}

Episode step 7290, time diff 0.6769576072692871, total time dif 554.1203417778015)
step: 7290 @ episode report: {'average_total_reward': np.float32(8.863334), 'reward_variance': np.float32(4.1538534), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10651382729411125), 'actor_loss': np.float64(-1.0247905015945435), 'hyper_actor_loss': np.float64(0.00915726600214839), 'behavior_loss': np.float64(1.6460638165473938)}

Episode step 7300, time diff 0.6821980476379395, total time dif 554.7972993850708)
step: 7300 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.482983), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10601678043603897), 'actor_loss': np.float64(-1.0151255011558533), 'hyper_actor_loss': np.float64(0.008965942915529013), 'behavior_loss': np.float64(1.6593960285186768)}

Episode step 7310, time diff 0.6693723201751709, total time dif 555.4794974327087)
step: 7310 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(3.4826171), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09178626984357834), 'actor_loss': np.float64(-0.9895667552947998), 'hyper_actor_loss': np.float64(0.008900262322276831), 'behavior_loss': np.float64(1.6497191190719604)}

Episode step 7320, time diff 0.7543771266937256, total time dif 556.1488697528839)
step: 7320 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.6045434), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09974802136421204), 'actor_loss': np.float64(-1.0001993417739867), 'hyper_actor_loss': np.float64(0.008658497966825962), 'behavior_loss': np.float64(1.7014292359352112)}

Episode step 7330, time diff 0.8562955856323242, total time dif 556.9032468795776)
step: 7330 @ episode report: {'average_total_reward': np.float32(10.324446), 'reward_variance': np.float32(2.5330815), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10082555413246155), 'actor_loss': np.float64(-0.9824469089508057), 'hyper_actor_loss': np.float64(0.008591247163712979), 'behavior_loss': np.float64(1.7316356539726256)}

Episode step 7340, time diff 0.7299849987030029, total time dif 557.75954246521)
step: 7340 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(3.6480746), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09727123454213142), 'actor_loss': np.float64(-0.9930886209011078), 'hyper_actor_loss': np.float64(0.008613384887576103), 'behavior_loss': np.float64(1.6184722304344177)}

Episode step 7350, time diff 0.711068868637085, total time dif 558.489527463913)
step: 7350 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(4.2501), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09769812375307083), 'actor_loss': np.float64(-0.9877700328826904), 'hyper_actor_loss': np.float64(0.008532826229929924), 'behavior_loss': np.float64(1.6335352420806886)}

Episode step 7360, time diff 0.787930965423584, total time dif 559.20059633255)
step: 7360 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(2.6785789), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09051369801163674), 'actor_loss': np.float64(-0.9849491775035858), 'hyper_actor_loss': np.float64(0.008209292497485875), 'behavior_loss': np.float64(1.508286476135254)}

Episode step 7370, time diff 0.6988742351531982, total time dif 559.9885272979736)
step: 7370 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(1.0560997), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11670056879520416), 'actor_loss': np.float64(-1.0234160423278809), 'hyper_actor_loss': np.float64(0.00807007378898561), 'behavior_loss': np.float64(1.7127764225006104)}

Episode step 7380, time diff 0.7247724533081055, total time dif 560.6874015331268)
step: 7380 @ episode report: {'average_total_reward': np.float32(10.573334), 'reward_variance': np.float32(1.7226965), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.411112), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09239736236631871), 'actor_loss': np.float64(-0.976221626996994), 'hyper_actor_loss': np.float64(0.007828995352610946), 'behavior_loss': np.float64(1.6890417098999024)}

Episode step 7390, time diff 0.7051219940185547, total time dif 561.4121739864349)
step: 7390 @ episode report: {'average_total_reward': np.float32(10.197778), 'reward_variance': np.float32(2.272465), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11992567852139473), 'actor_loss': np.float64(-1.0231208086013794), 'hyper_actor_loss': np.float64(0.007732345536351204), 'behavior_loss': np.float64(1.6886640429496764)}

Episode step 7400, time diff 0.6687641143798828, total time dif 562.1172959804535)
step: 7400 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(4.871453), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10583982318639755), 'actor_loss': np.float64(-0.994725477695465), 'hyper_actor_loss': np.float64(0.0074918170925229784), 'behavior_loss': np.float64(1.7628365635871888)}

Episode step 7410, time diff 0.7112348079681396, total time dif 562.7860600948334)
step: 7410 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(3.6228657), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0904541164636612), 'actor_loss': np.float64(-0.9732555627822876), 'hyper_actor_loss': np.float64(0.007317490782588721), 'behavior_loss': np.float64(1.781370794773102)}

Episode step 7420, time diff 0.698289155960083, total time dif 563.4972949028015)
step: 7420 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(3.7869446), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08958739414811134), 'actor_loss': np.float64(-0.9703998386859893), 'hyper_actor_loss': np.float64(0.007032048515975475), 'behavior_loss': np.float64(1.671900200843811)}

Episode step 7430, time diff 0.7055351734161377, total time dif 564.1955840587616)
step: 7430 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(1.6849744), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09649154171347618), 'actor_loss': np.float64(-0.992379778623581), 'hyper_actor_loss': np.float64(0.0068598016630858185), 'behavior_loss': np.float64(1.7651730060577393)}

Episode step 7440, time diff 0.6629712581634521, total time dif 564.9011192321777)
step: 7440 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.9239569), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07971354238688946), 'actor_loss': np.float64(-0.929362291097641), 'hyper_actor_loss': np.float64(0.006848985329270363), 'behavior_loss': np.float64(1.6748040199279786)}

Episode step 7450, time diff 0.6761584281921387, total time dif 565.5640904903412)
step: 7450 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(2.6353626), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08640649244189262), 'actor_loss': np.float64(-1.0062312006950378), 'hyper_actor_loss': np.float64(0.006784175243228674), 'behavior_loss': np.float64(1.741976511478424)}

Episode step 7460, time diff 0.646000862121582, total time dif 566.2402489185333)
step: 7460 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(4.353236), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10986177772283554), 'actor_loss': np.float64(-1.025930392742157), 'hyper_actor_loss': np.float64(0.006686415802687406), 'behavior_loss': np.float64(1.7295762658119203)}

Episode step 7470, time diff 0.7313916683197021, total time dif 566.8862497806549)
step: 7470 @ episode report: {'average_total_reward': np.float32(8.738889), 'reward_variance': np.float32(3.5913148), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09156470000743866), 'actor_loss': np.float64(-0.9856606662273407), 'hyper_actor_loss': np.float64(0.006763513712212443), 'behavior_loss': np.float64(1.7290541410446167)}

Episode step 7480, time diff 0.6938185691833496, total time dif 567.6176414489746)
step: 7480 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(2.0206122), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09026662334799766), 'actor_loss': np.float64(-0.9985429346561432), 'hyper_actor_loss': np.float64(0.006683064810931683), 'behavior_loss': np.float64(1.7797382354736329)}

Episode step 7490, time diff 0.729039192199707, total time dif 568.311460018158)
step: 7490 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(3.0759058), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09484845697879792), 'actor_loss': np.float64(-0.9792915046215057), 'hyper_actor_loss': np.float64(0.006711497902870178), 'behavior_loss': np.float64(1.7667157053947449)}

Episode step 7500, time diff 0.8371949195861816, total time dif 569.0404992103577)
step: 7500 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(3.8893876), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10321172550320626), 'actor_loss': np.float64(-1.018703669309616), 'hyper_actor_loss': np.float64(0.006679046247154474), 'behavior_loss': np.float64(1.7259747982025146)}

Episode step 7510, time diff 0.6744389533996582, total time dif 569.8776941299438)
step: 7510 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(1.7827911), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10079625248908997), 'actor_loss': np.float64(-0.9855208039283753), 'hyper_actor_loss': np.float64(0.006968331709504128), 'behavior_loss': np.float64(1.78785879611969)}

Episode step 7520, time diff 0.7278239727020264, total time dif 570.5521330833435)
step: 7520 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(6.0017486), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10375947132706642), 'actor_loss': np.float64(-1.0277520060539245), 'hyper_actor_loss': np.float64(0.007082766341045499), 'behavior_loss': np.float64(1.72863107919693)}

Episode step 7530, time diff 0.77396559715271, total time dif 571.2799570560455)
step: 7530 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(2.039462), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0956203818321228), 'actor_loss': np.float64(-1.0081706821918488), 'hyper_actor_loss': np.float64(0.00698822638951242), 'behavior_loss': np.float64(1.648569941520691)}

Episode step 7540, time diff 0.6830563545227051, total time dif 572.0539226531982)
step: 7540 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(0.86158645), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09003382921218872), 'actor_loss': np.float64(-1.008945220708847), 'hyper_actor_loss': np.float64(0.007013450609520078), 'behavior_loss': np.float64(1.5890393257141113)}

Episode step 7550, time diff 0.7419722080230713, total time dif 572.736979007721)
step: 7550 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(0.8300295), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08876433297991752), 'actor_loss': np.float64(-0.9698522388935089), 'hyper_actor_loss': np.float64(0.0069315577391535045), 'behavior_loss': np.float64(1.6023269891738892)}

Episode step 7560, time diff 0.6892974376678467, total time dif 573.478951215744)
step: 7560 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(3.7418835), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10478811487555503), 'actor_loss': np.float64(-1.0239422917366028), 'hyper_actor_loss': np.float64(0.00698925880715251), 'behavior_loss': np.float64(1.797126865386963)}

Episode step 7570, time diff 0.6976175308227539, total time dif 574.1682486534119)
step: 7570 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(2.0078526), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09879830479621887), 'actor_loss': np.float64(-1.0235780715942382), 'hyper_actor_loss': np.float64(0.0069611378014087675), 'behavior_loss': np.float64(1.7876762628555298)}

Episode step 7580, time diff 0.733100175857544, total time dif 574.8658661842346)
step: 7580 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(2.8266182), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08718561604619027), 'actor_loss': np.float64(-1.0015677213668823), 'hyper_actor_loss': np.float64(0.0071196996606886385), 'behavior_loss': np.float64(1.7484749913215638)}

Episode step 7590, time diff 0.6723711490631104, total time dif 575.5989663600922)
step: 7590 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(3.2066376), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09522575438022614), 'actor_loss': np.float64(-1.0199798762798309), 'hyper_actor_loss': np.float64(0.007781443605199456), 'behavior_loss': np.float64(1.7330787897109985)}

Episode step 7600, time diff 0.6999151706695557, total time dif 576.2713375091553)
step: 7600 @ episode report: {'average_total_reward': np.float32(9.251111), 'reward_variance': np.float32(2.2213874), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10749002620577812), 'actor_loss': np.float64(-0.9673950254917145), 'hyper_actor_loss': np.float64(0.00814970675855875), 'behavior_loss': np.float64(1.6678539633750915)}

Episode step 7610, time diff 0.6977057456970215, total time dif 576.9712526798248)
step: 7610 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.8395869), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09080904759466649), 'actor_loss': np.float64(-0.9998315572738647), 'hyper_actor_loss': np.float64(0.00814067255705595), 'behavior_loss': np.float64(1.5406128287315368)}

Episode step 7620, time diff 0.6510090827941895, total time dif 577.6689584255219)
step: 7620 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(3.4475605), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08732396811246872), 'actor_loss': np.float64(-0.9675903975963592), 'hyper_actor_loss': np.float64(0.007930899783968926), 'behavior_loss': np.float64(1.6146267175674438)}

Episode step 7630, time diff 0.7022325992584229, total time dif 578.319967508316)
step: 7630 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(3.2860851), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09489627256989479), 'actor_loss': np.float64(-0.9812016248703003), 'hyper_actor_loss': np.float64(0.007687753485515714), 'behavior_loss': np.float64(1.7022531867027282)}

Episode step 7640, time diff 0.6859288215637207, total time dif 579.0222001075745)
step: 7640 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(0.834548), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0998107559978962), 'actor_loss': np.float64(-1.0082308411598206), 'hyper_actor_loss': np.float64(0.0075306678656488655), 'behavior_loss': np.float64(1.6443804860115052)}

Episode step 7650, time diff 0.6842217445373535, total time dif 579.7081289291382)
step: 7650 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.0141492), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.11013167947530747), 'actor_loss': np.float64(-1.0156343817710876), 'hyper_actor_loss': np.float64(0.007257710117846728), 'behavior_loss': np.float64(1.6672323346138)}

Episode step 7660, time diff 0.845991849899292, total time dif 580.3923506736755)
step: 7660 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(4.791231), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10600216463208198), 'actor_loss': np.float64(-1.049597930908203), 'hyper_actor_loss': np.float64(0.007292172592133284), 'behavior_loss': np.float64(1.668015444278717)}

Episode step 7670, time diff 0.6963400840759277, total time dif 581.2383425235748)
step: 7670 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.8994385), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0965972151607275), 'actor_loss': np.float64(-0.9757775962352753), 'hyper_actor_loss': np.float64(0.007427701214328408), 'behavior_loss': np.float64(1.6201512217521667)}

Episode step 7680, time diff 0.7043735980987549, total time dif 581.9346826076508)
step: 7680 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.769858), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08811070621013642), 'actor_loss': np.float64(-1.0251737833023071), 'hyper_actor_loss': np.float64(0.0076119279023259875), 'behavior_loss': np.float64(1.6559982180595398)}

Episode step 7690, time diff 0.6865818500518799, total time dif 582.6390562057495)
step: 7690 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(6.1894183), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09871776849031448), 'actor_loss': np.float64(-1.0053735256195069), 'hyper_actor_loss': np.float64(0.007612406695261598), 'behavior_loss': np.float64(1.7045692443847655)}

Episode step 7700, time diff 0.6887047290802002, total time dif 583.3256380558014)
step: 7700 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(4.765705), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08683896400034427), 'actor_loss': np.float64(-0.9889120697975159), 'hyper_actor_loss': np.float64(0.007650407962501049), 'behavior_loss': np.float64(1.5951999425888062)}

Episode step 7710, time diff 0.6963233947753906, total time dif 584.0143427848816)
step: 7710 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(3.2444682), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09470237754285335), 'actor_loss': np.float64(-0.9859870672225952), 'hyper_actor_loss': np.float64(0.007800526777282357), 'behavior_loss': np.float64(1.6548389434814452)}

Episode step 7720, time diff 0.70615553855896, total time dif 584.710666179657)
step: 7720 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(0.78866136), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09167929589748383), 'actor_loss': np.float64(-0.9830111503601074), 'hyper_actor_loss': np.float64(0.007903383672237396), 'behavior_loss': np.float64(1.7932533740997314)}

Episode step 7730, time diff 0.694512128829956, total time dif 585.4168217182159)
step: 7730 @ episode report: {'average_total_reward': np.float32(9.775557), 'reward_variance': np.float32(3.3953032), 'max_total_reward': np.float32(14.388888), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09556315541267395), 'actor_loss': np.float64(-0.9973421990871429), 'hyper_actor_loss': np.float64(0.007984821032732726), 'behavior_loss': np.float64(1.684503436088562)}

Episode step 7740, time diff 0.7062094211578369, total time dif 586.1113338470459)
step: 7740 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(1.9442661), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10379180908203126), 'actor_loss': np.float64(-1.0226641654968263), 'hyper_actor_loss': np.float64(0.00782454088330269), 'behavior_loss': np.float64(1.7007684707641602)}

Episode step 7750, time diff 0.7122135162353516, total time dif 586.8175432682037)
step: 7750 @ episode report: {'average_total_reward': np.float32(10.036668), 'reward_variance': np.float32(4.6834083), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07985826954245567), 'actor_loss': np.float64(-0.9831603288650512), 'hyper_actor_loss': np.float64(0.007480784505605698), 'behavior_loss': np.float64(1.6389643192291259)}

Episode step 7760, time diff 0.7024364471435547, total time dif 587.5297567844391)
step: 7760 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(3.137116), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09067448787391186), 'actor_loss': np.float64(-1.009353905916214), 'hyper_actor_loss': np.float64(0.007208259031176567), 'behavior_loss': np.float64(1.6763165950775147)}

Episode step 7770, time diff 0.6694703102111816, total time dif 588.2321932315826)
step: 7770 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(3.8680596), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10575737804174423), 'actor_loss': np.float64(-1.0333173274993896), 'hyper_actor_loss': np.float64(0.007011059671640396), 'behavior_loss': np.float64(1.7298977494239807)}

Episode step 7780, time diff 0.662670373916626, total time dif 588.9016635417938)
step: 7780 @ episode report: {'average_total_reward': np.float32(9.561112), 'reward_variance': np.float32(3.7121549), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10088580846786499), 'actor_loss': np.float64(-0.9773678183555603), 'hyper_actor_loss': np.float64(0.006760019063949585), 'behavior_loss': np.float64(1.724116814136505)}

Episode step 7790, time diff 0.6776406764984131, total time dif 589.5643339157104)
step: 7790 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.309877), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.108062032610178), 'actor_loss': np.float64(-1.0320747256278993), 'hyper_actor_loss': np.float64(0.006695132050663233), 'behavior_loss': np.float64(1.673503589630127)}

Episode step 7800, time diff 0.6769912242889404, total time dif 590.2419745922089)
step: 7800 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(3.9125195), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10320683866739273), 'actor_loss': np.float64(-0.9469089925289154), 'hyper_actor_loss': np.float64(0.006468250136822462), 'behavior_loss': np.float64(1.7315022230148316)}

Episode step 7810, time diff 0.6906077861785889, total time dif 590.9189658164978)
step: 7810 @ episode report: {'average_total_reward': np.float32(11.556668), 'reward_variance': np.float32(2.3533213), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0813467875123024), 'actor_loss': np.float64(-0.9475523293018341), 'hyper_actor_loss': np.float64(0.006204537767916918), 'behavior_loss': np.float64(1.7151268005371094)}

Episode step 7820, time diff 0.6773824691772461, total time dif 591.6095736026764)
step: 7820 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.6416001), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09794501364231109), 'actor_loss': np.float64(-0.9858596205711365), 'hyper_actor_loss': np.float64(0.005891001829877496), 'behavior_loss': np.float64(1.623779833316803)}

Episode step 7830, time diff 0.8192875385284424, total time dif 592.2869560718536)
step: 7830 @ episode report: {'average_total_reward': np.float32(8.526668), 'reward_variance': np.float32(1.5327207), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09690673872828484), 'actor_loss': np.float64(-1.0156558394432067), 'hyper_actor_loss': np.float64(0.005732676992192864), 'behavior_loss': np.float64(1.7844391345977784)}

Episode step 7840, time diff 0.7467703819274902, total time dif 593.1062436103821)
step: 7840 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(3.462648), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09375666975975036), 'actor_loss': np.float64(-0.9676685392856598), 'hyper_actor_loss': np.float64(0.005733282305300236), 'behavior_loss': np.float64(1.6840048551559448)}

Episode step 7850, time diff 0.8065180778503418, total time dif 593.8530139923096)
step: 7850 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(1.8884197), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10076599270105362), 'actor_loss': np.float64(-1.0045426905155181), 'hyper_actor_loss': np.float64(0.00568620259873569), 'behavior_loss': np.float64(1.6648227453231812)}

Episode step 7860, time diff 0.843888521194458, total time dif 594.6595320701599)
step: 7860 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(3.4187493), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09285520538687705), 'actor_loss': np.float64(-1.0049892783164978), 'hyper_actor_loss': np.float64(0.005838187923654914), 'behavior_loss': np.float64(1.6814132571220397)}

Episode step 7870, time diff 0.6903393268585205, total time dif 595.5034205913544)
step: 7870 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(1.5836589), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07488709352910519), 'actor_loss': np.float64(-0.9551349997520446), 'hyper_actor_loss': np.float64(0.0055679297540336846), 'behavior_loss': np.float64(1.6903367638587952)}

Episode step 7880, time diff 0.7227978706359863, total time dif 596.1937599182129)
step: 7880 @ episode report: {'average_total_reward': np.float32(8.9777775), 'reward_variance': np.float32(2.8308642), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09651955887675286), 'actor_loss': np.float64(-0.9806140005588532), 'hyper_actor_loss': np.float64(0.0054736819583922625), 'behavior_loss': np.float64(1.7066652655601502)}

Episode step 7890, time diff 0.6803936958312988, total time dif 596.9165577888489)
step: 7890 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(1.9766867), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09800521731376648), 'actor_loss': np.float64(-1.0001042008399963), 'hyper_actor_loss': np.float64(0.005388721590861678), 'behavior_loss': np.float64(1.809919285774231)}

Episode step 7900, time diff 0.6711621284484863, total time dif 597.5969514846802)
step: 7900 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(4.342366), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10059499219059945), 'actor_loss': np.float64(-0.9477038979530334), 'hyper_actor_loss': np.float64(0.005223521823063492), 'behavior_loss': np.float64(1.7437925577163695)}

Episode step 7910, time diff 0.6768224239349365, total time dif 598.2681136131287)
step: 7910 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(1.4426434), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10123421922326088), 'actor_loss': np.float64(-1.004880666732788), 'hyper_actor_loss': np.float64(0.005136275058612227), 'behavior_loss': np.float64(1.77545725107193)}

Episode step 7920, time diff 0.8115990161895752, total time dif 598.9449360370636)
step: 7920 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(2.4354687), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08989564552903176), 'actor_loss': np.float64(-0.9637783765792847), 'hyper_actor_loss': np.float64(0.005081243580207229), 'behavior_loss': np.float64(1.6325586199760438)}

Episode step 7930, time diff 0.818864107131958, total time dif 599.7565350532532)
step: 7930 @ episode report: {'average_total_reward': np.float32(9.936668), 'reward_variance': np.float32(1.5203717), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09400594457983971), 'actor_loss': np.float64(-1.0151955723762511), 'hyper_actor_loss': np.float64(0.005074461549520492), 'behavior_loss': np.float64(1.6185900211334228)}

Episode step 7940, time diff 0.755059003829956, total time dif 600.5753991603851)
step: 7940 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.888988), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08677333891391754), 'actor_loss': np.float64(-0.9912906110286712), 'hyper_actor_loss': np.float64(0.00506116827018559), 'behavior_loss': np.float64(1.6617157697677611)}

Episode step 7950, time diff 0.7364134788513184, total time dif 601.3304581642151)
step: 7950 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(3.5194335), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09919337704777717), 'actor_loss': np.float64(-0.9680874288082123), 'hyper_actor_loss': np.float64(0.00491333375684917), 'behavior_loss': np.float64(1.7636308550834656)}

Episode step 7960, time diff 0.7042436599731445, total time dif 602.0668716430664)
step: 7960 @ episode report: {'average_total_reward': np.float32(9.348889), 'reward_variance': np.float32(4.729289), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0838699072599411), 'actor_loss': np.float64(-1.0127660274505614), 'hyper_actor_loss': np.float64(0.004847946017980576), 'behavior_loss': np.float64(1.6518614888191223)}

Episode step 7970, time diff 0.7234945297241211, total time dif 602.7711153030396)
step: 7970 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(2.6185975), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09441900700330734), 'actor_loss': np.float64(-0.9679404258728027), 'hyper_actor_loss': np.float64(0.004874715162441134), 'behavior_loss': np.float64(1.7321145057678222)}

Episode step 7980, time diff 0.7011346817016602, total time dif 603.4946098327637)
step: 7980 @ episode report: {'average_total_reward': np.float32(9.463333), 'reward_variance': np.float32(3.5645194), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10569722130894661), 'actor_loss': np.float64(-0.9906382977962493), 'hyper_actor_loss': np.float64(0.004826582968235016), 'behavior_loss': np.float64(1.7757667541503905)}

Episode step 7990, time diff 0.9168691635131836, total time dif 604.1957445144653)
step: 7990 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(1.4938972), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.1003039937466383), 'actor_loss': np.float64(-1.015961968898773), 'hyper_actor_loss': np.float64(0.004834050452336669), 'behavior_loss': np.float64(1.7023583054542542)}

Episode step 8000, time diff 0.8482382297515869, total time dif 605.1126136779785)
step: 8000 @ episode report: {'average_total_reward': np.float32(8.228889), 'reward_variance': np.float32(1.0151904), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09022879824042321), 'actor_loss': np.float64(-0.9863664746284485), 'hyper_actor_loss': np.float64(0.004685452580451966), 'behavior_loss': np.float64(1.7191985368728637)}

Episode step 8010, time diff 0.7203421592712402, total time dif 605.9608519077301)
step: 8010 @ episode report: {'average_total_reward': np.float32(8.32889), 'reward_variance': np.float32(1.4394125), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10035571604967117), 'actor_loss': np.float64(-1.0038921773433684), 'hyper_actor_loss': np.float64(0.004834627686068416), 'behavior_loss': np.float64(1.668635654449463)}

Episode step 8020, time diff 0.7729825973510742, total time dif 606.6811940670013)
step: 8020 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.2006376), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0898197416216135), 'actor_loss': np.float64(-0.9939928114414215), 'hyper_actor_loss': np.float64(0.0049451556988060474), 'behavior_loss': np.float64(1.6943106412887574)}

Episode step 8030, time diff 0.7689917087554932, total time dif 607.4541766643524)
step: 8030 @ episode report: {'average_total_reward': np.float32(9.3144455), 'reward_variance': np.float32(2.568643), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09931211546063423), 'actor_loss': np.float64(-0.9892375469207764), 'hyper_actor_loss': np.float64(0.004826038517057896), 'behavior_loss': np.float64(1.7403233408927918)}

Episode step 8040, time diff 0.7804334163665771, total time dif 608.2231683731079)
step: 8040 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(0.9381933), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0951334536075592), 'actor_loss': np.float64(-1.003429353237152), 'hyper_actor_loss': np.float64(0.0049951751716434956), 'behavior_loss': np.float64(1.7401342391967773)}

Episode step 8050, time diff 0.7845349311828613, total time dif 609.0036017894745)
step: 8050 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.1577733), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07630210891366004), 'actor_loss': np.float64(-0.9878660619258881), 'hyper_actor_loss': np.float64(0.005136658623814583), 'behavior_loss': np.float64(1.566652274131775)}

Episode step 8060, time diff 0.7265613079071045, total time dif 609.7881367206573)
step: 8060 @ episode report: {'average_total_reward': np.float32(9.673334), 'reward_variance': np.float32(2.776352), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09445451125502587), 'actor_loss': np.float64(-0.998662531375885), 'hyper_actor_loss': np.float64(0.005175586044788361), 'behavior_loss': np.float64(1.8319452285766602)}

Episode step 8070, time diff 0.7445309162139893, total time dif 610.5146980285645)
step: 8070 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.7972167), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07723906636238098), 'actor_loss': np.float64(-0.9749505579471588), 'hyper_actor_loss': np.float64(0.005047624371945858), 'behavior_loss': np.float64(1.7049276113510132)}

Episode step 8080, time diff 0.7860963344573975, total time dif 611.2592289447784)
step: 8080 @ episode report: {'average_total_reward': np.float32(10.573334), 'reward_variance': np.float32(0.99649906), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09554556421935559), 'actor_loss': np.float64(-0.9952125489711762), 'hyper_actor_loss': np.float64(0.005059189582243562), 'behavior_loss': np.float64(1.7408884763717651)}

Episode step 8090, time diff 0.780289888381958, total time dif 612.0453252792358)
step: 8090 @ episode report: {'average_total_reward': np.float32(10.1), 'reward_variance': np.float32(2.48474), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09188443571329116), 'actor_loss': np.float64(-1.0152282536029815), 'hyper_actor_loss': np.float64(0.0048823343124240635), 'behavior_loss': np.float64(1.7862451076507568)}

Episode step 8100, time diff 0.7541613578796387, total time dif 612.8256151676178)
step: 8100 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(2.05294), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09580284431576729), 'actor_loss': np.float64(-1.0097578406333922), 'hyper_actor_loss': np.float64(0.004788914695382118), 'behavior_loss': np.float64(1.6785376787185669)}

Episode step 8110, time diff 0.7596259117126465, total time dif 613.5797765254974)
step: 8110 @ episode report: {'average_total_reward': np.float32(8.416667), 'reward_variance': np.float32(2.649389), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09943202137947083), 'actor_loss': np.float64(-0.9928690075874329), 'hyper_actor_loss': np.float64(0.004776908922940493), 'behavior_loss': np.float64(1.6820661067962646)}

Episode step 8120, time diff 0.7246205806732178, total time dif 614.3394024372101)
step: 8120 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(3.8490577), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10097198337316513), 'actor_loss': np.float64(-1.0278122663497924), 'hyper_actor_loss': np.float64(0.0047373834531754255), 'behavior_loss': np.float64(1.7405970811843872)}

Episode step 8130, time diff 0.7900209426879883, total time dif 615.0640230178833)
step: 8130 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(1.4274768), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09260641634464264), 'actor_loss': np.float64(-0.9767156422138215), 'hyper_actor_loss': np.float64(0.004696925031021237), 'behavior_loss': np.float64(1.7338766694068908)}

Episode step 8140, time diff 0.7562122344970703, total time dif 615.8540439605713)
step: 8140 @ episode report: {'average_total_reward': np.float32(8.841112), 'reward_variance': np.float32(2.3099523), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08646626994013787), 'actor_loss': np.float64(-0.9681178092956543), 'hyper_actor_loss': np.float64(0.004557893658056855), 'behavior_loss': np.float64(1.7893277525901794)}

Episode step 8150, time diff 0.6848893165588379, total time dif 616.6102561950684)
step: 8150 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(4.1991167), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08972434252500534), 'actor_loss': np.float64(-0.9833795428276062), 'hyper_actor_loss': np.float64(0.0045301648788154125), 'behavior_loss': np.float64(1.7187100887298583)}

Episode step 8160, time diff 0.9303016662597656, total time dif 617.2951455116272)
step: 8160 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(2.624649), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08830615133047104), 'actor_loss': np.float64(-0.9925743281841278), 'hyper_actor_loss': np.float64(0.004437691438943148), 'behavior_loss': np.float64(1.675117552280426)}

Episode step 8170, time diff 0.7180497646331787, total time dif 618.225447177887)
step: 8170 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(6.2476244), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09883847534656524), 'actor_loss': np.float64(-1.012551087141037), 'hyper_actor_loss': np.float64(0.00457738540135324), 'behavior_loss': np.float64(1.7184649229049682)}

Episode step 8180, time diff 0.7425487041473389, total time dif 618.9434969425201)
step: 8180 @ episode report: {'average_total_reward': np.float32(8.763334), 'reward_variance': np.float32(2.856445), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0958576612174511), 'actor_loss': np.float64(-0.9816320180892945), 'hyper_actor_loss': np.float64(0.004686121409758925), 'behavior_loss': np.float64(1.7028700232505798)}

Episode step 8190, time diff 0.7714071273803711, total time dif 619.6860456466675)
step: 8190 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(2.2409675), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09007473811507224), 'actor_loss': np.float64(-0.996781450510025), 'hyper_actor_loss': np.float64(0.004783557029440999), 'behavior_loss': np.float64(1.6958716273307801)}

Episode step 8200, time diff 0.7538352012634277, total time dif 620.4574527740479)
step: 8200 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.3872852), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0845660500228405), 'actor_loss': np.float64(-1.0087461113929748), 'hyper_actor_loss': np.float64(0.0050033234991133215), 'behavior_loss': np.float64(1.6346007585525513)}

Episode step 8210, time diff 0.7255983352661133, total time dif 621.2112879753113)
step: 8210 @ episode report: {'average_total_reward': np.float32(8.192223), 'reward_variance': np.float32(1.4523963), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.28889), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10078092701733113), 'actor_loss': np.float64(-0.9905130088329315), 'hyper_actor_loss': np.float64(0.005118338065221906), 'behavior_loss': np.float64(1.7590677380561828)}

Episode step 8220, time diff 0.775949239730835, total time dif 621.9368863105774)
step: 8220 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(1.3011119), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08754078708589078), 'actor_loss': np.float64(-1.007905775308609), 'hyper_actor_loss': np.float64(0.0054511003196239475), 'behavior_loss': np.float64(1.6240389943122864)}

Episode step 8230, time diff 0.7312262058258057, total time dif 622.7128355503082)
step: 8230 @ episode report: {'average_total_reward': np.float32(9.463334), 'reward_variance': np.float32(1.0487418), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08455187305808068), 'actor_loss': np.float64(-0.9814685225486756), 'hyper_actor_loss': np.float64(0.005546819092705846), 'behavior_loss': np.float64(1.6255738258361816)}

Episode step 8240, time diff 0.7594797611236572, total time dif 623.444061756134)
step: 8240 @ episode report: {'average_total_reward': np.float32(8.526668), 'reward_variance': np.float32(3.9497337), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0729123368859291), 'actor_loss': np.float64(-0.974010455608368), 'hyper_actor_loss': np.float64(0.005747594265267253), 'behavior_loss': np.float64(1.600040364265442)}

Episode step 8250, time diff 0.7341701984405518, total time dif 624.2035415172577)
step: 8250 @ episode report: {'average_total_reward': np.float32(9.475557), 'reward_variance': np.float32(1.4029087), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08483515232801438), 'actor_loss': np.float64(-1.0034440517425538), 'hyper_actor_loss': np.float64(0.005618082545697689), 'behavior_loss': np.float64(1.597636914253235)}

Episode step 8260, time diff 0.7806873321533203, total time dif 624.9377117156982)
step: 8260 @ episode report: {'average_total_reward': np.float32(8.589999), 'reward_variance': np.float32(2.4325786), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08536711186170579), 'actor_loss': np.float64(-1.0057854533195496), 'hyper_actor_loss': np.float64(0.005552292941138149), 'behavior_loss': np.float64(1.5724227666854858)}

Episode step 8270, time diff 0.8233592510223389, total time dif 625.7183990478516)
step: 8270 @ episode report: {'average_total_reward': np.float32(7.653334), 'reward_variance': np.float32(0.81569874), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08260363265872002), 'actor_loss': np.float64(-0.9662020266056061), 'hyper_actor_loss': np.float64(0.005290198931470513), 'behavior_loss': np.float64(1.6482517838478088)}

Episode step 8280, time diff 0.7592673301696777, total time dif 626.5417582988739)
step: 8280 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(2.614637), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09850878454744816), 'actor_loss': np.float64(-0.9955831408500672), 'hyper_actor_loss': np.float64(0.005256710899993777), 'behavior_loss': np.float64(1.6505757808685302)}

Episode step 8290, time diff 0.8355987071990967, total time dif 627.3010256290436)
step: 8290 @ episode report: {'average_total_reward': np.float32(8.39), 'reward_variance': np.float32(3.1150231), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08381225094199181), 'actor_loss': np.float64(-0.9899585902690887), 'hyper_actor_loss': np.float64(0.005390175664797425), 'behavior_loss': np.float64(1.6239317655563354)}

Episode step 8300, time diff 0.7950537204742432, total time dif 628.1366243362427)
step: 8300 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(1.2975079), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08451355323195457), 'actor_loss': np.float64(-0.9945732176303863), 'hyper_actor_loss': np.float64(0.005799695290625096), 'behavior_loss': np.float64(1.581329345703125)}

Episode step 8310, time diff 0.7592267990112305, total time dif 628.9316780567169)
step: 8310 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(2.4727178), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08143800683319569), 'actor_loss': np.float64(-0.9989780068397522), 'hyper_actor_loss': np.float64(0.005878835357725621), 'behavior_loss': np.float64(1.5258615016937256)}

Episode step 8320, time diff 0.9821605682373047, total time dif 629.6909048557281)
step: 8320 @ episode report: {'average_total_reward': np.float32(8.526668), 'reward_variance': np.float32(2.460919), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09263777136802673), 'actor_loss': np.float64(-0.9861470520496368), 'hyper_actor_loss': np.float64(0.005707576824352145), 'behavior_loss': np.float64(1.597753608226776)}

Episode step 8330, time diff 0.6991088390350342, total time dif 630.6730654239655)
step: 8330 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(1.1112646), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08202536255121232), 'actor_loss': np.float64(-0.9971657752990722), 'hyper_actor_loss': np.float64(0.005359644303098321), 'behavior_loss': np.float64(1.6334365129470825)}

Episode step 8340, time diff 0.7313761711120605, total time dif 631.3721742630005)
step: 8340 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(4.266159), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08007764592766761), 'actor_loss': np.float64(-0.9828069865703583), 'hyper_actor_loss': np.float64(0.005099320458248258), 'behavior_loss': np.float64(1.665395736694336)}

Episode step 8350, time diff 0.7506535053253174, total time dif 632.1035504341125)
step: 8350 @ episode report: {'average_total_reward': np.float32(10.26111), 'reward_variance': np.float32(4.4784274), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09380723163485527), 'actor_loss': np.float64(-0.9921835124492645), 'hyper_actor_loss': np.float64(0.004742722073569894), 'behavior_loss': np.float64(1.565032947063446)}

Episode step 8360, time diff 0.7151072025299072, total time dif 632.8542039394379)
step: 8360 @ episode report: {'average_total_reward': np.float32(10.112223), 'reward_variance': np.float32(5.197963), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10201092809438705), 'actor_loss': np.float64(-1.0231219589710236), 'hyper_actor_loss': np.float64(0.004633912444114685), 'behavior_loss': np.float64(1.7431536555290221)}

Episode step 8370, time diff 0.7411580085754395, total time dif 633.5693111419678)
step: 8370 @ episode report: {'average_total_reward': np.float32(10.173333), 'reward_variance': np.float32(3.631092), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08992106951773167), 'actor_loss': np.float64(-0.9690158188343048), 'hyper_actor_loss': np.float64(0.004387313174083829), 'behavior_loss': np.float64(1.6734852194786072)}

Episode step 8380, time diff 0.7768926620483398, total time dif 634.3104691505432)
step: 8380 @ episode report: {'average_total_reward': np.float32(9.463335), 'reward_variance': np.float32(3.608891), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08299227394163608), 'actor_loss': np.float64(-0.9864928126335144), 'hyper_actor_loss': np.float64(0.004015679541043937), 'behavior_loss': np.float64(1.6442086577415467)}

Episode step 8390, time diff 0.7486343383789062, total time dif 635.0873618125916)
step: 8390 @ episode report: {'average_total_reward': np.float32(10.224444), 'reward_variance': np.float32(3.1498961), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0911525920033455), 'actor_loss': np.float64(-0.999403840303421), 'hyper_actor_loss': np.float64(0.004054692946374417), 'behavior_loss': np.float64(1.6341686367988586)}

Episode step 8400, time diff 0.7275621891021729, total time dif 635.8359961509705)
step: 8400 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(3.411057), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08896521478891373), 'actor_loss': np.float64(-0.9931375026702881), 'hyper_actor_loss': np.float64(0.004094717209227383), 'behavior_loss': np.float64(1.7375546455383302)}

Episode step 8410, time diff 0.7206742763519287, total time dif 636.5635583400726)
step: 8410 @ episode report: {'average_total_reward': np.float32(9.663334), 'reward_variance': np.float32(1.4696801), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07716120146214962), 'actor_loss': np.float64(-0.9622289419174195), 'hyper_actor_loss': np.float64(0.00430204994045198), 'behavior_loss': np.float64(1.771188735961914)}

Episode step 8420, time diff 0.7577404975891113, total time dif 637.2842326164246)
step: 8420 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(2.2416902), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09543250352144242), 'actor_loss': np.float64(-1.0200215876102448), 'hyper_actor_loss': np.float64(0.004570381250232458), 'behavior_loss': np.float64(1.570651078224182)}

Episode step 8430, time diff 0.7519896030426025, total time dif 638.0419731140137)
step: 8430 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(3.2342086), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09741213135421276), 'actor_loss': np.float64(-1.0291132271289825), 'hyper_actor_loss': np.float64(0.0045297117438167335), 'behavior_loss': np.float64(1.5892329931259155)}

Episode step 8440, time diff 0.7649538516998291, total time dif 638.7939627170563)
step: 8440 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(4.3158345), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08550873771309853), 'actor_loss': np.float64(-0.9610140562057495), 'hyper_actor_loss': np.float64(0.004320152569562197), 'behavior_loss': np.float64(1.627843451499939)}

Episode step 8450, time diff 0.7554295063018799, total time dif 639.5589165687561)
step: 8450 @ episode report: {'average_total_reward': np.float32(10.24889), 'reward_variance': np.float32(1.8296845), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07881573177874088), 'actor_loss': np.float64(-0.9660375833511352), 'hyper_actor_loss': np.float64(0.004321063822135329), 'behavior_loss': np.float64(1.616707170009613)}

Episode step 8460, time diff 0.7073593139648438, total time dif 640.314346075058)
step: 8460 @ episode report: {'average_total_reward': np.float32(10.322222), 'reward_variance': np.float32(2.8384933), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08443165011703968), 'actor_loss': np.float64(-0.9981308996677398), 'hyper_actor_loss': np.float64(0.004051323817111552), 'behavior_loss': np.float64(1.5306607961654664)}

Episode step 8470, time diff 0.68060302734375, total time dif 641.0217053890228)
step: 8470 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(2.2183995), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08674435839056968), 'actor_loss': np.float64(-0.9798101544380188), 'hyper_actor_loss': np.float64(0.003959639091044664), 'behavior_loss': np.float64(1.6679341793060303)}

Episode step 8480, time diff 0.9028370380401611, total time dif 641.7023084163666)
step: 8480 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.0146222), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08802722468972206), 'actor_loss': np.float64(-0.9877131521701813), 'hyper_actor_loss': np.float64(0.0040624204091727735), 'behavior_loss': np.float64(1.6256287217140197)}

Episode step 8490, time diff 0.7852513790130615, total time dif 642.6051454544067)
step: 8490 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(2.108277), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08816147148609162), 'actor_loss': np.float64(-1.0147918105125426), 'hyper_actor_loss': np.float64(0.004365450888872147), 'behavior_loss': np.float64(1.6555859446525574)}

Episode step 8500, time diff 0.6973485946655273, total time dif 643.3903968334198)
step: 8500 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(2.391295), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08706493973731995), 'actor_loss': np.float64(-1.0004386603832245), 'hyper_actor_loss': np.float64(0.004440721636638045), 'behavior_loss': np.float64(1.5986555218696594)}

Episode step 8510, time diff 0.7152700424194336, total time dif 644.0877454280853)
step: 8510 @ episode report: {'average_total_reward': np.float32(10.561111), 'reward_variance': np.float32(3.2906978), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0920161172747612), 'actor_loss': np.float64(-0.9696027636528015), 'hyper_actor_loss': np.float64(0.004169044550508261), 'behavior_loss': np.float64(1.7560336589813232)}

Episode step 8520, time diff 0.7444291114807129, total time dif 644.8030154705048)
step: 8520 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(1.7382023), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09703003987669945), 'actor_loss': np.float64(-0.9911657214164734), 'hyper_actor_loss': np.float64(0.0038298807106912137), 'behavior_loss': np.float64(1.6731882691383362)}

Episode step 8530, time diff 0.7792329788208008, total time dif 645.5474445819855)
step: 8530 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(3.170243), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08725406974554062), 'actor_loss': np.float64(-0.9965132057666779), 'hyper_actor_loss': np.float64(0.0038008867762982846), 'behavior_loss': np.float64(1.664954626560211)}

Episode step 8540, time diff 0.7045567035675049, total time dif 646.3266775608063)
step: 8540 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.7617533), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08329082652926445), 'actor_loss': np.float64(-0.9863510310649872), 'hyper_actor_loss': np.float64(0.0038037348538637163), 'behavior_loss': np.float64(1.723458194732666)}

Episode step 8550, time diff 0.7208170890808105, total time dif 647.0312342643738)
step: 8550 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(3.5916321), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08684740588068962), 'actor_loss': np.float64(-0.9686518251895905), 'hyper_actor_loss': np.float64(0.003604525956325233), 'behavior_loss': np.float64(1.7249085187911988)}

Episode step 8560, time diff 0.7326581478118896, total time dif 647.7520513534546)
step: 8560 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(2.5273285), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07675281576812268), 'actor_loss': np.float64(-0.9982923090457916), 'hyper_actor_loss': np.float64(0.003634509351104498), 'behavior_loss': np.float64(1.691052758693695)}

Episode step 8570, time diff 0.7524063587188721, total time dif 648.4847095012665)
step: 8570 @ episode report: {'average_total_reward': np.float32(10.646667), 'reward_variance': np.float32(3.172736), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07499343045055866), 'actor_loss': np.float64(-0.9775908529758454), 'hyper_actor_loss': np.float64(0.0037332023959606884), 'behavior_loss': np.float64(1.5616527557373048)}

Episode step 8580, time diff 0.7601592540740967, total time dif 649.2371158599854)
step: 8580 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(1.0529085), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08569187484681606), 'actor_loss': np.float64(-1.0217517495155335), 'hyper_actor_loss': np.float64(0.0038875150261446834), 'behavior_loss': np.float64(1.6493322014808656)}

Episode step 8590, time diff 0.7595939636230469, total time dif 649.9972751140594)
step: 8590 @ episode report: {'average_total_reward': np.float32(9.573334), 'reward_variance': np.float32(5.20561), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08482637442648411), 'actor_loss': np.float64(-1.000553584098816), 'hyper_actor_loss': np.float64(0.004054567590355873), 'behavior_loss': np.float64(1.8079275131225585)}

Episode step 8600, time diff 0.7140483856201172, total time dif 650.7568690776825)
step: 8600 @ episode report: {'average_total_reward': np.float32(8.514444), 'reward_variance': np.float32(3.167853), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09164822474122047), 'actor_loss': np.float64(-1.0062743306159974), 'hyper_actor_loss': np.float64(0.0041385470423847435), 'behavior_loss': np.float64(1.6564445972442627)}

Episode step 8610, time diff 0.7264673709869385, total time dif 651.4709174633026)
step: 8610 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.2156537), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0804935198277235), 'actor_loss': np.float64(-0.9688452661037446), 'hyper_actor_loss': np.float64(0.0041984835173934695), 'behavior_loss': np.float64(1.6368627548217773)}

Episode step 8620, time diff 0.7350895404815674, total time dif 652.1973848342896)
step: 8620 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(2.2430418), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07752840854227543), 'actor_loss': np.float64(-0.9783522188663483), 'hyper_actor_loss': np.float64(0.003985399357043206), 'behavior_loss': np.float64(1.6698571681976317)}

Episode step 8630, time diff 0.7435586452484131, total time dif 652.9324743747711)
step: 8630 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(3.5578396), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07475693821907044), 'actor_loss': np.float64(-0.9825990617275238), 'hyper_actor_loss': np.float64(0.003957876656204462), 'behavior_loss': np.float64(1.7315611362457275)}

Episode step 8640, time diff 0.7604773044586182, total time dif 653.6760330200195)
step: 8640 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(2.6429744), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08881793767213822), 'actor_loss': np.float64(-0.9844990193843841), 'hyper_actor_loss': np.float64(0.003876799833960831), 'behavior_loss': np.float64(1.9320939779281616)}

Episode step 8650, time diff 0.9113702774047852, total time dif 654.4365103244781)
step: 8650 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(3.7105305), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08411636352539062), 'actor_loss': np.float64(-0.9924526572227478), 'hyper_actor_loss': np.float64(0.0038245653500780462), 'behavior_loss': np.float64(1.752407193183899)}

Episode step 8660, time diff 0.7642247676849365, total time dif 655.3478806018829)
step: 8660 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(3.0381846), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08651846684515477), 'actor_loss': np.float64(-1.0040685832500458), 'hyper_actor_loss': np.float64(0.00381821112241596), 'behavior_loss': np.float64(1.8317639470100402)}

Episode step 8670, time diff 0.6990160942077637, total time dif 656.1121053695679)
step: 8670 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(3.910323), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08818011842668057), 'actor_loss': np.float64(-0.9776704251766205), 'hyper_actor_loss': np.float64(0.0035162837943062187), 'behavior_loss': np.float64(1.8491926670074463)}

Episode step 8680, time diff 0.7604351043701172, total time dif 656.8111214637756)
step: 8680 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(4.0754185), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07803475260734558), 'actor_loss': np.float64(-0.9721557140350342), 'hyper_actor_loss': np.float64(0.003390752314589918), 'behavior_loss': np.float64(1.8909211039543152)}

Episode step 8690, time diff 0.7083675861358643, total time dif 657.5715565681458)
step: 8690 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(4.3838644), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08198542669415473), 'actor_loss': np.float64(-0.9974108517169953), 'hyper_actor_loss': np.float64(0.0033798921620473267), 'behavior_loss': np.float64(1.793749785423279)}

Episode step 8700, time diff 0.7001166343688965, total time dif 658.2799241542816)
step: 8700 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(2.487986), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09948041588068009), 'actor_loss': np.float64(-0.9601451754570007), 'hyper_actor_loss': np.float64(0.0032619444886222483), 'behavior_loss': np.float64(2.2592417716979982)}

Episode step 8710, time diff 0.725724458694458, total time dif 658.9800407886505)
step: 8710 @ episode report: {'average_total_reward': np.float32(8.863335), 'reward_variance': np.float32(1.4869642), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08537221252918244), 'actor_loss': np.float64(-0.9906071901321412), 'hyper_actor_loss': np.float64(0.003392364643514156), 'behavior_loss': np.float64(2.0214149236679075)}

Episode step 8720, time diff 0.7145836353302002, total time dif 659.705765247345)
step: 8720 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(2.4825015), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08303828239440918), 'actor_loss': np.float64(-0.9617038667201996), 'hyper_actor_loss': np.float64(0.0035043168114498257), 'behavior_loss': np.float64(2.0519547700881957)}

Episode step 8730, time diff 0.727471113204956, total time dif 660.4203488826752)
step: 8730 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.1107917), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07952040135860443), 'actor_loss': np.float64(-0.9864501953125), 'hyper_actor_loss': np.float64(0.003580570314079523), 'behavior_loss': np.float64(2.01540869474411)}

Episode step 8740, time diff 0.6802716255187988, total time dif 661.1478199958801)
step: 8740 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.5658536), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09724251851439476), 'actor_loss': np.float64(-0.994036215543747), 'hyper_actor_loss': np.float64(0.0034602686064317822), 'behavior_loss': np.float64(1.8625918865203857)}

Episode step 8750, time diff 0.7817361354827881, total time dif 661.8280916213989)
step: 8750 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(1.0295664), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0901565294712782), 'actor_loss': np.float64(-0.9943995654582978), 'hyper_actor_loss': np.float64(0.0034482717281207444), 'behavior_loss': np.float64(2.1151513934135435)}

Episode step 8760, time diff 0.7414805889129639, total time dif 662.6098277568817)
step: 8760 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(3.4645698), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07801852561533451), 'actor_loss': np.float64(-0.9253015697002411), 'hyper_actor_loss': np.float64(0.0031907513039186597), 'behavior_loss': np.float64(2.0148343801498414)}

Episode step 8770, time diff 0.7194793224334717, total time dif 663.3513083457947)
step: 8770 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(1.7329195), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07914629727602004), 'actor_loss': np.float64(-0.9802763402462006), 'hyper_actor_loss': np.float64(0.0031345521798357367), 'behavior_loss': np.float64(2.1285221576690674)}

Episode step 8780, time diff 0.7575609683990479, total time dif 664.0707876682281)
step: 8780 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(1.9839756), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0786865245550871), 'actor_loss': np.float64(-0.9721231281757354), 'hyper_actor_loss': np.float64(0.003272587084211409), 'behavior_loss': np.float64(2.0743733406066895)}

Episode step 8790, time diff 0.7058277130126953, total time dif 664.8283486366272)
step: 8790 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(3.0736127), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09299861900508404), 'actor_loss': np.float64(-1.0099173367023468), 'hyper_actor_loss': np.float64(0.003267391747795045), 'behavior_loss': np.float64(2.083343577384949)}

Episode step 8800, time diff 0.6995816230773926, total time dif 665.5341763496399)
step: 8800 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(5.3443513), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09824056252837181), 'actor_loss': np.float64(-0.9863454103469849), 'hyper_actor_loss': np.float64(0.0031248979503288865), 'behavior_loss': np.float64(2.0966086864471434)}

Episode step 8810, time diff 0.6857895851135254, total time dif 666.2337579727173)
step: 8810 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(3.3961105), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08602278344333172), 'actor_loss': np.float64(-0.9622178137302398), 'hyper_actor_loss': np.float64(0.0029679900500923393), 'behavior_loss': np.float64(1.9841131329536439)}

Episode step 8820, time diff 0.8288319110870361, total time dif 666.9195475578308)
step: 8820 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.9467605), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07823034264147281), 'actor_loss': np.float64(-0.9487668097019195), 'hyper_actor_loss': np.float64(0.0027313983999192716), 'behavior_loss': np.float64(2.066857671737671)}

Episode step 8830, time diff 0.7226862907409668, total time dif 667.7483794689178)
step: 8830 @ episode report: {'average_total_reward': np.float32(8.987778), 'reward_variance': np.float32(3.7103317), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09222839921712875), 'actor_loss': np.float64(-0.9625607788562774), 'hyper_actor_loss': np.float64(0.0028095237677916883), 'behavior_loss': np.float64(2.321599066257477)}

Episode step 8840, time diff 0.7272348403930664, total time dif 668.4710657596588)
step: 8840 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(2.1212544), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0815532635897398), 'actor_loss': np.float64(-0.9954833567142487), 'hyper_actor_loss': np.float64(0.00274048347491771), 'behavior_loss': np.float64(1.9862782597541808)}

Episode step 8850, time diff 0.7375936508178711, total time dif 669.1983006000519)
step: 8850 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(2.510292), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0779235940426588), 'actor_loss': np.float64(-1.0001466870307922), 'hyper_actor_loss': np.float64(0.0028205297654494642), 'behavior_loss': np.float64(1.9521918535232543)}

Episode step 8860, time diff 0.7949843406677246, total time dif 669.9358942508698)
step: 8860 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(4.518827), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09202419444918633), 'actor_loss': np.float64(-0.9623461663722992), 'hyper_actor_loss': np.float64(0.002803820394910872), 'behavior_loss': np.float64(2.1415773391723634)}

Episode step 8870, time diff 0.705876350402832, total time dif 670.7308785915375)
step: 8870 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(5.992376), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08959046304225922), 'actor_loss': np.float64(-0.9944181323051453), 'hyper_actor_loss': np.float64(0.002766678994521499), 'behavior_loss': np.float64(2.0286205291748045)}

Episode step 8880, time diff 0.7355680465698242, total time dif 671.4367549419403)
step: 8880 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(4.0240545), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07592241242527961), 'actor_loss': np.float64(-0.9724720120429993), 'hyper_actor_loss': np.float64(0.0027571865823119877), 'behavior_loss': np.float64(1.9601235866546631)}

Episode step 8890, time diff 0.693619966506958, total time dif 672.1723229885101)
step: 8890 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.0141497), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0898470938205719), 'actor_loss': np.float64(-0.9795151054859161), 'hyper_actor_loss': np.float64(0.0027461241465061904), 'behavior_loss': np.float64(2.002182495594025)}

Episode step 8900, time diff 0.6783876419067383, total time dif 672.8659429550171)
step: 8900 @ episode report: {'average_total_reward': np.float32(10.061112), 'reward_variance': np.float32(2.6032658), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07785902991890907), 'actor_loss': np.float64(-0.9842584669589997), 'hyper_actor_loss': np.float64(0.002746615861542523), 'behavior_loss': np.float64(2.125760781764984)}

Episode step 8910, time diff 0.6827805042266846, total time dif 673.5443305969238)
step: 8910 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.1108693), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08625723868608474), 'actor_loss': np.float64(-0.9595050692558289), 'hyper_actor_loss': np.float64(0.0028437326662242414), 'behavior_loss': np.float64(2.1107710361480714)}

Episode step 8920, time diff 0.6661019325256348, total time dif 674.2271111011505)
step: 8920 @ episode report: {'average_total_reward': np.float32(10.036668), 'reward_variance': np.float32(1.9342244), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08823059014976024), 'actor_loss': np.float64(-0.9837586581707001), 'hyper_actor_loss': np.float64(0.00301410595420748), 'behavior_loss': np.float64(2.053359663486481)}

Episode step 8930, time diff 0.6578879356384277, total time dif 674.8932130336761)
step: 8930 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(6.930445), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07986986376345158), 'actor_loss': np.float64(-0.9883796513080597), 'hyper_actor_loss': np.float64(0.0032318503828719257), 'behavior_loss': np.float64(1.923492157459259)}

Episode step 8940, time diff 0.7022542953491211, total time dif 675.5511009693146)
step: 8940 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(2.3427026), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0763820730149746), 'actor_loss': np.float64(-0.9914440929889679), 'hyper_actor_loss': np.float64(0.0034336793469265103), 'behavior_loss': np.float64(2.1491050481796266)}

Episode step 8950, time diff 0.7196228504180908, total time dif 676.2533552646637)
step: 8950 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(2.302515), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07773093730211258), 'actor_loss': np.float64(-0.9852030813694), 'hyper_actor_loss': np.float64(0.003632893692702055), 'behavior_loss': np.float64(2.0579773664474486)}

Episode step 8960, time diff 0.6907384395599365, total time dif 676.9729781150818)
step: 8960 @ episode report: {'average_total_reward': np.float32(9.35111), 'reward_variance': np.float32(2.6381786), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08918992541730404), 'actor_loss': np.float64(-0.9979159891605377), 'hyper_actor_loss': np.float64(0.0035740054911002515), 'behavior_loss': np.float64(1.9433687329292297)}

Episode step 8970, time diff 0.6715502738952637, total time dif 677.6637165546417)
step: 8970 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(5.397649), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07865900658071041), 'actor_loss': np.float64(-0.9862316370010376), 'hyper_actor_loss': np.float64(0.0036122196819633246), 'behavior_loss': np.float64(1.993401038646698)}

Episode step 8980, time diff 0.8787844181060791, total time dif 678.335266828537)
step: 8980 @ episode report: {'average_total_reward': np.float32(9.451113), 'reward_variance': np.float32(0.76309174), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08613647371530533), 'actor_loss': np.float64(-0.9788419425487518), 'hyper_actor_loss': np.float64(0.0034469623351469636), 'behavior_loss': np.float64(2.0388402462005617)}

Episode step 8990, time diff 0.7346045970916748, total time dif 679.2140512466431)
step: 8990 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(2.4986968), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0764448270201683), 'actor_loss': np.float64(-0.9952167630195617), 'hyper_actor_loss': np.float64(0.003393821557983756), 'behavior_loss': np.float64(1.955868089199066)}

Episode step 9000, time diff 0.7509293556213379, total time dif 679.9486558437347)
step: 9000 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(1.1412886), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09204337038099766), 'actor_loss': np.float64(-0.9845867574214935), 'hyper_actor_loss': np.float64(0.0033558384049683808), 'behavior_loss': np.float64(2.057153022289276)}

Episode step 9010, time diff 0.7585327625274658, total time dif 680.6995851993561)
step: 9010 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(1.0401595), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07661626301705837), 'actor_loss': np.float64(-0.9871716618537902), 'hyper_actor_loss': np.float64(0.003364796796813607), 'behavior_loss': np.float64(2.0423343181610107)}

Episode step 9020, time diff 0.7336099147796631, total time dif 681.4581179618835)
step: 9020 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(1.3392049), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07380236834287643), 'actor_loss': np.float64(-0.9754271745681763), 'hyper_actor_loss': np.float64(0.0032008168287575244), 'behavior_loss': np.float64(1.8626414656639099)}

Episode step 9030, time diff 0.6920902729034424, total time dif 682.1917278766632)
step: 9030 @ episode report: {'average_total_reward': np.float32(9.910001), 'reward_variance': np.float32(1.1208504), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08714261725544929), 'actor_loss': np.float64(-1.0061072945594787), 'hyper_actor_loss': np.float64(0.00316031516995281), 'behavior_loss': np.float64(2.1351739168167114)}

Episode step 9040, time diff 0.6900293827056885, total time dif 682.8838181495667)
step: 9040 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(2.718771), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07944497801363468), 'actor_loss': np.float64(-0.9910966455936432), 'hyper_actor_loss': np.float64(0.0032990201143547893), 'behavior_loss': np.float64(1.8234748005867005)}

Episode step 9050, time diff 0.8418941497802734, total time dif 683.5738475322723)
step: 9050 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.5454443), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07960401810705661), 'actor_loss': np.float64(-1.0048140823841094), 'hyper_actor_loss': np.float64(0.0035484601045027374), 'behavior_loss': np.float64(1.9721875429153441)}

Episode step 9060, time diff 0.7604825496673584, total time dif 684.4157416820526)
step: 9060 @ episode report: {'average_total_reward': np.float32(9.775556), 'reward_variance': np.float32(2.084045), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07973240613937378), 'actor_loss': np.float64(-0.970101797580719), 'hyper_actor_loss': np.float64(0.0036894074408337476), 'behavior_loss': np.float64(2.2909744501113893)}

Episode step 9070, time diff 0.8072299957275391, total time dif 685.17622423172)
step: 9070 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.9552399), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07576060369610786), 'actor_loss': np.float64(-0.9778781533241272), 'hyper_actor_loss': np.float64(0.0037530524423345923), 'behavior_loss': np.float64(1.8074102640151977)}

Episode step 9080, time diff 0.7423496246337891, total time dif 685.9834542274475)
step: 9080 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(1.283437), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07494126670062543), 'actor_loss': np.float64(-1.0001457512378693), 'hyper_actor_loss': np.float64(0.0037236534990370274), 'behavior_loss': np.float64(1.9637335538864136)}

Episode step 9090, time diff 0.7530958652496338, total time dif 686.7258038520813)
step: 9090 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(3.0173645), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08496297709643841), 'actor_loss': np.float64(-0.98392254114151), 'hyper_actor_loss': np.float64(0.003956243465654552), 'behavior_loss': np.float64(1.9821284651756286)}

Episode step 9100, time diff 0.7179687023162842, total time dif 687.4788997173309)
step: 9100 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(1.2345788), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08245560452342034), 'actor_loss': np.float64(-1.0182262301445006), 'hyper_actor_loss': np.float64(0.004041251237504184), 'behavior_loss': np.float64(1.7330813527107238)}

Episode step 9110, time diff 0.7500395774841309, total time dif 688.1968684196472)
step: 9110 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.8075507), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08745918422937393), 'actor_loss': np.float64(-0.9987935900688172), 'hyper_actor_loss': np.float64(0.004092268808744848), 'behavior_loss': np.float64(1.7295956254005431)}

Episode step 9120, time diff 0.7632477283477783, total time dif 688.9469079971313)
step: 9120 @ episode report: {'average_total_reward': np.float32(10.561111), 'reward_variance': np.float32(1.4158343), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07574912831187249), 'actor_loss': np.float64(-1.0027093231678008), 'hyper_actor_loss': np.float64(0.00425687893293798), 'behavior_loss': np.float64(1.8329958081245423)}

Episode step 9130, time diff 0.7908022403717041, total time dif 689.7101557254791)
step: 9130 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(4.0356245), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07802556715905666), 'actor_loss': np.float64(-0.9801787793636322), 'hyper_actor_loss': np.float64(0.00422740988433361), 'behavior_loss': np.float64(1.706720221042633)}

Episode step 9140, time diff 0.9200038909912109, total time dif 690.5009579658508)
step: 9140 @ episode report: {'average_total_reward': np.float32(9.700002), 'reward_variance': np.float32(4.541136), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07184319980442525), 'actor_loss': np.float64(-1.0140905618667602), 'hyper_actor_loss': np.float64(0.004232759564183652), 'behavior_loss': np.float64(1.8119146347045898)}

Episode step 9150, time diff 0.7293345928192139, total time dif 691.420961856842)
step: 9150 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.9786227), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08147804774343967), 'actor_loss': np.float64(-0.9764705181121827), 'hyper_actor_loss': np.float64(0.004046144429594278), 'behavior_loss': np.float64(2.016948163509369)}

Episode step 9160, time diff 0.7571983337402344, total time dif 692.1502964496613)
step: 9160 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(3.120371), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0797240249812603), 'actor_loss': np.float64(-1.0035272777080535), 'hyper_actor_loss': np.float64(0.003898813226260245), 'behavior_loss': np.float64(1.8815186381340028)}

Episode step 9170, time diff 0.7350296974182129, total time dif 692.9074947834015)
step: 9170 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(1.0221841), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07062245160341263), 'actor_loss': np.float64(-0.9821047425270081), 'hyper_actor_loss': np.float64(0.003681085491552949), 'behavior_loss': np.float64(1.7536044359207152)}

Episode step 9180, time diff 0.7673604488372803, total time dif 693.6425244808197)
step: 9180 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(1.2853389), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08277020864188671), 'actor_loss': np.float64(-0.9868100643157959), 'hyper_actor_loss': np.float64(0.0034478524466976523), 'behavior_loss': np.float64(1.8751269578933716)}

Episode step 9190, time diff 0.7209925651550293, total time dif 694.409884929657)
step: 9190 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(4.5273046), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0785616684705019), 'actor_loss': np.float64(-0.9954760730266571), 'hyper_actor_loss': np.float64(0.0032560392515733837), 'behavior_loss': np.float64(1.9361698269844054)}

Episode step 9200, time diff 0.7402822971343994, total time dif 695.130877494812)
step: 9200 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(1.1540293), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07080100104212761), 'actor_loss': np.float64(-0.9759072303771973), 'hyper_actor_loss': np.float64(0.0031582297990098594), 'behavior_loss': np.float64(1.73578679561615)}

Episode step 9210, time diff 0.7879102230072021, total time dif 695.8711597919464)
step: 9210 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(1.9931803), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08178119026124478), 'actor_loss': np.float64(-0.9921348214149475), 'hyper_actor_loss': np.float64(0.00315799571108073), 'behavior_loss': np.float64(1.731833851337433)}

Episode step 9220, time diff 0.7952187061309814, total time dif 696.6590700149536)
step: 9220 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.1353295), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08519531488418579), 'actor_loss': np.float64(-0.9992744088172912), 'hyper_actor_loss': np.float64(0.003150370204821229), 'behavior_loss': np.float64(1.9632441759109498)}

Episode step 9230, time diff 0.7581276893615723, total time dif 697.4542887210846)
step: 9230 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.3658535), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08046074584126472), 'actor_loss': np.float64(-0.9963102340698242), 'hyper_actor_loss': np.float64(0.003229950508102775), 'behavior_loss': np.float64(1.8659536719322205)}

Episode step 9240, time diff 0.8085031509399414, total time dif 698.2124164104462)
step: 9240 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(4.378896), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07216228023171425), 'actor_loss': np.float64(-0.9716709613800049), 'hyper_actor_loss': np.float64(0.003246640879660845), 'behavior_loss': np.float64(1.8245808005332946)}

Episode step 9250, time diff 0.8129968643188477, total time dif 699.0209195613861)
step: 9250 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.3589256), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07676872126758098), 'actor_loss': np.float64(-0.984220165014267), 'hyper_actor_loss': np.float64(0.0030872268136590717), 'behavior_loss': np.float64(1.7661418318748474)}

Episode step 9260, time diff 0.786980390548706, total time dif 699.833916425705)
step: 9260 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(3.649592), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07968455962836743), 'actor_loss': np.float64(-1.0034260511398316), 'hyper_actor_loss': np.float64(0.003129513282328844), 'behavior_loss': np.float64(1.8173918962478637)}

Episode step 9270, time diff 0.7476305961608887, total time dif 700.6208968162537)
step: 9270 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(2.266498), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08718825317919254), 'actor_loss': np.float64(-0.9803472101688385), 'hyper_actor_loss': np.float64(0.0030355317518115043), 'behavior_loss': np.float64(1.865491473674774)}

Episode step 9280, time diff 0.7586607933044434, total time dif 701.3685274124146)
step: 9280 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(5.4008417), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07676176205277443), 'actor_loss': np.float64(-0.9950821280479432), 'hyper_actor_loss': np.float64(0.002958549931645393), 'behavior_loss': np.float64(1.9831804633140564)}

Episode step 9290, time diff 0.7359867095947266, total time dif 702.127188205719)
step: 9290 @ episode report: {'average_total_reward': np.float32(8.502223), 'reward_variance': np.float32(1.2894274), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08144459798932076), 'actor_loss': np.float64(-0.9616247236728668), 'hyper_actor_loss': np.float64(0.0027197342831641437), 'behavior_loss': np.float64(1.9583397388458252)}

Episode step 9300, time diff 0.7648417949676514, total time dif 702.8631749153137)
step: 9300 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.7761236), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06715022511780262), 'actor_loss': np.float64(-0.9852584600448608), 'hyper_actor_loss': np.float64(0.002686614799313247), 'behavior_loss': np.float64(1.8784698247909546)}

Episode step 9310, time diff 0.9205844402313232, total time dif 703.6280167102814)
step: 9310 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(1.889147), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08943870067596435), 'actor_loss': np.float64(-0.9836717247962952), 'hyper_actor_loss': np.float64(0.002739701606333256), 'behavior_loss': np.float64(1.9347717761993408)}

Episode step 9320, time diff 0.7447466850280762, total time dif 704.5486011505127)
step: 9320 @ episode report: {'average_total_reward': np.float32(10.246668), 'reward_variance': np.float32(3.7277732), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0719508532434702), 'actor_loss': np.float64(-1.0113036692142487), 'hyper_actor_loss': np.float64(0.002888704952783883), 'behavior_loss': np.float64(1.9391327381134034)}

Episode step 9330, time diff 0.7922053337097168, total time dif 705.2933478355408)
step: 9330 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(2.1252599), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06184974703937769), 'actor_loss': np.float64(-0.9632129728794098), 'hyper_actor_loss': np.float64(0.0029908867087215184), 'behavior_loss': np.float64(1.9502816915512085)}

Episode step 9340, time diff 0.8416306972503662, total time dif 706.0855531692505)
step: 9340 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(2.64803), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08590395599603654), 'actor_loss': np.float64(-0.9789059579372406), 'hyper_actor_loss': np.float64(0.003289544233120978), 'behavior_loss': np.float64(2.1546586871147158)}

Episode step 9350, time diff 0.8468198776245117, total time dif 706.9271838665009)
step: 9350 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(1.1882975), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0702026929706335), 'actor_loss': np.float64(-1.0078999280929566), 'hyper_actor_loss': np.float64(0.0031816686736419798), 'behavior_loss': np.float64(1.8416311860084533)}

Episode step 9360, time diff 0.8646233081817627, total time dif 707.7740037441254)
step: 9360 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(1.9472796), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07952507436275483), 'actor_loss': np.float64(-0.9808639049530029), 'hyper_actor_loss': np.float64(0.0028119003167375923), 'behavior_loss': np.float64(1.9697298169136048)}

Episode step 9370, time diff 0.7912440299987793, total time dif 708.6386270523071)
step: 9370 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(3.1398227), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0723611406981945), 'actor_loss': np.float64(-0.9823410928249359), 'hyper_actor_loss': np.float64(0.0026846768567338588), 'behavior_loss': np.float64(1.8628443717956542)}

Episode step 9380, time diff 0.7471704483032227, total time dif 709.4298710823059)
step: 9380 @ episode report: {'average_total_reward': np.float32(7.9044447), 'reward_variance': np.float32(2.4139063), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07798594422638416), 'actor_loss': np.float64(-0.9941175937652588), 'hyper_actor_loss': np.float64(0.0025848712772130965), 'behavior_loss': np.float64(1.7570764780044557)}

Episode step 9390, time diff 0.7419333457946777, total time dif 710.1770415306091)
step: 9390 @ episode report: {'average_total_reward': np.float32(7.992223), 'reward_variance': np.float32(2.937335), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08516208492219449), 'actor_loss': np.float64(-0.9870952486991882), 'hyper_actor_loss': np.float64(0.0026879086624830963), 'behavior_loss': np.float64(2.185099649429321)}

Episode step 9400, time diff 0.7867481708526611, total time dif 710.9189748764038)
step: 9400 @ episode report: {'average_total_reward': np.float32(6.957778), 'reward_variance': np.float32(3.3565388), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0817300621420145), 'actor_loss': np.float64(-0.9931321442127228), 'hyper_actor_loss': np.float64(0.002708578249439597), 'behavior_loss': np.float64(2.5201985359191896)}

Episode step 9410, time diff 0.7310616970062256, total time dif 711.7057230472565)
step: 9410 @ episode report: {'average_total_reward': np.float32(6.794444), 'reward_variance': np.float32(1.9682531), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07924542427062989), 'actor_loss': np.float64(-1.0058779060840606), 'hyper_actor_loss': np.float64(0.0027894837548956275), 'behavior_loss': np.float64(2.4042308807373045)}

Episode step 9420, time diff 0.7517802715301514, total time dif 712.4367847442627)
step: 9420 @ episode report: {'average_total_reward': np.float32(7.4433336), 'reward_variance': np.float32(1.6063322), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0795880552381277), 'actor_loss': np.float64(-0.981656801700592), 'hyper_actor_loss': np.float64(0.0029081542743369937), 'behavior_loss': np.float64(2.779674601554871)}

Episode step 9430, time diff 0.7508833408355713, total time dif 713.1885650157928)
step: 9430 @ episode report: {'average_total_reward': np.float32(5.6211114), 'reward_variance': np.float32(0.5820602), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06999297328293323), 'actor_loss': np.float64(-0.9808102369308471), 'hyper_actor_loss': np.float64(0.0028297560289502143), 'behavior_loss': np.float64(3.7029814958572387)}

Episode step 9440, time diff 0.7277581691741943, total time dif 713.9394483566284)
step: 9440 @ episode report: {'average_total_reward': np.float32(4.7355556), 'reward_variance': np.float32(0.820168), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(6.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0801579363644123), 'actor_loss': np.float64(-0.9949929237365722), 'hyper_actor_loss': np.float64(0.00285104694776237), 'behavior_loss': np.float64(4.4631136655807495)}

Episode step 9450, time diff 0.7735848426818848, total time dif 714.6672065258026)
step: 9450 @ episode report: {'average_total_reward': np.float32(4.0255556), 'reward_variance': np.float32(0.61313707), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07523033656179905), 'actor_loss': np.float64(-0.9600778520107269), 'hyper_actor_loss': np.float64(0.002795840473845601), 'behavior_loss': np.float64(4.472112655639648)}

Episode step 9460, time diff 0.7418351173400879, total time dif 715.4407913684845)
step: 9460 @ episode report: {'average_total_reward': np.float32(4.1377783), 'reward_variance': np.float32(0.7596839), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06979074589908123), 'actor_loss': np.float64(-0.9824698388576507), 'hyper_actor_loss': np.float64(0.0026933409040793776), 'behavior_loss': np.float64(4.707590627670288)}

Episode step 9470, time diff 0.9411237239837646, total time dif 716.1826264858246)
step: 9470 @ episode report: {'average_total_reward': np.float32(4.037778), 'reward_variance': np.float32(0.82543695), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(2.9222224), 'average_n_step': np.float32(5.7), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07969580851495266), 'actor_loss': np.float64(-0.9688079833984375), 'hyper_actor_loss': np.float64(0.0027656658086925743), 'behavior_loss': np.float64(4.598880171775818)}

Episode step 9480, time diff 0.7382230758666992, total time dif 717.1237502098083)
step: 9480 @ episode report: {'average_total_reward': np.float32(4.25), 'reward_variance': np.float32(0.46901855), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08176474869251252), 'actor_loss': np.float64(-1.0048227310180664), 'hyper_actor_loss': np.float64(0.0029208852211013436), 'behavior_loss': np.float64(4.51245527267456)}

Episode step 9490, time diff 0.7753138542175293, total time dif 717.861973285675)
step: 9490 @ episode report: {'average_total_reward': np.float32(4.137778), 'reward_variance': np.float32(1.0419804), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.166667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0735978677868843), 'actor_loss': np.float64(-0.9812062084674835), 'hyper_actor_loss': np.float64(0.002936001867055893), 'behavior_loss': np.float64(4.271407699584961)}

Episode step 9500, time diff 0.7407679557800293, total time dif 718.6372871398926)
step: 9500 @ episode report: {'average_total_reward': np.float32(3.0522223), 'reward_variance': np.float32(1.4168161), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(1.1666667), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07109701111912728), 'actor_loss': np.float64(-0.9794229030609131), 'hyper_actor_loss': np.float64(0.0026257903780788185), 'behavior_loss': np.float64(4.773730802536011)}

Episode step 9510, time diff 0.8041896820068359, total time dif 719.3780550956726)
step: 9510 @ episode report: {'average_total_reward': np.float32(3.6011112), 'reward_variance': np.float32(1.0863078), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07998759858310223), 'actor_loss': np.float64(-0.9713714003562928), 'hyper_actor_loss': np.float64(0.0024013981223106385), 'behavior_loss': np.float64(4.90767617225647)}

Episode step 9520, time diff 0.7395436763763428, total time dif 720.1822447776794)
step: 9520 @ episode report: {'average_total_reward': np.float32(3.5133336), 'reward_variance': np.float32(0.6867852), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07576963119208813), 'actor_loss': np.float64(-0.9714863181114197), 'hyper_actor_loss': np.float64(0.0022302302066236736), 'behavior_loss': np.float64(4.568439197540283)}

Episode step 9530, time diff 0.7259244918823242, total time dif 720.9217884540558)
step: 9530 @ episode report: {'average_total_reward': np.float32(3.676667), 'reward_variance': np.float32(0.33939382), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07360437363386155), 'actor_loss': np.float64(-0.97502361536026), 'hyper_actor_loss': np.float64(0.002181049552746117), 'behavior_loss': np.float64(4.78212959766388)}

Episode step 9540, time diff 0.7088963985443115, total time dif 721.6477129459381)
step: 9540 @ episode report: {'average_total_reward': np.float32(3.6766667), 'reward_variance': np.float32(0.7149494), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08260966204106808), 'actor_loss': np.float64(-0.9666996240615845), 'hyper_actor_loss': np.float64(0.002147603570483625), 'behavior_loss': np.float64(4.597852206230163)}

Episode step 9550, time diff 0.7305862903594971, total time dif 722.3566093444824)
step: 9550 @ episode report: {'average_total_reward': np.float32(3.9500003), 'reward_variance': np.float32(1.2542284), 'max_total_reward': np.float32(5.4111114), 'min_total_reward': np.float32(2.166667), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08356933258473873), 'actor_loss': np.float64(-1.0050691366195679), 'hyper_actor_loss': np.float64(0.0021814826177433133), 'behavior_loss': np.float64(5.1999849557876585)}

Episode step 9560, time diff 0.723546028137207, total time dif 723.0871956348419)
step: 9560 @ episode report: {'average_total_reward': np.float32(3.442222), 'reward_variance': np.float32(0.43846425), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08924060836434364), 'actor_loss': np.float64(-0.9775409758090973), 'hyper_actor_loss': np.float64(0.002361930371262133), 'behavior_loss': np.float64(5.033126306533814)}

Episode step 9570, time diff 0.7381160259246826, total time dif 723.8107416629791)
step: 9570 @ episode report: {'average_total_reward': np.float32(3.9255555), 'reward_variance': np.float32(0.456989), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(5.6), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07513844072818757), 'actor_loss': np.float64(-0.9649304687976837), 'hyper_actor_loss': np.float64(0.002450624341145158), 'behavior_loss': np.float64(4.73951575756073)}

Episode step 9580, time diff 0.7149550914764404, total time dif 724.5488576889038)
step: 9580 @ episode report: {'average_total_reward': np.float32(4.4744444), 'reward_variance': np.float32(1.4875076), 'max_total_reward': np.float32(6.533334), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08821429833769798), 'actor_loss': np.float64(-0.98558811545372), 'hyper_actor_loss': np.float64(0.002404220262542367), 'behavior_loss': np.float64(4.788822937011719)}

Episode step 9590, time diff 0.696429967880249, total time dif 725.2638127803802)
step: 9590 @ episode report: {'average_total_reward': np.float32(3.7766666), 'reward_variance': np.float32(0.23260382), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08565069548785686), 'actor_loss': np.float64(-1.000312054157257), 'hyper_actor_loss': np.float64(0.0024932440370321273), 'behavior_loss': np.float64(4.2307047843933105)}

Episode step 9600, time diff 0.7090606689453125, total time dif 725.9602427482605)
step: 9600 @ episode report: {'average_total_reward': np.float32(4.313333), 'reward_variance': np.float32(1.2003902), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(3.1666665), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09163730144500733), 'actor_loss': np.float64(-0.9782080948352814), 'hyper_actor_loss': np.float64(0.0023510803002864122), 'behavior_loss': np.float64(4.717259407043457)}

Episode step 9610, time diff 0.7199742794036865, total time dif 726.6693034172058)
step: 9610 @ episode report: {'average_total_reward': np.float32(3.8011107), 'reward_variance': np.float32(0.8510729), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.5), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07467826120555401), 'actor_loss': np.float64(-0.9617960453033447), 'hyper_actor_loss': np.float64(0.0022644340991973878), 'behavior_loss': np.float64(4.287775468826294)}

Episode step 9620, time diff 0.7558746337890625, total time dif 727.3892776966095)
step: 9620 @ episode report: {'average_total_reward': np.float32(4.225556), 'reward_variance': np.float32(0.71042097), 'max_total_reward': np.float32(5.5333333), 'min_total_reward': np.float32(2.8), 'average_n_step': np.float32(5.9), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08062000200152397), 'actor_loss': np.float64(-0.9851203501224518), 'hyper_actor_loss': np.float64(0.002236235304735601), 'behavior_loss': np.float64(4.82039704322815)}

Episode step 9630, time diff 0.9462146759033203, total time dif 728.1451523303986)
step: 9630 @ episode report: {'average_total_reward': np.float32(4.2233334), 'reward_variance': np.float32(0.37220857), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06604059226810932), 'actor_loss': np.float64(-0.9619973301887512), 'hyper_actor_loss': np.float64(0.0022311825538054108), 'behavior_loss': np.float64(4.508803939819336)}

Episode step 9640, time diff 0.8789479732513428, total time dif 729.0913670063019)
step: 9640 @ episode report: {'average_total_reward': np.float32(4.0644445), 'reward_variance': np.float32(0.4069333), 'max_total_reward': np.float32(5.0444446), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07163910456001758), 'actor_loss': np.float64(-0.9391728520393372), 'hyper_actor_loss': np.float64(0.002266114763915539), 'behavior_loss': np.float64(4.668205785751343)}

Episode step 9650, time diff 0.7908651828765869, total time dif 729.9703149795532)
step: 9650 @ episode report: {'average_total_reward': np.float32(4.374444), 'reward_variance': np.float32(1.0405197), 'max_total_reward': np.float32(5.6555557), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(6.0), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08134157992899418), 'actor_loss': np.float64(-0.97715322971344), 'hyper_actor_loss': np.float64(0.002216638531535864), 'behavior_loss': np.float64(4.865363073348999)}

Episode step 9660, time diff 0.7280280590057373, total time dif 730.7611801624298)
step: 9660 @ episode report: {'average_total_reward': np.float32(4.547778), 'reward_variance': np.float32(0.06733455), 'max_total_reward': np.float32(5.288889), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.075476598367095), 'actor_loss': np.float64(-0.9606109321117401), 'hyper_actor_loss': np.float64(0.0023575926665216685), 'behavior_loss': np.float64(3.438556456565857)}

Episode step 9670, time diff 0.6942229270935059, total time dif 731.4892082214355)
step: 9670 @ episode report: {'average_total_reward': np.float32(6.967778), 'reward_variance': np.float32(3.650975), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06928039900958538), 'actor_loss': np.float64(-0.9750837683677673), 'hyper_actor_loss': np.float64(0.002891822112724185), 'behavior_loss': np.float64(2.019519877433777)}

Episode step 9680, time diff 0.6632215976715088, total time dif 732.183431148529)
step: 9680 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(1.4894276), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07685810029506683), 'actor_loss': np.float64(-0.9549202144145965), 'hyper_actor_loss': np.float64(0.0028199694119393826), 'behavior_loss': np.float64(2.276853394508362)}

Episode step 9690, time diff 0.6767227649688721, total time dif 732.8466527462006)
step: 9690 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(6.413729), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0768318884074688), 'actor_loss': np.float64(-0.9420174539089203), 'hyper_actor_loss': np.float64(0.0027196316281333567), 'behavior_loss': np.float64(2.161924684047699)}

Episode step 9700, time diff 0.6485612392425537, total time dif 733.5233755111694)
step: 9700 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(3.399221), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07913057617843151), 'actor_loss': np.float64(-0.9732261002063751), 'hyper_actor_loss': np.float64(0.002731052250601351), 'behavior_loss': np.float64(2.219716477394104)}

Episode step 9710, time diff 0.6628799438476562, total time dif 734.171936750412)
step: 9710 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.1743021), 'max_total_reward': np.float32(12.1444435), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07644241899251938), 'actor_loss': np.float64(-0.9422930598258972), 'hyper_actor_loss': np.float64(0.0030466628493741156), 'behavior_loss': np.float64(1.8753551602363587)}

Episode step 9720, time diff 0.6473939418792725, total time dif 734.8348166942596)
step: 9720 @ episode report: {'average_total_reward': np.float32(10.622224), 'reward_variance': np.float32(1.8914073), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08101690337061881), 'actor_loss': np.float64(-0.9723884224891662), 'hyper_actor_loss': np.float64(0.003316439129412174), 'behavior_loss': np.float64(2.1125251173973085)}

Episode step 9730, time diff 0.652418851852417, total time dif 735.4822106361389)
step: 9730 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.783699), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0666787315160036), 'actor_loss': np.float64(-0.960885614156723), 'hyper_actor_loss': np.float64(0.0038100746693089604), 'behavior_loss': np.float64(1.7673197031021117)}

Episode step 9740, time diff 0.654895544052124, total time dif 736.1346294879913)
step: 9740 @ episode report: {'average_total_reward': np.float32(9.163333), 'reward_variance': np.float32(1.2760509), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07178419306874276), 'actor_loss': np.float64(-0.9395626604557037), 'hyper_actor_loss': np.float64(0.004137748898938298), 'behavior_loss': np.float64(1.885731828212738)}

Episode step 9750, time diff 0.6605021953582764, total time dif 736.7895250320435)
step: 9750 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.8714824), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07677270136773587), 'actor_loss': np.float64(-0.9707767724990845), 'hyper_actor_loss': np.float64(0.004544029990211129), 'behavior_loss': np.float64(1.9554490447044373)}

Episode step 9760, time diff 0.6752140522003174, total time dif 737.4500272274017)
step: 9760 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(1.4115175), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186173070222139), 'actor_loss': np.float64(-0.9395868837833404), 'hyper_actor_loss': np.float64(0.005004970962181688), 'behavior_loss': np.float64(1.850944995880127)}

Episode step 9770, time diff 0.6591804027557373, total time dif 738.125241279602)
step: 9770 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(1.0116419), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08122253566980361), 'actor_loss': np.float64(-0.9842327892780304), 'hyper_actor_loss': np.float64(0.0057140534278005365), 'behavior_loss': np.float64(1.754871129989624)}

Episode step 9780, time diff 0.6729764938354492, total time dif 738.7844216823578)
step: 9780 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(2.2799861), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06949282512068748), 'actor_loss': np.float64(-0.9623865842819214), 'hyper_actor_loss': np.float64(0.006780694099143147), 'behavior_loss': np.float64(1.6874744296073914)}

Episode step 9790, time diff 0.8425052165985107, total time dif 739.4573981761932)
step: 9790 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(2.3777537), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07056577373296022), 'actor_loss': np.float64(-0.9446446299552917), 'hyper_actor_loss': np.float64(0.007422622665762902), 'behavior_loss': np.float64(1.4417670309543609)}

Episode step 9800, time diff 0.6604695320129395, total time dif 740.2999033927917)
step: 9800 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(3.2779808), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06946044638752938), 'actor_loss': np.float64(-0.9810456991195678), 'hyper_actor_loss': np.float64(0.00696850405074656), 'behavior_loss': np.float64(1.5005829095840455)}

Episode step 9810, time diff 0.6586401462554932, total time dif 740.9603729248047)
step: 9810 @ episode report: {'average_total_reward': np.float32(9.463333), 'reward_variance': np.float32(1.300619), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08710846453905105), 'actor_loss': np.float64(-0.9831101655960083), 'hyper_actor_loss': np.float64(0.006073753302916885), 'behavior_loss': np.float64(1.7641696214675904)}

Episode step 9820, time diff 0.6592235565185547, total time dif 741.6190130710602)
step: 9820 @ episode report: {'average_total_reward': np.float32(10.697779), 'reward_variance': np.float32(1.8521183), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06710120663046837), 'actor_loss': np.float64(-0.9714069306850434), 'hyper_actor_loss': np.float64(0.005571153573691845), 'behavior_loss': np.float64(1.9369264006614686)}

Episode step 9830, time diff 0.641005277633667, total time dif 742.2782366275787)
step: 9830 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(0.94164944), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07023735977709293), 'actor_loss': np.float64(-0.9398499846458435), 'hyper_actor_loss': np.float64(0.005789165711030364), 'behavior_loss': np.float64(1.6019761323928834)}

Episode step 9840, time diff 0.6736929416656494, total time dif 742.9192419052124)
step: 9840 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(4.590154), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07347745634615421), 'actor_loss': np.float64(-1.002978938817978), 'hyper_actor_loss': np.float64(0.006384007632732391), 'behavior_loss': np.float64(1.3601102352142334)}

Episode step 9850, time diff 0.7756149768829346, total time dif 743.592934846878)
step: 9850 @ episode report: {'average_total_reward': np.float32(10.310001), 'reward_variance': np.float32(6.206851), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0660391017794609), 'actor_loss': np.float64(-0.9422210931777955), 'hyper_actor_loss': np.float64(0.006600604671984911), 'behavior_loss': np.float64(1.456600594520569)}

Episode step 9860, time diff 0.6857531070709229, total time dif 744.368549823761)
step: 9860 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.7792296), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08225938938558101), 'actor_loss': np.float64(-0.9973549544811249), 'hyper_actor_loss': np.float64(0.006515790103003382), 'behavior_loss': np.float64(1.3796446442604064)}

Episode step 9870, time diff 0.7044627666473389, total time dif 745.0543029308319)
step: 9870 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(2.7123013), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07243088893592357), 'actor_loss': np.float64(-0.9832526445388794), 'hyper_actor_loss': np.float64(0.006250000698491931), 'behavior_loss': np.float64(1.3124194145202637)}

Episode step 9880, time diff 0.6908004283905029, total time dif 745.7587656974792)
step: 9880 @ episode report: {'average_total_reward': np.float32(11.607779), 'reward_variance': np.float32(3.5701745), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06752376276999712), 'actor_loss': np.float64(-0.9677602648735046), 'hyper_actor_loss': np.float64(0.006268811598420143), 'behavior_loss': np.float64(1.3469055056571961)}

Episode step 9890, time diff 0.668001651763916, total time dif 746.4495661258698)
step: 9890 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(3.1152885), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06726765930652619), 'actor_loss': np.float64(-0.9836054623126984), 'hyper_actor_loss': np.float64(0.0061897292733192446), 'behavior_loss': np.float64(1.3433757543563842)}

Episode step 9900, time diff 0.6714339256286621, total time dif 747.1175677776337)
step: 9900 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(4.1808653), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08729265704751014), 'actor_loss': np.float64(-0.99544877409935), 'hyper_actor_loss': np.float64(0.0060305310878902675), 'behavior_loss': np.float64(1.400826346874237)}

Episode step 9910, time diff 0.74013352394104, total time dif 747.7890017032623)
step: 9910 @ episode report: {'average_total_reward': np.float32(10.385557), 'reward_variance': np.float32(5.9893103), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07867168709635734), 'actor_loss': np.float64(-0.9996931433677674), 'hyper_actor_loss': np.float64(0.0062758004758507015), 'behavior_loss': np.float64(1.3412818312644958)}

Episode step 9920, time diff 0.6661677360534668, total time dif 748.5291352272034)
step: 9920 @ episode report: {'average_total_reward': np.float32(9.475557), 'reward_variance': np.float32(1.8712549), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06857866235077381), 'actor_loss': np.float64(-0.9781556844711303), 'hyper_actor_loss': np.float64(0.00636593196541071), 'behavior_loss': np.float64(1.280911386013031)}

Episode step 9930, time diff 0.6694059371948242, total time dif 749.1953029632568)
step: 9930 @ episode report: {'average_total_reward': np.float32(11.346666), 'reward_variance': np.float32(2.5224643), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07960811369121075), 'actor_loss': np.float64(-0.9840591788291931), 'hyper_actor_loss': np.float64(0.005913225840777159), 'behavior_loss': np.float64(1.4528675079345703)}

Episode step 9940, time diff 0.6751534938812256, total time dif 749.8647089004517)
step: 9940 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(4.866345), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07657508403062821), 'actor_loss': np.float64(-0.9675209701061249), 'hyper_actor_loss': np.float64(0.00531296762637794), 'behavior_loss': np.float64(1.4885727524757386)}

Episode step 9950, time diff 0.6630744934082031, total time dif 750.5398623943329)
step: 9950 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(1.0479262), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06441812813282013), 'actor_loss': np.float64(-0.9617324411869049), 'hyper_actor_loss': np.float64(0.0047977851703763005), 'behavior_loss': np.float64(1.5000000596046448)}

Episode step 9960, time diff 0.8105826377868652, total time dif 751.2029368877411)
step: 9960 @ episode report: {'average_total_reward': np.float32(11.283335), 'reward_variance': np.float32(3.8575883), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08903916627168655), 'actor_loss': np.float64(-0.9663129985332489), 'hyper_actor_loss': np.float64(0.004546231217682361), 'behavior_loss': np.float64(1.4447967886924744)}

Episode step 9970, time diff 0.6482441425323486, total time dif 752.013519525528)
step: 9970 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.934455), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07739346362650394), 'actor_loss': np.float64(-1.0120178759098053), 'hyper_actor_loss': np.float64(0.004502742039039731), 'behavior_loss': np.float64(1.4329875707626343)}

Episode step 9980, time diff 0.6836724281311035, total time dif 752.6617636680603)
step: 9980 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(4.2157793), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07635723985731602), 'actor_loss': np.float64(-0.9646554827690125), 'hyper_actor_loss': np.float64(0.004979294119402766), 'behavior_loss': np.float64(1.3686857223510742)}

Episode step 9990, time diff 0.6805927753448486, total time dif 753.3454360961914)
step: 9990 @ episode report: {'average_total_reward': np.float32(9.912223), 'reward_variance': np.float32(4.272752), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06841561235487462), 'actor_loss': np.float64(-0.9828138947486877), 'hyper_actor_loss': np.float64(0.004886000975966453), 'behavior_loss': np.float64(1.2757981419563293)}

Episode step 10000, time diff 0.6934621334075928, total time dif 754.0260288715363)
step: 10000 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(2.3887167), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08541429191827773), 'actor_loss': np.float64(-1.004625540971756), 'hyper_actor_loss': np.float64(0.004698962392285466), 'behavior_loss': np.float64(1.2929632902145385)}

Episode step 10010, time diff 0.659393310546875, total time dif 754.7194910049438)
step: 10010 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(2.6887174), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06929673850536347), 'actor_loss': np.float64(-0.9835950374603272), 'hyper_actor_loss': np.float64(0.004482097690925002), 'behavior_loss': np.float64(1.4143211483955382)}

Episode step 10020, time diff 0.690150260925293, total time dif 755.3788843154907)
step: 10020 @ episode report: {'average_total_reward': np.float32(11.644445), 'reward_variance': np.float32(2.2954316), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(9.900002), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07310040816664695), 'actor_loss': np.float64(-0.9676121413707733), 'hyper_actor_loss': np.float64(0.004473738931119442), 'behavior_loss': np.float64(1.3642476677894593)}

Episode step 10030, time diff 0.6971278190612793, total time dif 756.069034576416)
step: 10030 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(2.32489), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06909765154123307), 'actor_loss': np.float64(-0.9698161959648133), 'hyper_actor_loss': np.float64(0.0042083834297955034), 'behavior_loss': np.float64(1.3606322884559632)}

Episode step 10040, time diff 0.7046222686767578, total time dif 756.7661623954773)
step: 10040 @ episode report: {'average_total_reward': np.float32(10.35889), 'reward_variance': np.float32(3.7907424), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07990241572260856), 'actor_loss': np.float64(-1.0023965716362), 'hyper_actor_loss': np.float64(0.0040622943779453635), 'behavior_loss': np.float64(1.344521701335907)}

Episode step 10050, time diff 0.6867766380310059, total time dif 757.470784664154)
step: 10050 @ episode report: {'average_total_reward': np.float32(10.361111), 'reward_variance': np.float32(3.2802033), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07976633943617344), 'actor_loss': np.float64(-0.9700210690498352), 'hyper_actor_loss': np.float64(0.004051504703238606), 'behavior_loss': np.float64(1.3995095252990724)}

Episode step 10060, time diff 0.6654090881347656, total time dif 758.1575613021851)
step: 10060 @ episode report: {'average_total_reward': np.float32(11.893334), 'reward_variance': np.float32(2.8165479), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06672682370990515), 'actor_loss': np.float64(-0.9866067051887513), 'hyper_actor_loss': np.float64(0.004034592118114233), 'behavior_loss': np.float64(1.3506712317466736)}

Episode step 10070, time diff 0.6772599220275879, total time dif 758.8229703903198)
step: 10070 @ episode report: {'average_total_reward': np.float32(11.52), 'reward_variance': np.float32(5.4670577), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07429887056350708), 'actor_loss': np.float64(-0.9847662389278412), 'hyper_actor_loss': np.float64(0.003974428772926331), 'behavior_loss': np.float64(1.3595659494400025)}

Episode step 10080, time diff 0.675877571105957, total time dif 759.5002303123474)
step: 10080 @ episode report: {'average_total_reward': np.float32(11.917778), 'reward_variance': np.float32(1.7688439), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06143034547567368), 'actor_loss': np.float64(-0.9380766868591308), 'hyper_actor_loss': np.float64(0.003908396465703845), 'behavior_loss': np.float64(1.4248518586158752)}

Episode step 10090, time diff 0.7138242721557617, total time dif 760.1761078834534)
step: 10090 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(1.4923894), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08590304777026177), 'actor_loss': np.float64(-1.0046902358531953), 'hyper_actor_loss': np.float64(0.0038286115741357207), 'behavior_loss': np.float64(1.387230360507965)}

Episode step 10100, time diff 0.678455114364624, total time dif 760.8899321556091)
step: 10100 @ episode report: {'average_total_reward': np.float32(11.744446), 'reward_variance': np.float32(1.9238274), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07877894341945649), 'actor_loss': np.float64(-0.9817666649818421), 'hyper_actor_loss': np.float64(0.0040146478218957785), 'behavior_loss': np.float64(1.3917036056518555)}

Episode step 10110, time diff 0.6794018745422363, total time dif 761.5683872699738)
step: 10110 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(1.2693694), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07701401561498641), 'actor_loss': np.float64(-0.9722838521003723), 'hyper_actor_loss': np.float64(0.0042490037623792885), 'behavior_loss': np.float64(1.3765251636505127)}

Episode step 10120, time diff 0.6519625186920166, total time dif 762.247789144516)
step: 10120 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(3.7600255), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0798367753624916), 'actor_loss': np.float64(-0.9841314911842346), 'hyper_actor_loss': np.float64(0.004027404775843024), 'behavior_loss': np.float64(1.4223152637481689)}

Episode step 10130, time diff 0.8544816970825195, total time dif 762.899751663208)
step: 10130 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(4.924321), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06974050328135491), 'actor_loss': np.float64(-0.9706186830997467), 'hyper_actor_loss': np.float64(0.003921775612980127), 'behavior_loss': np.float64(1.3277629137039184)}

Episode step 10140, time diff 0.6750011444091797, total time dif 763.7542333602905)
step: 10140 @ episode report: {'average_total_reward': np.float32(11.595556), 'reward_variance': np.float32(2.1538813), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07573638521134854), 'actor_loss': np.float64(-0.973971563577652), 'hyper_actor_loss': np.float64(0.0037636541295796634), 'behavior_loss': np.float64(1.3919440746307372)}

Episode step 10150, time diff 0.7055063247680664, total time dif 764.4292345046997)
step: 10150 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(2.817921), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07703864127397538), 'actor_loss': np.float64(-0.9960406839847564), 'hyper_actor_loss': np.float64(0.003621968370862305), 'behavior_loss': np.float64(1.447854793071747)}

Episode step 10160, time diff 0.6890256404876709, total time dif 765.1347408294678)
step: 10160 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(4.3084207), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06970886066555977), 'actor_loss': np.float64(-0.9658623576164246), 'hyper_actor_loss': np.float64(0.0036346000386402013), 'behavior_loss': np.float64(1.4283701539039613)}

Episode step 10170, time diff 0.6705071926116943, total time dif 765.8237664699554)
step: 10170 @ episode report: {'average_total_reward': np.float32(10.671112), 'reward_variance': np.float32(5.8514113), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06609984375536442), 'actor_loss': np.float64(-0.9693009197711945), 'hyper_actor_loss': np.float64(0.003551311930641532), 'behavior_loss': np.float64(1.4328862071037292)}

Episode step 10180, time diff 0.6902635097503662, total time dif 766.4942736625671)
step: 10180 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(1.6115259), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07870502211153507), 'actor_loss': np.float64(-0.9948221683502197), 'hyper_actor_loss': np.float64(0.003567317174747586), 'behavior_loss': np.float64(1.3397680580615998)}

Episode step 10190, time diff 0.6558759212493896, total time dif 767.1845371723175)
step: 10190 @ episode report: {'average_total_reward': np.float32(10.995556), 'reward_variance': np.float32(1.701807), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06869972012937069), 'actor_loss': np.float64(-0.9590693771839142), 'hyper_actor_loss': np.float64(0.003547758259810507), 'behavior_loss': np.float64(1.3837480425834656)}

Episode step 10200, time diff 0.6693353652954102, total time dif 767.8404130935669)
step: 10200 @ episode report: {'average_total_reward': np.float32(11.320001), 'reward_variance': np.float32(1.4315504), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07219172157347202), 'actor_loss': np.float64(-0.9822475850582123), 'hyper_actor_loss': np.float64(0.0033569194143638014), 'behavior_loss': np.float64(1.3111135244369507)}

Episode step 10210, time diff 0.7030642032623291, total time dif 768.5097484588623)
step: 10210 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.2529404), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08286205716431141), 'actor_loss': np.float64(-0.9983834862709046), 'hyper_actor_loss': np.float64(0.0032774109626188875), 'behavior_loss': np.float64(1.4871997117996216)}

Episode step 10220, time diff 0.6539702415466309, total time dif 769.2128126621246)
step: 10220 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.1314635), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07119110524654389), 'actor_loss': np.float64(-0.9808355331420898), 'hyper_actor_loss': np.float64(0.0031833937391638756), 'behavior_loss': np.float64(1.4824752330780029)}

Episode step 10230, time diff 0.6740195751190186, total time dif 769.8667829036713)
step: 10230 @ episode report: {'average_total_reward': np.float32(11.607779), 'reward_variance': np.float32(0.5446676), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06821850277483463), 'actor_loss': np.float64(-0.9612908244132996), 'hyper_actor_loss': np.float64(0.003264619642868638), 'behavior_loss': np.float64(1.3372924327850342)}

Episode step 10240, time diff 0.6548383235931396, total time dif 770.5408024787903)
step: 10240 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(2.931265), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0772171787917614), 'actor_loss': np.float64(-1.0024941504001617), 'hyper_actor_loss': np.float64(0.003378894366323948), 'behavior_loss': np.float64(1.2834668159484863)}

Episode step 10250, time diff 0.6813023090362549, total time dif 771.1956408023834)
step: 10250 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(5.2200994), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07073670364916325), 'actor_loss': np.float64(-0.9902360737323761), 'hyper_actor_loss': np.float64(0.003667451813817024), 'behavior_loss': np.float64(1.2835180699825286)}

Episode step 10260, time diff 0.6526610851287842, total time dif 771.8769431114197)
step: 10260 @ episode report: {'average_total_reward': np.float32(9.6877775), 'reward_variance': np.float32(5.542111), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06949230134487153), 'actor_loss': np.float64(-0.984513235092163), 'hyper_actor_loss': np.float64(0.004025655635632575), 'behavior_loss': np.float64(1.2915977835655212)}

Episode step 10270, time diff 0.6894681453704834, total time dif 772.5296041965485)
step: 10270 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(3.955413), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07528251893818379), 'actor_loss': np.float64(-0.9919060111045838), 'hyper_actor_loss': np.float64(0.004184281779453158), 'behavior_loss': np.float64(1.3306381464004517)}

Episode step 10280, time diff 0.6442177295684814, total time dif 773.219072341919)
step: 10280 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(1.258599), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.073019590228796), 'actor_loss': np.float64(-0.9752542734146118), 'hyper_actor_loss': np.float64(0.004136967565864324), 'behavior_loss': np.float64(1.1766829013824462)}

Episode step 10290, time diff 0.8401949405670166, total time dif 773.8632900714874)
step: 10290 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(3.5846372), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06821003407239914), 'actor_loss': np.float64(-0.9910333395004273), 'hyper_actor_loss': np.float64(0.003978256857953966), 'behavior_loss': np.float64(1.332395327091217)}

Episode step 10300, time diff 0.6741073131561279, total time dif 774.7034850120544)
step: 10300 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(2.0495663), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06789729557931423), 'actor_loss': np.float64(-0.9723441362380981), 'hyper_actor_loss': np.float64(0.003723158733919263), 'behavior_loss': np.float64(1.2592222452163697)}

Episode step 10310, time diff 0.679210901260376, total time dif 775.3775923252106)
step: 10310 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(2.0429645), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06587786115705967), 'actor_loss': np.float64(-0.9827602922916412), 'hyper_actor_loss': np.float64(0.0035527972504496574), 'behavior_loss': np.float64(1.325288724899292)}

Episode step 10320, time diff 0.6802628040313721, total time dif 776.056803226471)
step: 10320 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(2.9353135), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07376100532710553), 'actor_loss': np.float64(-0.987114667892456), 'hyper_actor_loss': np.float64(0.003417676477693021), 'behavior_loss': np.float64(1.3407328844070434)}

Episode step 10330, time diff 0.6822950839996338, total time dif 776.7370660305023)
step: 10330 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(5.367106), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06854619048535823), 'actor_loss': np.float64(-0.989183908700943), 'hyper_actor_loss': np.float64(0.0032679746625944974), 'behavior_loss': np.float64(1.2355339169502257)}

Episode step 10340, time diff 0.7007660865783691, total time dif 777.419361114502)
step: 10340 @ episode report: {'average_total_reward': np.float32(8.963335), 'reward_variance': np.float32(0.85610044), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08441031016409398), 'actor_loss': np.float64(-0.9910975217819213), 'hyper_actor_loss': np.float64(0.003254670067690313), 'behavior_loss': np.float64(1.289934766292572)}

Episode step 10350, time diff 0.694401741027832, total time dif 778.1201272010803)
step: 10350 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(1.3628403), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07348563447594643), 'actor_loss': np.float64(-1.0004128992557526), 'hyper_actor_loss': np.float64(0.0033446950372308494), 'behavior_loss': np.float64(1.2947109818458558)}

Episode step 10360, time diff 0.6621780395507812, total time dif 778.8145289421082)
step: 10360 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(4.138471), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07734618708491325), 'actor_loss': np.float64(-0.9867147326469421), 'hyper_actor_loss': np.float64(0.003333206777460873), 'behavior_loss': np.float64(1.2811613202095031)}

Episode step 10370, time diff 0.6659274101257324, total time dif 779.4767069816589)
step: 10370 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(1.2633939), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06894632503390312), 'actor_loss': np.float64(-0.9749673187732697), 'hyper_actor_loss': np.float64(0.003369890223257244), 'behavior_loss': np.float64(1.323785412311554)}

Episode step 10380, time diff 0.6970741748809814, total time dif 780.1426343917847)
step: 10380 @ episode report: {'average_total_reward': np.float32(10.21), 'reward_variance': np.float32(2.991739), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06894886195659637), 'actor_loss': np.float64(-0.9709946155548096), 'hyper_actor_loss': np.float64(0.0037164238281548025), 'behavior_loss': np.float64(1.3058602809906006)}

Episode step 10390, time diff 0.6774072647094727, total time dif 780.8397085666656)
step: 10390 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(2.503881), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07443199045956135), 'actor_loss': np.float64(-1.0038772344589233), 'hyper_actor_loss': np.float64(0.00369094293564558), 'behavior_loss': np.float64(1.3101398706436158)}

Episode step 10400, time diff 0.6960339546203613, total time dif 781.5171158313751)
step: 10400 @ episode report: {'average_total_reward': np.float32(8.951111), 'reward_variance': np.float32(3.1153874), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06030112579464912), 'actor_loss': np.float64(-0.9577612936496734), 'hyper_actor_loss': np.float64(0.003908696514554322), 'behavior_loss': np.float64(1.1991276502609254)}

Episode step 10410, time diff 0.6804659366607666, total time dif 782.2131497859955)
step: 10410 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(3.0187478), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058526307716965674), 'actor_loss': np.float64(-0.9750109493732453), 'hyper_actor_loss': np.float64(0.0038290676893666387), 'behavior_loss': np.float64(1.2716748476028443)}

Episode step 10420, time diff 0.6737723350524902, total time dif 782.8936157226562)
step: 10420 @ episode report: {'average_total_reward': np.float32(8.887777), 'reward_variance': np.float32(3.1689994), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07039519734680652), 'actor_loss': np.float64(-0.9719247996807099), 'hyper_actor_loss': np.float64(0.004049161449074745), 'behavior_loss': np.float64(1.3488589167594909)}

Episode step 10430, time diff 0.6654949188232422, total time dif 783.5673880577087)
step: 10430 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(2.5458386), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06914840899407863), 'actor_loss': np.float64(-0.978767454624176), 'hyper_actor_loss': np.float64(0.003924983995966613), 'behavior_loss': np.float64(1.3322377443313598)}

Episode step 10440, time diff 0.6690106391906738, total time dif 784.232882976532)
step: 10440 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(0.75277156), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08037387281656265), 'actor_loss': np.float64(-0.9939276218414307), 'hyper_actor_loss': np.float64(0.003967691562138498), 'behavior_loss': np.float64(1.2430344581604005)}

Episode step 10450, time diff 0.8662331104278564, total time dif 784.9018936157227)
step: 10450 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(0.7871965), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055720174312591554), 'actor_loss': np.float64(-0.9651164948940277), 'hyper_actor_loss': np.float64(0.003920184937305749), 'behavior_loss': np.float64(1.2089159607887268)}

Episode step 10460, time diff 0.7230498790740967, total time dif 785.7681267261505)
step: 10460 @ episode report: {'average_total_reward': np.float32(8.651113), 'reward_variance': np.float32(1.8523012), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0751254554837942), 'actor_loss': np.float64(-0.9840881586074829), 'hyper_actor_loss': np.float64(0.0037501025712117554), 'behavior_loss': np.float64(1.328685110807419)}

Episode step 10470, time diff 0.7150082588195801, total time dif 786.4911766052246)
step: 10470 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(2.5398335), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0642844520509243), 'actor_loss': np.float64(-0.9947792708873748), 'hyper_actor_loss': np.float64(0.0038965079933404923), 'behavior_loss': np.float64(1.1992913365364075)}

Episode step 10480, time diff 0.6957063674926758, total time dif 787.2061848640442)
step: 10480 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(2.8807607), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07938349694013595), 'actor_loss': np.float64(-0.9927302122116088), 'hyper_actor_loss': np.float64(0.004261195054277777), 'behavior_loss': np.float64(1.3044740676879882)}

Episode step 10490, time diff 0.6821024417877197, total time dif 787.9018912315369)
step: 10490 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(1.5549939), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08084805384278297), 'actor_loss': np.float64(-1.0188575208187103), 'hyper_actor_loss': np.float64(0.004899448901414871), 'behavior_loss': np.float64(1.246646249294281)}

Episode step 10500, time diff 0.7046821117401123, total time dif 788.5839936733246)
step: 10500 @ episode report: {'average_total_reward': np.float32(9.763334), 'reward_variance': np.float32(2.048939), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059651372209191325), 'actor_loss': np.float64(-0.9460003972053528), 'hyper_actor_loss': np.float64(0.004858080530539155), 'behavior_loss': np.float64(1.3401433110237122)}

Episode step 10510, time diff 0.7118644714355469, total time dif 789.2886757850647)
step: 10510 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(3.5699024), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0681373879313469), 'actor_loss': np.float64(-0.9782410025596618), 'hyper_actor_loss': np.float64(0.00411576284095645), 'behavior_loss': np.float64(1.3315586566925048)}

Episode step 10520, time diff 0.6743800640106201, total time dif 790.0005402565002)
step: 10520 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(2.1352146), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06593828238546848), 'actor_loss': np.float64(-0.9687472283840179), 'hyper_actor_loss': np.float64(0.003966517350636423), 'behavior_loss': np.float64(1.3648707032203675)}

Episode step 10530, time diff 0.7016534805297852, total time dif 790.6749203205109)
step: 10530 @ episode report: {'average_total_reward': np.float32(9.775557), 'reward_variance': np.float32(3.3573768), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06574787274003029), 'actor_loss': np.float64(-0.9746460139751434), 'hyper_actor_loss': np.float64(0.0038927179528400304), 'behavior_loss': np.float64(1.254604971408844)}

Episode step 10540, time diff 0.6610622406005859, total time dif 791.3765738010406)
step: 10540 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(2.7644796), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06344517432153225), 'actor_loss': np.float64(-0.9829915285110473), 'hyper_actor_loss': np.float64(0.0038171908119693397), 'behavior_loss': np.float64(1.353213095664978)}

Episode step 10550, time diff 0.6727886199951172, total time dif 792.0376360416412)
step: 10550 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(2.6296048), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06746060214936733), 'actor_loss': np.float64(-0.9815662264823913), 'hyper_actor_loss': np.float64(0.0037851407192647456), 'behavior_loss': np.float64(1.2461363136768342)}

Episode step 10560, time diff 0.6902718544006348, total time dif 792.7104246616364)
step: 10560 @ episode report: {'average_total_reward': np.float32(8.877779), 'reward_variance': np.float32(1.3613336), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07602542378008366), 'actor_loss': np.float64(-0.9949260473251342), 'hyper_actor_loss': np.float64(0.004018027288839221), 'behavior_loss': np.float64(1.3588078498840332)}

Episode step 10570, time diff 0.6726334095001221, total time dif 793.400696516037)
step: 10570 @ episode report: {'average_total_reward': np.float32(8.887777), 'reward_variance': np.float32(1.260727), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07311723493039608), 'actor_loss': np.float64(-1.013267743587494), 'hyper_actor_loss': np.float64(0.004481952544301749), 'behavior_loss': np.float64(1.3066545605659485)}

Episode step 10580, time diff 0.675499677658081, total time dif 794.0733299255371)
step: 10580 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(2.2973142), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06909721344709396), 'actor_loss': np.float64(-0.9774484992027282), 'hyper_actor_loss': np.float64(0.005106729408726096), 'behavior_loss': np.float64(1.4747719645500184)}

Episode step 10590, time diff 0.6930227279663086, total time dif 794.7488296031952)
step: 10590 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(2.9620295), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07927793227136135), 'actor_loss': np.float64(-1.001902437210083), 'hyper_actor_loss': np.float64(0.005818026419728994), 'behavior_loss': np.float64(1.3764259338378906)}

Episode step 10600, time diff 0.679542064666748, total time dif 795.4418523311615)
step: 10600 @ episode report: {'average_total_reward': np.float32(9.090001), 'reward_variance': np.float32(2.4279869), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08345885798335076), 'actor_loss': np.float64(-1.0305881023406982), 'hyper_actor_loss': np.float64(0.00759098706766963), 'behavior_loss': np.float64(1.280475389957428)}

Episode step 10610, time diff 0.6671788692474365, total time dif 796.1213943958282)
step: 10610 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(2.869758), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07770255096256733), 'actor_loss': np.float64(-0.999527531862259), 'hyper_actor_loss': np.float64(0.011006957851350307), 'behavior_loss': np.float64(1.3191960096359252)}

Episode step 10620, time diff 0.8591880798339844, total time dif 796.7885732650757)
step: 10620 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(2.329369), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06427440010011196), 'actor_loss': np.float64(-0.9833833515644074), 'hyper_actor_loss': np.float64(0.010949245560914278), 'behavior_loss': np.float64(1.2775099158287049)}

Episode step 10630, time diff 0.6781916618347168, total time dif 797.6477613449097)
step: 10630 @ episode report: {'average_total_reward': np.float32(9.375557), 'reward_variance': np.float32(3.4424896), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07930821962654591), 'actor_loss': np.float64(-1.0250404477119446), 'hyper_actor_loss': np.float64(0.011280738841742277), 'behavior_loss': np.float64(1.3312865853309632)}

Episode step 10640, time diff 0.6951043605804443, total time dif 798.3259530067444)
step: 10640 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(2.161634), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07763711102306843), 'actor_loss': np.float64(-1.0376073956489562), 'hyper_actor_loss': np.float64(0.012725147139281034), 'behavior_loss': np.float64(1.296424639225006)}

Episode step 10650, time diff 0.6834652423858643, total time dif 799.0210573673248)
step: 10650 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.8649582), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08034781888127326), 'actor_loss': np.float64(-1.0075803816318512), 'hyper_actor_loss': np.float64(0.012662579212337733), 'behavior_loss': np.float64(1.1749640822410583)}

Episode step 10660, time diff 0.6886014938354492, total time dif 799.7045226097107)
step: 10660 @ episode report: {'average_total_reward': np.float32(8.490001), 'reward_variance': np.float32(1.3389008), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07468685619533062), 'actor_loss': np.float64(-0.9922880828380585), 'hyper_actor_loss': np.float64(0.011343868914991617), 'behavior_loss': np.float64(1.210192370414734)}

Episode step 10670, time diff 0.7134718894958496, total time dif 800.3931241035461)
step: 10670 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(1.2968894), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07103576194494962), 'actor_loss': np.float64(-0.9687526524066925), 'hyper_actor_loss': np.float64(0.009852037578821183), 'behavior_loss': np.float64(1.3031069755554199)}

Episode step 10680, time diff 0.6967096328735352, total time dif 801.106595993042)
step: 10680 @ episode report: {'average_total_reward': np.float32(9.463335), 'reward_variance': np.float32(1.700619), 'max_total_reward': np.float32(11.900002), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07470922581851483), 'actor_loss': np.float64(-0.9827263236045838), 'hyper_actor_loss': np.float64(0.008590817917138338), 'behavior_loss': np.float64(1.198610246181488)}

Episode step 10690, time diff 0.687934160232544, total time dif 801.8033056259155)
step: 10690 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(2.6296048), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.077055424451828), 'actor_loss': np.float64(-0.9906637310981751), 'hyper_actor_loss': np.float64(0.007845577970147132), 'behavior_loss': np.float64(1.2245108246803285)}

Episode step 10700, time diff 0.6938471794128418, total time dif 802.4912397861481)
step: 10700 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(3.4973598), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06767721362411976), 'actor_loss': np.float64(-0.9908538401126862), 'hyper_actor_loss': np.float64(0.007205484388396144), 'behavior_loss': np.float64(1.245505702495575)}

Episode step 10710, time diff 0.6756360530853271, total time dif 803.1850869655609)
step: 10710 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.5623518), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08853988572955132), 'actor_loss': np.float64(-0.9918493211269379), 'hyper_actor_loss': np.float64(0.00685722385533154), 'behavior_loss': np.float64(1.2835758447647094)}

Episode step 10720, time diff 0.6730020046234131, total time dif 803.8607230186462)
step: 10720 @ episode report: {'average_total_reward': np.float32(8.875555), 'reward_variance': np.float32(4.708736), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06814519613981247), 'actor_loss': np.float64(-0.9984623849391937), 'hyper_actor_loss': np.float64(0.00683555593714118), 'behavior_loss': np.float64(1.2419936776161193)}

Episode step 10730, time diff 0.7045702934265137, total time dif 804.5337250232697)
step: 10730 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(2.387729), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08098515570163727), 'actor_loss': np.float64(-0.9941010534763336), 'hyper_actor_loss': np.float64(0.006840950483456254), 'behavior_loss': np.float64(1.3010675072669984)}

Episode step 10740, time diff 0.6714217662811279, total time dif 805.2382953166962)
step: 10740 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(3.458456), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07065544612705707), 'actor_loss': np.float64(-0.9652312755584717), 'hyper_actor_loss': np.float64(0.006353226490318775), 'behavior_loss': np.float64(1.2526694893836976)}

Episode step 10750, time diff 0.7116649150848389, total time dif 805.9097170829773)
step: 10750 @ episode report: {'average_total_reward': np.float32(8.653334), 'reward_variance': np.float32(1.1039699), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060490117967128755), 'actor_loss': np.float64(-0.969111031293869), 'hyper_actor_loss': np.float64(0.005506845656782389), 'behavior_loss': np.float64(1.2038703918457032)}

Episode step 10760, time diff 0.6823136806488037, total time dif 806.6213819980621)
step: 10760 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.474594), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06293653622269631), 'actor_loss': np.float64(-0.9981157660484314), 'hyper_actor_loss': np.float64(0.005140390619635582), 'behavior_loss': np.float64(1.1973218858242034)}

Episode step 10770, time diff 0.7058525085449219, total time dif 807.3036956787109)
step: 10770 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.5318227), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0858816597610712), 'actor_loss': np.float64(-0.9898968040943146), 'hyper_actor_loss': np.float64(0.005107388272881508), 'behavior_loss': np.float64(1.379004919528961)}

Episode step 10780, time diff 0.8774573802947998, total time dif 808.0095481872559)
step: 10780 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(2.0479207), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07875761911273002), 'actor_loss': np.float64(-1.001177763938904), 'hyper_actor_loss': np.float64(0.00495722140185535), 'behavior_loss': np.float64(1.3295410096645355)}

Episode step 10790, time diff 0.6924598217010498, total time dif 808.8870055675507)
step: 10790 @ episode report: {'average_total_reward': np.float32(9.975556), 'reward_variance': np.float32(1.3167363), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0650755163282156), 'actor_loss': np.float64(-0.9996147096157074), 'hyper_actor_loss': np.float64(0.0047970038373023275), 'behavior_loss': np.float64(1.2716035842895508)}

Episode step 10800, time diff 0.6827285289764404, total time dif 809.5794653892517)
step: 10800 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(2.498696), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07353252172470093), 'actor_loss': np.float64(-0.9788407385349274), 'hyper_actor_loss': np.float64(0.004710953868925572), 'behavior_loss': np.float64(1.4623353600502014)}

Episode step 10810, time diff 0.7040705680847168, total time dif 810.2621939182281)
step: 10810 @ episode report: {'average_total_reward': np.float32(9.6877775), 'reward_variance': np.float32(1.0602088), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07487173117697239), 'actor_loss': np.float64(-1.0012563109397887), 'hyper_actor_loss': np.float64(0.004668897669762373), 'behavior_loss': np.float64(1.3688805937767028)}

Episode step 10820, time diff 0.7178714275360107, total time dif 810.9662644863129)
step: 10820 @ episode report: {'average_total_reward': np.float32(9.514444), 'reward_variance': np.float32(2.1169646), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07447003573179245), 'actor_loss': np.float64(-0.9786524713039398), 'hyper_actor_loss': np.float64(0.004714737180620432), 'behavior_loss': np.float64(1.4430236577987672)}

Episode step 10830, time diff 0.681652307510376, total time dif 811.6841359138489)
step: 10830 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(2.1138566), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08121299967169762), 'actor_loss': np.float64(-1.0108365535736084), 'hyper_actor_loss': np.float64(0.0047417980618774894), 'behavior_loss': np.float64(1.4503490686416627)}

Episode step 10840, time diff 0.6867434978485107, total time dif 812.3657882213593)
step: 10840 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(1.4011722), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08158145062625408), 'actor_loss': np.float64(-0.9990844488143921), 'hyper_actor_loss': np.float64(0.0045770973898470405), 'behavior_loss': np.float64(1.4928167700767516)}

Episode step 10850, time diff 0.6872048377990723, total time dif 813.0525317192078)
step: 10850 @ episode report: {'average_total_reward': np.float32(8.963335), 'reward_variance': np.float32(2.042149), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0732606016099453), 'actor_loss': np.float64(-1.006503200531006), 'hyper_actor_loss': np.float64(0.004656643373891711), 'behavior_loss': np.float64(1.5666405797004699)}

Episode step 10860, time diff 0.7010655403137207, total time dif 813.7397365570068)
step: 10860 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(1.3089886), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07449259348213673), 'actor_loss': np.float64(-0.9899391174316406), 'hyper_actor_loss': np.float64(0.005126424692571163), 'behavior_loss': np.float64(1.547323989868164)}

Episode step 10870, time diff 0.6928117275238037, total time dif 814.4408020973206)
step: 10870 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(7.7161307), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07362107932567596), 'actor_loss': np.float64(-1.0296835780143738), 'hyper_actor_loss': np.float64(0.005593436909839511), 'behavior_loss': np.float64(1.513205623626709)}

Episode step 10880, time diff 0.7023279666900635, total time dif 815.1336138248444)
step: 10880 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(1.6819515), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07478755004703999), 'actor_loss': np.float64(-0.9963643729686738), 'hyper_actor_loss': np.float64(0.006076291343197227), 'behavior_loss': np.float64(1.6735986709594726)}

Episode step 10890, time diff 0.6847212314605713, total time dif 815.8359417915344)
step: 10890 @ episode report: {'average_total_reward': np.float32(8.902223), 'reward_variance': np.float32(1.9633539), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07029408812522889), 'actor_loss': np.float64(-0.9798131823539734), 'hyper_actor_loss': np.float64(0.0058877513278275725), 'behavior_loss': np.float64(1.454660451412201)}

Episode step 10900, time diff 0.6628484725952148, total time dif 816.520663022995)
step: 10900 @ episode report: {'average_total_reward': np.float32(8.365557), 'reward_variance': np.float32(3.7199128), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058492923900485036), 'actor_loss': np.float64(-0.9778551876544952), 'hyper_actor_loss': np.float64(0.0051993710920214655), 'behavior_loss': np.float64(1.6085427045822143)}

Episode step 10910, time diff 0.7012679576873779, total time dif 817.1835114955902)
step: 10910 @ episode report: {'average_total_reward': np.float32(8.341112), 'reward_variance': np.float32(1.9065939), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06561788395047188), 'actor_loss': np.float64(-0.9699100732803345), 'hyper_actor_loss': np.float64(0.004588774358853698), 'behavior_loss': np.float64(1.6344762325286866)}

Episode step 10920, time diff 0.6969361305236816, total time dif 817.8847794532776)
step: 10920 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(1.2250732), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.7777777), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08370922282338142), 'actor_loss': np.float64(-1.0125296354293822), 'hyper_actor_loss': np.float64(0.004516209755092859), 'behavior_loss': np.float64(1.4754202842712403)}

Episode step 10930, time diff 0.6509268283843994, total time dif 818.5817155838013)
step: 10930 @ episode report: {'average_total_reward': np.float32(7.38), 'reward_variance': np.float32(2.3901925), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07325187753885984), 'actor_loss': np.float64(-1.0316262722015381), 'hyper_actor_loss': np.float64(0.005472207628190518), 'behavior_loss': np.float64(1.5738906383514404)}

Episode step 10940, time diff 0.8869030475616455, total time dif 819.2326424121857)
step: 10940 @ episode report: {'average_total_reward': np.float32(6.731111), 'reward_variance': np.float32(4.56271), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08798847272992134), 'actor_loss': np.float64(-1.0645880341529845), 'hyper_actor_loss': np.float64(0.00990259083919227), 'behavior_loss': np.float64(1.2667879462242126)}

Episode step 10950, time diff 0.707190990447998, total time dif 820.1195454597473)
step: 10950 @ episode report: {'average_total_reward': np.float32(5.421111), 'reward_variance': np.float32(2.1512704), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07752280309796333), 'actor_loss': np.float64(-1.0300620436668395), 'hyper_actor_loss': np.float64(0.014120982121676206), 'behavior_loss': np.float64(1.2965850293636323)}

Episode step 10960, time diff 0.6836059093475342, total time dif 820.8267364501953)
step: 10960 @ episode report: {'average_total_reward': np.float32(5.323333), 'reward_variance': np.float32(1.3835918), 'max_total_reward': np.float32(6.5333343), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.9), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07203140184283256), 'actor_loss': np.float64(-0.9903783023357391), 'hyper_actor_loss': np.float64(0.015471359435468911), 'behavior_loss': np.float64(1.2275250554084778)}

Episode step 10970, time diff 0.6645467281341553, total time dif 821.5103423595428)
step: 10970 @ episode report: {'average_total_reward': np.float32(5.4722223), 'reward_variance': np.float32(1.2800555), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0666220586746931), 'actor_loss': np.float64(-0.9748648464679718), 'hyper_actor_loss': np.float64(0.013445539679378272), 'behavior_loss': np.float64(1.2631761193275453)}

Episode step 10980, time diff 0.7317798137664795, total time dif 822.174889087677)
step: 10980 @ episode report: {'average_total_reward': np.float32(7.0188894), 'reward_variance': np.float32(1.6585197), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06889350116252899), 'actor_loss': np.float64(-1.0041628181934357), 'hyper_actor_loss': np.float64(0.011683559976518154), 'behavior_loss': np.float64(1.1875922322273254)}

Episode step 10990, time diff 0.7361409664154053, total time dif 822.9066689014435)
step: 10990 @ episode report: {'average_total_reward': np.float32(6.7555556), 'reward_variance': np.float32(1.2456301), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07329235337674618), 'actor_loss': np.float64(-0.9613441109657288), 'hyper_actor_loss': np.float64(0.00975527660921216), 'behavior_loss': np.float64(1.309312653541565)}

Episode step 11000, time diff 0.703862190246582, total time dif 823.6428098678589)
step: 11000 @ episode report: {'average_total_reward': np.float32(7.3922224), 'reward_variance': np.float32(2.6716075), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08092203065752983), 'actor_loss': np.float64(-0.9840149581432343), 'hyper_actor_loss': np.float64(0.00802110219374299), 'behavior_loss': np.float64(1.385308998823166)}

Episode step 11010, time diff 0.679041862487793, total time dif 824.3466720581055)
step: 11010 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(3.081507), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06950906291604042), 'actor_loss': np.float64(-0.9629562318325042), 'hyper_actor_loss': np.float64(0.0072674098890274765), 'behavior_loss': np.float64(1.34746652841568)}

Episode step 11020, time diff 0.7208826541900635, total time dif 825.0257139205933)
step: 11020 @ episode report: {'average_total_reward': np.float32(8.526667), 'reward_variance': np.float32(1.5571649), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07152742519974709), 'actor_loss': np.float64(-0.9675638496875762), 'hyper_actor_loss': np.float64(0.007412918284535408), 'behavior_loss': np.float64(1.3198435723781585)}

Episode step 11030, time diff 0.723698616027832, total time dif 825.7465965747833)
step: 11030 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(0.5232889), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08449221774935722), 'actor_loss': np.float64(-0.9906688094139099), 'hyper_actor_loss': np.float64(0.007099051820114255), 'behavior_loss': np.float64(1.3117175698280334)}

Episode step 11040, time diff 0.7140369415283203, total time dif 826.4702951908112)
step: 11040 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.3187459), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07374775521457196), 'actor_loss': np.float64(-0.9999375581741333), 'hyper_actor_loss': np.float64(0.006130689475685358), 'behavior_loss': np.float64(1.264203095436096)}

Episode step 11050, time diff 0.718195915222168, total time dif 827.1843321323395)
step: 11050 @ episode report: {'average_total_reward': np.float32(10.212223), 'reward_variance': np.float32(1.0234678), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07539333961904049), 'actor_loss': np.float64(-0.9663717031478882), 'hyper_actor_loss': np.float64(0.005353273497894407), 'behavior_loss': np.float64(1.3007412075996398)}

Episode step 11060, time diff 0.9036412239074707, total time dif 827.9025280475616)
step: 11060 @ episode report: {'average_total_reward': np.float32(9.102223), 'reward_variance': np.float32(1.2052295), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06587729454040528), 'actor_loss': np.float64(-0.9904504537582397), 'hyper_actor_loss': np.float64(0.004853763151913881), 'behavior_loss': np.float64(1.4867030143737794)}

Episode step 11070, time diff 0.7349939346313477, total time dif 828.8061692714691)
step: 11070 @ episode report: {'average_total_reward': np.float32(8.902224), 'reward_variance': np.float32(1.4625877), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0765751551836729), 'actor_loss': np.float64(-0.9732660233974457), 'hyper_actor_loss': np.float64(0.004483866877853871), 'behavior_loss': np.float64(1.6084464192390442)}

Episode step 11080, time diff 0.7209060192108154, total time dif 829.5411632061005)
step: 11080 @ episode report: {'average_total_reward': np.float32(8.390001), 'reward_variance': np.float32(0.84912264), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07386531420052052), 'actor_loss': np.float64(-1.0005325734615327), 'hyper_actor_loss': np.float64(0.004339141491800547), 'behavior_loss': np.float64(1.7708895087242127)}

Episode step 11090, time diff 0.7027852535247803, total time dif 830.2620692253113)
step: 11090 @ episode report: {'average_total_reward': np.float32(9.102223), 'reward_variance': np.float32(3.36239), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07236875183880329), 'actor_loss': np.float64(-0.9661980450153351), 'hyper_actor_loss': np.float64(0.004241868993267417), 'behavior_loss': np.float64(1.530557429790497)}

Episode step 11100, time diff 0.847428560256958, total time dif 830.9648544788361)
step: 11100 @ episode report: {'average_total_reward': np.float32(8.353334), 'reward_variance': np.float32(1.5919459), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06578106470406056), 'actor_loss': np.float64(-0.9766213715076446), 'hyper_actor_loss': np.float64(0.004438246600329876), 'behavior_loss': np.float64(1.768888485431671)}

Episode step 11110, time diff 0.7184114456176758, total time dif 831.812283039093)
step: 11110 @ episode report: {'average_total_reward': np.float32(9.751112), 'reward_variance': np.float32(2.3821042), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.288889), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06532794497907161), 'actor_loss': np.float64(-0.9652727305889129), 'hyper_actor_loss': np.float64(0.004383720038458705), 'behavior_loss': np.float64(1.5845648646354675)}

Episode step 11120, time diff 0.6863930225372314, total time dif 832.5306944847107)
step: 11120 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(6.112564), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07500925622880458), 'actor_loss': np.float64(-0.9799403071403503), 'hyper_actor_loss': np.float64(0.004376792162656784), 'behavior_loss': np.float64(1.5806029796600343)}

Episode step 11130, time diff 0.6895437240600586, total time dif 833.2170875072479)
step: 11130 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(2.7638047), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07424982227385044), 'actor_loss': np.float64(-0.9865136444568634), 'hyper_actor_loss': np.float64(0.004281204706057906), 'behavior_loss': np.float64(1.688197124004364)}

Episode step 11140, time diff 0.7354803085327148, total time dif 833.906631231308)
step: 11140 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(3.5358326), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07957668229937553), 'actor_loss': np.float64(-0.988101190328598), 'hyper_actor_loss': np.float64(0.0039389247074723245), 'behavior_loss': np.float64(1.8263242721557618)}

Episode step 11150, time diff 0.7076680660247803, total time dif 834.6421115398407)
step: 11150 @ episode report: {'average_total_reward': np.float32(9.536668), 'reward_variance': np.float32(2.0081744), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07915443032979966), 'actor_loss': np.float64(-0.9927935659885406), 'hyper_actor_loss': np.float64(0.0034072892973199486), 'behavior_loss': np.float64(1.8638857841491698)}

Episode step 11160, time diff 0.7264039516448975, total time dif 835.3497796058655)
step: 11160 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(1.949295), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06975210011005402), 'actor_loss': np.float64(-0.9905300915241242), 'hyper_actor_loss': np.float64(0.003109572664834559), 'behavior_loss': np.float64(1.7440423011779784)}

Episode step 11170, time diff 0.7377774715423584, total time dif 836.0761835575104)
step: 11170 @ episode report: {'average_total_reward': np.float32(8.202223), 'reward_variance': np.float32(2.6837487), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07819733992218972), 'actor_loss': np.float64(-0.9983256340026856), 'hyper_actor_loss': np.float64(0.0029055540915578605), 'behavior_loss': np.float64(1.8122885346412658)}

Episode step 11180, time diff 0.6976537704467773, total time dif 836.8139610290527)
step: 11180 @ episode report: {'average_total_reward': np.float32(8.204445), 'reward_variance': np.float32(1.2122024), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06976886093616486), 'actor_loss': np.float64(-0.9860094845294952), 'hyper_actor_loss': np.float64(0.002848564274609089), 'behavior_loss': np.float64(1.8110091924667358)}

Episode step 11190, time diff 0.7120330333709717, total time dif 837.5116147994995)
step: 11190 @ episode report: {'average_total_reward': np.float32(8.365556), 'reward_variance': np.float32(2.476999), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0694297518581152), 'actor_loss': np.float64(-0.9735239505767822), 'hyper_actor_loss': np.float64(0.0027295031351968644), 'behavior_loss': np.float64(1.8104564189910888)}

Episode step 11200, time diff 0.7064733505249023, total time dif 838.2236478328705)
step: 11200 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(1.7357826), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062332776933908464), 'actor_loss': np.float64(-0.9741146504878998), 'hyper_actor_loss': np.float64(0.0026967278914526104), 'behavior_loss': np.float64(1.9374966621398926)}

Episode step 11210, time diff 0.7117781639099121, total time dif 838.9301211833954)
step: 11210 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(1.943024), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0685400802642107), 'actor_loss': np.float64(-0.9681799232959747), 'hyper_actor_loss': np.float64(0.00271197366528213), 'behavior_loss': np.float64(1.932526671886444)}

Episode step 11220, time diff 0.7002718448638916, total time dif 839.6418993473053)
step: 11220 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(3.2195358), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06107522174715996), 'actor_loss': np.float64(-0.9706679105758667), 'hyper_actor_loss': np.float64(0.002966805547475815), 'behavior_loss': np.float64(1.613519048690796)}

Episode step 11230, time diff 0.6850659847259521, total time dif 840.3421711921692)
step: 11230 @ episode report: {'average_total_reward': np.float32(8.477778), 'reward_variance': np.float32(1.7850864), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07623658776283264), 'actor_loss': np.float64(-0.9841028928756714), 'hyper_actor_loss': np.float64(0.0032312947325408457), 'behavior_loss': np.float64(1.8792271494865418)}

Episode step 11240, time diff 0.6883459091186523, total time dif 841.0272371768951)
step: 11240 @ episode report: {'average_total_reward': np.float32(8.502223), 'reward_variance': np.float32(2.0206132), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07501761578023433), 'actor_loss': np.float64(-0.965845936536789), 'hyper_actor_loss': np.float64(0.003328881203196943), 'behavior_loss': np.float64(1.841800820827484)}

Episode step 11250, time diff 0.7355940341949463, total time dif 841.7155830860138)
step: 11250 @ episode report: {'average_total_reward': np.float32(7.341111), 'reward_variance': np.float32(2.699163), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07337953336536884), 'actor_loss': np.float64(-0.9893514037132263), 'hyper_actor_loss': np.float64(0.0032413760898634793), 'behavior_loss': np.float64(1.835476815700531)}

Episode step 11260, time diff 0.8224391937255859, total time dif 842.4511771202087)
step: 11260 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(2.5338273), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06969809085130692), 'actor_loss': np.float64(-0.9692415058612823), 'hyper_actor_loss': np.float64(0.003185438085347414), 'behavior_loss': np.float64(1.9229864001274108)}

Episode step 11270, time diff 0.7100687026977539, total time dif 843.2736163139343)
step: 11270 @ episode report: {'average_total_reward': np.float32(7.5922227), 'reward_variance': np.float32(0.7827667), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07543974258005619), 'actor_loss': np.float64(-1.0122435092926025), 'hyper_actor_loss': np.float64(0.0032462060684338213), 'behavior_loss': np.float64(1.735037088394165)}

Episode step 11280, time diff 0.6966845989227295, total time dif 843.9836850166321)
step: 11280 @ episode report: {'average_total_reward': np.float32(7.6288896), 'reward_variance': np.float32(1.4394368), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06973804123699665), 'actor_loss': np.float64(-0.9914751827716828), 'hyper_actor_loss': np.float64(0.003399320971220732), 'behavior_loss': np.float64(1.752776861190796)}

Episode step 11290, time diff 0.6967630386352539, total time dif 844.6803696155548)
step: 11290 @ episode report: {'average_total_reward': np.float32(8.577778), 'reward_variance': np.float32(1.8498023), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07300543300807476), 'actor_loss': np.float64(-0.9930287361145019), 'hyper_actor_loss': np.float64(0.0035437448415905235), 'behavior_loss': np.float64(2.083981120586395)}

Episode step 11300, time diff 0.7038824558258057, total time dif 845.3771326541901)
step: 11300 @ episode report: {'average_total_reward': np.float32(7.6288896), 'reward_variance': np.float32(2.2195115), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07788460403680801), 'actor_loss': np.float64(-0.976773601770401), 'hyper_actor_loss': np.float64(0.003596934210509062), 'behavior_loss': np.float64(1.8335601925849914)}

Episode step 11310, time diff 0.7042014598846436, total time dif 846.0810151100159)
step: 11310 @ episode report: {'average_total_reward': np.float32(8.441112), 'reward_variance': np.float32(2.296421), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08324592411518097), 'actor_loss': np.float64(-1.0366024851799012), 'hyper_actor_loss': np.float64(0.003733148635365069), 'behavior_loss': np.float64(1.8488950371742248)}

Episode step 11320, time diff 0.6902484893798828, total time dif 846.7852165699005)
step: 11320 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(0.64602596), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07557239830493927), 'actor_loss': np.float64(-0.9644428372383118), 'hyper_actor_loss': np.float64(0.003935322631150484), 'behavior_loss': np.float64(1.9690744519233703)}

Episode step 11330, time diff 0.7121713161468506, total time dif 847.4754650592804)
step: 11330 @ episode report: {'average_total_reward': np.float32(8.002222), 'reward_variance': np.float32(2.1920695), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07669972442090511), 'actor_loss': np.float64(-0.9912821412086487), 'hyper_actor_loss': np.float64(0.003988640778698027), 'behavior_loss': np.float64(1.700324046611786)}

Episode step 11340, time diff 0.6858024597167969, total time dif 848.1876363754272)
step: 11340 @ episode report: {'average_total_reward': np.float32(8.765556), 'reward_variance': np.float32(3.0408013), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06367556806653737), 'actor_loss': np.float64(-1.0002426326274871), 'hyper_actor_loss': np.float64(0.003807613695971668), 'behavior_loss': np.float64(1.7988069534301758)}

Episode step 11350, time diff 0.7452459335327148, total time dif 848.873438835144)
step: 11350 @ episode report: {'average_total_reward': np.float32(8.29), 'reward_variance': np.float32(1.4221838), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07285417802631855), 'actor_loss': np.float64(-0.9953857421875), 'hyper_actor_loss': np.float64(0.0037077640416100623), 'behavior_loss': np.float64(1.9450669169425965)}

Episode step 11360, time diff 0.7006213665008545, total time dif 849.6186847686768)
step: 11360 @ episode report: {'average_total_reward': np.float32(8.204445), 'reward_variance': np.float32(1.4855359), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060473385453224185), 'actor_loss': np.float64(-0.9643533527851105), 'hyper_actor_loss': np.float64(0.003551709488965571), 'behavior_loss': np.float64(1.7489598631858825)}

Episode step 11370, time diff 0.69392991065979, total time dif 850.3193061351776)
step: 11370 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(2.1409402), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09444927126169204), 'actor_loss': np.float64(-0.9993627071380615), 'hyper_actor_loss': np.float64(0.003567176917567849), 'behavior_loss': np.float64(1.8961708784103393)}

Episode step 11380, time diff 0.701545238494873, total time dif 851.0132360458374)
step: 11380 @ episode report: {'average_total_reward': np.float32(8.126667), 'reward_variance': np.float32(3.4252403), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07894685827195644), 'actor_loss': np.float64(-1.0261443257331848), 'hyper_actor_loss': np.float64(0.0033134811790660025), 'behavior_loss': np.float64(1.907495880126953)}

Episode step 11390, time diff 0.6684627532958984, total time dif 851.7147812843323)
step: 11390 @ episode report: {'average_total_reward': np.float32(7.8411117), 'reward_variance': np.float32(2.308965), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07754906266927719), 'actor_loss': np.float64(-0.9424878001213074), 'hyper_actor_loss': np.float64(0.0031450729817152022), 'behavior_loss': np.float64(2.004204285144806)}

Episode step 11400, time diff 0.7248504161834717, total time dif 852.3832440376282)
step: 11400 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(2.2871614), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0753042072057724), 'actor_loss': np.float64(-0.9827792704105377), 'hyper_actor_loss': np.float64(0.002895942842587829), 'behavior_loss': np.float64(1.9825650453567505)}

Episode step 11410, time diff 0.678492546081543, total time dif 853.1080944538116)
step: 11410 @ episode report: {'average_total_reward': np.float32(8.202223), 'reward_variance': np.float32(2.431872), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07073978371918202), 'actor_loss': np.float64(-0.9800996601581573), 'hyper_actor_loss': np.float64(0.0027884830720722674), 'behavior_loss': np.float64(1.8001168012619018)}

Episode step 11420, time diff 0.8503327369689941, total time dif 853.7865869998932)
step: 11420 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(1.5033634), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07494454979896545), 'actor_loss': np.float64(-0.9742806911468506), 'hyper_actor_loss': np.float64(0.002899121306836605), 'behavior_loss': np.float64(2.1023722887039185)}

Episode step 11430, time diff 0.684645414352417, total time dif 854.6369197368622)
step: 11430 @ episode report: {'average_total_reward': np.float32(8.241112), 'reward_variance': np.float32(3.3048165), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.068895448371768), 'actor_loss': np.float64(-0.9906447350978851), 'hyper_actor_loss': np.float64(0.002996451430954039), 'behavior_loss': np.float64(1.8929907560348511)}

Episode step 11440, time diff 0.7015824317932129, total time dif 855.3215651512146)
step: 11440 @ episode report: {'average_total_reward': np.float32(6.731111), 'reward_variance': np.float32(3.464934), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06312757954001427), 'actor_loss': np.float64(-0.972330754995346), 'hyper_actor_loss': np.float64(0.0033515198854729535), 'behavior_loss': np.float64(1.7781624794006348)}

Episode step 11450, time diff 0.7264814376831055, total time dif 856.0231475830078)
step: 11450 @ episode report: {'average_total_reward': np.float32(8.214445), 'reward_variance': np.float32(3.854792), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06822318397462368), 'actor_loss': np.float64(-0.9785127997398376), 'hyper_actor_loss': np.float64(0.0036915201460942628), 'behavior_loss': np.float64(1.8755079507827759)}

Episode step 11460, time diff 0.6823484897613525, total time dif 856.7496290206909)
step: 11460 @ episode report: {'average_total_reward': np.float32(8.477778), 'reward_variance': np.float32(1.1147407), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08218967095017433), 'actor_loss': np.float64(-1.005858051776886), 'hyper_actor_loss': np.float64(0.0035027618985623123), 'behavior_loss': np.float64(1.8197751641273499)}

Episode step 11470, time diff 0.7047479152679443, total time dif 857.4319775104523)
step: 11470 @ episode report: {'average_total_reward': np.float32(7.553334), 'reward_variance': np.float32(3.6514518), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08179153613746167), 'actor_loss': np.float64(-0.9969190120697021), 'hyper_actor_loss': np.float64(0.003186611388809979), 'behavior_loss': np.float64(1.8832977414131165)}

Episode step 11480, time diff 0.6901125907897949, total time dif 858.1367254257202)
step: 11480 @ episode report: {'average_total_reward': np.float32(7.5166674), 'reward_variance': np.float32(0.99867284), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08217354863882065), 'actor_loss': np.float64(-1.0079332709312439), 'hyper_actor_loss': np.float64(0.0029655913822352886), 'behavior_loss': np.float64(1.9567752122879027)}

Episode step 11490, time diff 0.6780228614807129, total time dif 858.82683801651)
step: 11490 @ episode report: {'average_total_reward': np.float32(8.49), 'reward_variance': np.float32(2.5299368), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05931151434779167), 'actor_loss': np.float64(-0.9771432220935822), 'hyper_actor_loss': np.float64(0.0027690620627254248), 'behavior_loss': np.float64(1.994507098197937)}

Episode step 11500, time diff 0.7134602069854736, total time dif 859.5048608779907)
step: 11500 @ episode report: {'average_total_reward': np.float32(7.7922225), 'reward_variance': np.float32(1.9601252), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07603630721569062), 'actor_loss': np.float64(-0.9954814076423645), 'hyper_actor_loss': np.float64(0.0027779364259913563), 'behavior_loss': np.float64(2.2578871726989744)}

Episode step 11510, time diff 0.7060394287109375, total time dif 860.2183210849762)
step: 11510 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(1.9138467), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07797392420470714), 'actor_loss': np.float64(-0.9887396156787872), 'hyper_actor_loss': np.float64(0.0028479220811277626), 'behavior_loss': np.float64(2.280486297607422)}

Episode step 11520, time diff 0.6971895694732666, total time dif 860.9243605136871)
step: 11520 @ episode report: {'average_total_reward': np.float32(7.5922227), 'reward_variance': np.float32(0.53387797), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0634328380227089), 'actor_loss': np.float64(-0.9867346227169037), 'hyper_actor_loss': np.float64(0.002960228850133717), 'behavior_loss': np.float64(1.8657951474189758)}

Episode step 11530, time diff 0.6905550956726074, total time dif 861.6215500831604)
step: 11530 @ episode report: {'average_total_reward': np.float32(6.567778), 'reward_variance': np.float32(1.7257401), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07336881421506405), 'actor_loss': np.float64(-1.0048082232475282), 'hyper_actor_loss': np.float64(0.0035070442594587804), 'behavior_loss': np.float64(1.8909294724464416)}

Episode step 11540, time diff 0.6740598678588867, total time dif 862.312105178833)
step: 11540 @ episode report: {'average_total_reward': np.float32(7.428889), 'reward_variance': np.float32(2.9244742), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06670390851795674), 'actor_loss': np.float64(-0.9783188819885253), 'hyper_actor_loss': np.float64(0.00401475876569748), 'behavior_loss': np.float64(1.9546870946884156)}

Episode step 11550, time diff 0.7148292064666748, total time dif 862.9861650466919)
step: 11550 @ episode report: {'average_total_reward': np.float32(8.614445), 'reward_variance': np.float32(2.1409392), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06453163437545299), 'actor_loss': np.float64(-0.9698749482631683), 'hyper_actor_loss': np.float64(0.004477867158129811), 'behavior_loss': np.float64(1.89966539144516)}

Episode step 11560, time diff 0.6971108913421631, total time dif 863.7009942531586)
step: 11560 @ episode report: {'average_total_reward': np.float32(9.014444), 'reward_variance': np.float32(3.0562735), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07694881409406662), 'actor_loss': np.float64(-0.9732356667518616), 'hyper_actor_loss': np.float64(0.004665855085477233), 'behavior_loss': np.float64(2.051995837688446)}

Episode step 11570, time diff 0.6650524139404297, total time dif 864.3981051445007)
step: 11570 @ episode report: {'average_total_reward': np.float32(8.814445), 'reward_variance': np.float32(5.889557), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06554077304899693), 'actor_loss': np.float64(-0.998785388469696), 'hyper_actor_loss': np.float64(0.004574104491621256), 'behavior_loss': np.float64(1.6751227617263793)}

Episode step 11580, time diff 0.6916975975036621, total time dif 865.0631575584412)
step: 11580 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(5.405635), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060898811742663386), 'actor_loss': np.float64(-0.976765263080597), 'hyper_actor_loss': np.float64(0.0044153268914669756), 'behavior_loss': np.float64(2.0952120184898377)}

Episode step 11590, time diff 0.8524937629699707, total time dif 865.7548551559448)
step: 11590 @ episode report: {'average_total_reward': np.float32(9.612224), 'reward_variance': np.float32(1.3752954), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08517684191465377), 'actor_loss': np.float64(-1.0052047491073608), 'hyper_actor_loss': np.float64(0.004612270509824157), 'behavior_loss': np.float64(2.0536019921302797)}

Episode step 11600, time diff 0.6918313503265381, total time dif 866.6073489189148)
step: 11600 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(1.562352), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07502004764974117), 'actor_loss': np.float64(-1.0063317894935608), 'hyper_actor_loss': np.float64(0.004547639051452279), 'behavior_loss': np.float64(1.8479822039604188)}

Episode step 11610, time diff 0.7280409336090088, total time dif 867.2991802692413)
step: 11610 @ episode report: {'average_total_reward': np.float32(8.304445), 'reward_variance': np.float32(2.2472892), 'max_total_reward': np.float32(10.655556), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06593349054455758), 'actor_loss': np.float64(-0.9607663333415986), 'hyper_actor_loss': np.float64(0.004183732415549457), 'behavior_loss': np.float64(1.789419448375702)}

Episode step 11620, time diff 0.7121412754058838, total time dif 868.0272212028503)
step: 11620 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(2.5019367), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08214050382375718), 'actor_loss': np.float64(-0.9922436714172364), 'hyper_actor_loss': np.float64(0.003925632708705962), 'behavior_loss': np.float64(2.396837043762207)}

Episode step 11630, time diff 0.6927170753479004, total time dif 868.7393624782562)
step: 11630 @ episode report: {'average_total_reward': np.float32(8.065556), 'reward_variance': np.float32(3.0855665), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06740532703697681), 'actor_loss': np.float64(-0.9794481575489045), 'hyper_actor_loss': np.float64(0.0036338291130959986), 'behavior_loss': np.float64(2.1117710709571837)}

Episode step 11640, time diff 0.6964764595031738, total time dif 869.4320795536041)
step: 11640 @ episode report: {'average_total_reward': np.float32(8.041112), 'reward_variance': np.float32(2.9020262), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07351934164762497), 'actor_loss': np.float64(-0.9767367660999298), 'hyper_actor_loss': np.float64(0.0035800805781036615), 'behavior_loss': np.float64(2.01710706949234)}

Episode step 11650, time diff 0.691730260848999, total time dif 870.1285560131073)
step: 11650 @ episode report: {'average_total_reward': np.float32(7.555556), 'reward_variance': np.float32(2.4042468), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07470305413007736), 'actor_loss': np.float64(-1.009807860851288), 'hyper_actor_loss': np.float64(0.003566697775386274), 'behavior_loss': np.float64(2.192409908771515)}

Episode step 11660, time diff 0.6994554996490479, total time dif 870.8202862739563)
step: 11660 @ episode report: {'average_total_reward': np.float32(8.3144455), 'reward_variance': np.float32(3.1781497), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07461569793522357), 'actor_loss': np.float64(-0.9762194514274597), 'hyper_actor_loss': np.float64(0.003776911017484963), 'behavior_loss': np.float64(2.16008665561676)}

Episode step 11670, time diff 0.706230878829956, total time dif 871.5197417736053)
step: 11670 @ episode report: {'average_total_reward': np.float32(7.3922224), 'reward_variance': np.float32(2.146396), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0649255070835352), 'actor_loss': np.float64(-0.9745053470134735), 'hyper_actor_loss': np.float64(0.004160226415842771), 'behavior_loss': np.float64(2.195195531845093)}

Episode step 11680, time diff 0.677297830581665, total time dif 872.2259726524353)
step: 11680 @ episode report: {'average_total_reward': np.float32(8.3144455), 'reward_variance': np.float32(3.5751624), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07214596793055535), 'actor_loss': np.float64(-0.968606150150299), 'hyper_actor_loss': np.float64(0.004312007781118154), 'behavior_loss': np.float64(2.1019681096076965)}

Episode step 11690, time diff 0.6939949989318848, total time dif 872.903270483017)
step: 11690 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(1.834666), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07349272072315216), 'actor_loss': np.float64(-0.9858987867832184), 'hyper_actor_loss': np.float64(0.004302803985774517), 'behavior_loss': np.float64(2.1191003799438475)}

Episode step 11700, time diff 0.7404890060424805, total time dif 873.5972654819489)
step: 11700 @ episode report: {'average_total_reward': np.float32(7.4288893), 'reward_variance': np.float32(2.5414119), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06695381999015808), 'actor_loss': np.float64(-0.9680718123912812), 'hyper_actor_loss': np.float64(0.004139730939641595), 'behavior_loss': np.float64(2.374482214450836)}

Episode step 11710, time diff 0.7079405784606934, total time dif 874.3377544879913)
step: 11710 @ episode report: {'average_total_reward': np.float32(8.353334), 'reward_variance': np.float32(1.8682667), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08627911619842052), 'actor_loss': np.float64(-0.9894738554954529), 'hyper_actor_loss': np.float64(0.0037847172468900682), 'behavior_loss': np.float64(2.3065117239952087)}

Episode step 11720, time diff 0.6929481029510498, total time dif 875.045695066452)
step: 11720 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(3.6740203), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08302584290504456), 'actor_loss': np.float64(-1.0140279531478882), 'hyper_actor_loss': np.float64(0.003441233932971954), 'behavior_loss': np.float64(2.2644933104515075)}

Episode step 11730, time diff 0.6761839389801025, total time dif 875.7386431694031)
step: 11730 @ episode report: {'average_total_reward': np.float32(6.955556), 'reward_variance': np.float32(1.5656297), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07694225423038006), 'actor_loss': np.float64(-1.0101279258728026), 'hyper_actor_loss': np.float64(0.003169118380174041), 'behavior_loss': np.float64(2.366189253330231)}

Episode step 11740, time diff 0.6820478439331055, total time dif 876.4148271083832)
step: 11740 @ episode report: {'average_total_reward': np.float32(7.194445), 'reward_variance': np.float32(1.8512408), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07292764596641063), 'actor_loss': np.float64(-1.012648832798004), 'hyper_actor_loss': np.float64(0.003033860516734421), 'behavior_loss': np.float64(2.332650899887085)}

Episode step 11750, time diff 0.837287187576294, total time dif 877.0968749523163)
step: 11750 @ episode report: {'average_total_reward': np.float32(7.2555556), 'reward_variance': np.float32(1.4839013), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08952539414167404), 'actor_loss': np.float64(-1.0210739374160767), 'hyper_actor_loss': np.float64(0.0029219972202554345), 'behavior_loss': np.float64(2.3802358746528625)}

Episode step 11760, time diff 0.7077524662017822, total time dif 877.9341621398926)
step: 11760 @ episode report: {'average_total_reward': np.float32(7.7433333), 'reward_variance': np.float32(1.6339371), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07614388838410377), 'actor_loss': np.float64(-0.9972938239574433), 'hyper_actor_loss': np.float64(0.002761741168797016), 'behavior_loss': np.float64(2.8061838865280153)}

Episode step 11770, time diff 0.7171478271484375, total time dif 878.6419146060944)
step: 11770 @ episode report: {'average_total_reward': np.float32(6.7822227), 'reward_variance': np.float32(2.6383507), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07909454014152288), 'actor_loss': np.float64(-0.9827327072620392), 'hyper_actor_loss': np.float64(0.0026377957547083497), 'behavior_loss': np.float64(2.9447948932647705)}

Episode step 11780, time diff 0.7176034450531006, total time dif 879.3590624332428)
step: 11780 @ episode report: {'average_total_reward': np.float32(7.543334), 'reward_variance': np.float32(1.3632214), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07157002389431), 'actor_loss': np.float64(-0.9892733216285705), 'hyper_actor_loss': np.float64(0.0025843954412266613), 'behavior_loss': np.float64(2.98968448638916)}

Episode step 11790, time diff 0.6934802532196045, total time dif 880.0766658782959)
step: 11790 @ episode report: {'average_total_reward': np.float32(6.8188896), 'reward_variance': np.float32(1.8890632), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0763914555311203), 'actor_loss': np.float64(-0.9761035919189454), 'hyper_actor_loss': np.float64(0.002688501589000225), 'behavior_loss': np.float64(2.9703999757766724)}

Episode step 11800, time diff 0.696530818939209, total time dif 880.7701461315155)
step: 11800 @ episode report: {'average_total_reward': np.float32(6.3822227), 'reward_variance': np.float32(2.164203), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0681488310918212), 'actor_loss': np.float64(-0.9826316893100738), 'hyper_actor_loss': np.float64(0.0028218163177371027), 'behavior_loss': np.float64(2.9581066131591798)}

Episode step 11810, time diff 0.7126419544219971, total time dif 881.4666769504547)
step: 11810 @ episode report: {'average_total_reward': np.float32(7.231111), 'reward_variance': np.float32(2.2148595), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07763195559382438), 'actor_loss': np.float64(-0.9810976982116699), 'hyper_actor_loss': np.float64(0.003066674806177616), 'behavior_loss': np.float64(2.6829978108406065)}

Episode step 11820, time diff 0.6659862995147705, total time dif 882.1793189048767)
step: 11820 @ episode report: {'average_total_reward': np.float32(6.6433334), 'reward_variance': np.float32(1.8058628), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07444623932242393), 'actor_loss': np.float64(-1.007704848051071), 'hyper_actor_loss': np.float64(0.003205072064884007), 'behavior_loss': np.float64(2.815953719615936)}

Episode step 11830, time diff 0.7159333229064941, total time dif 882.8453052043915)
step: 11830 @ episode report: {'average_total_reward': np.float32(6.294445), 'reward_variance': np.float32(1.9920807), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07994111403822898), 'actor_loss': np.float64(-0.9866534948349), 'hyper_actor_loss': np.float64(0.003129627602174878), 'behavior_loss': np.float64(3.2760249614715575)}

Episode step 11840, time diff 0.6956992149353027, total time dif 883.561238527298)
step: 11840 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(4.194192), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07020943239331245), 'actor_loss': np.float64(-0.9802574098110199), 'hyper_actor_loss': np.float64(0.003093384369276464), 'behavior_loss': np.float64(3.7080416679382324)}

Episode step 11850, time diff 0.6985135078430176, total time dif 884.2569377422333)
step: 11850 @ episode report: {'average_total_reward': np.float32(5.9455557), 'reward_variance': np.float32(1.3976158), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07573825754225254), 'actor_loss': np.float64(-0.9679824709892273), 'hyper_actor_loss': np.float64(0.003078493080101907), 'behavior_loss': np.float64(3.890551543235779)}

Episode step 11860, time diff 0.6984105110168457, total time dif 884.9554512500763)
step: 11860 @ episode report: {'average_total_reward': np.float32(6.145556), 'reward_variance': np.float32(1.5483565), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0743816252797842), 'actor_loss': np.float64(-0.9978773176670075), 'hyper_actor_loss': np.float64(0.00316649719607085), 'behavior_loss': np.float64(3.8143141746520994)}

Episode step 11870, time diff 0.7042055130004883, total time dif 885.6538617610931)
step: 11870 @ episode report: {'average_total_reward': np.float32(5.87), 'reward_variance': np.float32(1.4564953), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08270759172737599), 'actor_loss': np.float64(-0.9997299253940582), 'hyper_actor_loss': np.float64(0.0033904703799635173), 'behavior_loss': np.float64(3.4993054628372193)}

Episode step 11880, time diff 0.7028582096099854, total time dif 886.3580672740936)
step: 11880 @ episode report: {'average_total_reward': np.float32(6.767778), 'reward_variance': np.float32(3.4595413), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07358994148671627), 'actor_loss': np.float64(-0.9676643967628479), 'hyper_actor_loss': np.float64(0.0032870145281776787), 'behavior_loss': np.float64(3.1868109464645387)}

Episode step 11890, time diff 0.6734564304351807, total time dif 887.0609254837036)
step: 11890 @ episode report: {'average_total_reward': np.float32(6.5311112), 'reward_variance': np.float32(2.6416743), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07082646265625954), 'actor_loss': np.float64(-0.9827600419521332), 'hyper_actor_loss': np.float64(0.0031182085862383247), 'behavior_loss': np.float64(3.6391488313674927)}

Episode step 11900, time diff 0.8325440883636475, total time dif 887.7343819141388)
step: 11900 @ episode report: {'average_total_reward': np.float32(6.618889), 'reward_variance': np.float32(1.731335), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09040524140000343), 'actor_loss': np.float64(-1.008537769317627), 'hyper_actor_loss': np.float64(0.0030477770371362565), 'behavior_loss': np.float64(3.6093140125274656)}

Episode step 11910, time diff 0.6994600296020508, total time dif 888.5669260025024)
step: 11910 @ episode report: {'average_total_reward': np.float32(7.11889), 'reward_variance': np.float32(2.3247418), 'max_total_reward': np.float32(9.655555), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09648530408740044), 'actor_loss': np.float64(-1.0006636381149292), 'hyper_actor_loss': np.float64(0.0032628006068989635), 'behavior_loss': np.float64(3.8531121015548706)}

Episode step 11920, time diff 0.7243273258209229, total time dif 889.2663860321045)
step: 11920 @ episode report: {'average_total_reward': np.float32(7.1433334), 'reward_variance': np.float32(3.4676163), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06703967563807964), 'actor_loss': np.float64(-1.0034215211868287), 'hyper_actor_loss': np.float64(0.0036418708506971596), 'behavior_loss': np.float64(4.166060566902161)}

Episode step 11930, time diff 0.7301781177520752, total time dif 889.9907133579254)
step: 11930 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(2.2954328), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08233113214373589), 'actor_loss': np.float64(-0.9515252351760864), 'hyper_actor_loss': np.float64(0.003999847988598049), 'behavior_loss': np.float64(3.974402666091919)}

Episode step 11940, time diff 0.7508678436279297, total time dif 890.7208914756775)
step: 11940 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(2.0179698), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08176388218998909), 'actor_loss': np.float64(-1.014604538679123), 'hyper_actor_loss': np.float64(0.003925588983111084), 'behavior_loss': np.float64(4.02413125038147)}

Episode step 11950, time diff 0.7148716449737549, total time dif 891.4717593193054)
step: 11950 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(2.5511909), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07813051454722882), 'actor_loss': np.float64(-0.9682060241699219), 'hyper_actor_loss': np.float64(0.003564131585881114), 'behavior_loss': np.float64(4.266033124923706)}

Episode step 11960, time diff 0.7222678661346436, total time dif 892.1866309642792)
step: 11960 @ episode report: {'average_total_reward': np.float32(7.3066673), 'reward_variance': np.float32(2.7638574), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07925126384943723), 'actor_loss': np.float64(-0.994029313325882), 'hyper_actor_loss': np.float64(0.0032388884807005524), 'behavior_loss': np.float64(3.8942888498306276)}

Episode step 11970, time diff 0.709650993347168, total time dif 892.9088988304138)
step: 11970 @ episode report: {'average_total_reward': np.float32(8.38), 'reward_variance': np.float32(2.09839), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07267132438719273), 'actor_loss': np.float64(-0.9870963096618652), 'hyper_actor_loss': np.float64(0.0031574374064803124), 'behavior_loss': np.float64(4.222427177429199)}

Episode step 11980, time diff 0.6676621437072754, total time dif 893.618549823761)
step: 11980 @ episode report: {'average_total_reward': np.float32(6.7555556), 'reward_variance': np.float32(3.222716), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08609546385705472), 'actor_loss': np.float64(-0.9694952368736267), 'hyper_actor_loss': np.float64(0.0032346899388357997), 'behavior_loss': np.float64(3.978040266036987)}

Episode step 11990, time diff 0.6850285530090332, total time dif 894.2862119674683)
step: 11990 @ episode report: {'average_total_reward': np.float32(7.2066665), 'reward_variance': np.float32(2.376573), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07324160300195218), 'actor_loss': np.float64(-0.9967191159725189), 'hyper_actor_loss': np.float64(0.0031074102502316237), 'behavior_loss': np.float64(4.06651725769043)}

Episode step 12000, time diff 0.706885814666748, total time dif 894.9712405204773)
step: 12000 @ episode report: {'average_total_reward': np.float32(7.331112), 'reward_variance': np.float32(1.2740448), 'max_total_reward': np.float32(8.655557), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07433469854295253), 'actor_loss': np.float64(-0.9702729344367981), 'hyper_actor_loss': np.float64(0.003003004635684192), 'behavior_loss': np.float64(4.086292457580567)}

Episode step 12010, time diff 0.7141077518463135, total time dif 895.678126335144)
step: 12010 @ episode report: {'average_total_reward': np.float32(7.104445), 'reward_variance': np.float32(2.0793383), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08603051491081715), 'actor_loss': np.float64(-0.9952717602252961), 'hyper_actor_loss': np.float64(0.002854421525262296), 'behavior_loss': np.float64(3.828703451156616)}

Episode step 12020, time diff 0.7217230796813965, total time dif 896.3922340869904)
step: 12020 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(2.52235), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08341739811003208), 'actor_loss': np.float64(-0.9978591740131378), 'hyper_actor_loss': np.float64(0.0026302306912839414), 'behavior_loss': np.float64(3.8928476572036743)}

Episode step 12030, time diff 0.7108750343322754, total time dif 897.1139571666718)
step: 12030 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(3.7284744), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0770847663283348), 'actor_loss': np.float64(-1.000152689218521), 'hyper_actor_loss': np.float64(0.002424287307076156), 'behavior_loss': np.float64(4.348906850814819)}

Episode step 12040, time diff 0.6885213851928711, total time dif 897.824832201004)
step: 12040 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(1.7442659), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0743323665112257), 'actor_loss': np.float64(-0.9737013936042785), 'hyper_actor_loss': np.float64(0.0023016648599877955), 'behavior_loss': np.float64(3.9904514074325563)}

Episode step 12050, time diff 0.6773967742919922, total time dif 898.5133535861969)
step: 12050 @ episode report: {'average_total_reward': np.float32(8.128889), 'reward_variance': np.float32(1.2961777), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07331956662237644), 'actor_loss': np.float64(-0.9752387583255768), 'hyper_actor_loss': np.float64(0.002358336909674108), 'behavior_loss': np.float64(4.162458920478821)}

Episode step 12060, time diff 0.885699987411499, total time dif 899.1907503604889)
step: 12060 @ episode report: {'average_total_reward': np.float32(8.565557), 'reward_variance': np.float32(2.4432712), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07491334266960621), 'actor_loss': np.float64(-0.9570582032203674), 'hyper_actor_loss': np.float64(0.002601331262849271), 'behavior_loss': np.float64(3.6173895359039308)}

Episode step 12070, time diff 0.6914305686950684, total time dif 900.0764503479004)
step: 12070 @ episode report: {'average_total_reward': np.float32(8.777777), 'reward_variance': np.float32(0.96459264), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06770665384829044), 'actor_loss': np.float64(-0.9825896322727203), 'hyper_actor_loss': np.float64(0.002677836548537016), 'behavior_loss': np.float64(3.497050738334656)}

Episode step 12080, time diff 0.7472615242004395, total time dif 900.7678809165955)
step: 12080 @ episode report: {'average_total_reward': np.float32(8.341112), 'reward_variance': np.float32(1.6457542), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08441042639315129), 'actor_loss': np.float64(-0.9951822996139527), 'hyper_actor_loss': np.float64(0.0027185226092115043), 'behavior_loss': np.float64(3.5981424331665037)}

Episode step 12090, time diff 0.698782205581665, total time dif 901.5151424407959)
step: 12090 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.124316), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07846910357475281), 'actor_loss': np.float64(-0.9844676673412323), 'hyper_actor_loss': np.float64(0.0027589241741225123), 'behavior_loss': np.float64(3.487908148765564)}

Episode step 12100, time diff 0.7199907302856445, total time dif 902.2139246463776)
step: 12100 @ episode report: {'average_total_reward': np.float32(9.338889), 'reward_variance': np.float32(2.8091671), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08505646251142025), 'actor_loss': np.float64(-0.9862970054149628), 'hyper_actor_loss': np.float64(0.0028545057866722345), 'behavior_loss': np.float64(3.5847300052642823)}

Episode step 12110, time diff 0.6683363914489746, total time dif 902.9339153766632)
step: 12110 @ episode report: {'average_total_reward': np.float32(9.202223), 'reward_variance': np.float32(3.5550823), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07977688722312451), 'actor_loss': np.float64(-1.0330686211585998), 'hyper_actor_loss': np.float64(0.003181373095139861), 'behavior_loss': np.float64(3.7066986560821533)}

Episode step 12120, time diff 0.7534501552581787, total time dif 903.6022517681122)
step: 12120 @ episode report: {'average_total_reward': np.float32(8.926667), 'reward_variance': np.float32(3.4210427), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0815451193600893), 'actor_loss': np.float64(-0.9961026012897491), 'hyper_actor_loss': np.float64(0.0034204578027129174), 'behavior_loss': np.float64(4.212258958816529)}

Episode step 12130, time diff 0.7212133407592773, total time dif 904.3557019233704)
step: 12130 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(3.268771), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07671126760542393), 'actor_loss': np.float64(-0.9581118822097778), 'hyper_actor_loss': np.float64(0.0037806165870279073), 'behavior_loss': np.float64(3.890980935096741)}

Episode step 12140, time diff 0.6930444240570068, total time dif 905.0769152641296)
step: 12140 @ episode report: {'average_total_reward': np.float32(8.353334), 'reward_variance': np.float32(1.7585375), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07711834087967873), 'actor_loss': np.float64(-0.9941350519657135), 'hyper_actor_loss': np.float64(0.0037234916118904946), 'behavior_loss': np.float64(3.69997091293335)}

Episode step 12150, time diff 0.6684350967407227, total time dif 905.7699596881866)
step: 12150 @ episode report: {'average_total_reward': np.float32(8.875555), 'reward_variance': np.float32(3.728661), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08253409266471863), 'actor_loss': np.float64(-0.9878278136253357), 'hyper_actor_loss': np.float64(0.0035477646626532076), 'behavior_loss': np.float64(3.858213472366333)}

Episode step 12160, time diff 0.6984679698944092, total time dif 906.4383947849274)
step: 12160 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(2.9841251), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07124160993844271), 'actor_loss': np.float64(-0.9734973192214966), 'hyper_actor_loss': np.float64(0.00334355840459466), 'behavior_loss': np.float64(4.059049105644226)}

Episode step 12170, time diff 0.6765046119689941, total time dif 907.1368627548218)
step: 12170 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(6.1574597), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07248474173247814), 'actor_loss': np.float64(-0.9786535799503326), 'hyper_actor_loss': np.float64(0.0030227765208110215), 'behavior_loss': np.float64(3.397241997718811)}

Episode step 12180, time diff 0.7434597015380859, total time dif 907.8133673667908)
step: 12180 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(4.009592), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06749778166413307), 'actor_loss': np.float64(-0.9840228736400605), 'hyper_actor_loss': np.float64(0.0030104564735665917), 'behavior_loss': np.float64(3.4050110816955566)}

Episode step 12190, time diff 0.7088682651519775, total time dif 908.5568270683289)
step: 12190 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(2.1707165), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08484842479228974), 'actor_loss': np.float64(-0.9782003700733185), 'hyper_actor_loss': np.float64(0.0031719040824100377), 'behavior_loss': np.float64(3.121327209472656)}

Episode step 12200, time diff 0.6926724910736084, total time dif 909.2656953334808)
step: 12200 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(1.9627352), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06783018968999385), 'actor_loss': np.float64(-0.9752642929553985), 'hyper_actor_loss': np.float64(0.0032776523847132923), 'behavior_loss': np.float64(3.64031503200531)}

Episode step 12210, time diff 0.680304765701294, total time dif 909.9583678245544)
step: 12210 @ episode report: {'average_total_reward': np.float32(8.5633335), 'reward_variance': np.float32(1.4445438), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07063183337450027), 'actor_loss': np.float64(-0.9407136082649231), 'hyper_actor_loss': np.float64(0.003631015005521476), 'behavior_loss': np.float64(3.2515519142150877)}

Episode step 12220, time diff 0.7118234634399414, total time dif 910.6386725902557)
step: 12220 @ episode report: {'average_total_reward': np.float32(9.112223), 'reward_variance': np.float32(3.4827278), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08053971529006958), 'actor_loss': np.float64(-0.979245638847351), 'hyper_actor_loss': np.float64(0.0032326434273272754), 'behavior_loss': np.float64(3.504337286949158)}

Episode step 12230, time diff 0.8688218593597412, total time dif 911.3504960536957)
step: 12230 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(1.367234), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07249344326555729), 'actor_loss': np.float64(-0.974746972322464), 'hyper_actor_loss': np.float64(0.0029249577783048153), 'behavior_loss': np.float64(3.527617621421814)}

Episode step 12240, time diff 0.7081854343414307, total time dif 912.2193179130554)
step: 12240 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(5.7939806), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06651978585869074), 'actor_loss': np.float64(-0.9796774566173554), 'hyper_actor_loss': np.float64(0.002983883977867663), 'behavior_loss': np.float64(3.4864891529083253)}

Episode step 12250, time diff 0.7254846096038818, total time dif 912.9275033473969)
step: 12250 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(2.388144), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07347216047346591), 'actor_loss': np.float64(-0.958294028043747), 'hyper_actor_loss': np.float64(0.0032976184040307997), 'behavior_loss': np.float64(3.569604182243347)}

Episode step 12260, time diff 0.7329709529876709, total time dif 913.6529879570007)
step: 12260 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(1.2431619), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07266390919685364), 'actor_loss': np.float64(-1.0111403048038483), 'hyper_actor_loss': np.float64(0.0033467098139226435), 'behavior_loss': np.float64(3.875729203224182)}

Episode step 12270, time diff 0.7228083610534668, total time dif 914.3859589099884)
step: 12270 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(1.2071059), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07468716651201249), 'actor_loss': np.float64(-0.9890011966228485), 'hyper_actor_loss': np.float64(0.003047341969795525), 'behavior_loss': np.float64(3.6533260107040406)}

Episode step 12280, time diff 0.7170143127441406, total time dif 915.1087672710419)
step: 12280 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(3.2709198), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08894720263779163), 'actor_loss': np.float64(-1.0076174020767212), 'hyper_actor_loss': np.float64(0.002818530355580151), 'behavior_loss': np.float64(3.460523533821106)}

Episode step 12290, time diff 0.6905591487884521, total time dif 915.825781583786)
step: 12290 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(4.3830185), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08167086653411389), 'actor_loss': np.float64(-0.9966872036457062), 'hyper_actor_loss': np.float64(0.0027504411526024343), 'behavior_loss': np.float64(3.6127164125442506)}

Episode step 12300, time diff 0.7182161808013916, total time dif 916.5163407325745)
step: 12300 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(4.3777485), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09001694619655609), 'actor_loss': np.float64(-0.9979063808918), 'hyper_actor_loss': np.float64(0.002693443582393229), 'behavior_loss': np.float64(4.020535182952881)}

Episode step 12310, time diff 0.7059817314147949, total time dif 917.2345569133759)
step: 12310 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(3.9895015), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07301385216414928), 'actor_loss': np.float64(-0.9962649822235108), 'hyper_actor_loss': np.float64(0.0024230416631326078), 'behavior_loss': np.float64(4.011923813819886)}

Episode step 12320, time diff 0.7113356590270996, total time dif 917.9405386447906)
step: 12320 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.7653401), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06733949072659015), 'actor_loss': np.float64(-0.982904452085495), 'hyper_actor_loss': np.float64(0.002444792608730495), 'behavior_loss': np.float64(4.298146200180054)}

Episode step 12330, time diff 0.6966385841369629, total time dif 918.6518743038177)
step: 12330 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(1.942406), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06586609147489071), 'actor_loss': np.float64(-0.9921225845813751), 'hyper_actor_loss': np.float64(0.0029302404960617422), 'behavior_loss': np.float64(3.678302001953125)}

Episode step 12340, time diff 0.7041018009185791, total time dif 919.3485128879547)
step: 12340 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(1.6325486), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07147115394473076), 'actor_loss': np.float64(-0.9855511426925659), 'hyper_actor_loss': np.float64(0.0036498869536444544), 'behavior_loss': np.float64(3.5536604642868044)}

Episode step 12350, time diff 0.7175564765930176, total time dif 920.0526146888733)
step: 12350 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(3.30444), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0716870803385973), 'actor_loss': np.float64(-0.9824911057949066), 'hyper_actor_loss': np.float64(0.004656314849853516), 'behavior_loss': np.float64(2.8536837100982666)}

Episode step 12360, time diff 0.6926157474517822, total time dif 920.7701711654663)
step: 12360 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(3.4930968), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08235682249069214), 'actor_loss': np.float64(-1.0072542428970337), 'hyper_actor_loss': np.float64(0.005888448562473059), 'behavior_loss': np.float64(2.43211225271225)}

Episode step 12370, time diff 0.6862082481384277, total time dif 921.4627869129181)
step: 12370 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.6277108), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07851486429572105), 'actor_loss': np.float64(-1.0106791615486146), 'hyper_actor_loss': np.float64(0.0061678561381995674), 'behavior_loss': np.float64(2.0409337401390077)}

Episode step 12380, time diff 0.731393575668335, total time dif 922.1489951610565)
step: 12380 @ episode report: {'average_total_reward': np.float32(8.975556), 'reward_variance': np.float32(1.9456985), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07711459193378686), 'actor_loss': np.float64(-1.012177789211273), 'hyper_actor_loss': np.float64(0.005766864493489265), 'behavior_loss': np.float64(2.0417094469070434)}

Episode step 12390, time diff 0.681818962097168, total time dif 922.8803887367249)
step: 12390 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(2.4598076), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0821998480707407), 'actor_loss': np.float64(-0.984780240058899), 'hyper_actor_loss': np.float64(0.005347526166588068), 'behavior_loss': np.float64(2.1440372586250307)}

Episode step 12400, time diff 0.8763735294342041, total time dif 923.562207698822)
step: 12400 @ episode report: {'average_total_reward': np.float32(10.64889), 'reward_variance': np.float32(3.669413), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07595380172133445), 'actor_loss': np.float64(-1.0100514352321626), 'hyper_actor_loss': np.float64(0.004998274007812142), 'behavior_loss': np.float64(1.8455803513526916)}

Episode step 12410, time diff 0.732459306716919, total time dif 924.4385812282562)
step: 12410 @ episode report: {'average_total_reward': np.float32(9.787779), 'reward_variance': np.float32(3.1782339), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07933446653187275), 'actor_loss': np.float64(-0.983905416727066), 'hyper_actor_loss': np.float64(0.004704028228297829), 'behavior_loss': np.float64(2.117936301231384)}

Episode step 12420, time diff 0.7514326572418213, total time dif 925.1710405349731)
step: 12420 @ episode report: {'average_total_reward': np.float32(9.451112), 'reward_variance': np.float32(5.8744245), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07665986977517605), 'actor_loss': np.float64(-0.974737697839737), 'hyper_actor_loss': np.float64(0.004576746234670281), 'behavior_loss': np.float64(1.9560667514801025)}

Episode step 12430, time diff 0.7002527713775635, total time dif 925.922473192215)
step: 12430 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(1.3584707), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07384730651974677), 'actor_loss': np.float64(-1.0010971188545228), 'hyper_actor_loss': np.float64(0.004262281232513487), 'behavior_loss': np.float64(2.046484184265137)}

Episode step 12440, time diff 0.7092907428741455, total time dif 926.6227259635925)
step: 12440 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.2619805), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07586864866316319), 'actor_loss': np.float64(-0.9546904981136322), 'hyper_actor_loss': np.float64(0.003464322816580534), 'behavior_loss': np.float64(2.35986385345459)}

Episode step 12450, time diff 0.7309558391571045, total time dif 927.3320167064667)
step: 12450 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(1.8604145), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0660996150225401), 'actor_loss': np.float64(-0.9624828398227692), 'hyper_actor_loss': np.float64(0.002937522390857339), 'behavior_loss': np.float64(2.078592574596405)}

Episode step 12460, time diff 0.6872720718383789, total time dif 928.0629725456238)
step: 12460 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(4.608489), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07674560993909836), 'actor_loss': np.float64(-0.9859957277774811), 'hyper_actor_loss': np.float64(0.0025979122146964073), 'behavior_loss': np.float64(2.064847195148468)}

Episode step 12470, time diff 0.6877775192260742, total time dif 928.7502446174622)
step: 12470 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.0815666), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0874277487397194), 'actor_loss': np.float64(-0.9954042077064514), 'hyper_actor_loss': np.float64(0.0025977766141295432), 'behavior_loss': np.float64(2.2946002006530763)}

Episode step 12480, time diff 0.7212533950805664, total time dif 929.4380221366882)
step: 12480 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(4.8195753), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07661751210689545), 'actor_loss': np.float64(-0.9969502449035644), 'hyper_actor_loss': np.float64(0.0026681934483349322), 'behavior_loss': np.float64(2.3502182006835937)}

Episode step 12490, time diff 0.6872191429138184, total time dif 930.1592755317688)
step: 12490 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.5348108), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06917285732924938), 'actor_loss': np.float64(-0.9874013960361481), 'hyper_actor_loss': np.float64(0.002827917691320181), 'behavior_loss': np.float64(2.259102463722229)}

Episode step 12500, time diff 0.7009789943695068, total time dif 930.8464946746826)
step: 12500 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.3008103), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07871788740158081), 'actor_loss': np.float64(-0.9717016577720642), 'hyper_actor_loss': np.float64(0.002861331100575626), 'behavior_loss': np.float64(2.1119645953178408)}

Episode step 12510, time diff 0.7059693336486816, total time dif 931.5474736690521)
step: 12510 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.647162), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08116879127919674), 'actor_loss': np.float64(-0.9984683394432068), 'hyper_actor_loss': np.float64(0.0029176756273955108), 'behavior_loss': np.float64(2.1453286528587343)}

Episode step 12520, time diff 0.6948776245117188, total time dif 932.2534430027008)
step: 12520 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(3.8449998), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08494991809129715), 'actor_loss': np.float64(-1.029794204235077), 'hyper_actor_loss': np.float64(0.0030103927478194238), 'behavior_loss': np.float64(2.2046647310256957)}

Episode step 12530, time diff 0.7079062461853027, total time dif 932.9483206272125)
step: 12530 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.7219622), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07439882531762124), 'actor_loss': np.float64(-0.9907797396183013), 'hyper_actor_loss': np.float64(0.003085783589631319), 'behavior_loss': np.float64(2.2480368852615356)}

Episode step 12540, time diff 0.7180709838867188, total time dif 933.6562268733978)
step: 12540 @ episode report: {'average_total_reward': np.float32(11.86889), 'reward_variance': np.float32(2.8311057), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07297458536922932), 'actor_loss': np.float64(-0.9716597676277161), 'hyper_actor_loss': np.float64(0.003121417644433677), 'behavior_loss': np.float64(1.9344488501548767)}

Episode step 12550, time diff 0.6953010559082031, total time dif 934.3742978572845)
step: 12550 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(4.6884756), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0792525339871645), 'actor_loss': np.float64(-1.0124066293239593), 'hyper_actor_loss': np.float64(0.0031627642922103403), 'behavior_loss': np.float64(1.9973578095436095)}

Episode step 12560, time diff 0.8837497234344482, total time dif 935.0695989131927)
step: 12560 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(3.9959996), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07692740820348262), 'actor_loss': np.float64(-0.9937391996383667), 'hyper_actor_loss': np.float64(0.0032680561766028405), 'behavior_loss': np.float64(2.184750235080719)}

Episode step 12570, time diff 0.68487548828125, total time dif 935.9533486366272)
step: 12570 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(3.2972298), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06952078230679035), 'actor_loss': np.float64(-0.975397652387619), 'hyper_actor_loss': np.float64(0.0033279160968959333), 'behavior_loss': np.float64(2.114038825035095)}

Episode step 12580, time diff 0.6929960250854492, total time dif 936.6382241249084)
step: 12580 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.9040751), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07006868794560432), 'actor_loss': np.float64(-0.9878294050693512), 'hyper_actor_loss': np.float64(0.003234336758032441), 'behavior_loss': np.float64(1.9969732642173768)}

Episode step 12590, time diff 0.7650861740112305, total time dif 937.3312201499939)
step: 12590 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(4.0675683), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061358536779880526), 'actor_loss': np.float64(-1.0049038708209992), 'hyper_actor_loss': np.float64(0.0031124153407290577), 'behavior_loss': np.float64(1.8859964847564696)}

Episode step 12600, time diff 0.7237722873687744, total time dif 938.0963063240051)
step: 12600 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(3.260446), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08704388737678528), 'actor_loss': np.float64(-0.9928199291229248), 'hyper_actor_loss': np.float64(0.0029656426049768925), 'behavior_loss': np.float64(2.0856855511665344)}

Episode step 12610, time diff 0.7304208278656006, total time dif 938.8200786113739)
step: 12610 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(3.7572885), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08130511306226254), 'actor_loss': np.float64(-1.0203266263008117), 'hyper_actor_loss': np.float64(0.002948221261613071), 'behavior_loss': np.float64(1.9733293056488037)}

Episode step 12620, time diff 0.7457365989685059, total time dif 939.5504994392395)
step: 12620 @ episode report: {'average_total_reward': np.float32(10.422224), 'reward_variance': np.float32(2.5617528), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06728606633841991), 'actor_loss': np.float64(-0.9885982513427735), 'hyper_actor_loss': np.float64(0.0028243229724466802), 'behavior_loss': np.float64(2.0859027743339538)}

Episode step 12630, time diff 0.6921896934509277, total time dif 940.296236038208)
step: 12630 @ episode report: {'average_total_reward': np.float32(10.622224), 'reward_variance': np.float32(1.445507), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0828146442770958), 'actor_loss': np.float64(-1.008882111310959), 'hyper_actor_loss': np.float64(0.002682256302796304), 'behavior_loss': np.float64(1.9677239179611206)}

Episode step 12640, time diff 0.7161805629730225, total time dif 940.9884257316589)
step: 12640 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(2.1526928), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06998109146952629), 'actor_loss': np.float64(-1.0051920235157012), 'hyper_actor_loss': np.float64(0.0027839574497193096), 'behavior_loss': np.float64(1.944817066192627)}

Episode step 12650, time diff 0.706104040145874, total time dif 941.704606294632)
step: 12650 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(1.1712407), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05942461676895618), 'actor_loss': np.float64(-0.9784426331520081), 'hyper_actor_loss': np.float64(0.0029044660041108727), 'behavior_loss': np.float64(1.958171796798706)}

Episode step 12660, time diff 0.7100579738616943, total time dif 942.4107103347778)
step: 12660 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.1683717), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07519045099616051), 'actor_loss': np.float64(-0.9869892299175262), 'hyper_actor_loss': np.float64(0.002920857397839427), 'behavior_loss': np.float64(2.12035870552063)}

Episode step 12670, time diff 0.7021980285644531, total time dif 943.1207683086395)
step: 12670 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(1.989088), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07002636231482029), 'actor_loss': np.float64(-1.0082927763462066), 'hyper_actor_loss': np.float64(0.003166412888094783), 'behavior_loss': np.float64(1.8665033340454102)}

Episode step 12680, time diff 0.6767110824584961, total time dif 943.822966337204)
step: 12680 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(0.8794383), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.075904094055295), 'actor_loss': np.float64(-0.9913169980049134), 'hyper_actor_loss': np.float64(0.003262210241518915), 'behavior_loss': np.float64(2.0976110458374024)}

Episode step 12690, time diff 0.7015552520751953, total time dif 944.4996774196625)
step: 12690 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(1.7185795), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0754257656633854), 'actor_loss': np.float64(-0.9805176615715027), 'hyper_actor_loss': np.float64(0.003083668719045818), 'behavior_loss': np.float64(1.9986234068870545)}

Episode step 12700, time diff 0.7262399196624756, total time dif 945.2012326717377)
step: 12700 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.583106), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07538952566683292), 'actor_loss': np.float64(-0.9949814200401306), 'hyper_actor_loss': np.float64(0.002893484872765839), 'behavior_loss': np.float64(1.9863638758659363)}

Episode step 12710, time diff 0.6825201511383057, total time dif 945.9274725914001)
step: 12710 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.9314635), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07326439768075943), 'actor_loss': np.float64(-0.9825670182704925), 'hyper_actor_loss': np.float64(0.002593085006810725), 'behavior_loss': np.float64(1.980621588230133)}

Episode step 12720, time diff 0.7015488147735596, total time dif 946.6099927425385)
step: 12720 @ episode report: {'average_total_reward': np.float32(8.538889), 'reward_variance': np.float32(2.71729), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06666718572378158), 'actor_loss': np.float64(-0.9770951747894288), 'hyper_actor_loss': np.float64(0.0023247580393217504), 'behavior_loss': np.float64(1.7615322113037108)}

Episode step 12730, time diff 0.8595054149627686, total time dif 947.311541557312)
step: 12730 @ episode report: {'average_total_reward': np.float32(9.400002), 'reward_variance': np.float32(4.669062), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08320138975977898), 'actor_loss': np.float64(-0.9990869879722595), 'hyper_actor_loss': np.float64(0.0020701869274489582), 'behavior_loss': np.float64(1.768141233921051)}

Episode step 12740, time diff 0.7253777980804443, total time dif 948.1710469722748)
step: 12740 @ episode report: {'average_total_reward': np.float32(9.263333), 'reward_variance': np.float32(2.7149143), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0768122710287571), 'actor_loss': np.float64(-1.018943578004837), 'hyper_actor_loss': np.float64(0.0019733471213839947), 'behavior_loss': np.float64(1.7046417832374572)}

Episode step 12750, time diff 0.7246849536895752, total time dif 948.8964247703552)
step: 12750 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(1.4327718), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08321887440979481), 'actor_loss': np.float64(-0.9700816392898559), 'hyper_actor_loss': np.float64(0.0019423411344178022), 'behavior_loss': np.float64(2.1898132085800173)}

Episode step 12760, time diff 0.7298626899719238, total time dif 949.6211097240448)
step: 12760 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(1.2604938), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0778643749654293), 'actor_loss': np.float64(-1.0244277954101562), 'hyper_actor_loss': np.float64(0.0019087270949967206), 'behavior_loss': np.float64(1.6475337862968444)}

Episode step 12770, time diff 0.7113075256347656, total time dif 950.3509724140167)
step: 12770 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.0172157), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07428680658340454), 'actor_loss': np.float64(-1.003402453660965), 'hyper_actor_loss': np.float64(0.001923577894922346), 'behavior_loss': np.float64(1.8809194087982177)}

Episode step 12780, time diff 0.7185125350952148, total time dif 951.0622799396515)
step: 12780 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.0205877), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07827764563262463), 'actor_loss': np.float64(-0.9920705199241638), 'hyper_actor_loss': np.float64(0.0019070677575655282), 'behavior_loss': np.float64(1.7780669689178468)}

Episode step 12790, time diff 0.6914875507354736, total time dif 951.7807924747467)
step: 12790 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.4370823), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06813765596598387), 'actor_loss': np.float64(-0.998249888420105), 'hyper_actor_loss': np.float64(0.0017881585634313523), 'behavior_loss': np.float64(1.6547172784805297)}

Episode step 12800, time diff 0.7083320617675781, total time dif 952.4722800254822)
step: 12800 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(6.447286), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08153188303112983), 'actor_loss': np.float64(-0.9878986895084381), 'hyper_actor_loss': np.float64(0.001702593546360731), 'behavior_loss': np.float64(1.9150227665901185)}

Episode step 12810, time diff 0.7300252914428711, total time dif 953.1806120872498)
step: 12810 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.0337303), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06955212503671646), 'actor_loss': np.float64(-1.0042252898216248), 'hyper_actor_loss': np.float64(0.001607554464135319), 'behavior_loss': np.float64(1.8843736410140992)}

Episode step 12820, time diff 0.6992266178131104, total time dif 953.9106373786926)
step: 12820 @ episode report: {'average_total_reward': np.float32(9.663333), 'reward_variance': np.float32(2.949532), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08614110127091408), 'actor_loss': np.float64(-1.0036215424537658), 'hyper_actor_loss': np.float64(0.0015498194494284689), 'behavior_loss': np.float64(1.9014100790023805)}

Episode step 12830, time diff 0.6874570846557617, total time dif 954.6098639965057)
step: 12830 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(6.627828), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0669298717752099), 'actor_loss': np.float64(-0.988152700662613), 'hyper_actor_loss': np.float64(0.0015268537797965109), 'behavior_loss': np.float64(1.846084976196289)}

Episode step 12840, time diff 0.685755729675293, total time dif 955.2973210811615)
step: 12840 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.9557788), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0678010130301118), 'actor_loss': np.float64(-0.9849233806133271), 'hyper_actor_loss': np.float64(0.0014815241564065218), 'behavior_loss': np.float64(1.8613902568817138)}

Episode step 12850, time diff 0.7400457859039307, total time dif 955.9830768108368)
step: 12850 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(2.4082837), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07815624848008156), 'actor_loss': np.float64(-0.9787308931350708), 'hyper_actor_loss': np.float64(0.001480877329595387), 'behavior_loss': np.float64(1.809804379940033)}

Episode step 12860, time diff 0.6923696994781494, total time dif 956.7231225967407)
step: 12860 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.2050318), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07247505262494087), 'actor_loss': np.float64(-1.0022946834564208), 'hyper_actor_loss': np.float64(0.0015048780012875795), 'behavior_loss': np.float64(1.9139148354530335)}

Episode step 12870, time diff 0.7047760486602783, total time dif 957.4154922962189)
step: 12870 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(3.3539128), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07989187091588974), 'actor_loss': np.float64(-1.0001587867736816), 'hyper_actor_loss': np.float64(0.001593428268097341), 'behavior_loss': np.float64(1.9648323774337768)}

Episode step 12880, time diff 0.7172451019287109, total time dif 958.1202683448792)
step: 12880 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.8016589), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07385416999459267), 'actor_loss': np.float64(-0.982066524028778), 'hyper_actor_loss': np.float64(0.0016964377835392952), 'behavior_loss': np.float64(1.7934733867645263)}

Episode step 12890, time diff 0.6894235610961914, total time dif 958.8375134468079)
step: 12890 @ episode report: {'average_total_reward': np.float32(8.041112), 'reward_variance': np.float32(1.5309144), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07767600305378437), 'actor_loss': np.float64(-0.9984802901744843), 'hyper_actor_loss': np.float64(0.0018209866830147803), 'behavior_loss': np.float64(1.8239464282989502)}

Episode step 12900, time diff 0.8575239181518555, total time dif 959.526937007904)
step: 12900 @ episode report: {'average_total_reward': np.float32(11.283335), 'reward_variance': np.float32(2.3208957), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07031007297337055), 'actor_loss': np.float64(-0.9909884512424469), 'hyper_actor_loss': np.float64(0.0019180827424861492), 'behavior_loss': np.float64(1.9981452822685242)}

Episode step 12910, time diff 0.7230539321899414, total time dif 960.3844609260559)
step: 12910 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.74282), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07139645777642727), 'actor_loss': np.float64(-0.9833039402961731), 'hyper_actor_loss': np.float64(0.0019777343259193003), 'behavior_loss': np.float64(1.9298791885375977)}

Episode step 12920, time diff 0.7002835273742676, total time dif 961.1075148582458)
step: 12920 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(3.7173836), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07231658771634102), 'actor_loss': np.float64(-0.9836367249488831), 'hyper_actor_loss': np.float64(0.0019430163083598018), 'behavior_loss': np.float64(1.970717763900757)}

Episode step 12930, time diff 0.7131710052490234, total time dif 961.8077983856201)
step: 12930 @ episode report: {'average_total_reward': np.float32(9.114445), 'reward_variance': np.float32(1.6594832), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058554339967668054), 'actor_loss': np.float64(-0.9741404473781585), 'hyper_actor_loss': np.float64(0.0017424831865355372), 'behavior_loss': np.float64(1.8766483426094056)}

Episode step 12940, time diff 0.7130703926086426, total time dif 962.5209693908691)
step: 12940 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(2.7036839), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06754381321370602), 'actor_loss': np.float64(-0.9647291004657745), 'hyper_actor_loss': np.float64(0.0015466227778233588), 'behavior_loss': np.float64(1.9215622544288635)}

Episode step 12950, time diff 0.6897857189178467, total time dif 963.2340397834778)
step: 12950 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(4.9269996), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06985873281955719), 'actor_loss': np.float64(-0.9850326299667358), 'hyper_actor_loss': np.float64(0.0014621071284636855), 'behavior_loss': np.float64(1.9297177076339722)}

Episode step 12960, time diff 0.738666296005249, total time dif 963.9238255023956)
step: 12960 @ episode report: {'average_total_reward': np.float32(9.675555), 'reward_variance': np.float32(3.6399465), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.288889), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07296571172773839), 'actor_loss': np.float64(-0.9879536390304565), 'hyper_actor_loss': np.float64(0.0014016123954206706), 'behavior_loss': np.float64(1.9537164211273192)}

Episode step 12970, time diff 0.7221128940582275, total time dif 964.6624917984009)
step: 12970 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(4.5104446), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07097126804292202), 'actor_loss': np.float64(-0.9915718495845794), 'hyper_actor_loss': np.float64(0.0013582564424723387), 'behavior_loss': np.float64(1.8607186913490295)}

Episode step 12980, time diff 0.7314732074737549, total time dif 965.3846046924591)
step: 12980 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.618005), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08592871986329556), 'actor_loss': np.float64(-0.9965825617313385), 'hyper_actor_loss': np.float64(0.0013815775513648988), 'behavior_loss': np.float64(2.0768364787101747)}

Episode step 12990, time diff 0.7126176357269287, total time dif 966.1160778999329)
step: 12990 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.7796847), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06385421156883239), 'actor_loss': np.float64(-0.9725635826587677), 'hyper_actor_loss': np.float64(0.0013274640310555696), 'behavior_loss': np.float64(2.0325864791870116)}

Episode step 13000, time diff 0.6898741722106934, total time dif 966.8286955356598)
step: 13000 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(2.2036595), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08606137186288834), 'actor_loss': np.float64(-0.9861982643604279), 'hyper_actor_loss': np.float64(0.0012923787697218358), 'behavior_loss': np.float64(1.803781259059906)}

Episode step 13010, time diff 0.7224276065826416, total time dif 967.5185697078705)
step: 13010 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(4.398692), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06622861996293068), 'actor_loss': np.float64(-1.0053884446620942), 'hyper_actor_loss': np.float64(0.0012490184046328068), 'behavior_loss': np.float64(1.7387310028076173)}

Episode step 13020, time diff 0.6765446662902832, total time dif 968.2409973144531)
step: 13020 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(3.822248), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07195289637893439), 'actor_loss': np.float64(-0.9590804100036621), 'hyper_actor_loss': np.float64(0.0012236128211952745), 'behavior_loss': np.float64(1.9812312722206116)}

Episode step 13030, time diff 0.6999588012695312, total time dif 968.9175419807434)
step: 13030 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(1.7002983), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07215348146855831), 'actor_loss': np.float64(-0.9795237123966217), 'hyper_actor_loss': np.float64(0.001116481446661055), 'behavior_loss': np.float64(1.9664743423461915)}

Episode step 13040, time diff 0.7173950672149658, total time dif 969.6175007820129)
step: 13040 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(1.3096503), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07193543054163457), 'actor_loss': np.float64(-0.9925690710544586), 'hyper_actor_loss': np.float64(0.0010638777748681605), 'behavior_loss': np.float64(1.8144806504249573)}

Episode step 13050, time diff 0.6933820247650146, total time dif 970.3348958492279)
step: 13050 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(2.7394385), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06342444084584713), 'actor_loss': np.float64(-0.9691438555717469), 'hyper_actor_loss': np.float64(0.0010488078754860909), 'behavior_loss': np.float64(1.9062251329421998)}

Episode step 13060, time diff 0.852942943572998, total time dif 971.0282778739929)
step: 13060 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.8281183), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0786938402801752), 'actor_loss': np.float64(-0.9756171464920044), 'hyper_actor_loss': np.float64(0.0010820574418175966), 'behavior_loss': np.float64(1.8558955192565918)}

Episode step 13070, time diff 0.6942968368530273, total time dif 971.8812208175659)
step: 13070 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(1.1956854), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0712563455104828), 'actor_loss': np.float64(-1.003728997707367), 'hyper_actor_loss': np.float64(0.0010898166801780462), 'behavior_loss': np.float64(1.87282053232193)}

Episode step 13080, time diff 0.6917955875396729, total time dif 972.575517654419)
step: 13080 @ episode report: {'average_total_reward': np.float32(9.224444), 'reward_variance': np.float32(1.5334275), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0780032992362976), 'actor_loss': np.float64(-0.9688625156879425), 'hyper_actor_loss': np.float64(0.001053540682187304), 'behavior_loss': np.float64(2.1032656908035277)}

Episode step 13090, time diff 0.7365920543670654, total time dif 973.2673132419586)
step: 13090 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(3.2767854), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06520943492650985), 'actor_loss': np.float64(-0.9813976109027862), 'hyper_actor_loss': np.float64(0.0010171612084377557), 'behavior_loss': np.float64(2.0025979042053224)}

Episode step 13100, time diff 0.6994137763977051, total time dif 974.0039052963257)
step: 13100 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(3.52565), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07298230826854706), 'actor_loss': np.float64(-1.0029939115047455), 'hyper_actor_loss': np.float64(0.0010115087497979403), 'behavior_loss': np.float64(1.7566824793815612)}

Episode step 13110, time diff 0.689211368560791, total time dif 974.7033190727234)
step: 13110 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.5747118), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07116485275328159), 'actor_loss': np.float64(-0.9920213639736175), 'hyper_actor_loss': np.float64(0.0010620993736665696), 'behavior_loss': np.float64(1.8945486783981322)}

Episode step 13120, time diff 0.724423885345459, total time dif 975.3925304412842)
step: 13120 @ episode report: {'average_total_reward': np.float32(10.871111), 'reward_variance': np.float32(3.315387), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08282222002744674), 'actor_loss': np.float64(-0.9879788935184479), 'hyper_actor_loss': np.float64(0.0010628403280861676), 'behavior_loss': np.float64(1.8198974609375)}

Episode step 13130, time diff 0.7040584087371826, total time dif 976.1169543266296)
step: 13130 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(3.4424953), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07523481585085393), 'actor_loss': np.float64(-1.0066421627998352), 'hyper_actor_loss': np.float64(0.0011460791109129786), 'behavior_loss': np.float64(1.676334810256958)}

Episode step 13140, time diff 0.698002815246582, total time dif 976.8210127353668)
step: 13140 @ episode report: {'average_total_reward': np.float32(10.373335), 'reward_variance': np.float32(5.1341777), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06739683486521245), 'actor_loss': np.float64(-0.9949899733066558), 'hyper_actor_loss': np.float64(0.0011423363466747105), 'behavior_loss': np.float64(1.728162705898285)}

Episode step 13150, time diff 0.7310400009155273, total time dif 977.5190155506134)
step: 13150 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.3438827), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06937368959188461), 'actor_loss': np.float64(-0.972896271944046), 'hyper_actor_loss': np.float64(0.001154303376097232), 'behavior_loss': np.float64(1.888422679901123)}

Episode step 13160, time diff 0.6846764087677002, total time dif 978.2500555515289)
step: 13160 @ episode report: {'average_total_reward': np.float32(9.74889), 'reward_variance': np.float32(1.3966223), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06164736039936543), 'actor_loss': np.float64(-0.981731653213501), 'hyper_actor_loss': np.float64(0.0011926406645216049), 'behavior_loss': np.float64(2.0786189675331115)}

Episode step 13170, time diff 0.7056059837341309, total time dif 978.9347319602966)
step: 13170 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.5429153), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0721635203808546), 'actor_loss': np.float64(-0.9725092828273774), 'hyper_actor_loss': np.float64(0.001189178042113781), 'behavior_loss': np.float64(1.8592243432998656)}

Episode step 13180, time diff 0.7148973941802979, total time dif 979.6403379440308)
step: 13180 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.7743471), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06838799472898245), 'actor_loss': np.float64(-0.9877152144908905), 'hyper_actor_loss': np.float64(0.0012166673666797578), 'behavior_loss': np.float64(1.8286965131759643)}

Episode step 13190, time diff 0.6701734066009521, total time dif 980.3552353382111)
step: 13190 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(2.1568015), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07823820598423481), 'actor_loss': np.float64(-0.989197313785553), 'hyper_actor_loss': np.float64(0.001331662351731211), 'behavior_loss': np.float64(1.7770328044891357)}

Episode step 13200, time diff 0.6717205047607422, total time dif 981.025408744812)
step: 13200 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(5.012228), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07370831854641438), 'actor_loss': np.float64(-0.9890651881694794), 'hyper_actor_loss': np.float64(0.001291595713701099), 'behavior_loss': np.float64(1.9273404955863953)}

Episode step 13210, time diff 0.705254077911377, total time dif 981.6971292495728)
step: 13210 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(4.964348), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06044172532856464), 'actor_loss': np.float64(-0.9695192635059356), 'hyper_actor_loss': np.float64(0.001229757978580892), 'behavior_loss': np.float64(1.79220312833786)}

Episode step 13220, time diff 0.6726453304290771, total time dif 982.4023833274841)
step: 13220 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(0.9772953), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05276294555515051), 'actor_loss': np.float64(-0.9599313259124755), 'hyper_actor_loss': np.float64(0.0012353353900834917), 'behavior_loss': np.float64(1.7456987500190735)}

Episode step 13230, time diff 0.8624565601348877, total time dif 983.0750286579132)
step: 13230 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(4.135903), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0586607038974762), 'actor_loss': np.float64(-0.9453474283218384), 'hyper_actor_loss': np.float64(0.0011771112331189215), 'behavior_loss': np.float64(2.033729577064514)}

Episode step 13240, time diff 0.7052607536315918, total time dif 983.9374852180481)
step: 13240 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(3.9662728), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0725692205131054), 'actor_loss': np.float64(-0.98105388879776), 'hyper_actor_loss': np.float64(0.0010951276461128145), 'behavior_loss': np.float64(1.843912708759308)}

Episode step 13250, time diff 0.7057993412017822, total time dif 984.6427459716797)
step: 13250 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(4.1560755), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06915141679346562), 'actor_loss': np.float64(-0.9953962385654449), 'hyper_actor_loss': np.float64(0.0010530473140534014), 'behavior_loss': np.float64(1.8876402854919434)}

Episode step 13260, time diff 0.7228608131408691, total time dif 985.3485453128815)
step: 13260 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(3.3539116), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06399584058672189), 'actor_loss': np.float64(-0.9650581061840058), 'hyper_actor_loss': np.float64(0.0010701334394980222), 'behavior_loss': np.float64(1.84578378200531)}

Episode step 13270, time diff 0.6840610504150391, total time dif 986.0714061260223)
step: 13270 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.1029391), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06834998689591884), 'actor_loss': np.float64(-0.9745015382766724), 'hyper_actor_loss': np.float64(0.0010530835017561913), 'behavior_loss': np.float64(1.7819683074951171)}

Episode step 13280, time diff 0.7081434726715088, total time dif 986.7554671764374)
step: 13280 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.8585143), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.533333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06988617368042468), 'actor_loss': np.float64(-0.9847210705280304), 'hyper_actor_loss': np.float64(0.0010269885940942913), 'behavior_loss': np.float64(2.066731894016266)}

Episode step 13290, time diff 0.7559740543365479, total time dif 987.4636106491089)
step: 13290 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(3.3613193), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06641232986003161), 'actor_loss': np.float64(-0.965629267692566), 'hyper_actor_loss': np.float64(0.0010421207058243453), 'behavior_loss': np.float64(1.8552891492843628)}

Episode step 13300, time diff 0.6908645629882812, total time dif 988.2195847034454)
step: 13300 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(4.3648844), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06069363430142403), 'actor_loss': np.float64(-0.9806064605712891), 'hyper_actor_loss': np.float64(0.0010642538429237903), 'behavior_loss': np.float64(1.8182605385780335)}

Episode step 13310, time diff 0.6890525817871094, total time dif 988.9104492664337)
step: 13310 @ episode report: {'average_total_reward': np.float32(9.551112), 'reward_variance': np.float32(3.8801033), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06001506969332695), 'actor_loss': np.float64(-0.9922798573970795), 'hyper_actor_loss': np.float64(0.0011139130918309092), 'behavior_loss': np.float64(1.8801441550254823)}

Episode step 13320, time diff 0.7399792671203613, total time dif 989.5995018482208)
step: 13320 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(1.6886966), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07703807838261127), 'actor_loss': np.float64(-0.9892174899578094), 'hyper_actor_loss': np.float64(0.0012697058729827404), 'behavior_loss': np.float64(1.952458930015564)}

Episode step 13330, time diff 0.7069740295410156, total time dif 990.3394811153412)
step: 13330 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(2.0491667), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07745240442454815), 'actor_loss': np.float64(-1.0047427892684937), 'hyper_actor_loss': np.float64(0.001289391051977873), 'behavior_loss': np.float64(1.8370489478111267)}

Episode step 13340, time diff 0.6923074722290039, total time dif 991.0464551448822)
step: 13340 @ episode report: {'average_total_reward': np.float32(10.422224), 'reward_variance': np.float32(3.5966923), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186521664261818), 'actor_loss': np.float64(-0.990452516078949), 'hyper_actor_loss': np.float64(0.0013014967669732868), 'behavior_loss': np.float64(1.7753679871559143)}

Episode step 13350, time diff 0.7004954814910889, total time dif 991.7387626171112)
step: 13350 @ episode report: {'average_total_reward': np.float32(10.758889), 'reward_variance': np.float32(5.4372606), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062086607329547405), 'actor_loss': np.float64(-0.9863506197929383), 'hyper_actor_loss': np.float64(0.0012416266952641308), 'behavior_loss': np.float64(1.598343014717102)}

Episode step 13360, time diff 0.6971478462219238, total time dif 992.4392580986023)
step: 13360 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(1.7500111), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06763149499893188), 'actor_loss': np.float64(-0.9655291736125946), 'hyper_actor_loss': np.float64(0.0011427708086557686), 'behavior_loss': np.float64(2.1347599983215333)}

Episode step 13370, time diff 0.7182648181915283, total time dif 993.1364059448242)
step: 13370 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(2.5453398), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061266953125596046), 'actor_loss': np.float64(-0.9603183567523956), 'hyper_actor_loss': np.float64(0.0010465396393556149), 'behavior_loss': np.float64(1.7850276947021484)}

Episode step 13380, time diff 0.6646895408630371, total time dif 993.8546707630157)
step: 13380 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(2.251641), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07810068242251873), 'actor_loss': np.float64(-0.9900321662425995), 'hyper_actor_loss': np.float64(0.0010373467404861004), 'behavior_loss': np.float64(2.039283263683319)}

Episode step 13390, time diff 0.8585662841796875, total time dif 994.5193603038788)
step: 13390 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(2.805196), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07739848233759403), 'actor_loss': np.float64(-0.9953105390071869), 'hyper_actor_loss': np.float64(0.0011677314876578748), 'behavior_loss': np.float64(1.8864543437957764)}

Episode step 13400, time diff 0.7203159332275391, total time dif 995.3779265880585)
step: 13400 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(4.5933347), 'max_total_reward': np.float32(14.144445), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062881238758564), 'actor_loss': np.float64(-0.9694531917572021), 'hyper_actor_loss': np.float64(0.0011600534431636333), 'behavior_loss': np.float64(1.8425041675567626)}

Episode step 13410, time diff 0.6810379028320312, total time dif 996.098242521286)
step: 13410 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.7871804), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06078023090958595), 'actor_loss': np.float64(-0.9610431730747223), 'hyper_actor_loss': np.float64(0.0011360101751051843), 'behavior_loss': np.float64(1.8430433988571167)}

Episode step 13420, time diff 0.7243971824645996, total time dif 996.779280424118)
step: 13420 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(2.6032948), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07666234150528908), 'actor_loss': np.float64(-0.9978572607040406), 'hyper_actor_loss': np.float64(0.0011267987079918384), 'behavior_loss': np.float64(1.6322665452957152)}

Episode step 13430, time diff 0.7470684051513672, total time dif 997.5036776065826)
step: 13430 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(1.2211865), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0648650225251913), 'actor_loss': np.float64(-0.9891339898109436), 'hyper_actor_loss': np.float64(0.0011505182716064154), 'behavior_loss': np.float64(1.7068535447120667)}

Episode step 13440, time diff 0.6802427768707275, total time dif 998.250746011734)
step: 13440 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(1.9867175), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06351512894034386), 'actor_loss': np.float64(-0.9810467064380646), 'hyper_actor_loss': np.float64(0.0011051087756641209), 'behavior_loss': np.float64(1.8952270865440368)}

Episode step 13450, time diff 0.7211887836456299, total time dif 998.9309887886047)
step: 13450 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(3.527782), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07142940871417522), 'actor_loss': np.float64(-0.9872717320919037), 'hyper_actor_loss': np.float64(0.0010987419867888094), 'behavior_loss': np.float64(1.834579348564148)}

Episode step 13460, time diff 0.6919348239898682, total time dif 999.6521775722504)
step: 13460 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.233689), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06216113101691008), 'actor_loss': np.float64(-0.9770512282848358), 'hyper_actor_loss': np.float64(0.0011368114151991903), 'behavior_loss': np.float64(1.7844089269638062)}

Episode step 13470, time diff 0.6942901611328125, total time dif 1000.3441123962402)
step: 13470 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(1.4909494), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08167250044643878), 'actor_loss': np.float64(-0.9814833641052246), 'hyper_actor_loss': np.float64(0.0011542370659299195), 'behavior_loss': np.float64(1.7475899934768677)}

Episode step 13480, time diff 0.7209768295288086, total time dif 1001.038402557373)
step: 13480 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.717279), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06739197485148907), 'actor_loss': np.float64(-0.985219168663025), 'hyper_actor_loss': np.float64(0.0011995405424386264), 'behavior_loss': np.float64(1.7505624890327454)}

Episode step 13490, time diff 0.7154226303100586, total time dif 1001.7593793869019)
step: 13490 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(4.024248), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.064230190590024), 'actor_loss': np.float64(-0.97042236328125), 'hyper_actor_loss': np.float64(0.0012195744668133557), 'behavior_loss': np.float64(1.8188716053962708)}

Episode step 13500, time diff 0.7442190647125244, total time dif 1002.4748020172119)
step: 13500 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(5.4726477), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08268715478479863), 'actor_loss': np.float64(-0.9703391134738922), 'hyper_actor_loss': np.float64(0.0012400990352034568), 'behavior_loss': np.float64(1.7706453204154968)}

Episode step 13510, time diff 0.6877715587615967, total time dif 1003.2190210819244)
step: 13510 @ episode report: {'average_total_reward': np.float32(9.663334), 'reward_variance': np.float32(2.4597297), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08201779834926129), 'actor_loss': np.float64(-1.0042321622371673), 'hyper_actor_loss': np.float64(0.0012719154125079512), 'behavior_loss': np.float64(1.7907785892486572)}

Episode step 13520, time diff 0.7065844535827637, total time dif 1003.906792640686)
step: 13520 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.0097544), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07830390594899654), 'actor_loss': np.float64(-1.0145797312259675), 'hyper_actor_loss': np.float64(0.0014319476438686251), 'behavior_loss': np.float64(1.8004366040229798)}

Episode step 13530, time diff 0.7089989185333252, total time dif 1004.6133770942688)
step: 13530 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(1.9505676), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08817863129079342), 'actor_loss': np.float64(-0.9836069643497467), 'hyper_actor_loss': np.float64(0.001502658473327756), 'behavior_loss': np.float64(1.8865924954414368)}

Episode step 13540, time diff 0.6981403827667236, total time dif 1005.3223760128021)
step: 13540 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.6724739), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08298603743314743), 'actor_loss': np.float64(-1.0073322653770447), 'hyper_actor_loss': np.float64(0.0014203295344486833), 'behavior_loss': np.float64(1.8104402780532838)}

Episode step 13550, time diff 0.6862196922302246, total time dif 1006.0205163955688)
step: 13550 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(2.667107), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07737496010959148), 'actor_loss': np.float64(-0.9617024242877961), 'hyper_actor_loss': np.float64(0.0013303698506206274), 'behavior_loss': np.float64(1.981020700931549)}

Episode step 13560, time diff 0.8508608341217041, total time dif 1006.7067360877991)
step: 13560 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.0345488), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07556427456438541), 'actor_loss': np.float64(-0.9962076663970947), 'hyper_actor_loss': np.float64(0.0012516359449364245), 'behavior_loss': np.float64(1.638450288772583)}

Episode step 13570, time diff 0.6879122257232666, total time dif 1007.5575969219208)
step: 13570 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(0.9545229), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07319104187190532), 'actor_loss': np.float64(-0.9890983939170838), 'hyper_actor_loss': np.float64(0.0011755295796319841), 'behavior_loss': np.float64(1.8902443885803222)}

Episode step 13580, time diff 0.7165703773498535, total time dif 1008.245509147644)
step: 13580 @ episode report: {'average_total_reward': np.float32(10.646668), 'reward_variance': np.float32(3.958786), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06932395529001951), 'actor_loss': np.float64(-0.9795470833778381), 'hyper_actor_loss': np.float64(0.0011971717583946883), 'behavior_loss': np.float64(1.8775498270988464)}

Episode step 13590, time diff 0.7204430103302002, total time dif 1008.9620795249939)
step: 13590 @ episode report: {'average_total_reward': np.float32(10.122223), 'reward_variance': np.float32(1.8805678), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07409575842320919), 'actor_loss': np.float64(-0.9783714592456818), 'hyper_actor_loss': np.float64(0.000917636405210942), 'behavior_loss': np.float64(1.650196921825409)}

Episode step 13600, time diff 0.6984100341796875, total time dif 1009.6825225353241)
step: 13600 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(7.995839), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06913420669734478), 'actor_loss': np.float64(-0.9883668899536133), 'hyper_actor_loss': np.float64(0.001028548670001328), 'behavior_loss': np.float64(1.7121815085411072)}

Episode step 13610, time diff 0.7246196269989014, total time dif 1010.3809325695038)
step: 13610 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(1.6451366), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07792387790977955), 'actor_loss': np.float64(-0.9681497693061829), 'hyper_actor_loss': np.float64(0.0010354305733926595), 'behavior_loss': np.float64(1.9706246376037597)}

Episode step 13620, time diff 0.753882646560669, total time dif 1011.1055521965027)
step: 13620 @ episode report: {'average_total_reward': np.float32(9.187779), 'reward_variance': np.float32(2.8307033), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06571057215332984), 'actor_loss': np.float64(-0.9829647183418274), 'hyper_actor_loss': np.float64(0.001012819295283407), 'behavior_loss': np.float64(1.8519582152366638)}

Episode step 13630, time diff 0.7170364856719971, total time dif 1011.8594348430634)
step: 13630 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(3.0434768), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059623251855373385), 'actor_loss': np.float64(-0.9796089291572571), 'hyper_actor_loss': np.float64(0.0010067304712720216), 'behavior_loss': np.float64(1.6786490559577942)}

Episode step 13640, time diff 0.7119119167327881, total time dif 1012.5764713287354)
step: 13640 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.9818089), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06919275298714637), 'actor_loss': np.float64(-0.9689951717853547), 'hyper_actor_loss': np.float64(0.0010224731173366307), 'behavior_loss': np.float64(1.7690833449363708)}

Episode step 13650, time diff 0.7098608016967773, total time dif 1013.2883832454681)
step: 13650 @ episode report: {'average_total_reward': np.float32(10.934445), 'reward_variance': np.float32(3.9053454), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808167174458504), 'actor_loss': np.float64(-0.973813945055008), 'hyper_actor_loss': np.float64(0.0010076667182147502), 'behavior_loss': np.float64(1.8540953636169433)}

Episode step 13660, time diff 0.7279136180877686, total time dif 1013.9982440471649)
step: 13660 @ episode report: {'average_total_reward': np.float32(9.636668), 'reward_variance': np.float32(4.2681494), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06267427615821361), 'actor_loss': np.float64(-0.9761193215847015), 'hyper_actor_loss': np.float64(0.001068120147101581), 'behavior_loss': np.float64(1.6563218712806702)}

Episode step 13670, time diff 0.7144434452056885, total time dif 1014.7261576652527)
step: 13670 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(4.0052214), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06918970532715321), 'actor_loss': np.float64(-0.9572758197784423), 'hyper_actor_loss': np.float64(0.0010966090601868927), 'behavior_loss': np.float64(1.8491440176963807)}

Episode step 13680, time diff 0.7343857288360596, total time dif 1015.4406011104584)
step: 13680 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(3.9363422), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07352258265018463), 'actor_loss': np.float64(-0.9904607474803925), 'hyper_actor_loss': np.float64(0.0011281169310677797), 'behavior_loss': np.float64(1.7763401031494142)}

Episode step 13690, time diff 0.712019681930542, total time dif 1016.1749868392944)
step: 13690 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(3.3893142), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.082417156919837), 'actor_loss': np.float64(-0.9926143229007721), 'hyper_actor_loss': np.float64(0.001320825458969921), 'behavior_loss': np.float64(1.5957016229629517)}

Episode step 13700, time diff 0.7166929244995117, total time dif 1016.887006521225)
step: 13700 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(1.1247212), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186552323400974), 'actor_loss': np.float64(-1.0046587824821471), 'hyper_actor_loss': np.float64(0.0015302236191928388), 'behavior_loss': np.float64(1.7083655834197997)}

Episode step 13710, time diff 0.699044942855835, total time dif 1017.6036994457245)
step: 13710 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(1.8559277), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07645106390118599), 'actor_loss': np.float64(-0.982888001203537), 'hyper_actor_loss': np.float64(0.0016944729490205646), 'behavior_loss': np.float64(1.7854450464248657)}

Episode step 13720, time diff 0.6846063137054443, total time dif 1018.3027443885803)
step: 13720 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.1070187), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.052918733842670915), 'actor_loss': np.float64(-0.9618634283542633), 'hyper_actor_loss': np.float64(0.0018126471317373217), 'behavior_loss': np.float64(1.5261708736419677)}

Episode step 13730, time diff 0.864154577255249, total time dif 1018.9873507022858)
step: 13730 @ episode report: {'average_total_reward': np.float32(11.371112), 'reward_variance': np.float32(2.2511408), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08647748045623302), 'actor_loss': np.float64(-0.9925052940845489), 'hyper_actor_loss': np.float64(0.001975218649022281), 'behavior_loss': np.float64(1.7826274752616882)}

Episode step 13740, time diff 0.7004859447479248, total time dif 1019.851505279541)
step: 13740 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(1.9114088), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06968585103750229), 'actor_loss': np.float64(-1.0247049689292909), 'hyper_actor_loss': np.float64(0.0025701266247779133), 'behavior_loss': np.float64(1.6962545275688172)}

Episode step 13750, time diff 0.711573600769043, total time dif 1020.5519912242889)
step: 13750 @ episode report: {'average_total_reward': np.float32(11.258889), 'reward_variance': np.float32(2.076198), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06911854855716229), 'actor_loss': np.float64(-0.9695536017417907), 'hyper_actor_loss': np.float64(0.003063486120663583), 'behavior_loss': np.float64(1.4371403932571412)}

Episode step 13760, time diff 0.6896927356719971, total time dif 1021.263564825058)
step: 13760 @ episode report: {'average_total_reward': np.float32(11.507779), 'reward_variance': np.float32(2.9618049), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07072166502475738), 'actor_loss': np.float64(-0.9999616324901581), 'hyper_actor_loss': np.float64(0.00327981214504689), 'behavior_loss': np.float64(1.3437103271484374)}

Episode step 13770, time diff 0.7308084964752197, total time dif 1021.95325756073)
step: 13770 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(1.3134636), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06986449584364891), 'actor_loss': np.float64(-1.0017205119132995), 'hyper_actor_loss': np.float64(0.0032388739520683883), 'behavior_loss': np.float64(1.3747616052627563)}

Episode step 13780, time diff 0.7366604804992676, total time dif 1022.6840660572052)
step: 13780 @ episode report: {'average_total_reward': np.float32(9.973334), 'reward_variance': np.float32(1.7118559), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07426456287503243), 'actor_loss': np.float64(-0.9967588245868683), 'hyper_actor_loss': np.float64(0.0032538306433707474), 'behavior_loss': np.float64(1.2561640381813048)}

Episode step 13790, time diff 0.7433121204376221, total time dif 1023.4207265377045)
step: 13790 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(2.2310917), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06704302914440632), 'actor_loss': np.float64(-1.019852888584137), 'hyper_actor_loss': np.float64(0.003166311397217214), 'behavior_loss': np.float64(1.281363046169281)}

Episode step 13800, time diff 0.7386012077331543, total time dif 1024.164038658142)
step: 13800 @ episode report: {'average_total_reward': np.float32(10.112223), 'reward_variance': np.float32(2.6293194), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0740924920886755), 'actor_loss': np.float64(-0.9729993283748627), 'hyper_actor_loss': np.float64(0.0030393155524507166), 'behavior_loss': np.float64(1.3257448434829713)}

Episode step 13810, time diff 0.7107903957366943, total time dif 1024.9026398658752)
step: 13810 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(1.5976546), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08231288455426693), 'actor_loss': np.float64(-1.0045914351940155), 'hyper_actor_loss': np.float64(0.0025255100801587103), 'behavior_loss': np.float64(1.3196346402168273)}

Episode step 13820, time diff 0.7382934093475342, total time dif 1025.613430261612)
step: 13820 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.9567711), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07739766724407673), 'actor_loss': np.float64(-0.9906179904937744), 'hyper_actor_loss': np.float64(0.0020154687110334633), 'behavior_loss': np.float64(1.3048540711402894)}

Episode step 13830, time diff 0.6907274723052979, total time dif 1026.3517236709595)
step: 13830 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.9214072), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0690376415848732), 'actor_loss': np.float64(-0.9723480105400085), 'hyper_actor_loss': np.float64(0.001544900774024427), 'behavior_loss': np.float64(1.3030376315116883)}

Episode step 13840, time diff 0.7210321426391602, total time dif 1027.0424511432648)
step: 13840 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(3.9520938), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08164207898080349), 'actor_loss': np.float64(-1.00561221241951), 'hyper_actor_loss': np.float64(0.001372893969528377), 'behavior_loss': np.float64(1.3061755895614624)}

Episode step 13850, time diff 0.7052857875823975, total time dif 1027.763483285904)
step: 13850 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(2.525339), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08289008438587189), 'actor_loss': np.float64(-0.9901430308818817), 'hyper_actor_loss': np.float64(0.0014190775342285632), 'behavior_loss': np.float64(1.3327991247177124)}

Episode step 13860, time diff 0.6914401054382324, total time dif 1028.4687690734863)
step: 13860 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(2.4605556), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07894467636942863), 'actor_loss': np.float64(-0.9942679345607758), 'hyper_actor_loss': np.float64(0.0015861891559325158), 'behavior_loss': np.float64(1.4396417617797852)}

Episode step 13870, time diff 0.7288572788238525, total time dif 1029.1602091789246)
step: 13870 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(3.5936844), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06969007588922978), 'actor_loss': np.float64(-0.9913269340991974), 'hyper_actor_loss': np.float64(0.0014496338204480707), 'behavior_loss': np.float64(1.2339797973632813)}

Episode step 13880, time diff 0.7329182624816895, total time dif 1029.8890664577484)
step: 13880 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(1.6921918), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06007071994245052), 'actor_loss': np.float64(-0.9747606337070465), 'hyper_actor_loss': np.float64(0.0013478865846991539), 'behavior_loss': np.float64(1.2538220942020417)}

Episode step 13890, time diff 0.8698809146881104, total time dif 1030.62198472023)
step: 13890 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(2.1756346), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07692119032144547), 'actor_loss': np.float64(-0.9935332715511322), 'hyper_actor_loss': np.float64(0.0013071691268123687), 'behavior_loss': np.float64(1.2540157079696654)}

Episode step 13900, time diff 0.6902587413787842, total time dif 1031.4918656349182)
step: 13900 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(4.1175127), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06581911779940128), 'actor_loss': np.float64(-0.9895307004451752), 'hyper_actor_loss': np.float64(0.0013153000036254525), 'behavior_loss': np.float64(1.321489405632019)}

Episode step 13910, time diff 0.7039074897766113, total time dif 1032.182124376297)
step: 13910 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(2.5758383), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07187403701245784), 'actor_loss': np.float64(-0.9806966364383698), 'hyper_actor_loss': np.float64(0.001324532029684633), 'behavior_loss': np.float64(1.26861732006073)}

Episode step 13920, time diff 0.7549362182617188, total time dif 1032.8860318660736)
step: 13920 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(1.8793602), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07819287851452827), 'actor_loss': np.float64(-1.0067975044250488), 'hyper_actor_loss': np.float64(0.0013252991018816828), 'behavior_loss': np.float64(1.2501127123832703)}

Episode step 13930, time diff 0.7440381050109863, total time dif 1033.6409680843353)
step: 13930 @ episode report: {'average_total_reward': np.float32(11.544445), 'reward_variance': np.float32(4.6346183), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06991560608148575), 'actor_loss': np.float64(-1.0133123993873596), 'hyper_actor_loss': np.float64(0.0013744857162237168), 'behavior_loss': np.float64(1.2613444983959199)}

Episode step 13940, time diff 0.7105391025543213, total time dif 1034.3850061893463)
step: 13940 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(5.959759), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08073839880526065), 'actor_loss': np.float64(-0.9782900094985962), 'hyper_actor_loss': np.float64(0.0012612048420123755), 'behavior_loss': np.float64(1.3040061235427856)}

Episode step 13950, time diff 0.7150380611419678, total time dif 1035.0955452919006)
step: 13950 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(2.5950923), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0828212346881628), 'actor_loss': np.float64(-1.0195250749588012), 'hyper_actor_loss': np.float64(0.0012111955555155873), 'behavior_loss': np.float64(1.2544821858406068)}

Episode step 13960, time diff 0.7078847885131836, total time dif 1035.8105833530426)
step: 13960 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(4.915977), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08067692741751671), 'actor_loss': np.float64(-0.9878011047840118), 'hyper_actor_loss': np.float64(0.0012214647606015206), 'behavior_loss': np.float64(1.2258528888225555)}

Episode step 13970, time diff 0.7335009574890137, total time dif 1036.5184681415558)
step: 13970 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(1.1334326), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07281480729579926), 'actor_loss': np.float64(-1.0010367274284362), 'hyper_actor_loss': np.float64(0.0012661283719353378), 'behavior_loss': np.float64(1.1760846734046937)}

Episode step 13980, time diff 0.6946580410003662, total time dif 1037.2519690990448)
step: 13980 @ episode report: {'average_total_reward': np.float32(11.271112), 'reward_variance': np.float32(2.7176595), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07545755356550217), 'actor_loss': np.float64(-0.9942429184913635), 'hyper_actor_loss': np.float64(0.001210238633211702), 'behavior_loss': np.float64(1.3393910765647887)}

Episode step 13990, time diff 0.7097442150115967, total time dif 1037.9466271400452)
step: 13990 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(3.2342079), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06817834377288819), 'actor_loss': np.float64(-0.9655694246292115), 'hyper_actor_loss': np.float64(0.0011707459227181972), 'behavior_loss': np.float64(1.238261604309082)}

Episode step 14000, time diff 0.7428038120269775, total time dif 1038.6563713550568)
step: 14000 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.6425234), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06679187230765819), 'actor_loss': np.float64(-0.9872341096401215), 'hyper_actor_loss': np.float64(0.001089286239584908), 'behavior_loss': np.float64(1.2608830392360688)}

Episode step 14010, time diff 0.7139420509338379, total time dif 1039.3991751670837)
step: 14010 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(1.5706229), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07741162702441215), 'actor_loss': np.float64(-0.9842656016349792), 'hyper_actor_loss': np.float64(0.001051120925694704), 'behavior_loss': np.float64(1.254835879802704)}

Episode step 14020, time diff 0.699458122253418, total time dif 1040.1131172180176)
step: 14020 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(7.5546865), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07241689339280129), 'actor_loss': np.float64(-0.9951420783996582), 'hyper_actor_loss': np.float64(0.0010546500911004842), 'behavior_loss': np.float64(1.2111520409584045)}

Episode step 14030, time diff 0.6925332546234131, total time dif 1040.812575340271)
step: 14030 @ episode report: {'average_total_reward': np.float32(11.544445), 'reward_variance': np.float32(2.786198), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053637607395648955), 'actor_loss': np.float64(-0.9721291899681092), 'hyper_actor_loss': np.float64(0.0010681894840672613), 'behavior_loss': np.float64(1.1440987050533296)}

Episode step 14040, time diff 0.6911184787750244, total time dif 1041.5051085948944)
step: 14040 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.7469151), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07619877718389034), 'actor_loss': np.float64(-0.9733861267566681), 'hyper_actor_loss': np.float64(0.0010541476309299468), 'behavior_loss': np.float64(1.2852627396583558)}

Episode step 14050, time diff 0.8789424896240234, total time dif 1042.1962270736694)
step: 14050 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(1.672006), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07450996860861778), 'actor_loss': np.float64(-1.008791947364807), 'hyper_actor_loss': np.float64(0.001043632859364152), 'behavior_loss': np.float64(1.1827711164951324)}

Episode step 14060, time diff 0.7077746391296387, total time dif 1043.0751695632935)
step: 14060 @ episode report: {'average_total_reward': np.float32(11.320001), 'reward_variance': np.float32(1.6834272), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07393454201519489), 'actor_loss': np.float64(-1.0001544535160065), 'hyper_actor_loss': np.float64(0.0010273013380356134), 'behavior_loss': np.float64(1.1754752814769744)}

Episode step 14070, time diff 0.7437806129455566, total time dif 1043.782944202423)
step: 14070 @ episode report: {'average_total_reward': np.float32(11.444446), 'reward_variance': np.float32(2.7836297), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0735930010676384), 'actor_loss': np.float64(-0.9798186719417572), 'hyper_actor_loss': np.float64(0.0009462615358643234), 'behavior_loss': np.float64(1.1554825246334075)}

Episode step 14080, time diff 0.7094495296478271, total time dif 1044.5267248153687)
step: 14080 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(3.071617), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07287469916045666), 'actor_loss': np.float64(-0.9906573116779327), 'hyper_actor_loss': np.float64(0.0009090637438930571), 'behavior_loss': np.float64(1.2009234726428986)}

Episode step 14090, time diff 0.7230029106140137, total time dif 1045.2361743450165)
step: 14090 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.1432843), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07290826998651027), 'actor_loss': np.float64(-0.9835970461368561), 'hyper_actor_loss': np.float64(0.0009114410437177867), 'behavior_loss': np.float64(1.1742590427398683)}

Episode step 14100, time diff 0.6942300796508789, total time dif 1045.9591772556305)
step: 14100 @ episode report: {'average_total_reward': np.float32(9.900002), 'reward_variance': np.float32(2.0838265), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0712508823722601), 'actor_loss': np.float64(-0.9916247546672821), 'hyper_actor_loss': np.float64(0.0009006129286717624), 'behavior_loss': np.float64(1.098654532432556)}

Episode step 14110, time diff 0.7280750274658203, total time dif 1046.6534073352814)
step: 14110 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(4.97489), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07989681474864482), 'actor_loss': np.float64(-0.9962279736995697), 'hyper_actor_loss': np.float64(0.0009578109544236213), 'behavior_loss': np.float64(1.2026503682136536)}

Episode step 14120, time diff 0.7142515182495117, total time dif 1047.3814823627472)
step: 14120 @ episode report: {'average_total_reward': np.float32(10.758889), 'reward_variance': np.float32(3.6192613), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07196522615849972), 'actor_loss': np.float64(-0.9795773983001709), 'hyper_actor_loss': np.float64(0.0009899978700559585), 'behavior_loss': np.float64(1.2006154537200928)}

Episode step 14130, time diff 0.7947649955749512, total time dif 1048.0957338809967)
step: 14130 @ episode report: {'average_total_reward': np.float32(10.346667), 'reward_variance': np.float32(2.3164897), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06822553984820842), 'actor_loss': np.float64(-0.9850317060947418), 'hyper_actor_loss': np.float64(0.0009992840292397886), 'behavior_loss': np.float64(1.1557586431503295)}

Episode step 14140, time diff 0.7031924724578857, total time dif 1048.8904988765717)
step: 14140 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(1.9320991), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05165237244218588), 'actor_loss': np.float64(-0.9775670766830444), 'hyper_actor_loss': np.float64(0.0010763846104964614), 'behavior_loss': np.float64(1.1033393919467926)}

Episode step 14150, time diff 0.7462303638458252, total time dif 1049.5936913490295)
step: 14150 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(4.180398), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08091529123485089), 'actor_loss': np.float64(-0.9874327421188355), 'hyper_actor_loss': np.float64(0.0012101456406526268), 'behavior_loss': np.float64(1.2142136991024017)}

Episode step 14160, time diff 0.7112913131713867, total time dif 1050.3399217128754)
step: 14160 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.1103654), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07834742702543736), 'actor_loss': np.float64(-1.00659282207489), 'hyper_actor_loss': np.float64(0.0013723789365030826), 'behavior_loss': np.float64(1.2442728877067566)}

Episode step 14170, time diff 0.7067327499389648, total time dif 1051.0512130260468)
step: 14170 @ episode report: {'average_total_reward': np.float32(10.934445), 'reward_variance': np.float32(1.6229737), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0765683550387621), 'actor_loss': np.float64(-0.9939338743686676), 'hyper_actor_loss': np.float64(0.0016225595376454295), 'behavior_loss': np.float64(1.1399812638759612)}

Episode step 14180, time diff 0.7040846347808838, total time dif 1051.7579457759857)
step: 14180 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(1.5383006), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07453966811299324), 'actor_loss': np.float64(-1.0221298098564149), 'hyper_actor_loss': np.float64(0.0016687049879692494), 'behavior_loss': np.float64(1.1712651193141936)}

Episode step 14190, time diff 0.724639892578125, total time dif 1052.4620304107666)
step: 14190 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(1.2131063), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06247519962489605), 'actor_loss': np.float64(-0.9862433850765229), 'hyper_actor_loss': np.float64(0.0020702871144749222), 'behavior_loss': np.float64(1.1509714424610138)}

Episode step 14200, time diff 0.7071843147277832, total time dif 1053.1866703033447)
step: 14200 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(1.5572345), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07545855045318603), 'actor_loss': np.float64(-0.9809460997581482), 'hyper_actor_loss': np.float64(0.0024689295329153537), 'behavior_loss': np.float64(1.1014513790607452)}

Episode step 14210, time diff 0.7285459041595459, total time dif 1053.8938546180725)
step: 14210 @ episode report: {'average_total_reward': np.float32(11.034445), 'reward_variance': np.float32(3.3455434), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0802579514682293), 'actor_loss': np.float64(-1.0122564792633058), 'hyper_actor_loss': np.float64(0.0016917381668463348), 'behavior_loss': np.float64(1.1334567189216613)}

Episode step 14220, time diff 0.8512883186340332, total time dif 1054.622400522232)
step: 14220 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(3.9987423), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07164423279464245), 'actor_loss': np.float64(-0.9923804342746735), 'hyper_actor_loss': np.float64(0.001276219158899039), 'behavior_loss': np.float64(1.2136119604110718)}

Episode step 14230, time diff 0.7212870121002197, total time dif 1055.473688840866)
step: 14230 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(4.4096007), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07113485150039196), 'actor_loss': np.float64(-0.9750994563102722), 'hyper_actor_loss': np.float64(0.0012613048776984215), 'behavior_loss': np.float64(1.1482548356056212)}

Episode step 14240, time diff 0.7411646842956543, total time dif 1056.1949758529663)
step: 14240 @ episode report: {'average_total_reward': np.float32(11.844445), 'reward_variance': np.float32(3.235506), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05554246660321951), 'actor_loss': np.float64(-0.9951916337013245), 'hyper_actor_loss': np.float64(0.0012204890605062245), 'behavior_loss': np.float64(1.1884911358356476)}

Episode step 14250, time diff 0.7508549690246582, total time dif 1056.936140537262)
step: 14250 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(3.1002173), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07690994516015053), 'actor_loss': np.float64(-0.9691991150379181), 'hyper_actor_loss': np.float64(0.001141926320269704), 'behavior_loss': np.float64(1.190891796350479)}

Episode step 14260, time diff 0.7332217693328857, total time dif 1057.6869955062866)
step: 14260 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(2.4853632), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07439117468893527), 'actor_loss': np.float64(-1.00517578125), 'hyper_actor_loss': np.float64(0.0010239072609692812), 'behavior_loss': np.float64(1.1671971917152404)}

Episode step 14270, time diff 0.7431466579437256, total time dif 1058.4202172756195)
step: 14270 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(3.417057), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07076091226190329), 'actor_loss': np.float64(-0.9895420789718627), 'hyper_actor_loss': np.float64(0.0009368966275360435), 'behavior_loss': np.float64(1.2237218379974366)}

Episode step 14280, time diff 0.7115850448608398, total time dif 1059.1633639335632)
step: 14280 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(4.0810385), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07362866327166558), 'actor_loss': np.float64(-0.9756505191326141), 'hyper_actor_loss': np.float64(0.000897373998304829), 'behavior_loss': np.float64(1.0577011823654174)}

Episode step 14290, time diff 0.7047896385192871, total time dif 1059.874948978424)
step: 14290 @ episode report: {'average_total_reward': np.float32(11.0222225), 'reward_variance': np.float32(6.5442705), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0714293945580721), 'actor_loss': np.float64(-0.9954899072647094), 'hyper_actor_loss': np.float64(0.000869531458010897), 'behavior_loss': np.float64(1.1249418139457703)}

Episode step 14300, time diff 0.7144200801849365, total time dif 1060.5797386169434)
step: 14300 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(4.247631), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07055226340889931), 'actor_loss': np.float64(-0.9905970752239227), 'hyper_actor_loss': np.float64(0.0008002796967048198), 'behavior_loss': np.float64(1.1630024313926697)}

Episode step 14310, time diff 0.6875958442687988, total time dif 1061.2941586971283)
step: 14310 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(1.1611376), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06245797798037529), 'actor_loss': np.float64(-0.9849932253360748), 'hyper_actor_loss': np.float64(0.0007705474330577999), 'behavior_loss': np.float64(1.137941950559616)}

Episode step 14320, time diff 0.7353687286376953, total time dif 1061.981754541397)
step: 14320 @ episode report: {'average_total_reward': np.float32(10.722222), 'reward_variance': np.float32(3.1237774), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07149880416691304), 'actor_loss': np.float64(-0.9674224257469177), 'hyper_actor_loss': np.float64(0.0008530794060789048), 'behavior_loss': np.float64(1.199126398563385)}

Episode step 14330, time diff 0.7329845428466797, total time dif 1062.7171232700348)
step: 14330 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.5409244), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06527110747992992), 'actor_loss': np.float64(-0.9932790279388428), 'hyper_actor_loss': np.float64(0.000924904877319932), 'behavior_loss': np.float64(1.1894236087799073)}

Episode step 14340, time diff 0.6988177299499512, total time dif 1063.4501078128815)
step: 14340 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(4.018046), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0742398403584957), 'actor_loss': np.float64(-0.9840299487113953), 'hyper_actor_loss': np.float64(0.0009177645901218056), 'behavior_loss': np.float64(1.1334286689758302)}

Episode step 14350, time diff 0.7340536117553711, total time dif 1064.1489255428314)
step: 14350 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.7592853), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06304833646863699), 'actor_loss': np.float64(-0.9904401183128357), 'hyper_actor_loss': np.float64(0.0010110852890647948), 'behavior_loss': np.float64(1.1690236628055573)}

Episode step 14360, time diff 0.7224321365356445, total time dif 1064.8829791545868)
step: 14360 @ episode report: {'average_total_reward': np.float32(10.21), 'reward_variance': np.float32(4.4207025), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06047538947314024), 'actor_loss': np.float64(-0.9684509217739106), 'hyper_actor_loss': np.float64(0.0010267820209264756), 'behavior_loss': np.float64(1.1150126218795777)}

Episode step 14370, time diff 0.6913294792175293, total time dif 1065.6054112911224)
step: 14370 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(1.1802237), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0607320286333561), 'actor_loss': np.float64(-0.9793508768081665), 'hyper_actor_loss': np.float64(0.001039569068234414), 'behavior_loss': np.float64(1.1356385111808778)}

Episode step 14380, time diff 0.881018877029419, total time dif 1066.29674077034)
step: 14380 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.121014), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07394577525556087), 'actor_loss': np.float64(-0.9956322371959686), 'hyper_actor_loss': np.float64(0.0009866663021966815), 'behavior_loss': np.float64(1.1471082508563994)}

Episode step 14390, time diff 0.7256860733032227, total time dif 1067.1777596473694)
step: 14390 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(4.345853), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0577207513153553), 'actor_loss': np.float64(-0.9922585606575012), 'hyper_actor_loss': np.float64(0.0010292859748005866), 'behavior_loss': np.float64(1.05938001871109)}

Episode step 14400, time diff 0.7479391098022461, total time dif 1067.9034457206726)
step: 14400 @ episode report: {'average_total_reward': np.float32(10.136668), 'reward_variance': np.float32(1.6931372), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06809368506073951), 'actor_loss': np.float64(-0.9812931716442108), 'hyper_actor_loss': np.float64(0.0010534131200984121), 'behavior_loss': np.float64(1.1643063426017761)}

Episode step 14410, time diff 0.7361879348754883, total time dif 1068.6513848304749)
step: 14410 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(0.8590883), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07334432676434517), 'actor_loss': np.float64(-0.9900720298290253), 'hyper_actor_loss': np.float64(0.0011324260267429054), 'behavior_loss': np.float64(1.1642897009849549)}

Episode step 14420, time diff 0.7218384742736816, total time dif 1069.3875727653503)
step: 14420 @ episode report: {'average_total_reward': np.float32(11.532224), 'reward_variance': np.float32(3.859691), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07819421738386154), 'actor_loss': np.float64(-1.0015860974788666), 'hyper_actor_loss': np.float64(0.0012297558831050992), 'behavior_loss': np.float64(1.130494660139084)}

Episode step 14430, time diff 0.6975593566894531, total time dif 1070.109411239624)
step: 14430 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(4.9578466), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08102496527135372), 'actor_loss': np.float64(-1.0262439727783204), 'hyper_actor_loss': np.float64(0.0013778115855529905), 'behavior_loss': np.float64(1.1203948259353638)}

Episode step 14440, time diff 0.7142202854156494, total time dif 1070.8069705963135)
step: 14440 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(1.8009093), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06653021536767482), 'actor_loss': np.float64(-1.0043398022651673), 'hyper_actor_loss': np.float64(0.0016082854010164738), 'behavior_loss': np.float64(1.0925999402999877)}

Episode step 14450, time diff 0.7330350875854492, total time dif 1071.5211908817291)
step: 14450 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.0187473), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07973411679267883), 'actor_loss': np.float64(-1.009688574075699), 'hyper_actor_loss': np.float64(0.002112646261230111), 'behavior_loss': np.float64(1.122597908973694)}

Episode step 14460, time diff 0.712820291519165, total time dif 1072.2542259693146)
step: 14460 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.7752395), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06566727571189404), 'actor_loss': np.float64(-1.0116362988948822), 'hyper_actor_loss': np.float64(0.002607459248974919), 'behavior_loss': np.float64(1.1196228921413423)}

Episode step 14470, time diff 0.73378586769104, total time dif 1072.9670462608337)
step: 14470 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(4.6114087), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07327709086239338), 'actor_loss': np.float64(-1.0013505935668945), 'hyper_actor_loss': np.float64(0.003248151740990579), 'behavior_loss': np.float64(1.0233363389968873)}

Episode step 14480, time diff 0.7125427722930908, total time dif 1073.7008321285248)
step: 14480 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(3.9607117), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07460146993398667), 'actor_loss': np.float64(-1.0094761490821837), 'hyper_actor_loss': np.float64(0.0023980014608241617), 'behavior_loss': np.float64(1.1657431423664093)}

Episode step 14490, time diff 0.7150371074676514, total time dif 1074.4133749008179)
step: 14490 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(1.6772302), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07316640317440033), 'actor_loss': np.float64(-0.9658393919467926), 'hyper_actor_loss': np.float64(0.0015511366887949406), 'behavior_loss': np.float64(1.1038086116313934)}

Episode step 14500, time diff 0.7424125671386719, total time dif 1075.1284120082855)
step: 14500 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.0525134), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08160707280039788), 'actor_loss': np.float64(-0.9905781209468841), 'hyper_actor_loss': np.float64(0.0011494329781271517), 'behavior_loss': np.float64(1.1420649766921998)}

Episode step 14510, time diff 0.7364680767059326, total time dif 1075.8708245754242)
step: 14510 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(1.2655079), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06459588780999184), 'actor_loss': np.float64(-1.002525955438614), 'hyper_actor_loss': np.float64(0.0010587410361040384), 'behavior_loss': np.float64(1.0624137043952941)}

Episode step 14520, time diff 0.7282228469848633, total time dif 1076.6072926521301)
step: 14520 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.4855669), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07260958068072795), 'actor_loss': np.float64(-0.9734284341335296), 'hyper_actor_loss': np.float64(0.0010494601621758193), 'behavior_loss': np.float64(1.0934327363967895)}

Episode step 14530, time diff 0.7345707416534424, total time dif 1077.335515499115)
step: 14530 @ episode report: {'average_total_reward': np.float32(9.785556), 'reward_variance': np.float32(4.065779), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08573521301150322), 'actor_loss': np.float64(-1.0173077583312988), 'hyper_actor_loss': np.float64(0.0010606102121528238), 'behavior_loss': np.float64(1.1038414776325225)}

Episode step 14540, time diff 0.7091970443725586, total time dif 1078.0700862407684)
step: 14540 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(2.9070487), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.075820816680789), 'actor_loss': np.float64(-1.0017219007015228), 'hyper_actor_loss': np.float64(0.0012295899563468994), 'behavior_loss': np.float64(1.0839502036571502)}

Episode step 14550, time diff 0.8664436340332031, total time dif 1078.779283285141)
step: 14550 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.4375255), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07783560939133168), 'actor_loss': np.float64(-1.0028677880764008), 'hyper_actor_loss': np.float64(0.001295085495803505), 'behavior_loss': np.float64(1.0708503603935242)}

Episode step 14560, time diff 0.6810288429260254, total time dif 1079.6457269191742)
step: 14560 @ episode report: {'average_total_reward': np.float32(9.887778), 'reward_variance': np.float32(1.1973449), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06756563596427441), 'actor_loss': np.float64(-1.0124018609523773), 'hyper_actor_loss': np.float64(0.001172123197466135), 'behavior_loss': np.float64(1.1098508536815643)}

Episode step 14570, time diff 0.7271432876586914, total time dif 1080.3267557621002)
step: 14570 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.0809141), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06527178063988685), 'actor_loss': np.float64(-0.9675449550151825), 'hyper_actor_loss': np.float64(0.0011273715179413557), 'behavior_loss': np.float64(1.0668331563472748)}

Episode step 14580, time diff 0.7316873073577881, total time dif 1081.053899049759)
step: 14580 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.8000441), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07574548292905092), 'actor_loss': np.float64(-0.9937556862831116), 'hyper_actor_loss': np.float64(0.0010170260502491146), 'behavior_loss': np.float64(1.0156483948230743)}

Episode step 14590, time diff 0.721437931060791, total time dif 1081.7855863571167)
step: 14590 @ episode report: {'average_total_reward': np.float32(9.351111), 'reward_variance': np.float32(3.5798583), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06426277942955494), 'actor_loss': np.float64(-0.9988649964332581), 'hyper_actor_loss': np.float64(0.0009363256918732077), 'behavior_loss': np.float64(1.0188304841518403)}

Episode step 14600, time diff 0.7352843284606934, total time dif 1082.5070242881775)
step: 14600 @ episode report: {'average_total_reward': np.float32(9.785556), 'reward_variance': np.float32(5.1281476), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0748304359614849), 'actor_loss': np.float64(-0.9827576518058777), 'hyper_actor_loss': np.float64(0.0008660930907353759), 'behavior_loss': np.float64(0.9970758378505706)}

Episode step 14610, time diff 0.6969809532165527, total time dif 1083.2423086166382)
step: 14610 @ episode report: {'average_total_reward': np.float32(10.285555), 'reward_variance': np.float32(5.026767), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0699388723820448), 'actor_loss': np.float64(-0.9963901400566101), 'hyper_actor_loss': np.float64(0.0008150692156050354), 'behavior_loss': np.float64(1.071610677242279)}

Episode step 14620, time diff 0.7126402854919434, total time dif 1083.9392895698547)
step: 14620 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(2.8092856), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06448116227984428), 'actor_loss': np.float64(-0.9832032322883606), 'hyper_actor_loss': np.float64(0.0008221658645197749), 'behavior_loss': np.float64(1.0793796598911285)}

Episode step 14630, time diff 0.727200984954834, total time dif 1084.6519298553467)
step: 14630 @ episode report: {'average_total_reward': np.float32(10.222222), 'reward_variance': np.float32(4.633927), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06923928633332252), 'actor_loss': np.float64(-0.9719795763492585), 'hyper_actor_loss': np.float64(0.0008393343072384596), 'behavior_loss': np.float64(1.0540364623069762)}

Episode step 14640, time diff 0.7189607620239258, total time dif 1085.3791308403015)
step: 14640 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(4.52918), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07235922086983919), 'actor_loss': np.float64(-0.9954107403755188), 'hyper_actor_loss': np.float64(0.0008757758128922433), 'behavior_loss': np.float64(1.1252862572669984)}

Episode step 14650, time diff 0.724600076675415, total time dif 1086.0980916023254)
step: 14650 @ episode report: {'average_total_reward': np.float32(9.700002), 'reward_variance': np.float32(2.6278775), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06486048474907875), 'actor_loss': np.float64(-0.983875036239624), 'hyper_actor_loss': np.float64(0.0008916616148781032), 'behavior_loss': np.float64(1.1528587937355042)}

Episode step 14660, time diff 0.6995306015014648, total time dif 1086.8226916790009)
step: 14660 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(7.5984282), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0697065208107233), 'actor_loss': np.float64(-0.9738986253738403), 'hyper_actor_loss': np.float64(0.0008881616580765694), 'behavior_loss': np.float64(1.1221122682094573)}

Episode step 14670, time diff 0.6938297748565674, total time dif 1087.5222222805023)
step: 14670 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(1.7877886), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0709854643791914), 'actor_loss': np.float64(-0.9968005895614624), 'hyper_actor_loss': np.float64(0.000870446307817474), 'behavior_loss': np.float64(1.0702640116214752)}

Episode step 14680, time diff 0.7523458003997803, total time dif 1088.216052055359)
step: 14680 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(4.209674), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06585224755108357), 'actor_loss': np.float64(-0.9941879093647004), 'hyper_actor_loss': np.float64(0.0008990931557491422), 'behavior_loss': np.float64(1.0290036082267762)}

Episode step 14690, time diff 0.7007086277008057, total time dif 1088.9683978557587)
step: 14690 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(2.5527704), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06749725043773651), 'actor_loss': np.float64(-0.978492820262909), 'hyper_actor_loss': np.float64(0.000918036833172664), 'behavior_loss': np.float64(0.949798172712326)}

Episode step 14700, time diff 0.7026522159576416, total time dif 1089.6691064834595)
step: 14700 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(2.5849833), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06518399119377136), 'actor_loss': np.float64(-0.9799596071243286), 'hyper_actor_loss': np.float64(0.0009517322410829365), 'behavior_loss': np.float64(1.1352893233299255)}

Episode step 14710, time diff 0.7056162357330322, total time dif 1090.3717586994171)
step: 14710 @ episode report: {'average_total_reward': np.float32(10.958889), 'reward_variance': np.float32(2.9563973), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0711615527048707), 'actor_loss': np.float64(-0.9855264544486999), 'hyper_actor_loss': np.float64(0.0009640469390433282), 'behavior_loss': np.float64(1.07980614900589)}

Episode step 14720, time diff 0.8473610877990723, total time dif 1091.0773749351501)
step: 14720 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(2.956197), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06820976603776216), 'actor_loss': np.float64(-0.9987282812595367), 'hyper_actor_loss': np.float64(0.0010958324768580497), 'behavior_loss': np.float64(1.1643355369567872)}

Episode step 14730, time diff 0.7380847930908203, total time dif 1091.9247360229492)
step: 14730 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(2.421204), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06734936647117137), 'actor_loss': np.float64(-0.9857106149196625), 'hyper_actor_loss': np.float64(0.0013131705578416585), 'behavior_loss': np.float64(1.0791747987270355)}

Episode step 14740, time diff 0.7158331871032715, total time dif 1092.66282081604)
step: 14740 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.6390562), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07248929515480995), 'actor_loss': np.float64(-0.9942702531814576), 'hyper_actor_loss': np.float64(0.0013994129956699907), 'behavior_loss': np.float64(1.1001298129558563)}

Episode step 14750, time diff 0.7052779197692871, total time dif 1093.3786540031433)
step: 14750 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.7668703), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07661750242114067), 'actor_loss': np.float64(-1.0142610788345336), 'hyper_actor_loss': np.float64(0.0017978274961933493), 'behavior_loss': np.float64(1.0705948352813721)}

Episode step 14760, time diff 0.7104928493499756, total time dif 1094.0839319229126)
step: 14760 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(1.9063947), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06242724359035492), 'actor_loss': np.float64(-0.9951878070831299), 'hyper_actor_loss': np.float64(0.0021166606107726692), 'behavior_loss': np.float64(1.0612229883670807)}

Episode step 14770, time diff 0.7472691535949707, total time dif 1094.7944247722626)
step: 14770 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(1.4874133), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06953769475221634), 'actor_loss': np.float64(-0.9820675373077392), 'hyper_actor_loss': np.float64(0.0018094207393005491), 'behavior_loss': np.float64(1.0370909333229066)}

Episode step 14780, time diff 0.7139358520507812, total time dif 1095.5416939258575)
step: 14780 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.285607), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06550251916050912), 'actor_loss': np.float64(-0.9870147585868836), 'hyper_actor_loss': np.float64(0.0014664993504993618), 'behavior_loss': np.float64(1.0205770492553712)}

Episode step 14790, time diff 0.7155847549438477, total time dif 1096.2556297779083)
step: 14790 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(1.4695817), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0669669046998024), 'actor_loss': np.float64(-0.9832351565361023), 'hyper_actor_loss': np.float64(0.0013881216640584172), 'behavior_loss': np.float64(1.0018267571926116)}

Episode step 14800, time diff 0.6887798309326172, total time dif 1096.9712145328522)
step: 14800 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(1.5568655), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06376000009477138), 'actor_loss': np.float64(-0.99396111369133), 'hyper_actor_loss': np.float64(0.0014056576997973024), 'behavior_loss': np.float64(1.053602361679077)}

Episode step 14810, time diff 0.7281272411346436, total time dif 1097.6599943637848)
step: 14810 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(3.602672), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07215216904878616), 'actor_loss': np.float64(-0.9843210756778717), 'hyper_actor_loss': np.float64(0.001334223838057369), 'behavior_loss': np.float64(1.0085594177246093)}

Episode step 14820, time diff 0.7179968357086182, total time dif 1098.3881216049194)
step: 14820 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(1.2890484), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0713341373950243), 'actor_loss': np.float64(-1.0068808019161224), 'hyper_actor_loss': np.float64(0.0013386599835939705), 'behavior_loss': np.float64(1.0314728677272798)}

Episode step 14830, time diff 0.705859899520874, total time dif 1099.106118440628)
step: 14830 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(2.9224749), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05903048887848854), 'actor_loss': np.float64(-0.9863953232765198), 'hyper_actor_loss': np.float64(0.0013150912360288202), 'behavior_loss': np.float64(1.0085982978343964)}

Episode step 14840, time diff 0.694892168045044, total time dif 1099.811978340149)
step: 14840 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(3.1746616), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0845393754541874), 'actor_loss': np.float64(-0.9999114573001862), 'hyper_actor_loss': np.float64(0.0015146486344747244), 'behavior_loss': np.float64(1.1946699380874635)}

Episode step 14850, time diff 0.7175271511077881, total time dif 1100.506870508194)
step: 14850 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(3.3962326), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0672997586429119), 'actor_loss': np.float64(-0.9940356552600861), 'hyper_actor_loss': np.float64(0.0013942929566837848), 'behavior_loss': np.float64(1.1786522805690764)}

Episode step 14860, time diff 0.6971228122711182, total time dif 1101.2243976593018)
step: 14860 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(1.8042482), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07524869367480277), 'actor_loss': np.float64(-0.9870746433734894), 'hyper_actor_loss': np.float64(0.001374741061590612), 'behavior_loss': np.float64(1.1035470843315125)}

Episode step 14870, time diff 0.6784441471099854, total time dif 1101.9215204715729)
step: 14870 @ episode report: {'average_total_reward': np.float32(10.797778), 'reward_variance': np.float32(2.683329), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06715673506259918), 'actor_loss': np.float64(-1.0065603613853455), 'hyper_actor_loss': np.float64(0.0011613746755756438), 'behavior_loss': np.float64(1.0721964299678803)}

Episode step 14880, time diff 0.8421566486358643, total time dif 1102.5999646186829)
step: 14880 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(1.4284642), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06419585309922696), 'actor_loss': np.float64(-0.9873785972595215), 'hyper_actor_loss': np.float64(0.0011819058679975569), 'behavior_loss': np.float64(1.1689355969429016)}

Episode step 14890, time diff 0.7162728309631348, total time dif 1103.4421212673187)
step: 14890 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.545414), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08782714381814002), 'actor_loss': np.float64(-0.9917515873908996), 'hyper_actor_loss': np.float64(0.0012106833280995488), 'behavior_loss': np.float64(1.065808230638504)}

Episode step 14900, time diff 0.6951503753662109, total time dif 1104.1583940982819)
step: 14900 @ episode report: {'average_total_reward': np.float32(10.185555), 'reward_variance': np.float32(2.9788651), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0697095688432455), 'actor_loss': np.float64(-1.0281333923339844), 'hyper_actor_loss': np.float64(0.0009115125692915171), 'behavior_loss': np.float64(0.9718924045562745)}

Episode step 14910, time diff 0.67386794090271, total time dif 1104.853544473648)
step: 14910 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(2.043956), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06186198610812425), 'actor_loss': np.float64(-0.9689859688282013), 'hyper_actor_loss': np.float64(0.0008799239818472415), 'behavior_loss': np.float64(1.1472466826438903)}

Episode step 14920, time diff 0.7321615219116211, total time dif 1105.5274124145508)
step: 14920 @ episode report: {'average_total_reward': np.float32(9.8977785), 'reward_variance': np.float32(6.778441), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06069956980645656), 'actor_loss': np.float64(-0.9519918918609619), 'hyper_actor_loss': np.float64(0.0008926257956773043), 'behavior_loss': np.float64(1.1402587950229646)}

Episode step 14930, time diff 0.7360711097717285, total time dif 1106.2595739364624)
step: 14930 @ episode report: {'average_total_reward': np.float32(11.483335), 'reward_variance': np.float32(4.479735), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056737134233117104), 'actor_loss': np.float64(-0.981293386220932), 'hyper_actor_loss': np.float64(0.0009043600934091955), 'behavior_loss': np.float64(1.001086711883545)}

Episode step 14940, time diff 0.7509400844573975, total time dif 1106.9956450462341)
step: 14940 @ episode report: {'average_total_reward': np.float32(11.207778), 'reward_variance': np.float32(5.2068157), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07575816176831722), 'actor_loss': np.float64(-0.9897373437881469), 'hyper_actor_loss': np.float64(0.0009438851033337414), 'behavior_loss': np.float64(1.0649111151695252)}

Episode step 14950, time diff 0.7121238708496094, total time dif 1107.7465851306915)
step: 14950 @ episode report: {'average_total_reward': np.float32(11.132223), 'reward_variance': np.float32(1.5400356), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08183355368673802), 'actor_loss': np.float64(-1.00099875330925), 'hyper_actor_loss': np.float64(0.0009074412926565855), 'behavior_loss': np.float64(1.105502712726593)}

Episode step 14960, time diff 0.7138276100158691, total time dif 1108.4587090015411)
step: 14960 @ episode report: {'average_total_reward': np.float32(10.41), 'reward_variance': np.float32(1.7743317), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06646618768572807), 'actor_loss': np.float64(-0.9959008574485779), 'hyper_actor_loss': np.float64(0.0008757721458096057), 'behavior_loss': np.float64(1.0640914976596831)}

Episode step 14970, time diff 0.7079377174377441, total time dif 1109.172536611557)
step: 14970 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(3.3756542), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06464620865881443), 'actor_loss': np.float64(-0.9867890536785126), 'hyper_actor_loss': np.float64(0.001062609872315079), 'behavior_loss': np.float64(1.0233232855796814)}

Episode step 14980, time diff 0.7213358879089355, total time dif 1109.8804743289948)
step: 14980 @ episode report: {'average_total_reward': np.float32(11.35889), 'reward_variance': np.float32(6.258619), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06633238792419434), 'actor_loss': np.float64(-0.9912690341472625), 'hyper_actor_loss': np.float64(0.0020808892440982163), 'behavior_loss': np.float64(1.052277672290802)}

Episode step 14990, time diff 0.7162578105926514, total time dif 1110.6018102169037)
step: 14990 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(1.7824256), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06333277560770512), 'actor_loss': np.float64(-0.9939319431781769), 'hyper_actor_loss': np.float64(0.0021692069247365), 'behavior_loss': np.float64(1.0749707102775574)}

Episode step 15000, time diff 0.6683704853057861, total time dif 1111.3180680274963)
step: 15000 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(4.946401), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06300463862717151), 'actor_loss': np.float64(-0.9726873457431793), 'hyper_actor_loss': np.float64(0.0012993516866117716), 'behavior_loss': np.float64(1.025320041179657)}

Episode step 15010, time diff 0.7070355415344238, total time dif 1111.9864385128021)
step: 15010 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(2.167156), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056793096289038655), 'actor_loss': np.float64(-0.9761878550052643), 'hyper_actor_loss': np.float64(0.000876542809419334), 'behavior_loss': np.float64(0.9835338652133941)}

Episode step 15020, time diff 0.7108728885650635, total time dif 1112.6934740543365)
step: 15020 @ episode report: {'average_total_reward': np.float32(11.034445), 'reward_variance': np.float32(4.539098), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05804846547544003), 'actor_loss': np.float64(-0.9983704924583435), 'hyper_actor_loss': np.float64(0.0008057221013586969), 'behavior_loss': np.float64(1.0178465008735658)}

Episode step 15030, time diff 0.7027084827423096, total time dif 1113.4043469429016)
step: 15030 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(2.9224806), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07343658693134784), 'actor_loss': np.float64(-0.9959600567817688), 'hyper_actor_loss': np.float64(0.000703448971034959), 'behavior_loss': np.float64(1.0231725990772247)}

Episode step 15040, time diff 0.741281270980835, total time dif 1114.107055425644)
step: 15040 @ episode report: {'average_total_reward': np.float32(10.073335), 'reward_variance': np.float32(2.01761), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08275817930698395), 'actor_loss': np.float64(-1.0079341292381288), 'hyper_actor_loss': np.float64(0.0007225629058666527), 'behavior_loss': np.float64(1.003571105003357)}

Episode step 15050, time diff 0.712329626083374, total time dif 1114.8483366966248)
step: 15050 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(2.2879558), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06872305683791638), 'actor_loss': np.float64(-0.9966996252536774), 'hyper_actor_loss': np.float64(0.0006990776397287846), 'behavior_loss': np.float64(1.0454201579093934)}

Episode step 15060, time diff 0.8776381015777588, total time dif 1115.5606663227081)
step: 15060 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(1.3354092), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06334260795265437), 'actor_loss': np.float64(-0.9752015113830567), 'hyper_actor_loss': np.float64(0.000756751507287845), 'behavior_loss': np.float64(1.0346600472927094)}

Episode step 15070, time diff 0.6953186988830566, total time dif 1116.438304424286)
step: 15070 @ episode report: {'average_total_reward': np.float32(12.081112), 'reward_variance': np.float32(3.2661247), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06176776718348265), 'actor_loss': np.float64(-0.9875955581665039), 'hyper_actor_loss': np.float64(0.0007554772135335952), 'behavior_loss': np.float64(1.0432921707630158)}

Episode step 15080, time diff 0.7107851505279541, total time dif 1117.133623123169)
step: 15080 @ episode report: {'average_total_reward': np.float32(10.336667), 'reward_variance': np.float32(1.2517301), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058022087439894676), 'actor_loss': np.float64(-0.9729758024215698), 'hyper_actor_loss': np.float64(0.0007483259541913867), 'behavior_loss': np.float64(0.9843596518039703)}

Episode step 15090, time diff 0.7160353660583496, total time dif 1117.844408273697)
step: 15090 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(2.7963958), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07328473404049873), 'actor_loss': np.float64(-0.9961246073246002), 'hyper_actor_loss': np.float64(0.0007453977828845382), 'behavior_loss': np.float64(1.0137092769145966)}

Episode step 15100, time diff 0.7159900665283203, total time dif 1118.5604436397552)
step: 15100 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.2446475), 'max_total_reward': np.float32(13.022222), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06376352533698082), 'actor_loss': np.float64(-1.008334493637085), 'hyper_actor_loss': np.float64(0.0007535294920671731), 'behavior_loss': np.float64(0.9825728416442872)}

Episode step 15110, time diff 0.7056691646575928, total time dif 1119.2764337062836)
step: 15110 @ episode report: {'average_total_reward': np.float32(11.881113), 'reward_variance': np.float32(4.1260014), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06129771023988724), 'actor_loss': np.float64(-0.978237634897232), 'hyper_actor_loss': np.float64(0.0007299009244889021), 'behavior_loss': np.float64(1.032968944311142)}

Episode step 15120, time diff 0.7326316833496094, total time dif 1119.9821028709412)
step: 15120 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(2.6222243), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061921328864991666), 'actor_loss': np.float64(-0.9907335460186004), 'hyper_actor_loss': np.float64(0.0007324515143409371), 'behavior_loss': np.float64(0.9716915190219879)}

Episode step 15130, time diff 0.7346537113189697, total time dif 1120.7147345542908)
step: 15130 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(4.812697), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0760000430047512), 'actor_loss': np.float64(-1.005589532852173), 'hyper_actor_loss': np.float64(0.0007896731491200626), 'behavior_loss': np.float64(1.1291534900665283)}

Episode step 15140, time diff 0.7057294845581055, total time dif 1121.4493882656097)
step: 15140 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(1.4525185), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06113396380096674), 'actor_loss': np.float64(-0.9867257297039032), 'hyper_actor_loss': np.float64(0.000830432161455974), 'behavior_loss': np.float64(1.026804941892624)}

Episode step 15150, time diff 0.7226588726043701, total time dif 1122.1551177501678)
step: 15150 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(2.2853634), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05808960571885109), 'actor_loss': np.float64(-0.9920107662677765), 'hyper_actor_loss': np.float64(0.0008737604017369449), 'behavior_loss': np.float64(1.0326517820358276)}

Episode step 15160, time diff 0.7034995555877686, total time dif 1122.8777766227722)
step: 15160 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(5.2753096), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07221984378993511), 'actor_loss': np.float64(-0.9968506872653962), 'hyper_actor_loss': np.float64(0.0010046504903584719), 'behavior_loss': np.float64(1.0553298711776733)}

Episode step 15170, time diff 0.6902754306793213, total time dif 1123.58127617836)
step: 15170 @ episode report: {'average_total_reward': np.float32(10.697779), 'reward_variance': np.float32(1.4246871), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06149689443409443), 'actor_loss': np.float64(-0.9977615952491761), 'hyper_actor_loss': np.float64(0.0012069371063262225), 'behavior_loss': np.float64(1.0337330222129821)}

Episode step 15180, time diff 0.685755729675293, total time dif 1124.2715516090393)
step: 15180 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(4.9802427), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07946503348648548), 'actor_loss': np.float64(-1.008852368593216), 'hyper_actor_loss': np.float64(0.0015145340119488537), 'behavior_loss': np.float64(1.0742497324943543)}

Episode step 15190, time diff 0.694455623626709, total time dif 1124.9573073387146)
step: 15190 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(3.6570325), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06773798819631338), 'actor_loss': np.float64(-1.0078961670398712), 'hyper_actor_loss': np.float64(0.001587645651306957), 'behavior_loss': np.float64(1.0122602045536042)}

Episode step 15200, time diff 0.689781904220581, total time dif 1125.6517629623413)
step: 15200 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.737907), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06249045990407467), 'actor_loss': np.float64(-0.9960135817527771), 'hyper_actor_loss': np.float64(0.0013175697182305156), 'behavior_loss': np.float64(0.9808767259120941)}

Episode step 15210, time diff 0.7177801132202148, total time dif 1126.341544866562)
step: 15210 @ episode report: {'average_total_reward': np.float32(10.497779), 'reward_variance': np.float32(5.5722175), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07720887027680874), 'actor_loss': np.float64(-0.9998311400413513), 'hyper_actor_loss': np.float64(0.001061893894802779), 'behavior_loss': np.float64(1.1457882702350617)}

Episode step 15220, time diff 0.6901400089263916, total time dif 1127.059324979782)
step: 15220 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.2445736), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05204623565077782), 'actor_loss': np.float64(-0.9768878638744354), 'hyper_actor_loss': np.float64(0.0008784462581388653), 'behavior_loss': np.float64(1.0630782783031463)}

Episode step 15230, time diff 0.9063427448272705, total time dif 1127.7494649887085)
step: 15230 @ episode report: {'average_total_reward': np.float32(9.936668), 'reward_variance': np.float32(1.9173844), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06626397743821144), 'actor_loss': np.float64(-0.9711427628993988), 'hyper_actor_loss': np.float64(0.000805375655181706), 'behavior_loss': np.float64(1.0521465957164764)}

Episode step 15240, time diff 0.7058427333831787, total time dif 1128.6558077335358)
step: 15240 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.171838), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06331128776073455), 'actor_loss': np.float64(-0.9922473669052124), 'hyper_actor_loss': np.float64(0.0007711796672083438), 'behavior_loss': np.float64(1.0407487332820893)}

Episode step 15250, time diff 0.7350261211395264, total time dif 1129.361650466919)
step: 15250 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(1.6663898), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06273427642881871), 'actor_loss': np.float64(-0.9839981198310852), 'hyper_actor_loss': np.float64(0.0007303981983568519), 'behavior_loss': np.float64(1.0310876667499542)}

Episode step 15260, time diff 0.7008376121520996, total time dif 1130.0966765880585)
step: 15260 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(2.4064202), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06439297050237655), 'actor_loss': np.float64(-0.9756037175655365), 'hyper_actor_loss': np.float64(0.0006733027461450547), 'behavior_loss': np.float64(1.0687534868717194)}

Episode step 15270, time diff 0.7088642120361328, total time dif 1130.7975142002106)
step: 15270 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(2.970024), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06985912695527077), 'actor_loss': np.float64(-0.9884444296360015), 'hyper_actor_loss': np.float64(0.0006937097641639411), 'behavior_loss': np.float64(1.0530229210853577)}

Episode step 15280, time diff 0.7441751956939697, total time dif 1131.5063784122467)
step: 15280 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(4.791033), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0749124277383089), 'actor_loss': np.float64(-1.0017869293689727), 'hyper_actor_loss': np.float64(0.0006525788572616875), 'behavior_loss': np.float64(1.014597487449646)}

Episode step 15290, time diff 0.718764066696167, total time dif 1132.2505536079407)
step: 15290 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(4.2954626), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07346628569066524), 'actor_loss': np.float64(-1.007563066482544), 'hyper_actor_loss': np.float64(0.0006848647084552794), 'behavior_loss': np.float64(0.9762702941894531)}

Episode step 15300, time diff 0.700486421585083, total time dif 1132.9693176746368)
step: 15300 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(4.908563), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0648041320964694), 'actor_loss': np.float64(-1.0074256002902984), 'hyper_actor_loss': np.float64(0.0006926870380993932), 'behavior_loss': np.float64(1.0063796997070313)}

Episode step 15310, time diff 0.7398576736450195, total time dif 1133.669804096222)
step: 15310 @ episode report: {'average_total_reward': np.float32(11.981112), 'reward_variance': np.float32(1.1608651), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06139636859297752), 'actor_loss': np.float64(-0.9794839859008789), 'hyper_actor_loss': np.float64(0.0007303782971575856), 'behavior_loss': np.float64(1.0215521812438966)}

Episode step 15320, time diff 0.6805059909820557, total time dif 1134.409661769867)
step: 15320 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(5.896421), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05911519918590784), 'actor_loss': np.float64(-0.9855327725410461), 'hyper_actor_loss': np.float64(0.0006901155051309615), 'behavior_loss': np.float64(0.9786225438117981)}

Episode step 15330, time diff 0.7182300090789795, total time dif 1135.090167760849)
step: 15330 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(4.7483253), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06335945501923561), 'actor_loss': np.float64(-0.9792751610279083), 'hyper_actor_loss': np.float64(0.0006743049481883645), 'behavior_loss': np.float64(1.0594205021858216)}

Episode step 15340, time diff 0.7151155471801758, total time dif 1135.808397769928)
step: 15340 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(2.0390973), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07476286254823208), 'actor_loss': np.float64(-0.9937979996204376), 'hyper_actor_loss': np.float64(0.0006476751994341611), 'behavior_loss': np.float64(1.0295382499694825)}

Episode step 15350, time diff 0.6970908641815186, total time dif 1136.5235133171082)
step: 15350 @ episode report: {'average_total_reward': np.float32(8.253334), 'reward_variance': np.float32(1.2324642), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060228376369923356), 'actor_loss': np.float64(-1.0022802233695984), 'hyper_actor_loss': np.float64(0.00069439253420569), 'behavior_loss': np.float64(1.0182175517082215)}

Episode step 15360, time diff 0.7143938541412354, total time dif 1137.2206041812897)
step: 15360 @ episode report: {'average_total_reward': np.float32(9.775557), 'reward_variance': np.float32(2.140908), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0660344198346138), 'actor_loss': np.float64(-0.9700669944286346), 'hyper_actor_loss': np.float64(0.0006623569584917277), 'behavior_loss': np.float64(0.9991579949855804)}

Episode step 15370, time diff 0.720740556716919, total time dif 1137.934998035431)
step: 15370 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.4800754), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06162768974900246), 'actor_loss': np.float64(-0.9820155918598175), 'hyper_actor_loss': np.float64(0.0006729363289196044), 'behavior_loss': np.float64(1.0422762095928193)}

Episode step 15380, time diff 0.6966614723205566, total time dif 1138.6557385921478)
step: 15380 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(1.6774933), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0608539741486311), 'actor_loss': np.float64(-0.9989293992519379), 'hyper_actor_loss': np.float64(0.0006869086762890219), 'behavior_loss': np.float64(1.0042657554149628)}

Episode step 15390, time diff 0.8651731014251709, total time dif 1139.3524000644684)
step: 15390 @ episode report: {'average_total_reward': np.float32(10.036668), 'reward_variance': np.float32(6.1063967), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05359028540551662), 'actor_loss': np.float64(-0.974919468164444), 'hyper_actor_loss': np.float64(0.000732526829233393), 'behavior_loss': np.float64(1.0918811678886413)}

Episode step 15400, time diff 0.7337026596069336, total time dif 1140.2175731658936)
step: 15400 @ episode report: {'average_total_reward': np.float32(9.951113), 'reward_variance': np.float32(4.282499), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06416153125464916), 'actor_loss': np.float64(-0.9817021548748016), 'hyper_actor_loss': np.float64(0.000750507420161739), 'behavior_loss': np.float64(0.9683793485164642)}

Episode step 15410, time diff 0.7594542503356934, total time dif 1140.9512758255005)
step: 15410 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.4945197), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0642018347978592), 'actor_loss': np.float64(-1.007558858394623), 'hyper_actor_loss': np.float64(0.000887233013054356), 'behavior_loss': np.float64(1.006786036491394)}

Episode step 15420, time diff 0.7309119701385498, total time dif 1141.7107300758362)
step: 15420 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(1.093937), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06772448383271694), 'actor_loss': np.float64(-0.994692039489746), 'hyper_actor_loss': np.float64(0.0009585304476786404), 'behavior_loss': np.float64(0.9382710576057434)}

Episode step 15430, time diff 0.7170510292053223, total time dif 1142.4416420459747)
step: 15430 @ episode report: {'average_total_reward': np.float32(8.1044445), 'reward_variance': np.float32(2.201018), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06282839737832546), 'actor_loss': np.float64(-0.9905933022499085), 'hyper_actor_loss': np.float64(0.0010027023847214878), 'behavior_loss': np.float64(0.9982588291168213)}

Episode step 15440, time diff 0.7044970989227295, total time dif 1143.15869307518)
step: 15440 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(3.5448265), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07201998569071293), 'actor_loss': np.float64(-1.0082789003849029), 'hyper_actor_loss': np.float64(0.0010092568001709879), 'behavior_loss': np.float64(0.9541593551635742)}

Episode step 15450, time diff 0.7145540714263916, total time dif 1143.8631901741028)
step: 15450 @ episode report: {'average_total_reward': np.float32(8.887778), 'reward_variance': np.float32(2.44379), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06595769450068474), 'actor_loss': np.float64(-1.0178699374198914), 'hyper_actor_loss': np.float64(0.0012541426694951952), 'behavior_loss': np.float64(0.9132703959941864)}

Episode step 15460, time diff 0.7345101833343506, total time dif 1144.5777442455292)
step: 15460 @ episode report: {'average_total_reward': np.float32(7.5433335), 'reward_variance': np.float32(2.5512702), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07336840592324734), 'actor_loss': np.float64(-0.9956640303134918), 'hyper_actor_loss': np.float64(0.0014193597133271396), 'behavior_loss': np.float64(1.0117686212062835)}

Episode step 15470, time diff 0.7158033847808838, total time dif 1145.3122544288635)
step: 15470 @ episode report: {'average_total_reward': np.float32(8.177778), 'reward_variance': np.float32(2.8166175), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07115395180881023), 'actor_loss': np.float64(-1.0087477445602417), 'hyper_actor_loss': np.float64(0.0015003426000475884), 'behavior_loss': np.float64(1.0341549217700958)}

Episode step 15480, time diff 0.7610571384429932, total time dif 1146.0280578136444)
step: 15480 @ episode report: {'average_total_reward': np.float32(8.38), 'reward_variance': np.float32(4.5817485), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060562312975525855), 'actor_loss': np.float64(-0.9984842836856842), 'hyper_actor_loss': np.float64(0.0014612446539103985), 'behavior_loss': np.float64(1.0473744571208954)}

Episode step 15490, time diff 0.7419347763061523, total time dif 1146.7891149520874)
step: 15490 @ episode report: {'average_total_reward': np.float32(8.190001), 'reward_variance': np.float32(3.6999867), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07211147285997868), 'actor_loss': np.float64(-0.9889534473419189), 'hyper_actor_loss': np.float64(0.001179019338451326), 'behavior_loss': np.float64(1.1095115423202515)}

Episode step 15500, time diff 0.7718594074249268, total time dif 1147.5310497283936)
step: 15500 @ episode report: {'average_total_reward': np.float32(7.6166673), 'reward_variance': np.float32(3.2521546), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06575619280338288), 'actor_loss': np.float64(-1.000307959318161), 'hyper_actor_loss': np.float64(0.0010661835316568614), 'behavior_loss': np.float64(1.078535532951355)}

Episode step 15510, time diff 0.7873356342315674, total time dif 1148.3029091358185)
step: 15510 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(2.0361743), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06412522532045842), 'actor_loss': np.float64(-0.9896035134792328), 'hyper_actor_loss': np.float64(0.0009955564455594867), 'behavior_loss': np.float64(1.0758101403713227)}

Episode step 15520, time diff 0.75528883934021, total time dif 1149.09024477005)
step: 15520 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(1.6496098), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06348604820668698), 'actor_loss': np.float64(-0.9834931492805481), 'hyper_actor_loss': np.float64(0.0008940956380683929), 'behavior_loss': np.float64(1.099104678630829)}

Episode step 15530, time diff 0.7868335247039795, total time dif 1149.8455336093903)
step: 15530 @ episode report: {'average_total_reward': np.float32(8.641111), 'reward_variance': np.float32(1.8269393), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0625738374888897), 'actor_loss': np.float64(-0.9704889237880707), 'hyper_actor_loss': np.float64(0.0008254556683823467), 'behavior_loss': np.float64(1.06098153591156)}

Episode step 15540, time diff 0.7615365982055664, total time dif 1150.6323671340942)
step: 15540 @ episode report: {'average_total_reward': np.float32(8.753334), 'reward_variance': np.float32(1.3255999), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06328905411064625), 'actor_loss': np.float64(-0.9905058979988098), 'hyper_actor_loss': np.float64(0.0007748063479084521), 'behavior_loss': np.float64(1.179428803920746)}

Episode step 15550, time diff 0.7813436985015869, total time dif 1151.3939037322998)
step: 15550 @ episode report: {'average_total_reward': np.float32(7.8288894), 'reward_variance': np.float32(1.4560049), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06836933456361294), 'actor_loss': np.float64(-0.9786935269832611), 'hyper_actor_loss': np.float64(0.000714060664176941), 'behavior_loss': np.float64(1.0363009691238403)}

Episode step 15560, time diff 0.9141120910644531, total time dif 1152.1752474308014)
step: 15560 @ episode report: {'average_total_reward': np.float32(7.5922217), 'reward_variance': np.float32(4.1703463), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06615614742040635), 'actor_loss': np.float64(-0.9951924622058869), 'hyper_actor_loss': np.float64(0.0006826005002949387), 'behavior_loss': np.float64(1.0904598236083984)}

Episode step 15570, time diff 0.7796468734741211, total time dif 1153.0893595218658)
step: 15570 @ episode report: {'average_total_reward': np.float32(7.9777784), 'reward_variance': np.float32(1.6193831), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058992974832654), 'actor_loss': np.float64(-0.9753314971923828), 'hyper_actor_loss': np.float64(0.0006650382827501744), 'behavior_loss': np.float64(1.0925214409828186)}

Episode step 15580, time diff 0.7879772186279297, total time dif 1153.86900639534)
step: 15580 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(2.2469437), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06269377376884222), 'actor_loss': np.float64(-0.960255753993988), 'hyper_actor_loss': np.float64(0.0006455155496951192), 'behavior_loss': np.float64(1.0409685134887696)}

Episode step 15590, time diff 0.8431687355041504, total time dif 1154.656983613968)
step: 15590 @ episode report: {'average_total_reward': np.float32(7.518889), 'reward_variance': np.float32(1.0444216), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059301503002643585), 'actor_loss': np.float64(-1.002978503704071), 'hyper_actor_loss': np.float64(0.0006633236538618803), 'behavior_loss': np.float64(1.0721177577972412)}

Episode step 15600, time diff 0.8086738586425781, total time dif 1155.500152349472)
step: 15600 @ episode report: {'average_total_reward': np.float32(8.067778), 'reward_variance': np.float32(0.731789), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06895008608698845), 'actor_loss': np.float64(-0.9654741942882538), 'hyper_actor_loss': np.float64(0.0006578599044587463), 'behavior_loss': np.float64(1.164356881380081)}

Episode step 15610, time diff 0.823387861251831, total time dif 1156.3088262081146)
step: 15610 @ episode report: {'average_total_reward': np.float32(8.453334), 'reward_variance': np.float32(1.966835), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0717535674571991), 'actor_loss': np.float64(-0.9757141828536987), 'hyper_actor_loss': np.float64(0.0006708077911753207), 'behavior_loss': np.float64(1.1016521453857422)}

Episode step 15620, time diff 0.8399910926818848, total time dif 1157.1322140693665)
step: 15620 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(3.2135613), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05474581234157085), 'actor_loss': np.float64(-0.974018108844757), 'hyper_actor_loss': np.float64(0.0007677373068872839), 'behavior_loss': np.float64(1.1364364922046661)}

Episode step 15630, time diff 0.8044471740722656, total time dif 1157.9722051620483)
step: 15630 @ episode report: {'average_total_reward': np.float32(8.265555), 'reward_variance': np.float32(4.308529), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061532742157578466), 'actor_loss': np.float64(-0.9685751378536225), 'hyper_actor_loss': np.float64(0.0007864999293815345), 'behavior_loss': np.float64(1.0198710560798645)}

Episode step 15640, time diff 0.8180899620056152, total time dif 1158.7766523361206)
step: 15640 @ episode report: {'average_total_reward': np.float32(8.526668), 'reward_variance': np.float32(3.8105736), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06368508189916611), 'actor_loss': np.float64(-0.9928539216518402), 'hyper_actor_loss': np.float64(0.0007841054350137711), 'behavior_loss': np.float64(1.0737860321998596)}

Episode step 15650, time diff 0.8119258880615234, total time dif 1159.5947422981262)
step: 15650 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(1.6067667), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06838912293314933), 'actor_loss': np.float64(-0.9850782692432404), 'hyper_actor_loss': np.float64(0.0007447674113791436), 'behavior_loss': np.float64(1.0477264106273652)}

Episode step 15660, time diff 0.8368360996246338, total time dif 1160.4066681861877)
step: 15660 @ episode report: {'average_total_reward': np.float32(8.416667), 'reward_variance': np.float32(0.50526565), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058368497714400294), 'actor_loss': np.float64(-0.9879914164543152), 'hyper_actor_loss': np.float64(0.0006863084912765771), 'behavior_loss': np.float64(0.9627750456333161)}

Episode step 15670, time diff 0.8360161781311035, total time dif 1161.2435042858124)
step: 15670 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(1.4314635), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.052388533018529415), 'actor_loss': np.float64(-0.9746950566768646), 'hyper_actor_loss': np.float64(0.0006587466748896986), 'behavior_loss': np.float64(1.0612766921520234)}

Episode step 15680, time diff 0.852999210357666, total time dif 1162.0795204639435)
step: 15680 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(0.7592151), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05469322046265006), 'actor_loss': np.float64(-0.9598828256130219), 'hyper_actor_loss': np.float64(0.0006946544745005667), 'behavior_loss': np.float64(1.0507197439670564)}

Episode step 15690, time diff 0.8511850833892822, total time dif 1162.9325196743011)
step: 15690 @ episode report: {'average_total_reward': np.float32(8.353333), 'reward_variance': np.float32(2.6573038), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058504540286958215), 'actor_loss': np.float64(-0.9650098979473114), 'hyper_actor_loss': np.float64(0.0007615446287672967), 'behavior_loss': np.float64(1.0334477245807647)}

Episode step 15700, time diff 0.8068068027496338, total time dif 1163.7837047576904)
step: 15700 @ episode report: {'average_total_reward': np.float32(10.224444), 'reward_variance': np.float32(3.5085132), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07114316672086715), 'actor_loss': np.float64(-0.9941684663295746), 'hyper_actor_loss': np.float64(0.0007658749935217201), 'behavior_loss': np.float64(0.9812208354473114)}

Episode step 15710, time diff 0.8027234077453613, total time dif 1164.59051156044)
step: 15710 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(2.8795118), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07177805379033089), 'actor_loss': np.float64(-1.0172529339790344), 'hyper_actor_loss': np.float64(0.0007197488739620894), 'behavior_loss': np.float64(1.023370122909546)}

Episode step 15720, time diff 0.9899191856384277, total time dif 1165.3932349681854)
step: 15720 @ episode report: {'average_total_reward': np.float32(9.961112), 'reward_variance': np.float32(1.5454134), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05723704844713211), 'actor_loss': np.float64(-0.9753696739673614), 'hyper_actor_loss': np.float64(0.0007798109028954059), 'behavior_loss': np.float64(0.9649513185024261)}

Episode step 15730, time diff 0.8889939785003662, total time dif 1166.3831541538239)
step: 15730 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(1.1156298), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06381638385355473), 'actor_loss': np.float64(-0.964760559797287), 'hyper_actor_loss': np.float64(0.0011704559030476958), 'behavior_loss': np.float64(1.0982861936092376)}

Episode step 15740, time diff 0.8256011009216309, total time dif 1167.2721481323242)
step: 15740 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(1.9824255), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07633892986923456), 'actor_loss': np.float64(-0.9953748285770416), 'hyper_actor_loss': np.float64(0.0011664583871606737), 'behavior_loss': np.float64(1.048066633939743)}

Episode step 15750, time diff 0.8212275505065918, total time dif 1168.0977492332458)
step: 15750 @ episode report: {'average_total_reward': np.float32(9.8977785), 'reward_variance': np.float32(3.6656494), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07810255754739046), 'actor_loss': np.float64(-1.0104094624519349), 'hyper_actor_loss': np.float64(0.000814340024953708), 'behavior_loss': np.float64(1.0257391512393952)}

Episode step 15760, time diff 0.8388795852661133, total time dif 1168.9189767837524)
step: 15760 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(3.5003505), 'max_total_reward': np.float32(14.388888), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0643107008188963), 'actor_loss': np.float64(-0.99229975938797), 'hyper_actor_loss': np.float64(0.0006493013584986329), 'behavior_loss': np.float64(0.999946939945221)}

Episode step 15770, time diff 0.8063359260559082, total time dif 1169.7578563690186)
step: 15770 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(3.0257154), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07219005674123764), 'actor_loss': np.float64(-0.9894978106021881), 'hyper_actor_loss': np.float64(0.0005817850935272873), 'behavior_loss': np.float64(0.9705006718635559)}

Episode step 15780, time diff 0.7998454570770264, total time dif 1170.5641922950745)
step: 15780 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(0.6018821), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07310041878372431), 'actor_loss': np.float64(-0.9932823300361633), 'hyper_actor_loss': np.float64(0.0005474551464430988), 'behavior_loss': np.float64(1.100604671239853)}

Episode step 15790, time diff 0.8272068500518799, total time dif 1171.3640377521515)
step: 15790 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(5.2967653), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07202502191066742), 'actor_loss': np.float64(-0.9910466790199279), 'hyper_actor_loss': np.float64(0.0005408195138443261), 'behavior_loss': np.float64(1.0190572917461396)}

Episode step 15800, time diff 0.8379037380218506, total time dif 1172.1912446022034)
step: 15800 @ episode report: {'average_total_reward': np.float32(11.371112), 'reward_variance': np.float32(6.057709), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061116059124469754), 'actor_loss': np.float64(-0.9892979443073273), 'hyper_actor_loss': np.float64(0.0005738079926231876), 'behavior_loss': np.float64(0.9988082885742188)}

Episode step 15810, time diff 0.8415522575378418, total time dif 1173.0291483402252)
step: 15810 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(2.8055573), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.051985624432563785), 'actor_loss': np.float64(-0.9603959023952484), 'hyper_actor_loss': np.float64(0.0005052899330621585), 'behavior_loss': np.float64(1.0079167425632476)}

Episode step 15820, time diff 0.8460569381713867, total time dif 1173.870700597763)
step: 15820 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(2.951067), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06918203234672546), 'actor_loss': np.float64(-0.9711711168289184), 'hyper_actor_loss': np.float64(0.0005358464928576723), 'behavior_loss': np.float64(1.0039904534816741)}

Episode step 15830, time diff 0.8594186305999756, total time dif 1174.7167575359344)
step: 15830 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(3.7058387), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07274785190820694), 'actor_loss': np.float64(-1.0106447041034698), 'hyper_actor_loss': np.float64(0.0005210930103203282), 'behavior_loss': np.float64(1.0348685026168822)}

Episode step 15840, time diff 0.8606648445129395, total time dif 1175.5761761665344)
step: 15840 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(3.269921), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06942614801228046), 'actor_loss': np.float64(-0.9940208375453949), 'hyper_actor_loss': np.float64(0.0005292775545967743), 'behavior_loss': np.float64(1.0898770630359649)}

Episode step 15850, time diff 0.8497247695922852, total time dif 1176.4368410110474)
step: 15850 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.540301), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0732592299580574), 'actor_loss': np.float64(-0.9664705336093903), 'hyper_actor_loss': np.float64(0.0016051431011874229), 'behavior_loss': np.float64(1.0015004217624663)}

Episode step 15860, time diff 0.8524022102355957, total time dif 1177.2865657806396)
step: 15860 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(2.7503319), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07182581461966038), 'actor_loss': np.float64(-0.9927293956279755), 'hyper_actor_loss': np.float64(0.001677855453453958), 'behavior_loss': np.float64(1.0165294229984283)}

Episode step 15870, time diff 0.876781702041626, total time dif 1178.1389679908752)
step: 15870 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(3.1833403), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0581444788724184), 'actor_loss': np.float64(-0.983703476190567), 'hyper_actor_loss': np.float64(0.0008811992011032999), 'behavior_loss': np.float64(0.9964376032352448)}

Episode step 15880, time diff 0.9248194694519043, total time dif 1179.0157496929169)
step: 15880 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.3314922), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0625589806586504), 'actor_loss': np.float64(-0.9699703931808472), 'hyper_actor_loss': np.float64(0.0006721590354572982), 'behavior_loss': np.float64(1.008287900686264)}

Episode step 15890, time diff 1.0872957706451416, total time dif 1179.9405691623688)
step: 15890 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(2.4924448), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081558518111705), 'actor_loss': np.float64(-0.9931502282619477), 'hyper_actor_loss': np.float64(0.000552538176998496), 'behavior_loss': np.float64(0.9847047984600067)}

Episode step 15900, time diff 0.9643886089324951, total time dif 1181.027864933014)
step: 15900 @ episode report: {'average_total_reward': np.float32(10.671112), 'reward_variance': np.float32(1.8213867), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07234797179698944), 'actor_loss': np.float64(-0.998287433385849), 'hyper_actor_loss': np.float64(0.0005112348619149998), 'behavior_loss': np.float64(0.9987465381622315)}

Episode step 15910, time diff 1.0231637954711914, total time dif 1181.9922535419464)
step: 15910 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(1.6630974), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06307590678334236), 'actor_loss': np.float64(-0.9897529304027557), 'hyper_actor_loss': np.float64(0.0004997351032216101), 'behavior_loss': np.float64(0.9881059885025024)}

Episode step 15920, time diff 1.130335807800293, total time dif 1183.0154173374176)
step: 15920 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(3.2684937), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07825250923633575), 'actor_loss': np.float64(-0.9815658926963806), 'hyper_actor_loss': np.float64(0.0005175176542252302), 'behavior_loss': np.float64(1.012060397863388)}

Episode step 15930, time diff 1.1353650093078613, total time dif 1184.145753145218)
step: 15930 @ episode report: {'average_total_reward': np.float32(9.336667), 'reward_variance': np.float32(4.234866), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0595244288444519), 'actor_loss': np.float64(-1.0092089295387268), 'hyper_actor_loss': np.float64(0.0004858067055465654), 'behavior_loss': np.float64(0.9534547209739686)}

Episode step 15940, time diff 1.1323604583740234, total time dif 1185.2811181545258)
step: 15940 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(3.2021687), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06688314974308014), 'actor_loss': np.float64(-0.9763083457946777), 'hyper_actor_loss': np.float64(0.0004760788811836392), 'behavior_loss': np.float64(0.9582471132278443)}

Episode step 15950, time diff 1.2192459106445312, total time dif 1186.4134786128998)
step: 15950 @ episode report: {'average_total_reward': np.float32(10.385555), 'reward_variance': np.float32(2.8246431), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08089327476918698), 'actor_loss': np.float64(-0.9975292205810546), 'hyper_actor_loss': np.float64(0.0004494802939007059), 'behavior_loss': np.float64(0.9606945633888244)}

Episode step 15960, time diff 1.1637020111083984, total time dif 1187.6327245235443)
step: 15960 @ episode report: {'average_total_reward': np.float32(10.012224), 'reward_variance': np.float32(3.903913), 'max_total_reward': np.float32(13.900002), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07045462466776371), 'actor_loss': np.float64(-1.0070876061916352), 'hyper_actor_loss': np.float64(0.00047321251768153163), 'behavior_loss': np.float64(0.9927041947841644)}

Episode step 15970, time diff 1.180673360824585, total time dif 1188.7964265346527)
step: 15970 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(3.4873824), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07354945950210094), 'actor_loss': np.float64(-0.988769280910492), 'hyper_actor_loss': np.float64(0.00045999712601769717), 'behavior_loss': np.float64(1.0281925797462463)}

Episode step 15980, time diff 1.1791975498199463, total time dif 1189.9770998954773)
step: 15980 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(4.623561), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0607585821300745), 'actor_loss': np.float64(-0.9843583881855011), 'hyper_actor_loss': np.float64(0.00046085059002507476), 'behavior_loss': np.float64(0.9633199572563171)}

Episode step 15990, time diff 1.2043826580047607, total time dif 1191.1562974452972)
step: 15990 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(4.165709), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06957650706171989), 'actor_loss': np.float64(-0.9861110150814056), 'hyper_actor_loss': np.float64(0.00044460257049649954), 'behavior_loss': np.float64(0.9419520616531372)}

Episode step 16000, time diff 1.1895346641540527, total time dif 1192.360680103302)
step: 16000 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.0753578), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06548687182366848), 'actor_loss': np.float64(-0.9972083985805511), 'hyper_actor_loss': np.float64(0.00043692274193745105), 'behavior_loss': np.float64(1.010994815826416)}

Episode step 16010, time diff 1.1982457637786865, total time dif 1193.550214767456)
step: 16010 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.965723), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06249710321426392), 'actor_loss': np.float64(-0.9852109849452972), 'hyper_actor_loss': np.float64(0.0004621408472303301), 'behavior_loss': np.float64(0.9633776307106018)}

Episode step 16020, time diff 1.203711748123169, total time dif 1194.7484605312347)
step: 16020 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.1316097), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07290877476334572), 'actor_loss': np.float64(-0.986783891916275), 'hyper_actor_loss': np.float64(0.0005025204736739397), 'behavior_loss': np.float64(0.9701830625534058)}

Episode step 16030, time diff 1.23264479637146, total time dif 1195.952172279358)
step: 16030 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(1.1395128), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061101964116096495), 'actor_loss': np.float64(-1.001346719264984), 'hyper_actor_loss': np.float64(0.0005237030942225829), 'behavior_loss': np.float64(0.9885214686393737)}

Episode step 16040, time diff 1.2036659717559814, total time dif 1197.1848170757294)
step: 16040 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(6.782599), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06360148228704929), 'actor_loss': np.float64(-0.9705792725086212), 'hyper_actor_loss': np.float64(0.0005274631665088236), 'behavior_loss': np.float64(0.8935752868652344)}

Episode step 16050, time diff 1.2202601432800293, total time dif 1198.3884830474854)
step: 16050 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(1.6879256), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06242117211222649), 'actor_loss': np.float64(-0.9875413417816162), 'hyper_actor_loss': np.float64(0.0005862218094989657), 'behavior_loss': np.float64(1.018750160932541)}

Episode step 16060, time diff 1.3958659172058105, total time dif 1199.6087431907654)
step: 16060 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.4281037), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07017647251486778), 'actor_loss': np.float64(-0.9893566370010376), 'hyper_actor_loss': np.float64(0.0005904865392949432), 'behavior_loss': np.float64(0.969799143075943)}

Episode step 16070, time diff 1.1912102699279785, total time dif 1201.0046091079712)
step: 16070 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(2.5491896), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06956609785556793), 'actor_loss': np.float64(-0.9926357269287109), 'hyper_actor_loss': np.float64(0.0005955776083283127), 'behavior_loss': np.float64(0.8989329874515534)}

Episode step 16080, time diff 1.2279841899871826, total time dif 1202.1958193778992)
step: 16080 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(2.5688448), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186168059706688), 'actor_loss': np.float64(-1.0031737208366394), 'hyper_actor_loss': np.float64(0.000621517573017627), 'behavior_loss': np.float64(0.9478319585323334)}

Episode step 16090, time diff 1.213815450668335, total time dif 1203.4238035678864)
step: 16090 @ episode report: {'average_total_reward': np.float32(11.744445), 'reward_variance': np.float32(4.8376045), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06716213710606098), 'actor_loss': np.float64(-0.9821626722812653), 'hyper_actor_loss': np.float64(0.0005836385767906904), 'behavior_loss': np.float64(0.9905509173870086)}

Episode step 16100, time diff 1.2118635177612305, total time dif 1204.6376190185547)
step: 16100 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(3.3363824), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664345059543848), 'actor_loss': np.float64(-0.9707840740680694), 'hyper_actor_loss': np.float64(0.000562979286769405), 'behavior_loss': np.float64(1.029871553182602)}

Episode step 16110, time diff 1.2305951118469238, total time dif 1205.849482536316)
step: 16110 @ episode report: {'average_total_reward': np.float32(10.983334), 'reward_variance': np.float32(3.158895), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07018422298133373), 'actor_loss': np.float64(-1.006990170478821), 'hyper_actor_loss': np.float64(0.0005335139343515038), 'behavior_loss': np.float64(0.9344675898551941)}

Episode step 16120, time diff 1.2217252254486084, total time dif 1207.0800776481628)
step: 16120 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(3.8962426), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06747409421950579), 'actor_loss': np.float64(-0.998171353340149), 'hyper_actor_loss': np.float64(0.0004725773906102404), 'behavior_loss': np.float64(1.0288928627967835)}

Episode step 16130, time diff 1.22243070602417, total time dif 1208.3018028736115)
step: 16130 @ episode report: {'average_total_reward': np.float32(10.658888), 'reward_variance': np.float32(3.9663231), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05615346468985081), 'actor_loss': np.float64(-0.9677876889705658), 'hyper_actor_loss': np.float64(0.00046887777280062436), 'behavior_loss': np.float64(0.9423357903957367)}

Episode step 16140, time diff 1.2409350872039795, total time dif 1209.5242335796356)
step: 16140 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(6.4935355), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07098370753228664), 'actor_loss': np.float64(-0.9809299588203431), 'hyper_actor_loss': np.float64(0.00048268880636896937), 'behavior_loss': np.float64(1.0180627346038817)}

Episode step 16150, time diff 1.2483711242675781, total time dif 1210.7651686668396)
step: 16150 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.3547364), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06978816539049149), 'actor_loss': np.float64(-0.9917490243911743), 'hyper_actor_loss': np.float64(0.0004996767209377139), 'behavior_loss': np.float64(0.9614781618118287)}

Episode step 16160, time diff 1.2706749439239502, total time dif 1212.0135397911072)
step: 16160 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(1.8720801), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07133393175899982), 'actor_loss': np.float64(-1.004457950592041), 'hyper_actor_loss': np.float64(0.0005061852862127125), 'behavior_loss': np.float64(0.9561104059219361)}

Episode step 16170, time diff 1.2355430126190186, total time dif 1213.2842147350311)
step: 16170 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.355938), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07269117310643196), 'actor_loss': np.float64(-0.9933216869831085), 'hyper_actor_loss': np.float64(0.0005011846922570839), 'behavior_loss': np.float64(0.9634464979171753)}

Episode step 16180, time diff 1.195206642150879, total time dif 1214.5197577476501)
step: 16180 @ episode report: {'average_total_reward': np.float32(11.132223), 'reward_variance': np.float32(1.7919124), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06380541883409023), 'actor_loss': np.float64(-0.9849861204624176), 'hyper_actor_loss': np.float64(0.0005677288223523647), 'behavior_loss': np.float64(1.0085058331489563)}

Episode step 16190, time diff 1.2700953483581543, total time dif 1215.714964389801)
step: 16190 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(1.2375264), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06492982991039753), 'actor_loss': np.float64(-0.9771431148052215), 'hyper_actor_loss': np.float64(0.0006017247796989977), 'behavior_loss': np.float64(0.9387995481491089)}

Episode step 16200, time diff 1.2454328536987305, total time dif 1216.9850597381592)
step: 16200 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(2.481567), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06010088138282299), 'actor_loss': np.float64(-0.9948046445846558), 'hyper_actor_loss': np.float64(0.0005508224829100072), 'behavior_loss': np.float64(0.9547724008560181)}

Episode step 16210, time diff 1.2795915603637695, total time dif 1218.230492591858)
step: 16210 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(3.433091), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07786599807441234), 'actor_loss': np.float64(-0.991996031999588), 'hyper_actor_loss': np.float64(0.0005129048513481393), 'behavior_loss': np.float64(0.9809725344181061)}

Episode step 16220, time diff 1.5001392364501953, total time dif 1219.5100841522217)
step: 16220 @ episode report: {'average_total_reward': np.float32(11.332223), 'reward_variance': np.float32(3.8346531), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06520586609840393), 'actor_loss': np.float64(-0.9951368391513824), 'hyper_actor_loss': np.float64(0.0005125793511979281), 'behavior_loss': np.float64(0.9525429308414459)}

Episode step 16230, time diff 1.37501859664917, total time dif 1221.0102233886719)
step: 16230 @ episode report: {'average_total_reward': np.float32(11.307778), 'reward_variance': np.float32(3.1484447), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06347256787121296), 'actor_loss': np.float64(-0.9745828628540039), 'hyper_actor_loss': np.float64(0.0005004041537176818), 'behavior_loss': np.float64(0.9343861699104309)}

Episode step 16240, time diff 1.4591312408447266, total time dif 1222.385241985321)
step: 16240 @ episode report: {'average_total_reward': np.float32(11.5199995), 'reward_variance': np.float32(4.9907365), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0699336625635624), 'actor_loss': np.float64(-0.9847603142261505), 'hyper_actor_loss': np.float64(0.0005418273096438498), 'behavior_loss': np.float64(0.9232645630836487)}

Episode step 16250, time diff 1.5068857669830322, total time dif 1223.8443732261658)
step: 16250 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(0.95936376), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06385901123285294), 'actor_loss': np.float64(-0.9967121660709382), 'hyper_actor_loss': np.float64(0.0005503241904079914), 'behavior_loss': np.float64(0.9927334129810333)}

Episode step 16260, time diff 1.5847752094268799, total time dif 1225.3512589931488)
step: 16260 @ episode report: {'average_total_reward': np.float32(11.92), 'reward_variance': np.float32(4.0768843), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06181594524532556), 'actor_loss': np.float64(-0.9612222611904144), 'hyper_actor_loss': np.float64(0.0005376863613491878), 'behavior_loss': np.float64(0.9508487522602082)}

Episode step 16270, time diff 1.7495293617248535, total time dif 1226.9360342025757)
step: 16270 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(4.613166), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06147810164839029), 'actor_loss': np.float64(-0.9842136204242706), 'hyper_actor_loss': np.float64(0.0005948322010226548), 'behavior_loss': np.float64(0.9276111960411072)}

Episode step 16280, time diff 1.7704105377197266, total time dif 1228.6855635643005)
step: 16280 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(2.9605389), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056900305300951005), 'actor_loss': np.float64(-0.9949939548969269), 'hyper_actor_loss': np.float64(0.0005417989566922187), 'behavior_loss': np.float64(0.9333378553390503)}

Episode step 16290, time diff 1.7517070770263672, total time dif 1230.4559741020203)
step: 16290 @ episode report: {'average_total_reward': np.float32(10.922223), 'reward_variance': np.float32(2.7568889), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058035419695079325), 'actor_loss': np.float64(-0.9607115626335144), 'hyper_actor_loss': np.float64(0.00048373343306593595), 'behavior_loss': np.float64(0.9286617398262024)}

Episode step 16300, time diff 1.8434057235717773, total time dif 1232.2076811790466)
step: 16300 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(0.990767), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06359060481190681), 'actor_loss': np.float64(-0.9908869564533234), 'hyper_actor_loss': np.float64(0.0004646242130547762), 'behavior_loss': np.float64(0.9056971073150635)}

Episode step 16310, time diff 1.86635422706604, total time dif 1234.0510869026184)
step: 16310 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.2033095), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06031381636857987), 'actor_loss': np.float64(-0.988510811328888), 'hyper_actor_loss': np.float64(0.00041317163268104193), 'behavior_loss': np.float64(0.9717767715454102)}

Episode step 16320, time diff 1.8431861400604248, total time dif 1235.9174411296844)
step: 16320 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(4.9630914), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06576317101716996), 'actor_loss': np.float64(-0.9680419087409973), 'hyper_actor_loss': np.float64(0.0004269000346539542), 'behavior_loss': np.float64(0.9890596568584442)}

Episode step 16330, time diff 1.9943599700927734, total time dif 1237.7606272697449)
step: 16330 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(4.3360105), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0734557993710041), 'actor_loss': np.float64(-0.9977345407009125), 'hyper_actor_loss': np.float64(0.00040998929471243174), 'behavior_loss': np.float64(0.8796574294567108)}

Episode step 16340, time diff 2.01869535446167, total time dif 1239.7549872398376)
step: 16340 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.025508), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07021163068711758), 'actor_loss': np.float64(-1.002216786146164), 'hyper_actor_loss': np.float64(0.00041258522833231837), 'behavior_loss': np.float64(0.9610734224319458)}

Episode step 16350, time diff 2.0502326488494873, total time dif 1241.7736825942993)
step: 16350 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(1.7786919), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06861676797270774), 'actor_loss': np.float64(-0.9885575115680695), 'hyper_actor_loss': np.float64(0.00039846732397563756), 'behavior_loss': np.float64(0.9801420509815216)}

Episode step 16360, time diff 2.0014004707336426, total time dif 1243.8239152431488)
step: 16360 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(3.0985458), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07832968309521675), 'actor_loss': np.float64(-0.9930276453495026), 'hyper_actor_loss': np.float64(0.0003994236991275102), 'behavior_loss': np.float64(0.924811202287674)}

Episode step 16370, time diff 2.0371780395507812, total time dif 1245.8253157138824)
step: 16370 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(1.8385947), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06777390539646148), 'actor_loss': np.float64(-1.0007052421569824), 'hyper_actor_loss': np.float64(0.0004170439729932696), 'behavior_loss': np.float64(0.9662005484104157)}

Episode step 16380, time diff 2.0451648235321045, total time dif 1247.8624937534332)
step: 16380 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.2725194), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05825367122888565), 'actor_loss': np.float64(-0.9844545185565948), 'hyper_actor_loss': np.float64(0.0003876340750139207), 'behavior_loss': np.float64(0.9758831083774566)}

Episode step 16390, time diff 2.2399370670318604, total time dif 1249.9076585769653)
step: 16390 @ episode report: {'average_total_reward': np.float32(10.746667), 'reward_variance': np.float32(2.5762422), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08556732460856438), 'actor_loss': np.float64(-0.9829152524471283), 'hyper_actor_loss': np.float64(0.0003902690164977685), 'behavior_loss': np.float64(0.9226210474967956)}

Episode step 16400, time diff 2.1046817302703857, total time dif 1252.1475956439972)
step: 16400 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.8934264), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06443757601082326), 'actor_loss': np.float64(-1.0199195086956023), 'hyper_actor_loss': np.float64(0.0003830401634331793), 'behavior_loss': np.float64(0.9735640227794647)}

Episode step 16410, time diff 2.064572334289551, total time dif 1254.2522773742676)
step: 16410 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.033729), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060528270155191424), 'actor_loss': np.float64(-0.9506171584129334), 'hyper_actor_loss': np.float64(0.00037979523185640576), 'behavior_loss': np.float64(0.9262612819671631)}

Episode step 16420, time diff 2.094236135482788, total time dif 1256.3168497085571)
step: 16420 @ episode report: {'average_total_reward': np.float32(11.66889), 'reward_variance': np.float32(2.4045634), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06663434710353613), 'actor_loss': np.float64(-0.9633241057395935), 'hyper_actor_loss': np.float64(0.00039411093457601963), 'behavior_loss': np.float64(0.9342206299304963)}

Episode step 16430, time diff 2.1000945568084717, total time dif 1258.41108584404)
step: 16430 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(4.330283), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07351189218461514), 'actor_loss': np.float64(-0.9946049332618714), 'hyper_actor_loss': np.float64(0.0003995951090473682), 'behavior_loss': np.float64(1.0572577595710755)}

Episode step 16440, time diff 2.037977695465088, total time dif 1260.5111804008484)
step: 16440 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(2.8020844), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07044843547046184), 'actor_loss': np.float64(-0.9922001421451568), 'hyper_actor_loss': np.float64(0.00038336218567565085), 'behavior_loss': np.float64(0.9015030682086944)}

Episode step 16450, time diff 2.0395240783691406, total time dif 1262.5491580963135)
step: 16450 @ episode report: {'average_total_reward': np.float32(11.744446), 'reward_variance': np.float32(5.3628154), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07767801359295845), 'actor_loss': np.float64(-1.0129707336425782), 'hyper_actor_loss': np.float64(0.00037351889768615364), 'behavior_loss': np.float64(0.9122940480709076)}

Episode step 16460, time diff 1.9186832904815674, total time dif 1264.5886821746826)
step: 16460 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(4.1716847), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07460251189768315), 'actor_loss': np.float64(-1.004214096069336), 'hyper_actor_loss': np.float64(0.0003800208098255098), 'behavior_loss': np.float64(0.9785926818847657)}

Episode step 16470, time diff 1.8252570629119873, total time dif 1266.5073654651642)
step: 16470 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(3.3962338), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06291468553245068), 'actor_loss': np.float64(-0.9832037448883056), 'hyper_actor_loss': np.float64(0.00035365240182727575), 'behavior_loss': np.float64(0.9382791221141815)}

Episode step 16480, time diff 1.7954812049865723, total time dif 1268.3326225280762)
step: 16480 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(3.2076538), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060459695383906366), 'actor_loss': np.float64(-0.9746493816375732), 'hyper_actor_loss': np.float64(0.00036982951278332623), 'behavior_loss': np.float64(0.9127392947673798)}

Episode step 16490, time diff 1.7938683032989502, total time dif 1270.1281037330627)
step: 16490 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(3.4781296), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062325403280556205), 'actor_loss': np.float64(-0.9945185482501984), 'hyper_actor_loss': np.float64(0.00037481604376807807), 'behavior_loss': np.float64(0.9157731592655182)}

Episode step 16500, time diff 1.8035850524902344, total time dif 1271.9219720363617)
step: 16500 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.1862533), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057939381524920466), 'actor_loss': np.float64(-0.9778403580188751), 'hyper_actor_loss': np.float64(0.00036228552635293455), 'behavior_loss': np.float64(0.9668700277805329)}

Episode step 16510, time diff 1.803790807723999, total time dif 1273.725557088852)
step: 16510 @ episode report: {'average_total_reward': np.float32(11.395556), 'reward_variance': np.float32(5.34129), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06914008483290672), 'actor_loss': np.float64(-0.9741580605506897), 'hyper_actor_loss': np.float64(0.000385922915302217), 'behavior_loss': np.float64(0.9375068426132203)}

Episode step 16520, time diff 1.7592856884002686, total time dif 1275.529347896576)
step: 16520 @ episode report: {'average_total_reward': np.float32(11.556667), 'reward_variance': np.float32(3.1363814), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07613494284451008), 'actor_loss': np.float64(-1.0077639877796174), 'hyper_actor_loss': np.float64(0.0003718395921168849), 'behavior_loss': np.float64(0.9363319039344787)}

Episode step 16530, time diff 1.666858434677124, total time dif 1277.2886335849762)
step: 16530 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(4.1833844), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06995734758675098), 'actor_loss': np.float64(-1.0047828257083893), 'hyper_actor_loss': np.float64(0.00036745836841873826), 'behavior_loss': np.float64(0.9286298274993896)}

Episode step 16540, time diff 1.609405755996704, total time dif 1278.9554920196533)
step: 16540 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.8909335), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04895082674920559), 'actor_loss': np.float64(-0.9691525101661682), 'hyper_actor_loss': np.float64(0.00036767148703802376), 'behavior_loss': np.float64(0.8293407678604126)}

Episode step 16550, time diff 1.6641252040863037, total time dif 1280.56489777565)
step: 16550 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(3.418129), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06588460318744183), 'actor_loss': np.float64(-0.9728402972221375), 'hyper_actor_loss': np.float64(0.00036168092919979243), 'behavior_loss': np.float64(0.8793038904666901)}

Episode step 16560, time diff 1.7396013736724854, total time dif 1282.2290229797363)
step: 16560 @ episode report: {'average_total_reward': np.float32(11.520001), 'reward_variance': np.float32(2.2525136), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07235526889562607), 'actor_loss': np.float64(-0.9955334424972534), 'hyper_actor_loss': np.float64(0.00034864367335103453), 'behavior_loss': np.float64(0.9692996203899383)}

Episode step 16570, time diff 1.6250760555267334, total time dif 1283.9686243534088)
step: 16570 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(0.6974186), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0618977976962924), 'actor_loss': np.float64(-0.9924944818019867), 'hyper_actor_loss': np.float64(0.00036336732446216045), 'behavior_loss': np.float64(0.8766322433948517)}

Episode step 16580, time diff 1.6590242385864258, total time dif 1285.5937004089355)
step: 16580 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(4.8307524), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06993929669260979), 'actor_loss': np.float64(-0.992731100320816), 'hyper_actor_loss': np.float64(0.0003622258343966678), 'behavior_loss': np.float64(0.9000120997428894)}

Episode step 16590, time diff 1.646552562713623, total time dif 1287.252724647522)
step: 16590 @ episode report: {'average_total_reward': np.float32(11.881112), 'reward_variance': np.float32(3.6222482), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062493448704481126), 'actor_loss': np.float64(-0.9963051438331604), 'hyper_actor_loss': np.float64(0.00036657305608969184), 'behavior_loss': np.float64(0.8797326683998108)}

Episode step 16600, time diff 1.6824681758880615, total time dif 1288.8992772102356)
step: 16600 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(2.988111), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06995396837592124), 'actor_loss': np.float64(-0.9892609477043152), 'hyper_actor_loss': np.float64(0.0003371113358298317), 'behavior_loss': np.float64(0.8850819051265717)}

Episode step 16610, time diff 1.642967939376831, total time dif 1290.5817453861237)
step: 16610 @ episode report: {'average_total_reward': np.float32(10.858889), 'reward_variance': np.float32(2.2232351), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05856966869905591), 'actor_loss': np.float64(-0.9773228883743286), 'hyper_actor_loss': np.float64(0.00033643061469774694), 'behavior_loss': np.float64(0.8932031512260437)}

Episode step 16620, time diff 1.660198450088501, total time dif 1292.2247133255005)
step: 16620 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.8251157), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06918641645461321), 'actor_loss': np.float64(-0.9810623586177826), 'hyper_actor_loss': np.float64(0.0003674061968922615), 'behavior_loss': np.float64(1.0003578126430512)}

Episode step 16630, time diff 1.6539812088012695, total time dif 1293.884911775589)
step: 16630 @ episode report: {'average_total_reward': np.float32(12.056667), 'reward_variance': np.float32(2.5038137), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06113837063312531), 'actor_loss': np.float64(-0.9821995615959167), 'hyper_actor_loss': np.float64(0.00035581025294959543), 'behavior_loss': np.float64(0.9194753408432007)}

Episode step 16640, time diff 1.6574628353118896, total time dif 1295.5388929843903)
step: 16640 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(2.2862175), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07688909694552422), 'actor_loss': np.float64(-0.9914168834686279), 'hyper_actor_loss': np.float64(0.00035060163063462826), 'behavior_loss': np.float64(0.9664291143417358)}

Episode step 16650, time diff 1.7045214176177979, total time dif 1297.1963558197021)
step: 16650 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.5529494), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058813360333442685), 'actor_loss': np.float64(-0.9956460118293762), 'hyper_actor_loss': np.float64(0.0003359815280418843), 'behavior_loss': np.float64(0.9217752456665039)}

Episode step 16660, time diff 1.6622533798217773, total time dif 1298.90087723732)
step: 16660 @ episode report: {'average_total_reward': np.float32(11.1466675), 'reward_variance': np.float32(1.7957728), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05613590553402901), 'actor_loss': np.float64(-0.9789492189884186), 'hyper_actor_loss': np.float64(0.0003514895623084158), 'behavior_loss': np.float64(0.8391409158706665)}

Episode step 16670, time diff 1.6464924812316895, total time dif 1300.5631306171417)
step: 16670 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(2.393166), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061753631383180616), 'actor_loss': np.float64(-0.9789746999740601), 'hyper_actor_loss': np.float64(0.00034637304488569496), 'behavior_loss': np.float64(0.9069938778877258)}

Episode step 16680, time diff 1.667046070098877, total time dif 1302.2096230983734)
step: 16680 @ episode report: {'average_total_reward': np.float32(10.983334), 'reward_variance': np.float32(3.0736115), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06907055899500847), 'actor_loss': np.float64(-0.9893169403076172), 'hyper_actor_loss': np.float64(0.00033502225123811514), 'behavior_loss': np.float64(0.8914146542549133)}

Episode step 16690, time diff 1.6814560890197754, total time dif 1303.8766691684723)
step: 16690 @ episode report: {'average_total_reward': np.float32(12.017778), 'reward_variance': np.float32(2.0109189), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06383177153766155), 'actor_loss': np.float64(-1.0081727802753448), 'hyper_actor_loss': np.float64(0.00033148735237773506), 'behavior_loss': np.float64(0.8914264142513275)}

Episode step 16700, time diff 1.7153773307800293, total time dif 1305.558125257492)
step: 16700 @ episode report: {'average_total_reward': np.float32(11.544445), 'reward_variance': np.float32(3.851556), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07594997212290763), 'actor_loss': np.float64(-0.9729932487010956), 'hyper_actor_loss': np.float64(0.0003371111553860828), 'behavior_loss': np.float64(0.9603629827499389)}

Episode step 16710, time diff 1.7183828353881836, total time dif 1307.273502588272)
step: 16710 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(1.6285633), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07194133587181568), 'actor_loss': np.float64(-0.9945477366447448), 'hyper_actor_loss': np.float64(0.00032311738759744913), 'behavior_loss': np.float64(0.9333624124526978)}

Episode step 16720, time diff 1.8721058368682861, total time dif 1308.9918854236603)
step: 16720 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.8655617), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06158234886825085), 'actor_loss': np.float64(-0.9952621698379517), 'hyper_actor_loss': np.float64(0.0003631479892646894), 'behavior_loss': np.float64(0.8818774878978729)}

Episode step 16730, time diff 1.7678112983703613, total time dif 1310.8639912605286)
step: 16730 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(3.8511872), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060059952177107336), 'actor_loss': np.float64(-0.9771844625473023), 'hyper_actor_loss': np.float64(0.00036890935443807394), 'behavior_loss': np.float64(0.8542142271995544)}

Episode step 16740, time diff 1.7841370105743408, total time dif 1312.631802558899)
step: 16740 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(4.458205), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0789383139461279), 'actor_loss': np.float64(-1.0052478551864623), 'hyper_actor_loss': np.float64(0.0004114884970476851), 'behavior_loss': np.float64(0.9087215900421143)}

Episode step 16750, time diff 1.817018747329712, total time dif 1314.4159395694733)
step: 16750 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.0635116), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07471324577927589), 'actor_loss': np.float64(-1.001030308008194), 'hyper_actor_loss': np.float64(0.00035185644519515337), 'behavior_loss': np.float64(0.8114155530929565)}

Episode step 16760, time diff 1.7764921188354492, total time dif 1316.232958316803)
step: 16760 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.0187476), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06434577442705632), 'actor_loss': np.float64(-0.9935085117816925), 'hyper_actor_loss': np.float64(0.0003331930260173976), 'behavior_loss': np.float64(0.9612475156784057)}

Episode step 16770, time diff 1.7664456367492676, total time dif 1318.0094504356384)
step: 16770 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(1.0444992), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07977773062884808), 'actor_loss': np.float64(-0.9766650497913361), 'hyper_actor_loss': np.float64(0.00032550410251133146), 'behavior_loss': np.float64(0.876705139875412)}

Episode step 16780, time diff 1.8132421970367432, total time dif 1319.7758960723877)
step: 16780 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.0551715), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054359065182507035), 'actor_loss': np.float64(-1.0077212035655976), 'hyper_actor_loss': np.float64(0.00031521679484285413), 'behavior_loss': np.float64(0.8369720578193665)}

Episode step 16790, time diff 1.7934045791625977, total time dif 1321.5891382694244)
step: 16790 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(1.7493836), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07965801358222961), 'actor_loss': np.float64(-0.9920338809490203), 'hyper_actor_loss': np.float64(0.0003040160896489397), 'behavior_loss': np.float64(0.9480856537818909)}

Episode step 16800, time diff 1.7897553443908691, total time dif 1323.382542848587)
step: 16800 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(5.106942), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07906667813658715), 'actor_loss': np.float64(-0.9950719118118286), 'hyper_actor_loss': np.float64(0.000299167528282851), 'behavior_loss': np.float64(0.8653054594993591)}

Episode step 16810, time diff 1.8351540565490723, total time dif 1325.172298192978)
step: 16810 @ episode report: {'average_total_reward': np.float32(10.971111), 'reward_variance': np.float32(1.110869), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07154037356376648), 'actor_loss': np.float64(-0.9993730187416077), 'hyper_actor_loss': np.float64(0.00031561803189106286), 'behavior_loss': np.float64(0.9565916538238526)}

Episode step 16820, time diff 1.8173325061798096, total time dif 1327.007452249527)
step: 16820 @ episode report: {'average_total_reward': np.float32(10.597779), 'reward_variance': np.float32(2.6238961), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06356549747288227), 'actor_loss': np.float64(-0.990232527256012), 'hyper_actor_loss': np.float64(0.0002955767995445058), 'behavior_loss': np.float64(0.8444903254508972)}

Episode step 16830, time diff 1.8251771926879883, total time dif 1328.8247847557068)
step: 16830 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.9287522), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061700953543186186), 'actor_loss': np.float64(-0.9900485277175903), 'hyper_actor_loss': np.float64(0.00030182980990502986), 'behavior_loss': np.float64(0.8272857367992401)}

Episode step 16840, time diff 1.8283982276916504, total time dif 1330.6499619483948)
step: 16840 @ episode report: {'average_total_reward': np.float32(10.910001), 'reward_variance': np.float32(2.4959378), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07147709242999553), 'actor_loss': np.float64(-0.9954821765422821), 'hyper_actor_loss': np.float64(0.00040196748450398444), 'behavior_loss': np.float64(0.8566099047660828)}

Episode step 16850, time diff 1.7901654243469238, total time dif 1332.4783601760864)
step: 16850 @ episode report: {'average_total_reward': np.float32(11.944446), 'reward_variance': np.float32(2.6173825), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06619776263833047), 'actor_loss': np.float64(-1.001246565580368), 'hyper_actor_loss': np.float64(0.0003437795705394819), 'behavior_loss': np.float64(0.8480178415775299)}

Episode step 16860, time diff 1.824944257736206, total time dif 1334.2685256004333)
step: 16860 @ episode report: {'average_total_reward': np.float32(11.0222225), 'reward_variance': np.float32(3.0160003), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056707050278782846), 'actor_loss': np.float64(-0.9790699481964111), 'hyper_actor_loss': np.float64(0.0003172433964209631), 'behavior_loss': np.float64(0.857640540599823)}

Episode step 16870, time diff 1.831928014755249, total time dif 1336.0934698581696)
step: 16870 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.145903), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06681210901588201), 'actor_loss': np.float64(-0.9745257794857025), 'hyper_actor_loss': np.float64(0.0003068521531531587), 'behavior_loss': np.float64(0.8356556355953216)}

Episode step 16880, time diff 1.8263368606567383, total time dif 1337.9253978729248)
step: 16880 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(2.8463454), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07377616241574288), 'actor_loss': np.float64(-1.004064792394638), 'hyper_actor_loss': np.float64(0.0003492075658868998), 'behavior_loss': np.float64(0.926930946111679)}

Episode step 16890, time diff 1.9686365127563477, total time dif 1339.7517347335815)
step: 16890 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(4.4804993), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06467008702456951), 'actor_loss': np.float64(-1.0029741525650024), 'hyper_actor_loss': np.float64(0.00030878625984769313), 'behavior_loss': np.float64(0.881197863817215)}

Episode step 16900, time diff 1.8542850017547607, total time dif 1341.720371246338)
step: 16900 @ episode report: {'average_total_reward': np.float32(11.756668), 'reward_variance': np.float32(3.6960855), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06638012565672398), 'actor_loss': np.float64(-0.9708847403526306), 'hyper_actor_loss': np.float64(0.000279137643519789), 'behavior_loss': np.float64(0.9601964354515076)}

Episode step 16910, time diff 1.8684744834899902, total time dif 1343.5746562480927)
step: 16910 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(4.343339), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.048847155272960664), 'actor_loss': np.float64(-0.9711191952228546), 'hyper_actor_loss': np.float64(0.00030920875142328443), 'behavior_loss': np.float64(0.8752811431884766)}

Episode step 16920, time diff 1.912170171737671, total time dif 1345.4431307315826)
step: 16920 @ episode report: {'average_total_reward': np.float32(11.968889), 'reward_variance': np.float32(1.0709337), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.9), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0729854553937912), 'actor_loss': np.float64(-0.9950759828090667), 'hyper_actor_loss': np.float64(0.0002987513435073197), 'behavior_loss': np.float64(0.8673380255699158)}

Episode step 16930, time diff 1.9138174057006836, total time dif 1347.3553009033203)
step: 16930 @ episode report: {'average_total_reward': np.float32(11.395556), 'reward_variance': np.float32(2.9292648), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07521913722157478), 'actor_loss': np.float64(-0.9996567487716674), 'hyper_actor_loss': np.float64(0.0002763673255685717), 'behavior_loss': np.float64(0.8668885111808777)}

Episode step 16940, time diff 1.938952922821045, total time dif 1349.269118309021)
step: 16940 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(9.865294), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061786132678389546), 'actor_loss': np.float64(-0.9912341475486756), 'hyper_actor_loss': np.float64(0.00029577425157185645), 'behavior_loss': np.float64(0.8448735892772674)}

Episode step 16950, time diff 1.8705289363861084, total time dif 1351.208071231842)
step: 16950 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(3.0525641), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.070503481477499), 'actor_loss': np.float64(-0.9922164082527161), 'hyper_actor_loss': np.float64(0.00028895683644805106), 'behavior_loss': np.float64(0.8472885370254517)}

Episode step 16960, time diff 1.928394079208374, total time dif 1353.0786001682281)
step: 16960 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(4.11284), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062424137443304065), 'actor_loss': np.float64(-0.9939957439899445), 'hyper_actor_loss': np.float64(0.0002931110677309334), 'behavior_loss': np.float64(0.9179600179195404)}

Episode step 16970, time diff 1.9083049297332764, total time dif 1355.0069942474365)
step: 16970 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(3.0197396), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05708880852907896), 'actor_loss': np.float64(-0.9603327929973602), 'hyper_actor_loss': np.float64(0.0002736577225732617), 'behavior_loss': np.float64(0.8761232137680054)}

Episode step 16980, time diff 1.8630166053771973, total time dif 1356.9152991771698)
step: 16980 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(1.568735), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06976199448108673), 'actor_loss': np.float64(-0.9917206943035126), 'hyper_actor_loss': np.float64(0.00026493524492252616), 'behavior_loss': np.float64(0.8349198758602142)}

Episode step 16990, time diff 1.8871700763702393, total time dif 1358.778315782547)
step: 16990 @ episode report: {'average_total_reward': np.float32(11.607779), 'reward_variance': np.float32(3.4799042), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06730396896600724), 'actor_loss': np.float64(-1.012982815504074), 'hyper_actor_loss': np.float64(0.0002734607711317949), 'behavior_loss': np.float64(0.9840392291545867)}

Episode step 17000, time diff 1.9165818691253662, total time dif 1360.6654858589172)
step: 17000 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(1.5791805), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06825881563127041), 'actor_loss': np.float64(-0.982062703371048), 'hyper_actor_loss': np.float64(0.00029250693041831254), 'behavior_loss': np.float64(0.8678403079509736)}

Episode step 17010, time diff 2.0324339866638184, total time dif 1362.5820677280426)
step: 17010 @ episode report: {'average_total_reward': np.float32(11.283335), 'reward_variance': np.float32(4.0575857), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07405834160745144), 'actor_loss': np.float64(-0.9891296923160553), 'hyper_actor_loss': np.float64(0.0002821890826453455), 'behavior_loss': np.float64(0.986348009109497)}

Episode step 17020, time diff 2.018183708190918, total time dif 1364.6145017147064)
step: 17020 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(2.2334867), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06407394856214524), 'actor_loss': np.float64(-0.9975055754184723), 'hyper_actor_loss': np.float64(0.00026957682857755574), 'behavior_loss': np.float64(0.8941019535064697)}

Episode step 17030, time diff 1.8864729404449463, total time dif 1366.6326854228973)
step: 17030 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(1.3970237), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07797520719468594), 'actor_loss': np.float64(-0.986430037021637), 'hyper_actor_loss': np.float64(0.00027294969040667636), 'behavior_loss': np.float64(0.9034734666347504)}

Episode step 17040, time diff 1.9582266807556152, total time dif 1368.5191583633423)
step: 17040 @ episode report: {'average_total_reward': np.float32(11.071113), 'reward_variance': np.float32(1.8669434), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05734361726790667), 'actor_loss': np.float64(-0.981080150604248), 'hyper_actor_loss': np.float64(0.0002611500531202182), 'behavior_loss': np.float64(0.8449101030826569)}

Episode step 17050, time diff 2.1147873401641846, total time dif 1370.477385044098)
step: 17050 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.512999), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060109618864953515), 'actor_loss': np.float64(-0.9829165279865265), 'hyper_actor_loss': np.float64(0.0002570834956713952), 'behavior_loss': np.float64(0.863414078950882)}

Episode step 17060, time diff 2.199707269668579, total time dif 1372.592172384262)
step: 17060 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.1183748), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06272744834423065), 'actor_loss': np.float64(-0.9916950464248657), 'hyper_actor_loss': np.float64(0.00026031382294604554), 'behavior_loss': np.float64(0.8316102385520935)}

Episode step 17070, time diff 2.1035995483398438, total time dif 1374.7918796539307)
step: 17070 @ episode report: {'average_total_reward': np.float32(11.856668), 'reward_variance': np.float32(1.9848019), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0608264435082674), 'actor_loss': np.float64(-0.989339143037796), 'hyper_actor_loss': np.float64(0.00029170623456593604), 'behavior_loss': np.float64(0.903051620721817)}

Episode step 17080, time diff 2.151632785797119, total time dif 1376.8954792022705)
step: 17080 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(3.6984394), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06591810248792171), 'actor_loss': np.float64(-0.9857708871364593), 'hyper_actor_loss': np.float64(0.000282070497632958), 'behavior_loss': np.float64(0.8184734523296356)}

Episode step 17090, time diff 2.1279423236846924, total time dif 1379.0471119880676)
step: 17090 @ episode report: {'average_total_reward': np.float32(12.081112), 'reward_variance': np.float32(5.956471), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06895986385643482), 'actor_loss': np.float64(-0.9970192730426788), 'hyper_actor_loss': np.float64(0.00027122050523757933), 'behavior_loss': np.float64(0.876157146692276)}

Episode step 17100, time diff 2.1491925716400146, total time dif 1381.1750543117523)
step: 17100 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.3602225), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055593781359493735), 'actor_loss': np.float64(-0.9781493961811065), 'hyper_actor_loss': np.float64(0.0002534750106860884), 'behavior_loss': np.float64(0.8775571703910827)}

Episode step 17110, time diff 2.1510136127471924, total time dif 1383.3242468833923)
step: 17110 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.392173), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056164903566241264), 'actor_loss': np.float64(-0.9653896629810333), 'hyper_actor_loss': np.float64(0.0002567193077993579), 'behavior_loss': np.float64(0.9547442674636841)}

Episode step 17120, time diff 2.1345088481903076, total time dif 1385.4752604961395)
step: 17120 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(3.1079533), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05825765021145344), 'actor_loss': np.float64(-0.9730220139026642), 'hyper_actor_loss': np.float64(0.00026391486171633004), 'behavior_loss': np.float64(0.8385238111019134)}

Episode step 17130, time diff 2.1579558849334717, total time dif 1387.6097693443298)
step: 17130 @ episode report: {'average_total_reward': np.float32(11.420001), 'reward_variance': np.float32(0.7337976), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07137377485632897), 'actor_loss': np.float64(-0.9864690959453583), 'hyper_actor_loss': np.float64(0.00029391833231784403), 'behavior_loss': np.float64(0.9703059554100036)}

Episode step 17140, time diff 2.0914018154144287, total time dif 1389.7677252292633)
step: 17140 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(2.840489), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060940833576023576), 'actor_loss': np.float64(-0.9932943046092987), 'hyper_actor_loss': np.float64(0.00026268469664501024), 'behavior_loss': np.float64(0.776482105255127)}

Episode step 17150, time diff 1.9725396633148193, total time dif 1391.8591270446777)
step: 17150 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(4.3434877), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06683850139379502), 'actor_loss': np.float64(-0.9962014019489288), 'hyper_actor_loss': np.float64(0.00026510250609135255), 'behavior_loss': np.float64(0.8421944141387939)}

Episode step 17160, time diff 1.9451634883880615, total time dif 1393.8316667079926)
step: 17160 @ episode report: {'average_total_reward': np.float32(11.668889), 'reward_variance': np.float32(2.2125382), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06256275586783885), 'actor_loss': np.float64(-0.9921314060688019), 'hyper_actor_loss': np.float64(0.00028911551198689266), 'behavior_loss': np.float64(0.8945578694343567)}

Episode step 17170, time diff 1.965381383895874, total time dif 1395.7768301963806)
step: 17170 @ episode report: {'average_total_reward': np.float32(11.76889), 'reward_variance': np.float32(1.9702181), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06329205799847841), 'actor_loss': np.float64(-0.9812630951404572), 'hyper_actor_loss': np.float64(0.00029983271670062093), 'behavior_loss': np.float64(0.8354824662208558)}

Episode step 17180, time diff 1.9267950057983398, total time dif 1397.7422115802765)
step: 17180 @ episode report: {'average_total_reward': np.float32(11.732223), 'reward_variance': np.float32(2.5024314), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06279048174619675), 'actor_loss': np.float64(-0.9895141065120697), 'hyper_actor_loss': np.float64(0.0002943881438113749), 'behavior_loss': np.float64(0.9255831241607666)}

Episode step 17190, time diff 1.9667384624481201, total time dif 1399.6690065860748)
step: 17190 @ episode report: {'average_total_reward': np.float32(11.917778), 'reward_variance': np.float32(3.334969), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06028257943689823), 'actor_loss': np.float64(-0.9763051569461823), 'hyper_actor_loss': np.float64(0.0002683457438251935), 'behavior_loss': np.float64(0.8939769983291626)}

Episode step 17200, time diff 1.9967787265777588, total time dif 1401.635745048523)
step: 17200 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(3.1427426), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06844445802271366), 'actor_loss': np.float64(-0.9907628536224365), 'hyper_actor_loss': np.float64(0.0002836881234543398), 'behavior_loss': np.float64(0.8233812630176545)}

Episode step 17210, time diff 1.9499640464782715, total time dif 1403.6325237751007)
step: 17210 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(2.707019), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0655125219374895), 'actor_loss': np.float64(-1.0023826777935028), 'hyper_actor_loss': np.float64(0.0002842738540493883), 'behavior_loss': np.float64(0.8464146256446838)}

Episode step 17220, time diff 2.005237102508545, total time dif 1405.582487821579)
step: 17220 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.7818534), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07062280289828778), 'actor_loss': np.float64(-0.9911316812038422), 'hyper_actor_loss': np.float64(0.00028852857649326326), 'behavior_loss': np.float64(0.8785391032695771)}

Episode step 17230, time diff 2.0540101528167725, total time dif 1407.5877249240875)
step: 17230 @ episode report: {'average_total_reward': np.float32(10.97111), 'reward_variance': np.float32(4.8047204), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05999033004045486), 'actor_loss': np.float64(-0.9799865365028382), 'hyper_actor_loss': np.float64(0.0002872447803383693), 'behavior_loss': np.float64(0.8303659081459045)}

Episode step 17240, time diff 1.9907093048095703, total time dif 1409.6417350769043)
step: 17240 @ episode report: {'average_total_reward': np.float32(10.846666), 'reward_variance': np.float32(6.8670807), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07104987725615501), 'actor_loss': np.float64(-0.9934509217739105), 'hyper_actor_loss': np.float64(0.00026853602612391113), 'behavior_loss': np.float64(0.862971031665802)}

Episode step 17250, time diff 1.9619338512420654, total time dif 1411.6324443817139)
step: 17250 @ episode report: {'average_total_reward': np.float32(9.912223), 'reward_variance': np.float32(1.5853941), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07875323332846165), 'actor_loss': np.float64(-0.9984961807727813), 'hyper_actor_loss': np.float64(0.0002638738282257691), 'behavior_loss': np.float64(0.8727989375591279)}

Episode step 17260, time diff 1.9677908420562744, total time dif 1413.594378232956)
step: 17260 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(4.50045), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06448735147714615), 'actor_loss': np.float64(-0.9919478893280029), 'hyper_actor_loss': np.float64(0.00027555255510378627), 'behavior_loss': np.float64(0.8591362178325653)}

Episode step 17270, time diff 2.1205735206604004, total time dif 1415.5621690750122)
step: 17270 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(2.411624), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05457870066165924), 'actor_loss': np.float64(-0.9759938240051269), 'hyper_actor_loss': np.float64(0.0003146971401292831), 'behavior_loss': np.float64(0.8436258852481842)}

Episode step 17280, time diff 2.181022882461548, total time dif 1417.6827425956726)
step: 17280 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(4.516457), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06794455945491791), 'actor_loss': np.float64(-0.9770346581935883), 'hyper_actor_loss': np.float64(0.00032502963440492747), 'behavior_loss': np.float64(0.8681709349155426)}

Episode step 17290, time diff 2.1767971515655518, total time dif 1419.8637654781342)
step: 17290 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(1.8021193), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06396542638540267), 'actor_loss': np.float64(-1.0008858919143677), 'hyper_actor_loss': np.float64(0.00030529592768289147), 'behavior_loss': np.float64(0.8709340333938599)}

Episode step 17300, time diff 2.150059938430786, total time dif 1422.0405626296997)
step: 17300 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(2.7497833), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06168606206774711), 'actor_loss': np.float64(-0.9887797355651855), 'hyper_actor_loss': np.float64(0.0002677452503121458), 'behavior_loss': np.float64(0.7949812829494476)}

Episode step 17310, time diff 2.1888961791992188, total time dif 1424.1906225681305)
step: 17310 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(3.0504649), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06339382901787757), 'actor_loss': np.float64(-0.9734611988067627), 'hyper_actor_loss': np.float64(0.0002878871775465086), 'behavior_loss': np.float64(0.8650989174842835)}

Episode step 17320, time diff 2.1993050575256348, total time dif 1426.3795187473297)
step: 17320 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(3.6382031), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06268364787101746), 'actor_loss': np.float64(-0.9885433793067933), 'hyper_actor_loss': np.float64(0.0002960971323773265), 'behavior_loss': np.float64(0.890776413679123)}

Episode step 17330, time diff 2.1676270961761475, total time dif 1428.5788238048553)
step: 17330 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(2.6829686), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0632358055561781), 'actor_loss': np.float64(-0.9773377418518067), 'hyper_actor_loss': np.float64(0.0002845165086910129), 'behavior_loss': np.float64(0.7794682204723358)}

Episode step 17340, time diff 2.1673641204833984, total time dif 1430.7464509010315)
step: 17340 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(5.55684), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06704190224409104), 'actor_loss': np.float64(-0.9991887331008911), 'hyper_actor_loss': np.float64(0.00030942462617531417), 'behavior_loss': np.float64(0.8315670669078827)}

Episode step 17350, time diff 2.185563087463379, total time dif 1432.913815021515)
step: 17350 @ episode report: {'average_total_reward': np.float32(11.234446), 'reward_variance': np.float32(2.0677156), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0636601809412241), 'actor_loss': np.float64(-0.9984356880187988), 'hyper_actor_loss': np.float64(0.0002686296735191718), 'behavior_loss': np.float64(0.8556906461715699)}

Episode step 17360, time diff 2.1570146083831787, total time dif 1435.0993781089783)
step: 17360 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(3.40477), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0567440340295434), 'actor_loss': np.float64(-0.9855238497257233), 'hyper_actor_loss': np.float64(0.0002781135714030825), 'behavior_loss': np.float64(0.8094711542129517)}

Episode step 17370, time diff 2.157640218734741, total time dif 1437.2563927173615)
step: 17370 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.9731354), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08203892596065998), 'actor_loss': np.float64(-0.991722822189331), 'hyper_actor_loss': np.float64(0.0002596627615275793), 'behavior_loss': np.float64(0.8538827419281005)}

Episode step 17380, time diff 2.1762237548828125, total time dif 1439.4140329360962)
step: 17380 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(2.6421752), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05686866007745266), 'actor_loss': np.float64(-1.0031757712364198), 'hyper_actor_loss': np.float64(0.00025684106803964825), 'behavior_loss': np.float64(0.8253106296062469)}

Episode step 17390, time diff 2.2025375366210938, total time dif 1441.590256690979)
step: 17390 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(0.91385716), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05744225978851318), 'actor_loss': np.float64(-0.9775866031646728), 'hyper_actor_loss': np.float64(0.00024079777213046328), 'behavior_loss': np.float64(0.7979275226593018)}

Episode step 17400, time diff 2.3260538578033447, total time dif 1443.7927942276)
step: 17400 @ episode report: {'average_total_reward': np.float32(11.332223), 'reward_variance': np.float32(3.4386284), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056980295293033126), 'actor_loss': np.float64(-0.9839283883571625), 'hyper_actor_loss': np.float64(0.00028809797076974065), 'behavior_loss': np.float64(0.9002323508262634)}

Episode step 17410, time diff 2.1419107913970947, total time dif 1446.1188480854034)
step: 17410 @ episode report: {'average_total_reward': np.float32(11.232223), 'reward_variance': np.float32(3.8993707), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0657203197479248), 'actor_loss': np.float64(-0.9883524775505066), 'hyper_actor_loss': np.float64(0.0002739147894317284), 'behavior_loss': np.float64(0.7780444502830506)}

Episode step 17420, time diff 2.1606528759002686, total time dif 1448.2607588768005)
step: 17420 @ episode report: {'average_total_reward': np.float32(12.417778), 'reward_variance': np.float32(1.7278076), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(10.9), 'average_n_step': np.float32(13.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07916278056800366), 'actor_loss': np.float64(-1.0096792936325074), 'hyper_actor_loss': np.float64(0.0002380321398959495), 'behavior_loss': np.float64(0.8255189418792724)}

Episode step 17430, time diff 2.1440086364746094, total time dif 1450.4214117527008)
step: 17430 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(2.2954323), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05852715224027634), 'actor_loss': np.float64(-0.9879096210002899), 'hyper_actor_loss': np.float64(0.00023616214166395367), 'behavior_loss': np.float64(0.8237324893474579)}

Episode step 17440, time diff 2.178964614868164, total time dif 1452.5654203891754)
step: 17440 @ episode report: {'average_total_reward': np.float32(11.195556), 'reward_variance': np.float32(4.609708), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0675053471699357), 'actor_loss': np.float64(-0.9691065609455108), 'hyper_actor_loss': np.float64(0.0002363585474085994), 'behavior_loss': np.float64(0.8052460372447967)}

Episode step 17450, time diff 2.11844539642334, total time dif 1454.7443850040436)
step: 17450 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(2.2458022), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.411112), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06508844532072544), 'actor_loss': np.float64(-1.0031725347042084), 'hyper_actor_loss': np.float64(0.00023801906500011682), 'behavior_loss': np.float64(0.9400272965431213)}

Episode step 17460, time diff 2.1202330589294434, total time dif 1456.862830400467)
step: 17460 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.250668), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0644527468830347), 'actor_loss': np.float64(-0.9779686629772186), 'hyper_actor_loss': np.float64(0.00021389584871940316), 'behavior_loss': np.float64(0.7695941984653473)}

Episode step 17470, time diff 2.0553436279296875, total time dif 1458.9830634593964)
step: 17470 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(6.7593627), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06320522017776967), 'actor_loss': np.float64(-0.982431960105896), 'hyper_actor_loss': np.float64(0.00023583986912854016), 'behavior_loss': np.float64(0.8440501987934113)}

Episode step 17480, time diff 1.934880256652832, total time dif 1461.038407087326)
step: 17480 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(7.2547774), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06850949339568616), 'actor_loss': np.float64(-0.9987616181373596), 'hyper_actor_loss': np.float64(0.00022316543181659653), 'behavior_loss': np.float64(0.8411494612693786)}

Episode step 17490, time diff 2.0412166118621826, total time dif 1462.9732873439789)
step: 17490 @ episode report: {'average_total_reward': np.float32(11.483334), 'reward_variance': np.float32(5.166548), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07069150917232037), 'actor_loss': np.float64(-0.9912598013877869), 'hyper_actor_loss': np.float64(0.000232000979303848), 'behavior_loss': np.float64(0.9064084470272065)}

Episode step 17500, time diff 1.9458355903625488, total time dif 1465.014503955841)
step: 17500 @ episode report: {'average_total_reward': np.float32(12.33), 'reward_variance': np.float32(2.1977792), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06263249665498734), 'actor_loss': np.float64(-0.9837591648101807), 'hyper_actor_loss': np.float64(0.00022351887164404616), 'behavior_loss': np.float64(0.8043144285678864)}

Episode step 17510, time diff 1.9769220352172852, total time dif 1466.9603395462036)
step: 17510 @ episode report: {'average_total_reward': np.float32(12.117779), 'reward_variance': np.float32(1.3980542), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06744140647351742), 'actor_loss': np.float64(-0.9885237693786622), 'hyper_actor_loss': np.float64(0.00021143424237379804), 'behavior_loss': np.float64(0.9063633859157563)}

Episode step 17520, time diff 2.0415122509002686, total time dif 1468.937261581421)
step: 17520 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(3.2454765), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05343190804123878), 'actor_loss': np.float64(-0.9785088181495667), 'hyper_actor_loss': np.float64(0.00024862364225555213), 'behavior_loss': np.float64(0.829312938451767)}

Episode step 17530, time diff 2.021216869354248, total time dif 1470.9787738323212)
step: 17530 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(6.3830914), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07135450169444084), 'actor_loss': np.float64(-0.9809821963310241), 'hyper_actor_loss': np.float64(0.0004881551110884175), 'behavior_loss': np.float64(0.8611369073390961)}

Episode step 17540, time diff 1.9942052364349365, total time dif 1472.9999907016754)
step: 17540 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(1.3279278), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05501602627336979), 'actor_loss': np.float64(-0.9922884047031403), 'hyper_actor_loss': np.float64(0.0003801507467869669), 'behavior_loss': np.float64(0.8095503985881806)}

Episode step 17550, time diff 2.004038095474243, total time dif 1474.9941959381104)
step: 17550 @ episode report: {'average_total_reward': np.float32(11.420001), 'reward_variance': np.float32(2.2176247), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808799654245376), 'actor_loss': np.float64(-0.9893585443496704), 'hyper_actor_loss': np.float64(0.00024392512132180854), 'behavior_loss': np.float64(0.7417292892932892)}

Episode step 17560, time diff 2.099360704421997, total time dif 1476.9982340335846)
step: 17560 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(2.513508), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08117574825882912), 'actor_loss': np.float64(-1.0109856724739075), 'hyper_actor_loss': np.float64(0.00022228610469028355), 'behavior_loss': np.float64(0.8644844770431519)}

Episode step 17570, time diff 1.92903470993042, total time dif 1479.0975947380066)
step: 17570 @ episode report: {'average_total_reward': np.float32(12.093335), 'reward_variance': np.float32(2.0635114), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06343956217169762), 'actor_loss': np.float64(-1.008441948890686), 'hyper_actor_loss': np.float64(0.0002363721388974227), 'behavior_loss': np.float64(0.7861825585365295)}

Episode step 17580, time diff 2.0147430896759033, total time dif 1481.026629447937)
step: 17580 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(1.4496167), 'max_total_reward': np.float32(13.022223), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07046434320509434), 'actor_loss': np.float64(-0.9795760631561279), 'hyper_actor_loss': np.float64(0.00022702517890138552), 'behavior_loss': np.float64(0.8053640484809875)}

Episode step 17590, time diff 2.0048651695251465, total time dif 1483.041372537613)
step: 17590 @ episode report: {'average_total_reward': np.float32(10.907778), 'reward_variance': np.float32(5.262323), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07205337807536125), 'actor_loss': np.float64(-0.9988884508609772), 'hyper_actor_loss': np.float64(0.0002052695752354339), 'behavior_loss': np.float64(0.8818910479545593)}

Episode step 17600, time diff 2.0346617698669434, total time dif 1485.046237707138)
step: 17600 @ episode report: {'average_total_reward': np.float32(11.593334), 'reward_variance': np.float32(6.0954137), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05857345946133137), 'actor_loss': np.float64(-0.979103684425354), 'hyper_actor_loss': np.float64(0.0002269530828925781), 'behavior_loss': np.float64(0.9073717415332794)}

Episode step 17610, time diff 2.049161434173584, total time dif 1487.080899477005)
step: 17610 @ episode report: {'average_total_reward': np.float32(10.671112), 'reward_variance': np.float32(5.1311903), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06169496066868305), 'actor_loss': np.float64(-0.9599272668361664), 'hyper_actor_loss': np.float64(0.00020534859941108153), 'behavior_loss': np.float64(0.8108610033988952)}

Episode step 17620, time diff 2.0161709785461426, total time dif 1489.1300609111786)
step: 17620 @ episode report: {'average_total_reward': np.float32(11.0222225), 'reward_variance': np.float32(2.7426677), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0687067236751318), 'actor_loss': np.float64(-1.0054866015911101), 'hyper_actor_loss': np.float64(0.00019650315371109173), 'behavior_loss': np.float64(0.7685401380062103)}

Episode step 17630, time diff 2.0765695571899414, total time dif 1491.1462318897247)
step: 17630 @ episode report: {'average_total_reward': np.float32(10.609999), 'reward_variance': np.float32(4.6506286), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06688041538000107), 'actor_loss': np.float64(-1.0054538071155548), 'hyper_actor_loss': np.float64(0.0001856740112998523), 'behavior_loss': np.float64(0.8264502346515655)}

Episode step 17640, time diff 2.016373634338379, total time dif 1493.2228014469147)
step: 17640 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(2.830698), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0675392884761095), 'actor_loss': np.float64(-0.9751189053058624), 'hyper_actor_loss': np.float64(0.00021705865947296843), 'behavior_loss': np.float64(0.7479207754135132)}

Episode step 17650, time diff 1.9929499626159668, total time dif 1495.239175081253)
step: 17650 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.435536), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05655694231390953), 'actor_loss': np.float64(-0.9932906150817871), 'hyper_actor_loss': np.float64(0.00021632828138535843), 'behavior_loss': np.float64(0.8460368871688843)}

Episode step 17660, time diff 1.9792096614837646, total time dif 1497.232125043869)
step: 17660 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(4.9544516), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07498198896646499), 'actor_loss': np.float64(-0.984823715686798), 'hyper_actor_loss': np.float64(0.00023169425257947297), 'behavior_loss': np.float64(0.8562992632389068)}

Episode step 17670, time diff 2.039116144180298, total time dif 1499.2113347053528)
step: 17670 @ episode report: {'average_total_reward': np.float32(11.283335), 'reward_variance': np.float32(2.6061795), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.066408159583807), 'actor_loss': np.float64(-0.9947127282619477), 'hyper_actor_loss': np.float64(0.00022163996036397293), 'behavior_loss': np.float64(0.7433318257331848)}

Episode step 17680, time diff 1.9776246547698975, total time dif 1501.250450849533)
step: 17680 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(3.81258), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05972795635461807), 'actor_loss': np.float64(-1.0031130850315093), 'hyper_actor_loss': np.float64(0.00022417239379137754), 'behavior_loss': np.float64(0.7927143454551697)}

Episode step 17690, time diff 2.008730173110962, total time dif 1503.228075504303)
step: 17690 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.9539025), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07278742920607328), 'actor_loss': np.float64(-0.9674980878829956), 'hyper_actor_loss': np.float64(0.00021387219021562487), 'behavior_loss': np.float64(0.8200988888740539)}

Episode step 17700, time diff 2.048461437225342, total time dif 1505.236805677414)
step: 17700 @ episode report: {'average_total_reward': np.float32(10.573334), 'reward_variance': np.float32(3.2160313), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07704170756042003), 'actor_loss': np.float64(-1.0150139570236205), 'hyper_actor_loss': np.float64(0.0002238712608232163), 'behavior_loss': np.float64(0.7695810794830322)}

Episode step 17710, time diff 2.0040972232818604, total time dif 1507.2852671146393)
step: 17710 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(1.2604944), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060941073670983316), 'actor_loss': np.float64(-0.9989539921283722), 'hyper_actor_loss': np.float64(0.0002472719963407144), 'behavior_loss': np.float64(0.8242960155010224)}

Episode step 17720, time diff 2.0361833572387695, total time dif 1509.2893643379211)
step: 17720 @ episode report: {'average_total_reward': np.float32(11.344445), 'reward_variance': np.float32(4.9456544), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07115625385195017), 'actor_loss': np.float64(-0.9714602410793305), 'hyper_actor_loss': np.float64(0.00021529855875996874), 'behavior_loss': np.float64(0.7990793347358703)}

Episode step 17730, time diff 2.2297275066375732, total time dif 1511.32554769516)
step: 17730 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(1.390273), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0710136815905571), 'actor_loss': np.float64(-1.0084483742713928), 'hyper_actor_loss': np.float64(0.00018061426089843735), 'behavior_loss': np.float64(0.7837763726711273)}

Episode step 17740, time diff 2.0778799057006836, total time dif 1513.5552752017975)
step: 17740 @ episode report: {'average_total_reward': np.float32(12.005556), 'reward_variance': np.float32(3.462574), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05914810672402382), 'actor_loss': np.float64(-0.9931253969669342), 'hyper_actor_loss': np.float64(0.00018494148680474608), 'behavior_loss': np.float64(0.733773386478424)}

Episode step 17750, time diff 2.0618045330047607, total time dif 1515.6331551074982)
step: 17750 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(0.6016046), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06810660399496556), 'actor_loss': np.float64(-0.9903640210628509), 'hyper_actor_loss': np.float64(0.0001897207708680071), 'behavior_loss': np.float64(0.7331512331962585)}

Episode step 17760, time diff 2.049276828765869, total time dif 1517.694959640503)
step: 17760 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(3.4960601), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07326020449399948), 'actor_loss': np.float64(-1.0111448526382447), 'hyper_actor_loss': np.float64(0.00020863690442638472), 'behavior_loss': np.float64(0.8158922791481018)}

Episode step 17770, time diff 2.1037769317626953, total time dif 1519.7442364692688)
step: 17770 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(2.0335555), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06989115998148918), 'actor_loss': np.float64(-0.9862134873867034), 'hyper_actor_loss': np.float64(0.0002045291505055502), 'behavior_loss': np.float64(0.8545377016067505)}

Episode step 17780, time diff 2.0415756702423096, total time dif 1521.8480134010315)
step: 17780 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(1.286061), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07008833140134811), 'actor_loss': np.float64(-0.9831710934638977), 'hyper_actor_loss': np.float64(0.00021467223996296525), 'behavior_loss': np.float64(0.8377180099487305)}

Episode step 17790, time diff 2.1063179969787598, total time dif 1523.8895890712738)
step: 17790 @ episode report: {'average_total_reward': np.float32(11.371112), 'reward_variance': np.float32(3.1439314), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05798060242086649), 'actor_loss': np.float64(-0.9948788285255432), 'hyper_actor_loss': np.float64(0.00021728300052927806), 'behavior_loss': np.float64(0.8192295491695404)}

Episode step 17800, time diff 2.103139638900757, total time dif 1525.9959070682526)
step: 17800 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(4.8760557), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06766665354371071), 'actor_loss': np.float64(-0.9704855144023895), 'hyper_actor_loss': np.float64(0.0002647490211529657), 'behavior_loss': np.float64(0.8252516746520996)}

Episode step 17810, time diff 2.071969985961914, total time dif 1528.0990467071533)
step: 17810 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(6.24444), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055574051849544046), 'actor_loss': np.float64(-0.9848336756229401), 'hyper_actor_loss': np.float64(0.0003087043558480218), 'behavior_loss': np.float64(0.7508752644062042)}

Episode step 17820, time diff 2.120663642883301, total time dif 1530.1710166931152)
step: 17820 @ episode report: {'average_total_reward': np.float32(11.132223), 'reward_variance': np.float32(4.0588007), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06773367207497358), 'actor_loss': np.float64(-0.9995826244354248), 'hyper_actor_loss': np.float64(0.0004177069349680096), 'behavior_loss': np.float64(0.8063304841518402)}

Episode step 17830, time diff 2.138732671737671, total time dif 1532.2916803359985)
step: 17830 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(6.354367), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0651589896529913), 'actor_loss': np.float64(-0.9852568328380584), 'hyper_actor_loss': np.float64(0.0003630196151789278), 'behavior_loss': np.float64(0.7958007037639618)}

Episode step 17840, time diff 2.1547365188598633, total time dif 1534.4304130077362)
step: 17840 @ episode report: {'average_total_reward': np.float32(11.681112), 'reward_variance': np.float32(2.5841253), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07836133427917957), 'actor_loss': np.float64(-1.0152104794979095), 'hyper_actor_loss': np.float64(0.0002995855888002552), 'behavior_loss': np.float64(0.7796522080898285)}

Episode step 17850, time diff 2.133820056915283, total time dif 1536.585149526596)
step: 17850 @ episode report: {'average_total_reward': np.float32(11.544445), 'reward_variance': np.float32(4.6894817), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05920933745801449), 'actor_loss': np.float64(-0.994491845369339), 'hyper_actor_loss': np.float64(0.00029918298532720654), 'behavior_loss': np.float64(0.8050387263298034)}

Episode step 17860, time diff 2.1487395763397217, total time dif 1538.7189695835114)
step: 17860 @ episode report: {'average_total_reward': np.float32(10.871111), 'reward_variance': np.float32(2.2255855), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07017663158476353), 'actor_loss': np.float64(-0.9626338183879852), 'hyper_actor_loss': np.float64(0.00030988770886324345), 'behavior_loss': np.float64(0.7923293769359588)}

Episode step 17870, time diff 2.1161375045776367, total time dif 1540.867709159851)
step: 17870 @ episode report: {'average_total_reward': np.float32(11.444446), 'reward_variance': np.float32(1.7791113), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07384500801563262), 'actor_loss': np.float64(-1.0023893594741822), 'hyper_actor_loss': np.float64(0.00031065364892128854), 'behavior_loss': np.float64(0.7831430971622467)}

Episode step 17880, time diff 2.172609806060791, total time dif 1542.9838466644287)
step: 17880 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(2.455536), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07079652100801467), 'actor_loss': np.float64(-1.0156610369682313), 'hyper_actor_loss': np.float64(0.00032367661478929224), 'behavior_loss': np.float64(0.7416706621646881)}

Episode step 17890, time diff 2.2971134185791016, total time dif 1545.1564564704895)
step: 17890 @ episode report: {'average_total_reward': np.float32(11.395556), 'reward_variance': np.float32(6.350796), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081345692276955), 'actor_loss': np.float64(-0.9818255960941314), 'hyper_actor_loss': np.float64(0.00032193702063523235), 'behavior_loss': np.float64(0.8166713535785675)}

Episode step 17900, time diff 2.1626243591308594, total time dif 1547.4535698890686)
step: 17900 @ episode report: {'average_total_reward': np.float32(9.912223), 'reward_variance': np.float32(1.8342825), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07082345150411129), 'actor_loss': np.float64(-0.9918305575847626), 'hyper_actor_loss': np.float64(0.0002981427503982559), 'behavior_loss': np.float64(0.8314346551895142)}

Episode step 17910, time diff 2.1566786766052246, total time dif 1549.6161942481995)
step: 17910 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.2828648), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06413608156144619), 'actor_loss': np.float64(-1.0005492329597474), 'hyper_actor_loss': np.float64(0.0003944297379348427), 'behavior_loss': np.float64(0.7792218506336213)}

Episode step 17920, time diff 2.181161403656006, total time dif 1551.7728729248047)
step: 17920 @ episode report: {'average_total_reward': np.float32(12.093334), 'reward_variance': np.float32(2.231091), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0700252614915371), 'actor_loss': np.float64(-0.9937456846237183), 'hyper_actor_loss': np.float64(0.0004182443371973932), 'behavior_loss': np.float64(0.8102481424808502)}

Episode step 17930, time diff 2.194791316986084, total time dif 1553.9540343284607)
step: 17930 @ episode report: {'average_total_reward': np.float32(11.72), 'reward_variance': np.float32(1.3400692), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0604760380461812), 'actor_loss': np.float64(-1.0001484215259553), 'hyper_actor_loss': np.float64(0.0003387149656191468), 'behavior_loss': np.float64(0.7848265767097473)}

Episode step 17940, time diff 2.2140510082244873, total time dif 1556.1488256454468)
step: 17940 @ episode report: {'average_total_reward': np.float32(11.720001), 'reward_variance': np.float32(3.4697986), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0538364602252841), 'actor_loss': np.float64(-0.9795766472816467), 'hyper_actor_loss': np.float64(0.00028406264173099773), 'behavior_loss': np.float64(0.770288348197937)}

Episode step 17950, time diff 2.163243532180786, total time dif 1558.3628766536713)
step: 17950 @ episode report: {'average_total_reward': np.float32(11.556667), 'reward_variance': np.float32(4.9593697), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060944913700222966), 'actor_loss': np.float64(-0.9677525103092194), 'hyper_actor_loss': np.float64(0.0002779461385216564), 'behavior_loss': np.float64(0.7787682175636291)}

Episode step 17960, time diff 2.21496319770813, total time dif 1560.526120185852)
step: 17960 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(3.9639034), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05866514667868614), 'actor_loss': np.float64(-0.9966418743133545), 'hyper_actor_loss': np.float64(0.00025097595644183455), 'behavior_loss': np.float64(0.8298661828041076)}

Episode step 17970, time diff 2.25648832321167, total time dif 1562.7410833835602)
step: 17970 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(1.6729431), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08205527476966382), 'actor_loss': np.float64(-0.9975940585136414), 'hyper_actor_loss': np.float64(0.00022603948164032773), 'behavior_loss': np.float64(0.8136473715305328)}

Episode step 17980, time diff 2.2900452613830566, total time dif 1564.9975717067719)
step: 17980 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.4202485), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06338657587766647), 'actor_loss': np.float64(-0.9960733950138092), 'hyper_actor_loss': np.float64(0.0002262786772917025), 'behavior_loss': np.float64(0.8040120124816894)}

Episode step 17990, time diff 2.271791458129883, total time dif 1567.287616968155)
step: 17990 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(2.8401845), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06273823343217373), 'actor_loss': np.float64(-0.9805048108100891), 'hyper_actor_loss': np.float64(0.00022413810656871647), 'behavior_loss': np.float64(0.7482953608036041)}

Episode step 18000, time diff 2.334836006164551, total time dif 1569.5594084262848)
step: 18000 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(2.549783), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0657875657081604), 'actor_loss': np.float64(-0.9925964713096619), 'hyper_actor_loss': np.float64(0.00025503443903289734), 'behavior_loss': np.float64(0.833243316411972)}

Episode step 18010, time diff 2.3768844604492188, total time dif 1571.8942444324493)
step: 18010 @ episode report: {'average_total_reward': np.float32(11.944446), 'reward_variance': np.float32(1.6742471), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060515562817454335), 'actor_loss': np.float64(-0.9735456049442291), 'hyper_actor_loss': np.float64(0.0005379330657888203), 'behavior_loss': np.float64(0.8529989302158356)}

Episode step 18020, time diff 2.284694194793701, total time dif 1574.2711288928986)
step: 18020 @ episode report: {'average_total_reward': np.float32(10.297777), 'reward_variance': np.float32(2.7458217), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06199225168675184), 'actor_loss': np.float64(-0.9792914092540741), 'hyper_actor_loss': np.float64(0.0006224952347110956), 'behavior_loss': np.float64(0.7558939754962921)}

Episode step 18030, time diff 2.3893423080444336, total time dif 1576.5558230876923)
step: 18030 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(5.231165), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07127829007804394), 'actor_loss': np.float64(-1.0030318081378937), 'hyper_actor_loss': np.float64(0.00030142431933199987), 'behavior_loss': np.float64(0.759324562549591)}

Episode step 18040, time diff 2.400087594985962, total time dif 1578.9451653957367)
step: 18040 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.9457724), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05542487483471632), 'actor_loss': np.float64(-0.9807598352432251), 'hyper_actor_loss': np.float64(0.000570669854641892), 'behavior_loss': np.float64(0.7592440366744995)}

Episode step 18050, time diff 2.33369517326355, total time dif 1581.3452529907227)
step: 18050 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(1.6693777), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06940513923764229), 'actor_loss': np.float64(-0.990571141242981), 'hyper_actor_loss': np.float64(0.001174104365054518), 'behavior_loss': np.float64(0.700668340921402)}

Episode step 18060, time diff 2.5073394775390625, total time dif 1583.6789481639862)
step: 18060 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.4017398), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0673721544444561), 'actor_loss': np.float64(-1.0018844723701477), 'hyper_actor_loss': np.float64(0.0006038540042936802), 'behavior_loss': np.float64(0.7457679629325866)}

Episode step 18070, time diff 2.3572094440460205, total time dif 1586.1862876415253)
step: 18070 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(1.2655077), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0598541185259819), 'actor_loss': np.float64(-0.9929630100727082), 'hyper_actor_loss': np.float64(0.0003128268086584285), 'behavior_loss': np.float64(0.7151538610458374)}

Episode step 18080, time diff 2.400895595550537, total time dif 1588.5434970855713)
step: 18080 @ episode report: {'average_total_reward': np.float32(11.1466675), 'reward_variance': np.float32(2.6396751), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06955286655575037), 'actor_loss': np.float64(-0.9857740998268127), 'hyper_actor_loss': np.float64(0.00020055972418049351), 'behavior_loss': np.float64(0.7449482083320618)}

Episode step 18090, time diff 2.42586350440979, total time dif 1590.9443926811218)
step: 18090 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.078346), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06677669771015644), 'actor_loss': np.float64(-1.001628816127777), 'hyper_actor_loss': np.float64(0.00017726839723763987), 'behavior_loss': np.float64(0.7503790915012359)}

Episode step 18100, time diff 2.4559590816497803, total time dif 1593.3702561855316)
step: 18100 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(3.0983405), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06678644642233848), 'actor_loss': np.float64(-1.0004884600639343), 'hyper_actor_loss': np.float64(0.00017829156713560222), 'behavior_loss': np.float64(0.7096501469612122)}

Episode step 18110, time diff 2.6022660732269287, total time dif 1595.8262152671814)
step: 18110 @ episode report: {'average_total_reward': np.float32(11.246668), 'reward_variance': np.float32(3.3202672), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0723323542624712), 'actor_loss': np.float64(-0.9980783581733703), 'hyper_actor_loss': np.float64(0.0001673957478487864), 'behavior_loss': np.float64(0.763583368062973)}

Episode step 18120, time diff 2.6015522480010986, total time dif 1598.4284813404083)
step: 18120 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(3.5563157), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07535932138562203), 'actor_loss': np.float64(-1.000220650434494), 'hyper_actor_loss': np.float64(0.00016376664862036706), 'behavior_loss': np.float64(0.7512091159820556)}

Episode step 18130, time diff 2.619385242462158, total time dif 1601.0300335884094)
step: 18130 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(1.5054932), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06224466543644667), 'actor_loss': np.float64(-0.9996584951877594), 'hyper_actor_loss': np.float64(0.0001633665815461427), 'behavior_loss': np.float64(0.7619217932224274)}

Episode step 18140, time diff 2.561485528945923, total time dif 1603.6494188308716)
step: 18140 @ episode report: {'average_total_reward': np.float32(11.732223), 'reward_variance': np.float32(1.2999127), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06443296261131763), 'actor_loss': np.float64(-0.9718221068382263), 'hyper_actor_loss': np.float64(0.00016694079095032067), 'behavior_loss': np.float64(0.8052111387252807)}

Episode step 18150, time diff 2.5809073448181152, total time dif 1606.2109043598175)
step: 18150 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(2.3323164), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05431363508105278), 'actor_loss': np.float64(-0.9702089011669159), 'hyper_actor_loss': np.float64(0.00017270962707698344), 'behavior_loss': np.float64(0.7346694886684417)}

Episode step 18160, time diff 2.624859094619751, total time dif 1608.7918117046356)
step: 18160 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(1.1737394), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06828584671020507), 'actor_loss': np.float64(-0.9967561364173889), 'hyper_actor_loss': np.float64(0.00017069019959308206), 'behavior_loss': np.float64(0.7701493740081787)}

Episode step 18170, time diff 2.602891206741333, total time dif 1611.4166707992554)
step: 18170 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(3.8635826), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07039577923715115), 'actor_loss': np.float64(-1.0112272024154663), 'hyper_actor_loss': np.float64(0.0001646953824092634), 'behavior_loss': np.float64(0.7738459169864654)}

Episode step 18180, time diff 2.5873851776123047, total time dif 1614.0195620059967)
step: 18180 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.8247225), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06722873263061047), 'actor_loss': np.float64(-0.977824604511261), 'hyper_actor_loss': np.float64(0.00016206906439037995), 'behavior_loss': np.float64(0.7311798989772796)}

Episode step 18190, time diff 2.688598155975342, total time dif 1616.606947183609)
step: 18190 @ episode report: {'average_total_reward': np.float32(11.195555), 'reward_variance': np.float32(5.6975107), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07230919897556305), 'actor_loss': np.float64(-0.9849368631839752), 'hyper_actor_loss': np.float64(0.00016448074457002803), 'behavior_loss': np.float64(0.8020682632923126)}

Episode step 18200, time diff 2.7194840908050537, total time dif 1619.2955453395844)
step: 18200 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(1.1194382), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04901054333895445), 'actor_loss': np.float64(-0.9924226999282837), 'hyper_actor_loss': np.float64(0.0001598318078322336), 'behavior_loss': np.float64(0.7419755399227143)}

Episode step 18210, time diff 2.6708340644836426, total time dif 1622.0150294303894)
step: 18210 @ episode report: {'average_total_reward': np.float32(10.673333), 'reward_variance': np.float32(6.363463), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06745579913258552), 'actor_loss': np.float64(-0.976610255241394), 'hyper_actor_loss': np.float64(0.00015389980399049819), 'behavior_loss': np.float64(0.6812464892864227)}

Episode step 18220, time diff 2.6259665489196777, total time dif 1624.685863494873)
step: 18220 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(2.3477092), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06341493465006351), 'actor_loss': np.float64(-1.0075226068496703), 'hyper_actor_loss': np.float64(0.00016532642184756695), 'behavior_loss': np.float64(0.6867859482765197)}

Episode step 18230, time diff 2.805784225463867, total time dif 1627.3118300437927)
step: 18230 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.4939463), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07495908364653588), 'actor_loss': np.float64(-1.0008447825908662), 'hyper_actor_loss': np.float64(0.00017054185300366954), 'behavior_loss': np.float64(0.7472126901149749)}

Episode step 18240, time diff 2.570246934890747, total time dif 1630.1176142692566)
step: 18240 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(0.65888405), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07022502757608891), 'actor_loss': np.float64(-0.9999758183956147), 'hyper_actor_loss': np.float64(0.00018021586147369816), 'behavior_loss': np.float64(0.7380580365657806)}

Episode step 18250, time diff 2.61344575881958, total time dif 1632.6878612041473)
step: 18250 @ episode report: {'average_total_reward': np.float32(11.817778), 'reward_variance': np.float32(2.821018), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07382260113954545), 'actor_loss': np.float64(-0.9951586425304413), 'hyper_actor_loss': np.float64(0.00017320573824690654), 'behavior_loss': np.float64(0.7738457381725311)}

Episode step 18260, time diff 2.6305322647094727, total time dif 1635.301306962967)
step: 18260 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(3.5842724), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06569345872849226), 'actor_loss': np.float64(-0.9858440577983856), 'hyper_actor_loss': np.float64(0.00020267743384465574), 'behavior_loss': np.float64(0.7686949133872986)}

Episode step 18270, time diff 2.585710048675537, total time dif 1637.9318392276764)
step: 18270 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(2.7823267), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06787214577198028), 'actor_loss': np.float64(-0.9912621557712555), 'hyper_actor_loss': np.float64(0.0002210783219197765), 'behavior_loss': np.float64(0.7247615218162536)}

Episode step 18280, time diff 2.6208903789520264, total time dif 1640.517549276352)
step: 18280 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(4.0122824), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07093112133443355), 'actor_loss': np.float64(-0.9939289629459381), 'hyper_actor_loss': np.float64(0.0002086984270135872), 'behavior_loss': np.float64(0.7369200408458709)}

Episode step 18290, time diff 2.593360424041748, total time dif 1643.138439655304)
step: 18290 @ episode report: {'average_total_reward': np.float32(11.644445), 'reward_variance': np.float32(4.6555805), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06265611089766025), 'actor_loss': np.float64(-0.9957519412040711), 'hyper_actor_loss': np.float64(0.00023698013828834518), 'behavior_loss': np.float64(0.7430165529251098)}

Episode step 18300, time diff 2.6667087078094482, total time dif 1645.7318000793457)
step: 18300 @ episode report: {'average_total_reward': np.float32(10.934445), 'reward_variance': np.float32(4.6893945), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06791415810585022), 'actor_loss': np.float64(-0.9852866113185883), 'hyper_actor_loss': np.float64(0.0002775423548882827), 'behavior_loss': np.float64(0.7268421530723572)}

Episode step 18310, time diff 2.6770026683807373, total time dif 1648.3985087871552)
step: 18310 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.0152204), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0768142968416214), 'actor_loss': np.float64(-1.001644116640091), 'hyper_actor_loss': np.float64(0.00028107800462748853), 'behavior_loss': np.float64(0.7435937404632569)}

Episode step 18320, time diff 2.6782448291778564, total time dif 1651.075511455536)
step: 18320 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(2.203188), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05894852988421917), 'actor_loss': np.float64(-0.9975727379322052), 'hyper_actor_loss': np.float64(0.00023323789791902528), 'behavior_loss': np.float64(0.6916790306568146)}

Episode step 18330, time diff 2.6861538887023926, total time dif 1653.7537562847137)
step: 18330 @ episode report: {'average_total_reward': np.float32(11.132223), 'reward_variance': np.float32(3.3031712), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07485654577612877), 'actor_loss': np.float64(-0.9977426946163177), 'hyper_actor_loss': np.float64(0.00022354875400196761), 'behavior_loss': np.float64(0.7239706933498382)}

Episode step 18340, time diff 2.8405895233154297, total time dif 1656.4399101734161)
step: 18340 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(3.9143944), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05489913616329432), 'actor_loss': np.float64(-1.0011540055274963), 'hyper_actor_loss': np.float64(0.00019032346608582884), 'behavior_loss': np.float64(0.7290723383426666)}

Episode step 18350, time diff 2.800581216812134, total time dif 1659.2804996967316)
step: 18350 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(4.508736), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05205445792526007), 'actor_loss': np.float64(-0.9636741995811462), 'hyper_actor_loss': np.float64(0.00021374409407144413), 'behavior_loss': np.float64(0.6834656476974488)}

Episode step 18360, time diff 2.8610916137695312, total time dif 1662.0810809135437)
step: 18360 @ episode report: {'average_total_reward': np.float32(10.561111), 'reward_variance': np.float32(1.5759075), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07015073262155055), 'actor_loss': np.float64(-0.9938878059387207), 'hyper_actor_loss': np.float64(0.0002654406809597276), 'behavior_loss': np.float64(0.6739075779914856)}

Episode step 18370, time diff 2.88440203666687, total time dif 1664.9421725273132)
step: 18370 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(3.8905063), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07148044109344483), 'actor_loss': np.float64(-1.0216789603233338), 'hyper_actor_loss': np.float64(0.00019463055068627), 'behavior_loss': np.float64(0.7378687560558319)}

Episode step 18380, time diff 2.848087787628174, total time dif 1667.82657456398)
step: 18380 @ episode report: {'average_total_reward': np.float32(10.834444), 'reward_variance': np.float32(4.643146), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0683664083480835), 'actor_loss': np.float64(-0.9857564985752105), 'hyper_actor_loss': np.float64(0.00017322062776656822), 'behavior_loss': np.float64(0.6854226052761078)}

Episode step 18390, time diff 2.9033796787261963, total time dif 1670.6746623516083)
step: 18390 @ episode report: {'average_total_reward': np.float32(10.783335), 'reward_variance': np.float32(2.3202782), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07583812847733498), 'actor_loss': np.float64(-1.0072844445705413), 'hyper_actor_loss': np.float64(0.00016650026227580384), 'behavior_loss': np.float64(0.722926789522171)}

Episode step 18400, time diff 3.0587127208709717, total time dif 1673.5780420303345)
step: 18400 @ episode report: {'average_total_reward': np.float32(11.45889), 'reward_variance': np.float32(2.688645), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.288889), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07003408744931221), 'actor_loss': np.float64(-1.004149842262268), 'hyper_actor_loss': np.float64(0.00015874057135079056), 'behavior_loss': np.float64(0.7724536836147309)}

Episode step 18410, time diff 2.892263889312744, total time dif 1676.6367547512054)
step: 18410 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(5.3908277), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06895561665296554), 'actor_loss': np.float64(-0.9827326595783233), 'hyper_actor_loss': np.float64(0.00015821458509890363), 'behavior_loss': np.float64(0.6719532489776612)}

Episode step 18420, time diff 2.920142889022827, total time dif 1679.5290186405182)
step: 18420 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(2.5323277), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06688638739287853), 'actor_loss': np.float64(-0.9977533757686615), 'hyper_actor_loss': np.float64(0.00016489980625919998), 'behavior_loss': np.float64(0.7483058154582978)}

Episode step 18430, time diff 3.219701051712036, total time dif 1682.449161529541)
step: 18430 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(4.1777477), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06429697759449482), 'actor_loss': np.float64(-0.9871881544589997), 'hyper_actor_loss': np.float64(0.00015439526468981058), 'behavior_loss': np.float64(0.7733111143112182)}

Episode step 18440, time diff 3.108616352081299, total time dif 1685.668862581253)
step: 18440 @ episode report: {'average_total_reward': np.float32(11.158891), 'reward_variance': np.float32(2.352693), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.050707886554300786), 'actor_loss': np.float64(-0.9750930845737458), 'hyper_actor_loss': np.float64(0.00015595573058817535), 'behavior_loss': np.float64(0.6711790025234222)}

Episode step 18450, time diff 2.949211835861206, total time dif 1688.7774789333344)
step: 18450 @ episode report: {'average_total_reward': np.float32(11.271112), 'reward_variance': np.float32(7.0574126), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07442294918000698), 'actor_loss': np.float64(-0.9851721286773681), 'hyper_actor_loss': np.float64(0.00016028755635488778), 'behavior_loss': np.float64(0.7226611018180847)}

Episode step 18460, time diff 2.9551196098327637, total time dif 1691.7266907691956)
step: 18460 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(2.158746), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664671815931797), 'actor_loss': np.float64(-1.013952898979187), 'hyper_actor_loss': np.float64(0.00014759399055037649), 'behavior_loss': np.float64(0.7049171805381775)}

Episode step 18470, time diff 3.0497965812683105, total time dif 1694.6818103790283)
step: 18470 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(5.7208605), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06075516529381275), 'actor_loss': np.float64(-0.9858764171600342), 'hyper_actor_loss': np.float64(0.00015355338691733778), 'behavior_loss': np.float64(0.7359640955924988)}

Episode step 18480, time diff 3.1501197814941406, total time dif 1697.7316069602966)
step: 18480 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(5.986796), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07024336531758309), 'actor_loss': np.float64(-0.981342887878418), 'hyper_actor_loss': np.float64(0.000140032269700896), 'behavior_loss': np.float64(0.7564363420009613)}

Episode step 18490, time diff 3.0382802486419678, total time dif 1700.8817267417908)
step: 18490 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(2.533443), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05539476275444031), 'actor_loss': np.float64(-0.9832078576087951), 'hyper_actor_loss': np.float64(0.00013744653697358444), 'behavior_loss': np.float64(0.7015389740467072)}

Episode step 18500, time diff 3.0378448963165283, total time dif 1703.9200069904327)
step: 18500 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(4.095581), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061045791022479534), 'actor_loss': np.float64(-0.9754621922969818), 'hyper_actor_loss': np.float64(0.00014652308309450745), 'behavior_loss': np.float64(0.7423272788524627)}

Episode step 18510, time diff 3.069528818130493, total time dif 1706.9578518867493)
step: 18510 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(5.048027), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06847788821905851), 'actor_loss': np.float64(-0.9969507277011871), 'hyper_actor_loss': np.float64(0.00014783558290218935), 'behavior_loss': np.float64(0.7337385654449463)}

Episode step 18520, time diff 3.134984254837036, total time dif 1710.0273807048798)
step: 18520 @ episode report: {'average_total_reward': np.float32(11.868891), 'reward_variance': np.float32(5.235157), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05718061737716198), 'actor_loss': np.float64(-0.985442316532135), 'hyper_actor_loss': np.float64(0.00015957854920998216), 'behavior_loss': np.float64(0.7330508530139923)}

Episode step 18530, time diff 3.02893328666687, total time dif 1713.1623649597168)
step: 18530 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(4.7136555), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.288889), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07534369602799415), 'actor_loss': np.float64(-0.9763944625854493), 'hyper_actor_loss': np.float64(0.00015682874218327924), 'behavior_loss': np.float64(0.7492552399635315)}

Episode step 18540, time diff 3.2255020141601562, total time dif 1716.1912982463837)
step: 18540 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(2.6980357), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05628838613629341), 'actor_loss': np.float64(-1.0036676406860352), 'hyper_actor_loss': np.float64(0.00013905469168093987), 'behavior_loss': np.float64(0.6888662964105606)}

Episode step 18550, time diff 3.2220520973205566, total time dif 1719.4168002605438)
step: 18550 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(5.7155867), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06085394509136677), 'actor_loss': np.float64(-0.9791772961616516), 'hyper_actor_loss': np.float64(0.0001457972903153859), 'behavior_loss': np.float64(0.7336827218532562)}

Episode step 18560, time diff 3.0998647212982178, total time dif 1722.6388523578644)
step: 18560 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(2.3531966), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07211237102746963), 'actor_loss': np.float64(-0.9866823196411133), 'hyper_actor_loss': np.float64(0.00015389539184980096), 'behavior_loss': np.float64(0.7650883257389068)}

Episode step 18570, time diff 3.2714905738830566, total time dif 1725.7387170791626)
step: 18570 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(1.5867176), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06520545519888402), 'actor_loss': np.float64(-1.0033892154693604), 'hyper_actor_loss': np.float64(0.0001455265868571587), 'behavior_loss': np.float64(0.7087204337120057)}

Episode step 18580, time diff 3.102334976196289, total time dif 1729.0102076530457)
step: 18580 @ episode report: {'average_total_reward': np.float32(11.868891), 'reward_variance': np.float32(2.6066623), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07348832637071609), 'actor_loss': np.float64(-1.0027948796749115), 'hyper_actor_loss': np.float64(0.0001428181436494924), 'behavior_loss': np.float64(0.7556723237037659)}

Episode step 18590, time diff 3.1704306602478027, total time dif 1732.112542629242)
step: 18590 @ episode report: {'average_total_reward': np.float32(11.158889), 'reward_variance': np.float32(4.5138297), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05526282489299774), 'actor_loss': np.float64(-0.984073406457901), 'hyper_actor_loss': np.float64(0.00015156549197854475), 'behavior_loss': np.float64(0.6602022230625153)}

Episode step 18600, time diff 3.1839802265167236, total time dif 1735.2829732894897)
step: 18600 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(1.0360495), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0661006985232234), 'actor_loss': np.float64(-0.99189572930336), 'hyper_actor_loss': np.float64(0.000135064720234368), 'behavior_loss': np.float64(0.6752980947494507)}

Episode step 18610, time diff 3.0998599529266357, total time dif 1738.4669535160065)
step: 18610 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(1.9937054), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07212073579430581), 'actor_loss': np.float64(-1.0064178645610808), 'hyper_actor_loss': np.float64(0.00014112803182797506), 'behavior_loss': np.float64(0.7001357913017273)}

Episode step 18620, time diff 3.036020278930664, total time dif 1741.566813468933)
step: 18620 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(5.7134776), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0692208606749773), 'actor_loss': np.float64(-0.9923525989055634), 'hyper_actor_loss': np.float64(0.00013949327258160337), 'behavior_loss': np.float64(0.6902927100658417)}

Episode step 18630, time diff 3.0715999603271484, total time dif 1744.6028337478638)
step: 18630 @ episode report: {'average_total_reward': np.float32(11.420001), 'reward_variance': np.float32(3.079996), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06758592128753663), 'actor_loss': np.float64(-0.9896685481071472), 'hyper_actor_loss': np.float64(0.00013759778521489353), 'behavior_loss': np.float64(0.665009081363678)}

Episode step 18640, time diff 3.1312808990478516, total time dif 1747.674433708191)
step: 18640 @ episode report: {'average_total_reward': np.float32(11.507779), 'reward_variance': np.float32(4.095977), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0725287575274706), 'actor_loss': np.float64(-1.0141277313232422), 'hyper_actor_loss': np.float64(0.00013710782368434594), 'behavior_loss': np.float64(0.684861832857132)}

Episode step 18650, time diff 3.133852481842041, total time dif 1750.8057146072388)
step: 18650 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.2072468), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04503139853477478), 'actor_loss': np.float64(-1.000031578540802), 'hyper_actor_loss': np.float64(0.000131910911295563), 'behavior_loss': np.float64(0.6256662607192993)}

Episode step 18660, time diff 3.054649829864502, total time dif 1753.9395670890808)
step: 18660 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(2.9563973), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05304212793707848), 'actor_loss': np.float64(-0.9466941952705383), 'hyper_actor_loss': np.float64(0.00013123716125846842), 'behavior_loss': np.float64(0.7041660606861114)}

Episode step 18670, time diff 3.0888619422912598, total time dif 1756.9942169189453)
step: 18670 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(1.3017982), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06496241465210914), 'actor_loss': np.float64(-0.98664670586586), 'hyper_actor_loss': np.float64(0.00013181373869883828), 'behavior_loss': np.float64(0.7364551603794098)}

Episode step 18680, time diff 3.0964765548706055, total time dif 1760.0830788612366)
step: 18680 @ episode report: {'average_total_reward': np.float32(11.207778), 'reward_variance': np.float32(2.4222236), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07788017839193344), 'actor_loss': np.float64(-1.0187196373939513), 'hyper_actor_loss': np.float64(0.0001386710413498804), 'behavior_loss': np.float64(0.7213632106781006)}

Episode step 18690, time diff 3.05964732170105, total time dif 1763.1795554161072)
step: 18690 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(2.3831909), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0689812783151865), 'actor_loss': np.float64(-0.985372120141983), 'hyper_actor_loss': np.float64(0.00013689736515516416), 'behavior_loss': np.float64(0.7646218597888946)}

Episode step 18700, time diff 3.0789711475372314, total time dif 1766.2392027378082)
step: 18700 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(0.74375325), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06480647884309292), 'actor_loss': np.float64(-0.9838796973228454), 'hyper_actor_loss': np.float64(0.00013696804307983257), 'behavior_loss': np.float64(0.6354673087596894)}

Episode step 18710, time diff 3.079633951187134, total time dif 1769.3181738853455)
step: 18710 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(3.4898827), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06877967193722725), 'actor_loss': np.float64(-1.0172036170959473), 'hyper_actor_loss': np.float64(0.00013510179705917836), 'behavior_loss': np.float64(0.6560706347227097)}

Episode step 18720, time diff 3.130939245223999, total time dif 1772.3978078365326)
step: 18720 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(2.5976841), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06233408972620964), 'actor_loss': np.float64(-0.9982445001602173), 'hyper_actor_loss': np.float64(0.00013123992393957452), 'behavior_loss': np.float64(0.7396588146686554)}

Episode step 18730, time diff 3.2450008392333984, total time dif 1775.5287470817566)
step: 18730 @ episode report: {'average_total_reward': np.float32(11.420001), 'reward_variance': np.float32(2.521378), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06484033912420273), 'actor_loss': np.float64(-0.9767577826976777), 'hyper_actor_loss': np.float64(0.0001229058761964552), 'behavior_loss': np.float64(0.7306190133094788)}

Episode step 18740, time diff 3.087789297103882, total time dif 1778.77374792099)
step: 18740 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.1353288), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05618597306311131), 'actor_loss': np.float64(-0.9953839898109436), 'hyper_actor_loss': np.float64(0.00012448066991055384), 'behavior_loss': np.float64(0.6407890409231186)}

Episode step 18750, time diff 3.1412289142608643, total time dif 1781.8615372180939)
step: 18750 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.4780557), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06315708197653294), 'actor_loss': np.float64(-0.9926955699920654), 'hyper_actor_loss': np.float64(0.0001273582638532389), 'behavior_loss': np.float64(0.6766980290412903)}

Episode step 18760, time diff 3.1894514560699463, total time dif 1785.0027661323547)
step: 18760 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(5.5288343), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06717366892844438), 'actor_loss': np.float64(-0.9890243709087372), 'hyper_actor_loss': np.float64(0.00012778120144503192), 'behavior_loss': np.float64(0.6487492263317108)}

Episode step 18770, time diff 3.1557774543762207, total time dif 1788.1922175884247)
step: 18770 @ episode report: {'average_total_reward': np.float32(11.56889), 'reward_variance': np.float32(3.8528595), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.051822411641478536), 'actor_loss': np.float64(-0.9909751057624817), 'hyper_actor_loss': np.float64(0.00012742770195472986), 'behavior_loss': np.float64(0.6772207021713257)}

Episode step 18780, time diff 3.1432299613952637, total time dif 1791.347995042801)
step: 18780 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.3322473), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06493119541555643), 'actor_loss': np.float64(-0.9827134788036347), 'hyper_actor_loss': np.float64(0.00013409605089691466), 'behavior_loss': np.float64(0.6850998878479004)}

Episode step 18790, time diff 3.1748015880584717, total time dif 1794.4912250041962)
step: 18790 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(3.2145145), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06452901437878608), 'actor_loss': np.float64(-1.0017573058605194), 'hyper_actor_loss': np.float64(0.00012214312591822817), 'behavior_loss': np.float64(0.6643526792526245)}

Episode step 18800, time diff 3.2184152603149414, total time dif 1797.6660265922546)
step: 18800 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.5577236), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06070317439734936), 'actor_loss': np.float64(-0.9932941257953644), 'hyper_actor_loss': np.float64(0.00012146145818405784), 'behavior_loss': np.float64(0.725612860918045)}

Episode step 18810, time diff 3.193286895751953, total time dif 1800.8844418525696)
step: 18810 @ episode report: {'average_total_reward': np.float32(11.371112), 'reward_variance': np.float32(6.4322767), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0666401207447052), 'actor_loss': np.float64(-0.9914548754692077), 'hyper_actor_loss': np.float64(0.00012482404927141033), 'behavior_loss': np.float64(0.6421902865171433)}

Episode step 18820, time diff 3.3074898719787598, total time dif 1804.0777287483215)
step: 18820 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(2.5653832), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07394901849329472), 'actor_loss': np.float64(-1.0254631638526917), 'hyper_actor_loss': np.float64(0.00012107212460250593), 'behavior_loss': np.float64(0.598626685142517)}

Episode step 18830, time diff 3.2348031997680664, total time dif 1807.3852186203003)
step: 18830 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(2.618248), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06715240254998207), 'actor_loss': np.float64(-1.0003988444805145), 'hyper_actor_loss': np.float64(0.00012313952174736187), 'behavior_loss': np.float64(0.6749549865722656)}

Episode step 18840, time diff 3.24298357963562, total time dif 1810.6200218200684)
step: 18840 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.9120502), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058258943259716034), 'actor_loss': np.float64(-0.9727432072162628), 'hyper_actor_loss': np.float64(0.00012890383222838864), 'behavior_loss': np.float64(0.6436407655477524)}

Episode step 18850, time diff 3.2120697498321533, total time dif 1813.863005399704)
step: 18850 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(1.3438823), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06211226359009743), 'actor_loss': np.float64(-1.0032310843467713), 'hyper_actor_loss': np.float64(0.0001250108027306851), 'behavior_loss': np.float64(0.6333483099937439)}

Episode step 18860, time diff 3.3775928020477295, total time dif 1817.0750751495361)
step: 18860 @ episode report: {'average_total_reward': np.float32(11.756668), 'reward_variance': np.float32(3.3893456), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06600174866616726), 'actor_loss': np.float64(-1.0122823238372802), 'hyper_actor_loss': np.float64(0.0001259752367332112), 'behavior_loss': np.float64(0.6511259615421295)}

Episode step 18870, time diff 3.3416965007781982, total time dif 1820.4526679515839)
step: 18870 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.1757035), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06266265287995339), 'actor_loss': np.float64(-0.9871388077735901), 'hyper_actor_loss': np.float64(0.0001328595717495773), 'behavior_loss': np.float64(0.7095063090324402)}

Episode step 18880, time diff 3.275507926940918, total time dif 1823.794364452362)
step: 18880 @ episode report: {'average_total_reward': np.float32(10.746667), 'reward_variance': np.float32(5.2950087), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06746599934995175), 'actor_loss': np.float64(-0.9817283391952515), 'hyper_actor_loss': np.float64(0.00013181154572521335), 'behavior_loss': np.float64(0.6318094611167908)}

Episode step 18890, time diff 3.242727756500244, total time dif 1827.069872379303)
step: 18890 @ episode report: {'average_total_reward': np.float32(11.481112), 'reward_variance': np.float32(2.2794826), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05216258000582456), 'actor_loss': np.float64(-0.9993991434574128), 'hyper_actor_loss': np.float64(0.00014122163338470273), 'behavior_loss': np.float64(0.6348642587661744)}

Episode step 18900, time diff 3.4111645221710205, total time dif 1830.3126001358032)
step: 18900 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(3.5735507), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664284348487854), 'actor_loss': np.float64(-0.9967892050743103), 'hyper_actor_loss': np.float64(0.00013755847030552104), 'behavior_loss': np.float64(0.6791308999061585)}

Episode step 18910, time diff 3.2834153175354004, total time dif 1833.7237646579742)
step: 18910 @ episode report: {'average_total_reward': np.float32(11.868889), 'reward_variance': np.float32(3.5837474), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06121154502034187), 'actor_loss': np.float64(-0.9933411359786988), 'hyper_actor_loss': np.float64(0.00012857303372584283), 'behavior_loss': np.float64(0.6313968420028686)}

Episode step 18920, time diff 3.2218918800354004, total time dif 1837.0071799755096)
step: 18920 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(0.7673202), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06415368802845478), 'actor_loss': np.float64(-0.9852170944213867), 'hyper_actor_loss': np.float64(0.00013349301589187234), 'behavior_loss': np.float64(0.6808473587036132)}

Episode step 18930, time diff 3.288126230239868, total time dif 1840.229071855545)
step: 18930 @ episode report: {'average_total_reward': np.float32(10.7711115), 'reward_variance': np.float32(0.5496594), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05570229422301054), 'actor_loss': np.float64(-0.9871076285839081), 'hyper_actor_loss': np.float64(0.00012866730758105406), 'behavior_loss': np.float64(0.6941829681396484)}

Episode step 18940, time diff 3.301272392272949, total time dif 1843.517198085785)
step: 18940 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(1.8743753), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053891807049512866), 'actor_loss': np.float64(-0.9846733033657074), 'hyper_actor_loss': np.float64(0.00013241408014437185), 'behavior_loss': np.float64(0.6737186968326568)}

Episode step 18950, time diff 3.3872032165527344, total time dif 1846.8184704780579)
step: 18950 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(4.3133445), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06559422891587019), 'actor_loss': np.float64(-0.9865198850631713), 'hyper_actor_loss': np.float64(0.00012540826792246663), 'behavior_loss': np.float64(0.6331994891166687)}

Episode step 18960, time diff 3.3351407051086426, total time dif 1850.2056736946106)
step: 18960 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(3.2080696), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07425536178052425), 'actor_loss': np.float64(-1.0156293511390686), 'hyper_actor_loss': np.float64(0.0001399206965288613), 'behavior_loss': np.float64(0.6620088756084442)}

Episode step 18970, time diff 3.3664846420288086, total time dif 1853.5408143997192)
step: 18970 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(3.427897), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062498484179377554), 'actor_loss': np.float64(-1.0168539524078368), 'hyper_actor_loss': np.float64(0.00013126816920703278), 'behavior_loss': np.float64(0.6423891246318817)}

Episode step 18980, time diff 3.4857938289642334, total time dif 1856.907299041748)
step: 18980 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.1619124), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06136992275714874), 'actor_loss': np.float64(-0.9832605659961701), 'hyper_actor_loss': np.float64(0.00012282496973057278), 'behavior_loss': np.float64(0.6505192160606384)}

Episode step 18990, time diff 3.5095226764678955, total time dif 1860.3930928707123)
step: 18990 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(5.325952), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06475538648664951), 'actor_loss': np.float64(-0.9795049607753754), 'hyper_actor_loss': np.float64(0.00012881485163234173), 'behavior_loss': np.float64(0.7489942848682404)}

Episode step 19000, time diff 3.5213816165924072, total time dif 1863.9026155471802)
step: 19000 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.7803817), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06065987832844257), 'actor_loss': np.float64(-0.9883297741413116), 'hyper_actor_loss': np.float64(0.00012113842676626518), 'behavior_loss': np.float64(0.6431369453668594)}

Episode step 19010, time diff 3.7144012451171875, total time dif 1867.4239971637726)
step: 19010 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.095309), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06643988899886608), 'actor_loss': np.float64(-1.0032496750354767), 'hyper_actor_loss': np.float64(0.00011798433042713441), 'behavior_loss': np.float64(0.6838227033615112)}

Episode step 19020, time diff 3.744105100631714, total time dif 1871.1383984088898)
step: 19020 @ episode report: {'average_total_reward': np.float32(10.783335), 'reward_variance': np.float32(3.1661792), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0736819103360176), 'actor_loss': np.float64(-0.9896513819694519), 'hyper_actor_loss': np.float64(0.00012410541166900656), 'behavior_loss': np.float64(0.6777159690856933)}

Episode step 19030, time diff 3.9282660484313965, total time dif 1874.8825035095215)
step: 19030 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(3.2849827), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06275173462927341), 'actor_loss': np.float64(-1.0004785478115081), 'hyper_actor_loss': np.float64(0.00012637802065000868), 'behavior_loss': np.float64(0.6035204231739044)}

Episode step 19040, time diff 3.591384172439575, total time dif 1878.8107695579529)
step: 19040 @ episode report: {'average_total_reward': np.float32(10.671112), 'reward_variance': np.float32(1.84882), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05980458315461874), 'actor_loss': np.float64(-1.0003553807735444), 'hyper_actor_loss': np.float64(0.0001269214706553612), 'behavior_loss': np.float64(0.6536900371313095)}

Episode step 19050, time diff 3.5973706245422363, total time dif 1882.4021537303925)
step: 19050 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(6.1937585), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05845348536968231), 'actor_loss': np.float64(-0.9876160383224487), 'hyper_actor_loss': np.float64(0.00012447237168089487), 'behavior_loss': np.float64(0.6282048165798187)}

Episode step 19060, time diff 3.5151941776275635, total time dif 1885.9995243549347)
step: 19060 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(2.511927), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05864595174789429), 'actor_loss': np.float64(-0.9924771368503571), 'hyper_actor_loss': np.float64(0.00012081858440069482), 'behavior_loss': np.float64(0.6694954365491868)}

Episode step 19070, time diff 3.815749168395996, total time dif 1889.5147185325623)
step: 19070 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.1402965), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05805639717727899), 'actor_loss': np.float64(-0.9880365014076233), 'hyper_actor_loss': np.float64(0.0001217848555825185), 'behavior_loss': np.float64(0.6093247771263123)}

Episode step 19080, time diff 3.7772061824798584, total time dif 1893.3304677009583)
step: 19080 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(1.2009145), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06291160099208355), 'actor_loss': np.float64(-0.9887532413005828), 'hyper_actor_loss': np.float64(0.00012217009643791243), 'behavior_loss': np.float64(0.6590158015489578)}

Episode step 19090, time diff 3.5886240005493164, total time dif 1897.107673883438)
step: 19090 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(0.4818965), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07049705609679222), 'actor_loss': np.float64(-1.0075666785240174), 'hyper_actor_loss': np.float64(0.00012168030443717725), 'behavior_loss': np.float64(0.6934103608131409)}

Episode step 19100, time diff 3.604825973510742, total time dif 1900.6962978839874)
step: 19100 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(4.0242586), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05874631404876709), 'actor_loss': np.float64(-0.9992960691452026), 'hyper_actor_loss': np.float64(0.00012900061847176403), 'behavior_loss': np.float64(0.6415882468223572)}

Episode step 19110, time diff 3.594228744506836, total time dif 1904.3011238574982)
step: 19110 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(2.8401845), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06223055310547352), 'actor_loss': np.float64(-0.9760397374629974), 'hyper_actor_loss': np.float64(0.00012043310998706147), 'behavior_loss': np.float64(0.6636908113956451)}

Episode step 19120, time diff 3.6560170650482178, total time dif 1907.895352602005)
step: 19120 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(2.6695175), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06268939878791571), 'actor_loss': np.float64(-1.0007454931735993), 'hyper_actor_loss': np.float64(0.0001256633608136326), 'behavior_loss': np.float64(0.6302255839109421)}

Episode step 19130, time diff 3.686192750930786, total time dif 1911.5513696670532)
step: 19130 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(4.6387267), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060812714695930484), 'actor_loss': np.float64(-0.9964008152484893), 'hyper_actor_loss': np.float64(0.00013645903527503833), 'behavior_loss': np.float64(0.608271986246109)}

Episode step 19140, time diff 3.6486024856567383, total time dif 1915.237562417984)
step: 19140 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(2.990992), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07474196292459964), 'actor_loss': np.float64(-0.9857378363609314), 'hyper_actor_loss': np.float64(0.00012885723044746554), 'behavior_loss': np.float64(0.6988142013549805)}

Episode step 19150, time diff 3.634913682937622, total time dif 1918.8861649036407)
step: 19150 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(4.29857), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06663220077753067), 'actor_loss': np.float64(-1.015282940864563), 'hyper_actor_loss': np.float64(0.00012704030523309485), 'behavior_loss': np.float64(0.6370697021484375)}

Episode step 19160, time diff 3.566195487976074, total time dif 1922.5210785865784)
step: 19160 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(3.6699212), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056732244044542315), 'actor_loss': np.float64(-0.9992777407169342), 'hyper_actor_loss': np.float64(0.000123150106810499), 'behavior_loss': np.float64(0.6463434159755707)}

Episode step 19170, time diff 3.5339298248291016, total time dif 1926.0872740745544)
step: 19170 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.5970571), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05940007148310542), 'actor_loss': np.float64(-0.9796404421329499), 'hyper_actor_loss': np.float64(0.00013089952044538223), 'behavior_loss': np.float64(0.6525995761156083)}

Episode step 19180, time diff 3.6756954193115234, total time dif 1929.6212038993835)
step: 19180 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(0.96684086), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0670749818906188), 'actor_loss': np.float64(-0.9902402579784393), 'hyper_actor_loss': np.float64(0.00012694655670202336), 'behavior_loss': np.float64(0.629399886727333)}

Episode step 19190, time diff 3.9093329906463623, total time dif 1933.296899318695)
step: 19190 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.129962), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0726937934756279), 'actor_loss': np.float64(-1.013154649734497), 'hyper_actor_loss': np.float64(0.0001344322576187551), 'behavior_loss': np.float64(0.6644028306007386)}

Episode step 19200, time diff 3.8312366008758545, total time dif 1937.2062323093414)
step: 19200 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(2.0933197), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056706879660487174), 'actor_loss': np.float64(-0.9963428258895874), 'hyper_actor_loss': np.float64(0.00012406299283611587), 'behavior_loss': np.float64(0.6482553303241729)}

Episode step 19210, time diff 3.784937620162964, total time dif 1941.0374689102173)
step: 19210 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.650159), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664397832006216), 'actor_loss': np.float64(-0.988489031791687), 'hyper_actor_loss': np.float64(0.0001410744822351262), 'behavior_loss': np.float64(0.624398472905159)}

Episode step 19220, time diff 3.490767002105713, total time dif 1944.8224065303802)
step: 19220 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(5.23207), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0786243174225092), 'actor_loss': np.float64(-1.014897209405899), 'hyper_actor_loss': np.float64(0.00014382837835000828), 'behavior_loss': np.float64(0.6433553755283355)}

Episode step 19230, time diff 3.7229697704315186, total time dif 1948.313173532486)
step: 19230 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(2.6141195), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06397483199834823), 'actor_loss': np.float64(-1.0169731438159944), 'hyper_actor_loss': np.float64(0.00016439153550891207), 'behavior_loss': np.float64(0.6139998137950897)}

Episode step 19240, time diff 3.718358039855957, total time dif 1952.0361433029175)
step: 19240 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(1.4860604), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0644030936062336), 'actor_loss': np.float64(-0.9917377293109894), 'hyper_actor_loss': np.float64(0.00015865540044615045), 'behavior_loss': np.float64(0.5972888439893722)}

Episode step 19250, time diff 3.629163980484009, total time dif 1955.7545013427734)
step: 19250 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(2.8311067), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055377531144768), 'actor_loss': np.float64(-0.9835085690021514), 'hyper_actor_loss': np.float64(0.00015533325204160066), 'behavior_loss': np.float64(0.6823721110820771)}

Episode step 19260, time diff 3.600102663040161, total time dif 1959.3836653232574)
step: 19260 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.3014927), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0711918979883194), 'actor_loss': np.float64(-0.9867962002754211), 'hyper_actor_loss': np.float64(0.00016970640572253615), 'behavior_loss': np.float64(0.5896508604288101)}

Episode step 19270, time diff 3.6408467292785645, total time dif 1962.9837679862976)
step: 19270 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(2.9647973), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06199379712343216), 'actor_loss': np.float64(-1.013782024383545), 'hyper_actor_loss': np.float64(0.0002404086000751704), 'behavior_loss': np.float64(0.6740843653678894)}

Episode step 19280, time diff 3.630981683731079, total time dif 1966.6246147155762)
step: 19280 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.9833405), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061865339428186415), 'actor_loss': np.float64(-0.9796292364597321), 'hyper_actor_loss': np.float64(0.0002766204663203098), 'behavior_loss': np.float64(0.6447046965360641)}

Episode step 19290, time diff 3.689016103744507, total time dif 1970.2555963993073)
step: 19290 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(1.3134933), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07001720778644085), 'actor_loss': np.float64(-0.9954699754714966), 'hyper_actor_loss': np.float64(0.00020981195993954315), 'behavior_loss': np.float64(0.6426866233348847)}

Episode step 19300, time diff 3.7533023357391357, total time dif 1973.9446125030518)
step: 19300 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(5.4610624), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06776187866926194), 'actor_loss': np.float64(-1.0228931427001953), 'hyper_actor_loss': np.float64(0.00020576172100845725), 'behavior_loss': np.float64(0.6162546902894974)}

Episode step 19310, time diff 3.912759780883789, total time dif 1977.697914838791)
step: 19310 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(3.0409932), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05975953061133623), 'actor_loss': np.float64(-0.986232990026474), 'hyper_actor_loss': np.float64(0.00017426878766855226), 'behavior_loss': np.float64(0.6635882496833801)}

Episode step 19320, time diff 3.9362311363220215, total time dif 1981.6106746196747)
step: 19320 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(4.3914623), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0596585800871253), 'actor_loss': np.float64(-0.987131017446518), 'hyper_actor_loss': np.float64(0.00016882426280062646), 'behavior_loss': np.float64(0.6766089260578155)}

Episode step 19330, time diff 3.764054536819458, total time dif 1985.5469057559967)
step: 19330 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(3.1263509), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06211010534316301), 'actor_loss': np.float64(-0.9858131825923919), 'hyper_actor_loss': np.float64(0.00015833016368560493), 'behavior_loss': np.float64(0.703067934513092)}

Episode step 19340, time diff 3.8264901638031006, total time dif 1989.3109602928162)
step: 19340 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.415625), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06668335311114788), 'actor_loss': np.float64(-0.9921682298183441), 'hyper_actor_loss': np.float64(0.00016376512649003417), 'behavior_loss': np.float64(0.6991836845874786)}

Episode step 19350, time diff 3.9495227336883545, total time dif 1993.1374504566193)
step: 19350 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(1.6608686), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05992071684449911), 'actor_loss': np.float64(-0.9905534625053406), 'hyper_actor_loss': np.float64(0.00015900461439741776), 'behavior_loss': np.float64(0.6812582314014435)}

Episode step 19360, time diff 3.842951774597168, total time dif 1997.0869731903076)
step: 19360 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.779735), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07536399997770786), 'actor_loss': np.float64(-0.9927872717380524), 'hyper_actor_loss': np.float64(0.0001609021899639629), 'behavior_loss': np.float64(0.6598754942417144)}

Episode step 19370, time diff 3.7711548805236816, total time dif 2000.9299249649048)
step: 19370 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(2.9467175), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06559559889137745), 'actor_loss': np.float64(-0.9970407366752625), 'hyper_actor_loss': np.float64(0.00016244278813246638), 'behavior_loss': np.float64(0.6232433706521988)}

Episode step 19380, time diff 3.772310256958008, total time dif 2004.7010798454285)
step: 19380 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(4.201679), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06969069764018058), 'actor_loss': np.float64(-1.0093200623989105), 'hyper_actor_loss': np.float64(0.00014691164906253107), 'behavior_loss': np.float64(0.635492742061615)}

Episode step 19390, time diff 3.8180296421051025, total time dif 2008.4733901023865)
step: 19390 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.5862534), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07023695297539234), 'actor_loss': np.float64(-1.0131505131721497), 'hyper_actor_loss': np.float64(0.0001520775731478352), 'behavior_loss': np.float64(0.6123598664999008)}

Episode step 19400, time diff 3.745558261871338, total time dif 2012.2914197444916)
step: 19400 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(0.56979793), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05479120668023825), 'actor_loss': np.float64(-0.9966238260269165), 'hyper_actor_loss': np.float64(0.00013071456778561695), 'behavior_loss': np.float64(0.6360294878482818)}

Episode step 19410, time diff 3.957948684692383, total time dif 2016.036978006363)
step: 19410 @ episode report: {'average_total_reward': np.float32(10.000001), 'reward_variance': np.float32(3.206765), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055954239889979364), 'actor_loss': np.float64(-0.9661796152591705), 'hyper_actor_loss': np.float64(0.00014054568164283409), 'behavior_loss': np.float64(0.6332447737455368)}

Episode step 19420, time diff 3.7690608501434326, total time dif 2019.9949266910553)
step: 19420 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(0.97430784), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05675352681428194), 'actor_loss': np.float64(-0.9799064993858337), 'hyper_actor_loss': np.float64(0.00013085729005979374), 'behavior_loss': np.float64(0.6520455360412598)}

Episode step 19430, time diff 3.777468681335449, total time dif 2023.7639875411987)
step: 19430 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(3.5020454), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07134721986949444), 'actor_loss': np.float64(-1.0032370805740356), 'hyper_actor_loss': np.float64(0.00013452652638079599), 'behavior_loss': np.float64(0.6697673320770263)}

Episode step 19440, time diff 3.8244879245758057, total time dif 2027.5414562225342)
step: 19440 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.5663325), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05704110153019428), 'actor_loss': np.float64(-1.0063646018505097), 'hyper_actor_loss': np.float64(0.00012918690699734726), 'behavior_loss': np.float64(0.5987103700637817)}

Episode step 19450, time diff 3.994809865951538, total time dif 2031.36594414711)
step: 19450 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(1.8923229), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06118225250393152), 'actor_loss': np.float64(-0.9856585443019867), 'hyper_actor_loss': np.float64(0.0001255607305211015), 'behavior_loss': np.float64(0.6399462938308715)}

Episode step 19460, time diff 3.895260810852051, total time dif 2035.3607540130615)
step: 19460 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(2.0217338), 'max_total_reward': np.float32(12.1444435), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056337471678853036), 'actor_loss': np.float64(-0.97431121468544), 'hyper_actor_loss': np.float64(0.00012340437533566728), 'behavior_loss': np.float64(0.6442371964454651)}

Episode step 19470, time diff 3.8665919303894043, total time dif 2039.2560148239136)
step: 19470 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(0.9347703), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05735300797969103), 'actor_loss': np.float64(-0.9844456374645233), 'hyper_actor_loss': np.float64(0.0001313486172875855), 'behavior_loss': np.float64(0.6268091797828674)}

Episode step 19480, time diff 3.8949286937713623, total time dif 2043.122606754303)
step: 19480 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.210094), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07558537740260363), 'actor_loss': np.float64(-0.9963839471340179), 'hyper_actor_loss': np.float64(0.00012740547681460156), 'behavior_loss': np.float64(0.6152714848518371)}

Episode step 19490, time diff 3.835376262664795, total time dif 2047.0175354480743)
step: 19490 @ episode report: {'average_total_reward': np.float32(10.561112), 'reward_variance': np.float32(2.6861787), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05586680602282286), 'actor_loss': np.float64(-1.0154383182525635), 'hyper_actor_loss': np.float64(0.00012081380555173382), 'behavior_loss': np.float64(0.6169068217277527)}

Episode step 19500, time diff 3.8405239582061768, total time dif 2050.852911710739)
step: 19500 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(2.292846), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07353889495134354), 'actor_loss': np.float64(-0.9855078160762787), 'hyper_actor_loss': np.float64(0.00010630511242197827), 'behavior_loss': np.float64(0.677013498544693)}

Episode step 19510, time diff 3.8757107257843018, total time dif 2054.6934356689453)
step: 19510 @ episode report: {'average_total_reward': np.float32(10.012224), 'reward_variance': np.float32(2.7528), 'max_total_reward': np.float32(13.1444435), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0620912853628397), 'actor_loss': np.float64(-1.0037765145301818), 'hyper_actor_loss': np.float64(0.0001098417502362281), 'behavior_loss': np.float64(0.6026908725500106)}

Episode step 19520, time diff 3.799368381500244, total time dif 2058.5691463947296)
step: 19520 @ episode report: {'average_total_reward': np.float32(9.973334), 'reward_variance': np.float32(3.6445737), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06760447360575199), 'actor_loss': np.float64(-0.9975804924964905), 'hyper_actor_loss': np.float64(0.00011092095082858578), 'behavior_loss': np.float64(0.6552123248577117)}

Episode step 19530, time diff 3.789135694503784, total time dif 2062.36851477623)
step: 19530 @ episode report: {'average_total_reward': np.float32(9.800001), 'reward_variance': np.float32(0.37827158), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05229287315160036), 'actor_loss': np.float64(-0.97040776014328), 'hyper_actor_loss': np.float64(0.00011127659672638401), 'behavior_loss': np.float64(0.6357713460922241)}

Episode step 19540, time diff 3.7311129570007324, total time dif 2066.1576504707336)
step: 19540 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.0673823), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07333673276007176), 'actor_loss': np.float64(-0.992002648115158), 'hyper_actor_loss': np.float64(0.00010595372659736313), 'behavior_loss': np.float64(0.6045905828475953)}

Episode step 19550, time diff 3.8910253047943115, total time dif 2069.8887634277344)
step: 19550 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(1.687562), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06945117339491844), 'actor_loss': np.float64(-1.0322614908218384), 'hyper_actor_loss': np.float64(0.00010910214477917179), 'behavior_loss': np.float64(0.6079300045967102)}

Episode step 19560, time diff 3.869399070739746, total time dif 2073.7797887325287)
step: 19560 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.0724888), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06653998382389545), 'actor_loss': np.float64(-0.9983616590499877), 'hyper_actor_loss': np.float64(0.00012008364938083105), 'behavior_loss': np.float64(0.6082692116498947)}

Episode step 19570, time diff 3.7336912155151367, total time dif 2077.6491878032684)
step: 19570 @ episode report: {'average_total_reward': np.float32(11.844445), 'reward_variance': np.float32(5.439557), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06416661366820335), 'actor_loss': np.float64(-0.9788164913654327), 'hyper_actor_loss': np.float64(0.00017906588182086124), 'behavior_loss': np.float64(0.6429039001464844)}

Episode step 19580, time diff 3.8169422149658203, total time dif 2081.3828790187836)
step: 19580 @ episode report: {'average_total_reward': np.float32(11.744444), 'reward_variance': np.float32(2.0091105), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.900002), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0693325337022543), 'actor_loss': np.float64(-1.0027783453464507), 'hyper_actor_loss': np.float64(0.00020514939242275432), 'behavior_loss': np.float64(0.6085256695747375)}

Episode step 19590, time diff 3.6501035690307617, total time dif 2085.1998212337494)
step: 19590 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.0062988), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06834323517978191), 'actor_loss': np.float64(-0.9927115201950073), 'hyper_actor_loss': np.float64(0.0001502627579611726), 'behavior_loss': np.float64(0.6571760892868042)}

Episode step 19600, time diff 3.7001149654388428, total time dif 2088.84992480278)
step: 19600 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(3.301437), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06476891450583935), 'actor_loss': np.float64(-0.9852716982364654), 'hyper_actor_loss': np.float64(0.00010647857270669192), 'behavior_loss': np.float64(0.6152394503355026)}

Episode step 19610, time diff 3.622572898864746, total time dif 2092.550039768219)
step: 19610 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(3.2785194), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061892253905534746), 'actor_loss': np.float64(-0.999449610710144), 'hyper_actor_loss': np.float64(9.113733685808256e-05), 'behavior_loss': np.float64(0.6098265171051025)}

Episode step 19620, time diff 3.759300708770752, total time dif 2096.1726126670837)
step: 19620 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(3.9532363), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07166839726269245), 'actor_loss': np.float64(-0.9997271656990051), 'hyper_actor_loss': np.float64(0.0001004102363367565), 'behavior_loss': np.float64(0.6169579982757568)}

Episode step 19630, time diff 3.5387380123138428, total time dif 2099.9319133758545)
step: 19630 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(3.0141537), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055310233123600484), 'actor_loss': np.float64(-0.9969270288944244), 'hyper_actor_loss': np.float64(8.462575351586565e-05), 'behavior_loss': np.float64(0.6372143179178238)}

Episode step 19640, time diff 3.5210020542144775, total time dif 2103.4706513881683)
step: 19640 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(1.7947025), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060216032154858115), 'actor_loss': np.float64(-0.9721203982830048), 'hyper_actor_loss': np.float64(9.506088172201998e-05), 'behavior_loss': np.float64(0.5597463876008988)}

Episode step 19650, time diff 3.5288634300231934, total time dif 2106.991653442383)
step: 19650 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(1.635536), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05592128792777658), 'actor_loss': np.float64(-0.9906025469303131), 'hyper_actor_loss': np.float64(8.320148554048501e-05), 'behavior_loss': np.float64(0.6106503903865814)}

Episode step 19660, time diff 3.521864652633667, total time dif 2110.520516872406)
step: 19660 @ episode report: {'average_total_reward': np.float32(11.532224), 'reward_variance': np.float32(1.0401595), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05161740705370903), 'actor_loss': np.float64(-0.9873879015445709), 'hyper_actor_loss': np.float64(8.070668918662705e-05), 'behavior_loss': np.float64(0.5466253727674484)}

Episode step 19670, time diff 3.5246918201446533, total time dif 2114.0423815250397)
step: 19670 @ episode report: {'average_total_reward': np.float32(9.910001), 'reward_variance': np.float32(3.6396165), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05703819431364536), 'actor_loss': np.float64(-0.9797038972377777), 'hyper_actor_loss': np.float64(8.355518220923842e-05), 'behavior_loss': np.float64(0.5482044845819474)}

Episode step 19680, time diff 3.4556028842926025, total time dif 2117.5670733451843)
step: 19680 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(3.6156898), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07784404940903186), 'actor_loss': np.float64(-1.007577383518219), 'hyper_actor_loss': np.float64(8.682335464982316e-05), 'behavior_loss': np.float64(0.6203079402446747)}

Episode step 19690, time diff 3.4570672512054443, total time dif 2121.022676229477)
step: 19690 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(1.7353443), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07845711782574653), 'actor_loss': np.float64(-1.0096308410167694), 'hyper_actor_loss': np.float64(7.575592062494251e-05), 'behavior_loss': np.float64(0.6947760701179504)}

Episode step 19700, time diff 3.422210931777954, total time dif 2124.4797434806824)
step: 19700 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(0.8814026), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07067688032984734), 'actor_loss': np.float64(-0.9998452246189118), 'hyper_actor_loss': np.float64(7.46092959161615e-05), 'behavior_loss': np.float64(0.5311920166015625)}

Episode step 19710, time diff 3.4285290241241455, total time dif 2127.9019544124603)
step: 19710 @ episode report: {'average_total_reward': np.float32(11.456668), 'reward_variance': np.float32(1.9367282), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06741121001541614), 'actor_loss': np.float64(-1.006412398815155), 'hyper_actor_loss': np.float64(8.061508851824329e-05), 'behavior_loss': np.float64(0.5733034580945968)}

Episode step 19720, time diff 3.844813108444214, total time dif 2131.3304834365845)
step: 19720 @ episode report: {'average_total_reward': np.float32(11.2711115), 'reward_variance': np.float32(3.281264), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059075478464365005), 'actor_loss': np.float64(-0.9967057287693024), 'hyper_actor_loss': np.float64(7.567773063783534e-05), 'behavior_loss': np.float64(0.622912609577179)}

Episode step 19730, time diff 3.5006144046783447, total time dif 2135.1752965450287)
step: 19730 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(3.6167526), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07305822372436524), 'actor_loss': np.float64(-0.9807172179222107), 'hyper_actor_loss': np.float64(6.882144225528464e-05), 'behavior_loss': np.float64(0.5613974243402481)}

Episode step 19740, time diff 3.38394832611084, total time dif 2138.675910949707)
step: 19740 @ episode report: {'average_total_reward': np.float32(11.756668), 'reward_variance': np.float32(2.6905792), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05478707514703274), 'actor_loss': np.float64(-1.0069463551044464), 'hyper_actor_loss': np.float64(7.574589326395654e-05), 'behavior_loss': np.float64(0.5982966184616089)}

Episode step 19750, time diff 3.6014297008514404, total time dif 2142.059859275818)
step: 19750 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.2998025), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05952223427593708), 'actor_loss': np.float64(-0.9772058963775635), 'hyper_actor_loss': np.float64(6.503806653199718e-05), 'behavior_loss': np.float64(0.612045431137085)}

Episode step 19760, time diff 3.4514670372009277, total time dif 2145.6612889766693)
step: 19760 @ episode report: {'average_total_reward': np.float32(11.705557), 'reward_variance': np.float32(1.3223519), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059191429428756236), 'actor_loss': np.float64(-0.9830632507801056), 'hyper_actor_loss': np.float64(7.06413611624157e-05), 'behavior_loss': np.float64(0.5849538385868073)}

Episode step 19770, time diff 3.483403205871582, total time dif 2149.1127560138702)
step: 19770 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(0.9377581), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0752976331859827), 'actor_loss': np.float64(-1.0150172114372253), 'hyper_actor_loss': np.float64(6.929290102561936e-05), 'behavior_loss': np.float64(0.6024610757827759)}

Episode step 19780, time diff 3.514007806777954, total time dif 2152.596159219742)
step: 19780 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(2.3703463), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053381997346878055), 'actor_loss': np.float64(-0.9991148769855499), 'hyper_actor_loss': np.float64(7.293319940799847e-05), 'behavior_loss': np.float64(0.6089761525392532)}

Episode step 19790, time diff 3.5201892852783203, total time dif 2156.11016702652)
step: 19790 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.5292614), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054878311976790425), 'actor_loss': np.float64(-0.9572764337062836), 'hyper_actor_loss': np.float64(6.667001616733614e-05), 'behavior_loss': np.float64(0.5864341795444489)}

Episode step 19800, time diff 3.478604316711426, total time dif 2159.630356311798)
step: 19800 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(5.9019065), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05469101797789335), 'actor_loss': np.float64(-0.9907005965709687), 'hyper_actor_loss': np.float64(8.034859638428316e-05), 'behavior_loss': np.float64(0.5435498148202896)}

Episode step 19810, time diff 3.5656440258026123, total time dif 2163.1089606285095)
step: 19810 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(3.4302242), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0638907540589571), 'actor_loss': np.float64(-0.9996692955493927), 'hyper_actor_loss': np.float64(6.985303916735574e-05), 'behavior_loss': np.float64(0.5929334163665771)}

Episode step 19820, time diff 3.8726861476898193, total time dif 2166.674604654312)
step: 19820 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(1.277926), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06102118194103241), 'actor_loss': np.float64(-0.9844432413578034), 'hyper_actor_loss': np.float64(6.725163584633264e-05), 'behavior_loss': np.float64(0.5834783971309662)}

Episode step 19830, time diff 3.728959560394287, total time dif 2170.547290802002)
step: 19830 @ episode report: {'average_total_reward': np.float32(11.944445), 'reward_variance': np.float32(3.8982227), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05929528623819351), 'actor_loss': np.float64(-0.9969433128833771), 'hyper_actor_loss': np.float64(7.465760827471968e-05), 'behavior_loss': np.float64(0.536926782131195)}

Episode step 19840, time diff 3.629638433456421, total time dif 2174.2762503623962)
step: 19840 @ episode report: {'average_total_reward': np.float32(11.320001), 'reward_variance': np.float32(5.1548347), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06667202338576317), 'actor_loss': np.float64(-0.9912753164768219), 'hyper_actor_loss': np.float64(6.861806068627629e-05), 'behavior_loss': np.float64(0.6157418191432953)}

Episode step 19850, time diff 3.6545965671539307, total time dif 2177.9058887958527)
step: 19850 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.6078143), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0557762548327446), 'actor_loss': np.float64(-0.9968130409717559), 'hyper_actor_loss': np.float64(6.935345190868247e-05), 'behavior_loss': np.float64(0.5325459778308869)}

Episode step 19860, time diff 3.874077320098877, total time dif 2181.5604853630066)
step: 19860 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.3357785), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05902415737509727), 'actor_loss': np.float64(-0.9831275165081024), 'hyper_actor_loss': np.float64(8.001378409971949e-05), 'behavior_loss': np.float64(0.6005672097206116)}

Episode step 19870, time diff 3.639786720275879, total time dif 2185.4345626831055)
step: 19870 @ episode report: {'average_total_reward': np.float32(11.35889), 'reward_variance': np.float32(2.0914338), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0676525179296732), 'actor_loss': np.float64(-0.9839094758033753), 'hyper_actor_loss': np.float64(7.5245946936775e-05), 'behavior_loss': np.float64(0.5700584083795548)}

Episode step 19880, time diff 3.6132571697235107, total time dif 2189.0743494033813)
step: 19880 @ episode report: {'average_total_reward': np.float32(10.112223), 'reward_variance': np.float32(1.2766777), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06472126096487045), 'actor_loss': np.float64(-1.0186047077178955), 'hyper_actor_loss': np.float64(6.166109633340966e-05), 'behavior_loss': np.float64(0.5706299304962158)}

Episode step 19890, time diff 3.665931224822998, total time dif 2192.687606573105)
step: 19890 @ episode report: {'average_total_reward': np.float32(12.978889), 'reward_variance': np.float32(2.3493946), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(13.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06593607626855373), 'actor_loss': np.float64(-0.9932157874107361), 'hyper_actor_loss': np.float64(7.37372763978783e-05), 'behavior_loss': np.float64(0.5878038883209229)}

Episode step 19900, time diff 3.6251163482666016, total time dif 2196.353537797928)
step: 19900 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(2.6662674), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06667550913989544), 'actor_loss': np.float64(-0.9985696017742157), 'hyper_actor_loss': np.float64(7.001187914283946e-05), 'behavior_loss': np.float64(0.603916597366333)}

Episode step 19910, time diff 3.461616039276123, total time dif 2199.9786541461945)
step: 19910 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(2.3597894), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06454529855400323), 'actor_loss': np.float64(-0.9949699819087983), 'hyper_actor_loss': np.float64(7.121397138689644e-05), 'behavior_loss': np.float64(0.5744732260704041)}

Episode step 19920, time diff 3.6804873943328857, total time dif 2203.4402701854706)
step: 19920 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(3.077024), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07042002268135547), 'actor_loss': np.float64(-1.0005933403968812), 'hyper_actor_loss': np.float64(6.740534736309201e-05), 'behavior_loss': np.float64(0.5851305991411209)}

Episode step 19930, time diff 3.5498344898223877, total time dif 2207.1207575798035)
step: 19930 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(1.270076), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05482508987188339), 'actor_loss': np.float64(-0.9840735137462616), 'hyper_actor_loss': np.float64(6.387779285432771e-05), 'behavior_loss': np.float64(0.5898237347602844)}

Episode step 19940, time diff 3.535029649734497, total time dif 2210.670592069626)
step: 19940 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.1072955), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05080680325627327), 'actor_loss': np.float64(-0.9659724235534668), 'hyper_actor_loss': np.float64(6.593487851205281e-05), 'behavior_loss': np.float64(0.5474003344774246)}

Episode step 19950, time diff 3.5513408184051514, total time dif 2214.2056217193604)
step: 19950 @ episode report: {'average_total_reward': np.float32(10.971111), 'reward_variance': np.float32(3.350325), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05066181868314743), 'actor_loss': np.float64(-0.983750706911087), 'hyper_actor_loss': np.float64(6.396890930773224e-05), 'behavior_loss': np.float64(0.5442677527666092)}

Episode step 19960, time diff 3.631413459777832, total time dif 2217.7569625377655)
step: 19960 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(3.985231), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058745127171278), 'actor_loss': np.float64(-0.9899666368961334), 'hyper_actor_loss': np.float64(6.725491184624843e-05), 'behavior_loss': np.float64(0.5520709782838822)}

Episode step 19970, time diff 3.504859447479248, total time dif 2221.3883759975433)
step: 19970 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(4.701064), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04404159020632505), 'actor_loss': np.float64(-0.9813891172409057), 'hyper_actor_loss': np.float64(6.89438445988344e-05), 'behavior_loss': np.float64(0.5242174834012985)}

Episode step 19980, time diff 3.553593158721924, total time dif 2224.8932354450226)
step: 19980 @ episode report: {'average_total_reward': np.float32(11.2322235), 'reward_variance': np.float32(2.9192948), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05418352577835321), 'actor_loss': np.float64(-0.9650695204734803), 'hyper_actor_loss': np.float64(6.623164372285829e-05), 'behavior_loss': np.float64(0.5775238156318665)}

Episode step 19990, time diff 3.490865707397461, total time dif 2228.4468286037445)
step: 19990 @ episode report: {'average_total_reward': np.float32(9.687779), 'reward_variance': np.float32(1.2063326), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05062760505825281), 'actor_loss': np.float64(-0.9870432257652283), 'hyper_actor_loss': np.float64(6.170854358060751e-05), 'behavior_loss': np.float64(0.5145894765853882)}

Episode step 20000, time diff 3.380668878555298, total time dif 2231.937694311142)
step: 20000 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(3.754851), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04502655379474163), 'actor_loss': np.float64(-0.9883103430271148), 'hyper_actor_loss': np.float64(6.762641787645407e-05), 'behavior_loss': np.float64(0.5538976401090622)}

Episode step 20010, time diff 3.4457223415374756, total time dif 2235.3183631896973)
step: 20010 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(2.2280002), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055088558420538905), 'actor_loss': np.float64(-0.9693141043186188), 'hyper_actor_loss': np.float64(6.944449414731934e-05), 'behavior_loss': np.float64(0.5955689549446106)}

Episode step 20020, time diff 3.4254822731018066, total time dif 2238.7640855312347)
step: 20020 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(3.2106977), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06568456664681435), 'actor_loss': np.float64(-0.9874128699302673), 'hyper_actor_loss': np.float64(6.479270487034228e-05), 'behavior_loss': np.float64(0.5643772304058075)}

Episode step 20030, time diff 3.495673179626465, total time dif 2242.1895678043365)
step: 20030 @ episode report: {'average_total_reward': np.float32(9.561111), 'reward_variance': np.float32(2.0283275), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06318558994680643), 'actor_loss': np.float64(-1.0084619760513305), 'hyper_actor_loss': np.float64(6.842252005299088e-05), 'behavior_loss': np.float64(0.5593097239732743)}

Episode step 20040, time diff 3.4772138595581055, total time dif 2245.685240983963)
step: 20040 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.5454142), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808734461665153), 'actor_loss': np.float64(-0.9973975121974945), 'hyper_actor_loss': np.float64(6.213076958374585e-05), 'behavior_loss': np.float64(0.5301405042409897)}

Episode step 20050, time diff 3.425554037094116, total time dif 2249.162454843521)
step: 20050 @ episode report: {'average_total_reward': np.float32(12.554445), 'reward_variance': np.float32(3.7998137), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(13.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06740922033786774), 'actor_loss': np.float64(-1.005709308385849), 'hyper_actor_loss': np.float64(5.975549429422244e-05), 'behavior_loss': np.float64(0.5998376280069351)}

Episode step 20060, time diff 3.42533802986145, total time dif 2252.5880088806152)
step: 20060 @ episode report: {'average_total_reward': np.float32(10.858889), 'reward_variance': np.float32(3.006299), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06333107016980648), 'actor_loss': np.float64(-1.0015676677227021), 'hyper_actor_loss': np.float64(5.551693648158107e-05), 'behavior_loss': np.float64(0.5633546441793442)}

Episode step 20070, time diff 3.353672504425049, total time dif 2256.0133469104767)
step: 20070 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(2.5129943), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0620219387114048), 'actor_loss': np.float64(-0.9863645195960998), 'hyper_actor_loss': np.float64(6.168774598336313e-05), 'behavior_loss': np.float64(0.5376464486122131)}

Episode step 20080, time diff 3.2620718479156494, total time dif 2259.3670194149017)
step: 20080 @ episode report: {'average_total_reward': np.float32(9.987779), 'reward_variance': np.float32(2.006036), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06451789289712906), 'actor_loss': np.float64(-0.9976545453071595), 'hyper_actor_loss': np.float64(6.139108190836851e-05), 'behavior_loss': np.float64(0.6046295374631881)}

Episode step 20090, time diff 3.477510929107666, total time dif 2262.6290912628174)
step: 20090 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(1.8499314), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06189383901655674), 'actor_loss': np.float64(-1.0031300783157349), 'hyper_actor_loss': np.float64(6.676584380329586e-05), 'behavior_loss': np.float64(0.5444340407848358)}

Episode step 20100, time diff 3.3150861263275146, total time dif 2266.106602191925)
step: 20100 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(5.7146006), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0651117853820324), 'actor_loss': np.float64(-0.9825494050979614), 'hyper_actor_loss': np.float64(5.826299857289996e-05), 'behavior_loss': np.float64(0.544448721408844)}

Episode step 20110, time diff 3.317540168762207, total time dif 2269.4216883182526)
step: 20110 @ episode report: {'average_total_reward': np.float32(11.246668), 'reward_variance': np.float32(1.1905386), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05918507892638445), 'actor_loss': np.float64(-0.9896889686584472), 'hyper_actor_loss': np.float64(5.473510864248965e-05), 'behavior_loss': np.float64(0.5677282780408859)}

Episode step 20120, time diff 3.3815701007843018, total time dif 2272.7392284870148)
step: 20120 @ episode report: {'average_total_reward': np.float32(10.45889), 'reward_variance': np.float32(3.6712604), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06026998795568943), 'actor_loss': np.float64(-0.9884209394454956), 'hyper_actor_loss': np.float64(6.359895123750903e-05), 'behavior_loss': np.float64(0.5660744369029999)}

Episode step 20130, time diff 3.3504385948181152, total time dif 2276.120798587799)
step: 20130 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(1.745013), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07438147570937872), 'actor_loss': np.float64(-0.9897943019866944), 'hyper_actor_loss': np.float64(5.527072171389591e-05), 'behavior_loss': np.float64(0.5706846296787262)}

Episode step 20140, time diff 3.3349435329437256, total time dif 2279.471237182617)
step: 20140 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.9976988), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05954532325267792), 'actor_loss': np.float64(-1.0080314755439759), 'hyper_actor_loss': np.float64(6.288954755291342e-05), 'behavior_loss': np.float64(0.5416903167963028)}

Episode step 20150, time diff 3.327446222305298, total time dif 2282.806180715561)
step: 20150 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.0016612), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06837122477591037), 'actor_loss': np.float64(-0.9910608112812043), 'hyper_actor_loss': np.float64(5.8036208429257385e-05), 'behavior_loss': np.float64(0.5868904531002045)}

Episode step 20160, time diff 3.4147937297821045, total time dif 2286.133626937866)
step: 20160 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.6276321), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.046651593409478664), 'actor_loss': np.float64(-0.972639000415802), 'hyper_actor_loss': np.float64(6.670993934676517e-05), 'behavior_loss': np.float64(0.5512390255928039)}

Episode step 20170, time diff 3.3441708087921143, total time dif 2289.5484206676483)
step: 20170 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(6.3493047), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0548377962782979), 'actor_loss': np.float64(-0.9662510275840759), 'hyper_actor_loss': np.float64(5.266650259727612e-05), 'behavior_loss': np.float64(0.5830907881259918)}

Episode step 20180, time diff 3.374143123626709, total time dif 2292.8925914764404)
step: 20180 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(3.2520454), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07080383747816085), 'actor_loss': np.float64(-0.9902083933353424), 'hyper_actor_loss': np.float64(5.65254500543233e-05), 'behavior_loss': np.float64(0.59245166182518)}

Episode step 20190, time diff 3.4043736457824707, total time dif 2296.266734600067)
step: 20190 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(1.1936343), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055873132403939964), 'actor_loss': np.float64(-1.0017612874507904), 'hyper_actor_loss': np.float64(6.386282329913228e-05), 'behavior_loss': np.float64(0.5520381718873978)}

Episode step 20200, time diff 3.425933361053467, total time dif 2299.6711082458496)
step: 20200 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(1.1021535), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06627196855843068), 'actor_loss': np.float64(-0.9880348026752472), 'hyper_actor_loss': np.float64(6.482877361122519e-05), 'behavior_loss': np.float64(0.5351419478654862)}

Episode step 20210, time diff 3.41062331199646, total time dif 2303.097041606903)
step: 20210 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.1514192), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06465301476418972), 'actor_loss': np.float64(-1.0027770280838013), 'hyper_actor_loss': np.float64(6.35667034657672e-05), 'behavior_loss': np.float64(0.5197183847427368)}

Episode step 20220, time diff 3.378917694091797, total time dif 2306.5076649188995)
step: 20220 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.5792303), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06372587494552136), 'actor_loss': np.float64(-1.0109709739685058), 'hyper_actor_loss': np.float64(6.001183537591714e-05), 'behavior_loss': np.float64(0.5194235533475876)}

Episode step 20230, time diff 3.3552398681640625, total time dif 2309.8865826129913)
step: 20230 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.1533139), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06789499968290329), 'actor_loss': np.float64(-0.9957670629024505), 'hyper_actor_loss': np.float64(5.6482695799786595e-05), 'behavior_loss': np.float64(0.5785729050636291)}

Episode step 20240, time diff 3.3598430156707764, total time dif 2313.2418224811554)
step: 20240 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(0.9608658), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07280661053955555), 'actor_loss': np.float64(-1.0068189442157744), 'hyper_actor_loss': np.float64(5.722404603147879e-05), 'behavior_loss': np.float64(0.5007110506296157)}

Episode step 20250, time diff 3.3583900928497314, total time dif 2316.601665496826)
step: 20250 @ episode report: {'average_total_reward': np.float32(10.036668), 'reward_variance': np.float32(1.0933101), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06914267279207706), 'actor_loss': np.float64(-1.0064344704151154), 'hyper_actor_loss': np.float64(5.924421566305682e-05), 'behavior_loss': np.float64(0.609260955452919)}

Episode step 20260, time diff 3.448211193084717, total time dif 2319.960055589676)
step: 20260 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.6923997), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06868160329759121), 'actor_loss': np.float64(-0.9878004848957062), 'hyper_actor_loss': np.float64(5.571704241447151e-05), 'behavior_loss': np.float64(0.5496231108903885)}

Episode step 20270, time diff 3.286423444747925, total time dif 2323.4082667827606)
step: 20270 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(2.714297), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06638462338596582), 'actor_loss': np.float64(-0.9922296226024627), 'hyper_actor_loss': np.float64(5.265282052278053e-05), 'behavior_loss': np.float64(0.528515774011612)}

Episode step 20280, time diff 3.2503535747528076, total time dif 2326.6946902275085)
step: 20280 @ episode report: {'average_total_reward': np.float32(10.922223), 'reward_variance': np.float32(2.900024), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062244567275047305), 'actor_loss': np.float64(-1.005515468120575), 'hyper_actor_loss': np.float64(6.060841951693874e-05), 'behavior_loss': np.float64(0.5201995849609375)}

Episode step 20290, time diff 3.348346710205078, total time dif 2329.9450438022614)
step: 20290 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(1.1164507), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06508257500827312), 'actor_loss': np.float64(-1.0076520204544068), 'hyper_actor_loss': np.float64(5.14035618834896e-05), 'behavior_loss': np.float64(0.5193187296390533)}

Episode step 20300, time diff 3.298963785171509, total time dif 2333.2933905124664)
step: 20300 @ episode report: {'average_total_reward': np.float32(11.905558), 'reward_variance': np.float32(1.9861052), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06176181174814701), 'actor_loss': np.float64(-0.996033352613449), 'hyper_actor_loss': np.float64(5.300239463394974e-05), 'behavior_loss': np.float64(0.482242950797081)}

Episode step 20310, time diff 3.2960455417633057, total time dif 2336.592354297638)
step: 20310 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(0.9156295), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06133892871439457), 'actor_loss': np.float64(-0.9964263498783111), 'hyper_actor_loss': np.float64(5.997268563078251e-05), 'behavior_loss': np.float64(0.518184232711792)}

Episode step 20320, time diff 3.2916839122772217, total time dif 2339.8883998394012)
step: 20320 @ episode report: {'average_total_reward': np.float32(10.461111), 'reward_variance': np.float32(6.021192), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06068652328103781), 'actor_loss': np.float64(-0.9971963763237), 'hyper_actor_loss': np.float64(5.481199732457753e-05), 'behavior_loss': np.float64(0.5579893440008163)}

Episode step 20330, time diff 3.216324806213379, total time dif 2343.1800837516785)
step: 20330 @ episode report: {'average_total_reward': np.float32(11.593334), 'reward_variance': np.float32(2.0653882), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062350905314087865), 'actor_loss': np.float64(-0.9857453227043151), 'hyper_actor_loss': np.float64(4.6690577801200564e-05), 'behavior_loss': np.float64(0.5514725625514985)}

Episode step 20340, time diff 3.2232797145843506, total time dif 2346.396408557892)
step: 20340 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(5.572598), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0658984512090683), 'actor_loss': np.float64(-0.9976435899734497), 'hyper_actor_loss': np.float64(4.963962055626325e-05), 'behavior_loss': np.float64(0.5638922989368439)}

Episode step 20350, time diff 3.256471633911133, total time dif 2349.619688272476)
step: 20350 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.305358), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06265974715352059), 'actor_loss': np.float64(-0.9947240650653839), 'hyper_actor_loss': np.float64(5.128261655045208e-05), 'behavior_loss': np.float64(0.5750620990991593)}

Episode step 20360, time diff 3.2506983280181885, total time dif 2352.8761599063873)
step: 20360 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(5.590377), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06344026736915112), 'actor_loss': np.float64(-0.9813577353954315), 'hyper_actor_loss': np.float64(5.013908703404013e-05), 'behavior_loss': np.float64(0.5540812879800796)}

Episode step 20370, time diff 3.304306745529175, total time dif 2356.1268582344055)
step: 20370 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.6659951), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07098120115697384), 'actor_loss': np.float64(-0.9961175680160522), 'hyper_actor_loss': np.float64(5.7190216830349526e-05), 'behavior_loss': np.float64(0.5307098239660263)}

Episode step 20380, time diff 3.3221211433410645, total time dif 2359.4311649799347)
step: 20380 @ episode report: {'average_total_reward': np.float32(11.432222), 'reward_variance': np.float32(5.7325296), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056722451001405716), 'actor_loss': np.float64(-1.0010902822017669), 'hyper_actor_loss': np.float64(5.084796648588963e-05), 'behavior_loss': np.float64(0.5255981594324112)}

Episode step 20390, time diff 3.315866708755493, total time dif 2362.7532861232758)
step: 20390 @ episode report: {'average_total_reward': np.float32(12.081113), 'reward_variance': np.float32(1.6970142), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0631645917892456), 'actor_loss': np.float64(-0.9968138635158539), 'hyper_actor_loss': np.float64(5.023639569117222e-05), 'behavior_loss': np.float64(0.5292563170194626)}

Episode step 20400, time diff 3.2796249389648438, total time dif 2366.0691528320312)
step: 20400 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(4.8998795), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06399723999202252), 'actor_loss': np.float64(-0.9876470506191254), 'hyper_actor_loss': np.float64(4.963428727933206e-05), 'behavior_loss': np.float64(0.5353085100650787)}

Episode step 20410, time diff 3.347280979156494, total time dif 2369.348777770996)
step: 20410 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(3.4182792), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05551145561039448), 'actor_loss': np.float64(-0.9813456833362579), 'hyper_actor_loss': np.float64(5.316189526638482e-05), 'behavior_loss': np.float64(0.5472600102424622)}

Episode step 20420, time diff 3.610025644302368, total time dif 2372.6960587501526)
step: 20420 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.6980557), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06145395319908857), 'actor_loss': np.float64(-0.9878528475761413), 'hyper_actor_loss': np.float64(6.492467437055894e-05), 'behavior_loss': np.float64(0.5703671276569366)}

Episode step 20430, time diff 3.494554281234741, total time dif 2376.306084394455)
step: 20430 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.0446782), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0796258881688118), 'actor_loss': np.float64(-1.0001685976982118), 'hyper_actor_loss': np.float64(4.989400295016821e-05), 'behavior_loss': np.float64(0.5697986513376236)}

Episode step 20440, time diff 3.2418110370635986, total time dif 2379.8006386756897)
step: 20440 @ episode report: {'average_total_reward': np.float32(12.517778), 'reward_variance': np.float32(2.2913387), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(13.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06504138931632042), 'actor_loss': np.float64(-1.0118058562278747), 'hyper_actor_loss': np.float64(4.967432105331682e-05), 'behavior_loss': np.float64(0.5347034692764282)}

Episode step 20450, time diff 3.273362159729004, total time dif 2383.0424497127533)
step: 20450 @ episode report: {'average_total_reward': np.float32(12.093334), 'reward_variance': np.float32(4.6675606), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056335457041859625), 'actor_loss': np.float64(-0.991516238451004), 'hyper_actor_loss': np.float64(4.823000090254936e-05), 'behavior_loss': np.float64(0.4964811086654663)}

Episode step 20460, time diff 3.1979804039001465, total time dif 2386.3158118724823)
step: 20460 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(5.6825757), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06638212502002716), 'actor_loss': np.float64(-0.9894177615642548), 'hyper_actor_loss': np.float64(4.407839114719536e-05), 'behavior_loss': np.float64(0.5689808100461959)}

Episode step 20470, time diff 3.220282554626465, total time dif 2389.5137922763824)
step: 20470 @ episode report: {'average_total_reward': np.float32(10.683334), 'reward_variance': np.float32(4.2677846), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07758686617016793), 'actor_loss': np.float64(-1.0101972460746764), 'hyper_actor_loss': np.float64(6.107407389208675e-05), 'behavior_loss': np.float64(0.5448519080877304)}

Episode step 20480, time diff 3.235823154449463, total time dif 2392.734074831009)
step: 20480 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(1.0689434), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05227174311876297), 'actor_loss': np.float64(-0.9949338495731354), 'hyper_actor_loss': np.float64(4.9313465933664705e-05), 'behavior_loss': np.float64(0.5619862258434296)}

Episode step 20490, time diff 3.33109974861145, total time dif 2395.9698979854584)
step: 20490 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.06615), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05332942847162485), 'actor_loss': np.float64(-0.9611211895942688), 'hyper_actor_loss': np.float64(4.865316986979451e-05), 'behavior_loss': np.float64(0.5026840150356293)}

Episode step 20500, time diff 3.5052056312561035, total time dif 2399.30099773407)
step: 20500 @ episode report: {'average_total_reward': np.float32(11.9811125), 'reward_variance': np.float32(4.682149), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07293258607387543), 'actor_loss': np.float64(-1.0014502584934235), 'hyper_actor_loss': np.float64(5.008993721276056e-05), 'behavior_loss': np.float64(0.5301969915628433)}

Episode step 20510, time diff 3.3735134601593018, total time dif 2402.806203365326)
step: 20510 @ episode report: {'average_total_reward': np.float32(11.781112), 'reward_variance': np.float32(2.1179028), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808841899037361), 'actor_loss': np.float64(-1.022581112384796), 'hyper_actor_loss': np.float64(4.695538627856877e-05), 'behavior_loss': np.float64(0.5756680697202683)}

Episode step 20520, time diff 3.3470301628112793, total time dif 2406.1797168254852)
step: 20520 @ episode report: {'average_total_reward': np.float32(12.530001), 'reward_variance': np.float32(5.8342714), 'max_total_reward': np.float32(17.755554), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(13.3), 'max_n_step': np.float32(18.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061077560484409335), 'actor_loss': np.float64(-0.979992252588272), 'hyper_actor_loss': np.float64(4.697398253483698e-05), 'behavior_loss': np.float64(0.5387285381555558)}

Episode step 20530, time diff 3.286789894104004, total time dif 2409.5267469882965)
step: 20530 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(4.5186434), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06837712340056896), 'actor_loss': np.float64(-0.9833151280879975), 'hyper_actor_loss': np.float64(4.7808963790885174e-05), 'behavior_loss': np.float64(0.5000882744789124)}

Episode step 20540, time diff 3.1884050369262695, total time dif 2412.8135368824005)
step: 20540 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.4904747), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05765351951122284), 'actor_loss': np.float64(-1.0150070548057557), 'hyper_actor_loss': np.float64(5.337234179023653e-05), 'behavior_loss': np.float64(0.5491978913545609)}

Episode step 20550, time diff 3.195237398147583, total time dif 2416.001941919327)
step: 20550 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(1.9706236), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08189758993685245), 'actor_loss': np.float64(-0.995286625623703), 'hyper_actor_loss': np.float64(4.593425001075957e-05), 'behavior_loss': np.float64(0.5352740526199341)}

Episode step 20560, time diff 3.2348146438598633, total time dif 2419.1971793174744)
step: 20560 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(0.5418781), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06364828422665596), 'actor_loss': np.float64(-1.0139472603797912), 'hyper_actor_loss': np.float64(5.248099223535974e-05), 'behavior_loss': np.float64(0.5317438662052154)}

Episode step 20570, time diff 3.4460246562957764, total time dif 2422.4319939613342)
step: 20570 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(0.79686564), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07418866902589798), 'actor_loss': np.float64(-0.9943084716796875), 'hyper_actor_loss': np.float64(4.5627479630638844e-05), 'behavior_loss': np.float64(0.4838660329580307)}

Episode step 20580, time diff 3.6102828979492188, total time dif 2425.87801861763)
step: 20580 @ episode report: {'average_total_reward': np.float32(11.344446), 'reward_variance': np.float32(2.7884953), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058187419548630714), 'actor_loss': np.float64(-1.0143751740455627), 'hyper_actor_loss': np.float64(4.9222354573430496e-05), 'behavior_loss': np.float64(0.5109187632799148)}

Episode step 20590, time diff 3.4381773471832275, total time dif 2429.488301515579)
step: 20590 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(1.918844), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06471990402787924), 'actor_loss': np.float64(-0.9912980258464813), 'hyper_actor_loss': np.float64(4.904997913399711e-05), 'behavior_loss': np.float64(0.5166222393512726)}

Episode step 20600, time diff 3.3867900371551514, total time dif 2432.9264788627625)
step: 20600 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(3.7144504), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06326750796288252), 'actor_loss': np.float64(-0.9867729008197784), 'hyper_actor_loss': np.float64(4.955605872964952e-05), 'behavior_loss': np.float64(0.5344599515199662)}

Episode step 20610, time diff 3.3609044551849365, total time dif 2436.3132688999176)
step: 20610 @ episode report: {'average_total_reward': np.float32(11.993334), 'reward_variance': np.float32(6.3379064), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07039039023220539), 'actor_loss': np.float64(-1.003434145450592), 'hyper_actor_loss': np.float64(5.1452369552862366e-05), 'behavior_loss': np.float64(0.4774173229932785)}

Episode step 20620, time diff 3.385195255279541, total time dif 2439.6741733551025)
step: 20620 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(1.7457736), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055666572600603106), 'actor_loss': np.float64(-1.0059752643108368), 'hyper_actor_loss': np.float64(6.274199222389143e-05), 'behavior_loss': np.float64(0.47234987318515775)}

Episode step 20630, time diff 3.417491912841797, total time dif 2443.059368610382)
step: 20630 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(1.7886919), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07772646993398666), 'actor_loss': np.float64(-1.0015969753265381), 'hyper_actor_loss': np.float64(4.096406883036252e-05), 'behavior_loss': np.float64(0.4922560900449753)}

Episode step 20640, time diff 3.3797223567962646, total time dif 2446.476860523224)
step: 20640 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.4520245), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.77778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059990870952606204), 'actor_loss': np.float64(-0.9991799592971802), 'hyper_actor_loss': np.float64(4.2216270594508386e-05), 'behavior_loss': np.float64(0.5474837929010391)}

Episode step 20650, time diff 3.3219070434570312, total time dif 2449.85658288002)
step: 20650 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(2.7069447), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059398554265499115), 'actor_loss': np.float64(-0.9816388010978698), 'hyper_actor_loss': np.float64(4.109811816306319e-05), 'behavior_loss': np.float64(0.4603460133075714)}

Episode step 20660, time diff 3.2321321964263916, total time dif 2453.178489923477)
step: 20660 @ episode report: {'average_total_reward': np.float32(10.997778), 'reward_variance': np.float32(2.7495751), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05586972208693623), 'actor_loss': np.float64(-0.9953718066215516), 'hyper_actor_loss': np.float64(4.2717082033050245e-05), 'behavior_loss': np.float64(0.4997496008872986)}

Episode step 20670, time diff 3.318932056427002, total time dif 2456.4106221199036)
step: 20670 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.775364), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06568213813006878), 'actor_loss': np.float64(-0.992242968082428), 'hyper_actor_loss': np.float64(5.258444107312243e-05), 'behavior_loss': np.float64(0.504637885093689)}

Episode step 20680, time diff 3.2864596843719482, total time dif 2459.7295541763306)
step: 20680 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(1.8965254), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05757902637124061), 'actor_loss': np.float64(-0.9916435122489929), 'hyper_actor_loss': np.float64(3.844806724373484e-05), 'behavior_loss': np.float64(0.5561352968215942)}

Episode step 20690, time diff 3.296064853668213, total time dif 2463.0160138607025)
step: 20690 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(2.310869), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06595334745943546), 'actor_loss': np.float64(-0.9844215512275696), 'hyper_actor_loss': np.float64(5.15335541422246e-05), 'behavior_loss': np.float64(0.5136945366859436)}

Episode step 20700, time diff 3.2846508026123047, total time dif 2466.3120787143707)
step: 20700 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(2.4405942), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0629085659980774), 'actor_loss': np.float64(-0.9910829961299896), 'hyper_actor_loss': np.float64(4.774120170623064e-05), 'behavior_loss': np.float64(0.5524635046720505)}

Episode step 20710, time diff 3.327664375305176, total time dif 2469.596729516983)
step: 20710 @ episode report: {'average_total_reward': np.float32(11.45889), 'reward_variance': np.float32(3.318618), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0681941643357277), 'actor_loss': np.float64(-1.000764113664627), 'hyper_actor_loss': np.float64(4.3725671639549545e-05), 'behavior_loss': np.float64(0.5121611982584)}

Episode step 20720, time diff 3.4478204250335693, total time dif 2472.924393892288)
step: 20720 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(3.2262535), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06737559773027897), 'actor_loss': np.float64(-1.0029582023620605), 'hyper_actor_loss': np.float64(4.2393416515551506e-05), 'behavior_loss': np.float64(0.5451490819454193)}

Episode step 20730, time diff 3.5008773803710938, total time dif 2476.372214317322)
step: 20730 @ episode report: {'average_total_reward': np.float32(9.885557), 'reward_variance': np.float32(2.2948408), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06957645863294601), 'actor_loss': np.float64(-0.9940994143486023), 'hyper_actor_loss': np.float64(5.259808895061724e-05), 'behavior_loss': np.float64(0.5423179894685746)}

Episode step 20740, time diff 3.396510124206543, total time dif 2479.873091697693)
step: 20740 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.3333285), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08144187405705453), 'actor_loss': np.float64(-1.0167691111564636), 'hyper_actor_loss': np.float64(3.828607004834339e-05), 'behavior_loss': np.float64(0.54443079829216)}

Episode step 20750, time diff 3.4373998641967773, total time dif 2483.2696018218994)
step: 20750 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.8884203), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06720101572573185), 'actor_loss': np.float64(-1.0058521926403046), 'hyper_actor_loss': np.float64(3.932773397536948e-05), 'behavior_loss': np.float64(0.5447320997714996)}

Episode step 20760, time diff 3.4691193103790283, total time dif 2486.707001686096)
step: 20760 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(1.9632852), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07929845862090587), 'actor_loss': np.float64(-0.9955564141273499), 'hyper_actor_loss': np.float64(3.763341410376597e-05), 'behavior_loss': np.float64(0.5445725977420807)}

Episode step 20770, time diff 3.409508228302002, total time dif 2490.176120996475)
step: 20770 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.2998023), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07268152348697185), 'actor_loss': np.float64(-1.004396915435791), 'hyper_actor_loss': np.float64(4.067741501785349e-05), 'behavior_loss': np.float64(0.5132260262966156)}

Episode step 20780, time diff 3.4820053577423096, total time dif 2493.585629224777)
step: 20780 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(4.0789194), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058981245011091234), 'actor_loss': np.float64(-0.9933413207530976), 'hyper_actor_loss': np.float64(4.3954581269645135e-05), 'behavior_loss': np.float64(0.5002711594104767)}

Episode step 20790, time diff 3.38456654548645, total time dif 2497.0676345825195)
step: 20790 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(2.5243657), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07399912774562836), 'actor_loss': np.float64(-1.009857952594757), 'hyper_actor_loss': np.float64(4.69920636533061e-05), 'behavior_loss': np.float64(0.5049729853868484)}

Episode step 20800, time diff 3.454944133758545, total time dif 2500.452201128006)
step: 20800 @ episode report: {'average_total_reward': np.float32(10.885556), 'reward_variance': np.float32(2.9995823), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06799607202410698), 'actor_loss': np.float64(-0.9965027153491974), 'hyper_actor_loss': np.float64(4.796487737621646e-05), 'behavior_loss': np.float64(0.517515343427658)}

Episode step 20810, time diff 3.577237367630005, total time dif 2503.9071452617645)
step: 20810 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.3378475), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06087030991911888), 'actor_loss': np.float64(-0.9909674406051636), 'hyper_actor_loss': np.float64(4.9399264389649035e-05), 'behavior_loss': np.float64(0.5498789489269257)}

Episode step 20820, time diff 3.4948880672454834, total time dif 2507.4843826293945)
step: 20820 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(5.115978), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06054314784705639), 'actor_loss': np.float64(-0.9928728997707367), 'hyper_actor_loss': np.float64(4.9185071475221775e-05), 'behavior_loss': np.float64(0.5014686912298203)}

Episode step 20830, time diff 3.457484006881714, total time dif 2510.97927069664)
step: 20830 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(1.8903993), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06489056050777435), 'actor_loss': np.float64(-0.9966692984104156), 'hyper_actor_loss': np.float64(5.370794569898862e-05), 'behavior_loss': np.float64(0.49538354575634)}

Episode step 20840, time diff 3.4206717014312744, total time dif 2514.4367547035217)
step: 20840 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(1.6152214), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05921444445848465), 'actor_loss': np.float64(-0.9964596509933472), 'hyper_actor_loss': np.float64(5.051101179560646e-05), 'behavior_loss': np.float64(0.5378926485776901)}

Episode step 20850, time diff 3.4226176738739014, total time dif 2517.857426404953)
step: 20850 @ episode report: {'average_total_reward': np.float32(10.910001), 'reward_variance': np.float32(1.406135), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06319118309766054), 'actor_loss': np.float64(-0.9787594556808472), 'hyper_actor_loss': np.float64(4.3297827141941524e-05), 'behavior_loss': np.float64(0.5122724384069443)}

Episode step 20860, time diff 3.4902119636535645, total time dif 2521.280044078827)
step: 20860 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(1.9731362), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06817599721252918), 'actor_loss': np.float64(-1.0098257660865784), 'hyper_actor_loss': np.float64(4.4082820750190876e-05), 'behavior_loss': np.float64(0.507723069190979)}

Episode step 20870, time diff 3.5217976570129395, total time dif 2524.7702560424805)
step: 20870 @ episode report: {'average_total_reward': np.float32(9.500002), 'reward_variance': np.float32(4.1064215), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06307201422750949), 'actor_loss': np.float64(-1.0104061007499694), 'hyper_actor_loss': np.float64(4.323075118009001e-05), 'behavior_loss': np.float64(0.4853910535573959)}

Episode step 20880, time diff 3.466498851776123, total time dif 2528.2920536994934)
step: 20880 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(1.61761), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05470554195344448), 'actor_loss': np.float64(-0.9822389602661132), 'hyper_actor_loss': np.float64(4.6447062777588145e-05), 'behavior_loss': np.float64(0.48856075406074523)}

Episode step 20890, time diff 3.4301939010620117, total time dif 2531.7585525512695)
step: 20890 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(5.6093583), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061775453574955466), 'actor_loss': np.float64(-0.989721155166626), 'hyper_actor_loss': np.float64(4.02939273044467e-05), 'behavior_loss': np.float64(0.5147801280021668)}

Episode step 20900, time diff 3.5311293601989746, total time dif 2535.1887464523315)
step: 20900 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(1.7837296), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06198917794972658), 'actor_loss': np.float64(-0.9935798823833466), 'hyper_actor_loss': np.float64(4.6193551679607483e-05), 'behavior_loss': np.float64(0.5381337165832519)}

Episode step 20910, time diff 3.496351718902588, total time dif 2538.7198758125305)
step: 20910 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(2.3058143), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0639060813933611), 'actor_loss': np.float64(-0.9775985777378082), 'hyper_actor_loss': np.float64(3.871102526318282e-05), 'behavior_loss': np.float64(0.49919151663780215)}

Episode step 20920, time diff 3.5099570751190186, total time dif 2542.216227531433)
step: 20920 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(3.3212864), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07170189209282399), 'actor_loss': np.float64(-1.006730741262436), 'hyper_actor_loss': np.float64(4.803022020496428e-05), 'behavior_loss': np.float64(0.5693210959434509)}

Episode step 20930, time diff 3.764620542526245, total time dif 2545.726184606552)
step: 20930 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(2.017641), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0653274793177843), 'actor_loss': np.float64(-1.0032652139663696), 'hyper_actor_loss': np.float64(7.539359867223539e-05), 'behavior_loss': np.float64(0.511951345205307)}

Episode step 20940, time diff 3.686840057373047, total time dif 2549.4908051490784)
step: 20940 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(1.5834185), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06816616021096707), 'actor_loss': np.float64(-0.9886601269245148), 'hyper_actor_loss': np.float64(0.00037497527810046447), 'behavior_loss': np.float64(0.48680849075317384)}

Episode step 20950, time diff 3.5960628986358643, total time dif 2553.1776452064514)
step: 20950 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(4.9908915), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060817517526447776), 'actor_loss': np.float64(-1.0001371204853058), 'hyper_actor_loss': np.float64(0.00014807792103965767), 'behavior_loss': np.float64(0.49215027987957)}

Episode step 20960, time diff 3.458864688873291, total time dif 2556.7737081050873)
step: 20960 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(2.9240022), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059307077527046205), 'actor_loss': np.float64(-0.9795132279396057), 'hyper_actor_loss': np.float64(7.996789863682353e-05), 'behavior_loss': np.float64(0.4984181135892868)}

Episode step 20970, time diff 3.3601746559143066, total time dif 2560.2325727939606)
step: 20970 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(4.2031803), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06106827668845653), 'actor_loss': np.float64(-0.9952742159366608), 'hyper_actor_loss': np.float64(4.698471093433909e-05), 'behavior_loss': np.float64(0.5015752285718917)}

Episode step 20980, time diff 3.333007335662842, total time dif 2563.592747449875)
step: 20980 @ episode report: {'average_total_reward': np.float32(11.46889), 'reward_variance': np.float32(1.7853043), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06382195018231869), 'actor_loss': np.float64(-1.0024574816226959), 'hyper_actor_loss': np.float64(4.193168351775967e-05), 'behavior_loss': np.float64(0.4920208275318146)}

Episode step 20990, time diff 3.294766664505005, total time dif 2566.9257547855377)
step: 20990 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(5.640033), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.288889), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07172091044485569), 'actor_loss': np.float64(-0.9968367993831635), 'hyper_actor_loss': np.float64(4.477590300666634e-05), 'behavior_loss': np.float64(0.5023343086242675)}

Episode step 21000, time diff 3.370969772338867, total time dif 2570.2205214500427)
step: 21000 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.9913597), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05407900083810091), 'actor_loss': np.float64(-0.9964954018592834), 'hyper_actor_loss': np.float64(4.833659295400139e-05), 'behavior_loss': np.float64(0.5367964595556259)}

Episode step 21010, time diff 3.3908493518829346, total time dif 2573.5914912223816)
step: 21010 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.77885), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059650620073080064), 'actor_loss': np.float64(-0.9758778512477875), 'hyper_actor_loss': np.float64(5.2697616956720596e-05), 'behavior_loss': np.float64(0.5074122339487076)}

Episode step 21020, time diff 3.4653706550598145, total time dif 2576.9823405742645)
step: 21020 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(4.106051), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0674815222620964), 'actor_loss': np.float64(-1.0047650396823884), 'hyper_actor_loss': np.float64(4.713600701506948e-05), 'behavior_loss': np.float64(0.5025490254163743)}

Episode step 21030, time diff 3.636554718017578, total time dif 2580.4477112293243)
step: 21030 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(4.2935915), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06091989427804947), 'actor_loss': np.float64(-1.0081673204898833), 'hyper_actor_loss': np.float64(3.886341910401825e-05), 'behavior_loss': np.float64(0.513201242685318)}

Episode step 21040, time diff 3.3860111236572266, total time dif 2584.084265947342)
step: 21040 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(2.0703971), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06600643359124661), 'actor_loss': np.float64(-0.9752755403518677), 'hyper_actor_loss': np.float64(4.573363330564462e-05), 'behavior_loss': np.float64(0.5260258167982101)}

Episode step 21050, time diff 3.4470643997192383, total time dif 2587.470277070999)
step: 21050 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(6.3853364), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07210769578814506), 'actor_loss': np.float64(-1.0009491860866546), 'hyper_actor_loss': np.float64(4.039638297399506e-05), 'behavior_loss': np.float64(0.513859161734581)}

Episode step 21060, time diff 3.4709231853485107, total time dif 2590.9173414707184)
step: 21060 @ episode report: {'average_total_reward': np.float32(10.112223), 'reward_variance': np.float32(3.3241105), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06304055284708739), 'actor_loss': np.float64(-0.997436797618866), 'hyper_actor_loss': np.float64(3.4265589056303726e-05), 'behavior_loss': np.float64(0.5071459650993347)}

Episode step 21070, time diff 3.428574323654175, total time dif 2594.388264656067)
step: 21070 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(3.6519628), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07196407578885555), 'actor_loss': np.float64(-1.0018760919570924), 'hyper_actor_loss': np.float64(3.59089517587563e-05), 'behavior_loss': np.float64(0.5532306969165802)}

Episode step 21080, time diff 3.4155004024505615, total time dif 2597.816838979721)
step: 21080 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(3.084105), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06255853362381458), 'actor_loss': np.float64(-1.0048534750938416), 'hyper_actor_loss': np.float64(3.83192345907446e-05), 'behavior_loss': np.float64(0.4997344702482224)}

Episode step 21090, time diff 3.564380168914795, total time dif 2601.2323393821716)
step: 21090 @ episode report: {'average_total_reward': np.float32(10.2733345), 'reward_variance': np.float32(1.3741038), 'max_total_reward': np.float32(11.900002), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058322323486208916), 'actor_loss': np.float64(-0.9799128830432892), 'hyper_actor_loss': np.float64(3.759747487492859e-05), 'behavior_loss': np.float64(0.48712166845798494)}

Episode step 21100, time diff 3.4638795852661133, total time dif 2604.7967195510864)
step: 21100 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.2826037), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06011931709945202), 'actor_loss': np.float64(-0.9852673053741455), 'hyper_actor_loss': np.float64(3.648161200544564e-05), 'behavior_loss': np.float64(0.5540445297956467)}

Episode step 21110, time diff 3.4748220443725586, total time dif 2608.2605991363525)
step: 21110 @ episode report: {'average_total_reward': np.float32(11.383334), 'reward_variance': np.float32(3.0958338), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07235359326004982), 'actor_loss': np.float64(-1.0000867903232575), 'hyper_actor_loss': np.float64(3.4383864112896843e-05), 'behavior_loss': np.float64(0.5089649707078934)}

Episode step 21120, time diff 3.548992156982422, total time dif 2611.735421180725)
step: 21120 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(2.4537587), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333347), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06449915133416653), 'actor_loss': np.float64(-1.004635763168335), 'hyper_actor_loss': np.float64(4.63481248516473e-05), 'behavior_loss': np.float64(0.48098982572555543)}

Episode step 21130, time diff 3.4753682613372803, total time dif 2615.2844133377075)
step: 21130 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(0.7771606), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07630602084100246), 'actor_loss': np.float64(-1.0062647044658661), 'hyper_actor_loss': np.float64(3.711627832672093e-05), 'behavior_loss': np.float64(0.5385426491498947)}

Episode step 21140, time diff 3.516479730606079, total time dif 2618.759781599045)
step: 21140 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(3.35929), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05387235227972269), 'actor_loss': np.float64(-0.9965642273426056), 'hyper_actor_loss': np.float64(3.4488017627154476e-05), 'behavior_loss': np.float64(0.48010257482528684)}

Episode step 21150, time diff 3.532809019088745, total time dif 2622.276261329651)
step: 21150 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(4.040095), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05214095115661621), 'actor_loss': np.float64(-0.966234815120697), 'hyper_actor_loss': np.float64(3.4891066752607004e-05), 'behavior_loss': np.float64(0.5263992965221405)}

Episode step 21160, time diff 3.527125835418701, total time dif 2625.8090703487396)
step: 21160 @ episode report: {'average_total_reward': np.float32(11.905556), 'reward_variance': np.float32(5.1457844), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07696818001568317), 'actor_loss': np.float64(-0.9988930761814118), 'hyper_actor_loss': np.float64(3.16932604619069e-05), 'behavior_loss': np.float64(0.4978341519832611)}

Episode step 21170, time diff 3.534506320953369, total time dif 2629.3361961841583)
step: 21170 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(3.0415502), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0637296687811613), 'actor_loss': np.float64(-1.023902142047882), 'hyper_actor_loss': np.float64(3.5179715814592784e-05), 'behavior_loss': np.float64(0.4789713203907013)}

Episode step 21180, time diff 3.485393762588501, total time dif 2632.8707025051117)
step: 21180 @ episode report: {'average_total_reward': np.float32(10.461112), 'reward_variance': np.float32(3.3159077), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06128822378814221), 'actor_loss': np.float64(-0.9730212271213532), 'hyper_actor_loss': np.float64(3.653263920568861e-05), 'behavior_loss': np.float64(0.5231049746274948)}

Episode step 21190, time diff 3.4513392448425293, total time dif 2636.3560962677)
step: 21190 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(1.2181989), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06776615194976329), 'actor_loss': np.float64(-0.990302461385727), 'hyper_actor_loss': np.float64(3.204266613465734e-05), 'behavior_loss': np.float64(0.44685861468315125)}

Episode step 21200, time diff 3.4925107955932617, total time dif 2639.8074355125427)
step: 21200 @ episode report: {'average_total_reward': np.float32(10.012222), 'reward_variance': np.float32(5.3324065), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0692735679447651), 'actor_loss': np.float64(-1.0181701064109803), 'hyper_actor_loss': np.float64(3.331371608510381e-05), 'behavior_loss': np.float64(0.5284688174724579)}

Episode step 21210, time diff 3.409493923187256, total time dif 2643.299946308136)
step: 21210 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(3.3863075), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05574126243591308), 'actor_loss': np.float64(-0.9765548944473267), 'hyper_actor_loss': np.float64(2.9189715132815763e-05), 'behavior_loss': np.float64(0.49731414020061493)}

Episode step 21220, time diff 3.4369590282440186, total time dif 2646.7094402313232)
step: 21220 @ episode report: {'average_total_reward': np.float32(11.720001), 'reward_variance': np.float32(2.3291068), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05130130723118782), 'actor_loss': np.float64(-0.9581583559513092), 'hyper_actor_loss': np.float64(3.511785416776547e-05), 'behavior_loss': np.float64(0.5111733794212341)}

Episode step 21230, time diff 3.456920623779297, total time dif 2650.1463992595673)
step: 21230 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(2.7670364), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08519642315804958), 'actor_loss': np.float64(-1.0182011008262635), 'hyper_actor_loss': np.float64(3.3722188527463e-05), 'behavior_loss': np.float64(0.4958935618400574)}

Episode step 21240, time diff 3.3608038425445557, total time dif 2653.6033198833466)
step: 21240 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(2.362538), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06491541229188443), 'actor_loss': np.float64(-1.026669842004776), 'hyper_actor_loss': np.float64(3.088078392465832e-05), 'behavior_loss': np.float64(0.5052787452936173)}

Episode step 21250, time diff 3.3249151706695557, total time dif 2656.964123725891)
step: 21250 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(2.178129), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06356127634644508), 'actor_loss': np.float64(-0.9719958662986755), 'hyper_actor_loss': np.float64(3.553362057573395e-05), 'behavior_loss': np.float64(0.4692735433578491)}

Episode step 21260, time diff 3.29160475730896, total time dif 2660.2890388965607)
step: 21260 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(3.9937038), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06141253737732768), 'actor_loss': np.float64(-0.9935695409774781), 'hyper_actor_loss': np.float64(4.094262530998094e-05), 'behavior_loss': np.float64(0.5137854516506195)}

Episode step 21270, time diff 3.411001682281494, total time dif 2663.5806436538696)
step: 21270 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(0.7480446), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06760099865496158), 'actor_loss': np.float64(-1.0132705926895142), 'hyper_actor_loss': np.float64(3.2799652399262416e-05), 'behavior_loss': np.float64(0.4716550409793854)}

Episode step 21280, time diff 3.327669143676758, total time dif 2666.991645336151)
step: 21280 @ episode report: {'average_total_reward': np.float32(10.895556), 'reward_variance': np.float32(2.2079306), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06024135313928127), 'actor_loss': np.float64(-0.991409420967102), 'hyper_actor_loss': np.float64(3.3221726880583446e-05), 'behavior_loss': np.float64(0.4999853402376175)}

Episode step 21290, time diff 3.3112950325012207, total time dif 2670.319314479828)
step: 21290 @ episode report: {'average_total_reward': np.float32(12.056667), 'reward_variance': np.float32(5.0774426), 'max_total_reward': np.float32(17.877777), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(18.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06373196430504321), 'actor_loss': np.float64(-0.9723317503929139), 'hyper_actor_loss': np.float64(2.9308487683010753e-05), 'behavior_loss': np.float64(0.5041179955005646)}

Episode step 21300, time diff 3.3163654804229736, total time dif 2673.630609512329)
step: 21300 @ episode report: {'average_total_reward': np.float32(11.468889), 'reward_variance': np.float32(2.2616243), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061291102692484854), 'actor_loss': np.float64(-0.9956045985221863), 'hyper_actor_loss': np.float64(3.442093511694111e-05), 'behavior_loss': np.float64(0.5049088448286057)}

Episode step 21310, time diff 3.328847646713257, total time dif 2676.946974992752)
step: 21310 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(2.4888585), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07696125581860543), 'actor_loss': np.float64(-1.011566936969757), 'hyper_actor_loss': np.float64(3.1071757075551434e-05), 'behavior_loss': np.float64(0.5433951139450073)}

Episode step 21320, time diff 3.3377342224121094, total time dif 2680.2758226394653)
step: 21320 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(1.9431871), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07047235183417797), 'actor_loss': np.float64(-1.0005259096622467), 'hyper_actor_loss': np.float64(3.665790536615532e-05), 'behavior_loss': np.float64(0.5145910412073136)}

Episode step 21330, time diff 3.3107829093933105, total time dif 2683.6135568618774)
step: 21330 @ episode report: {'average_total_reward': np.float32(11.395556), 'reward_variance': np.float32(2.2394621), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06958889327943325), 'actor_loss': np.float64(-1.0014942646026612), 'hyper_actor_loss': np.float64(2.9807297869410833e-05), 'behavior_loss': np.float64(0.49261104464530947)}

Episode step 21340, time diff 3.366610050201416, total time dif 2686.9243397712708)
step: 21340 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(6.2341433), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05554689057171345), 'actor_loss': np.float64(-0.9848701417446136), 'hyper_actor_loss': np.float64(3.5189175105188045e-05), 'behavior_loss': np.float64(0.5298612117767334)}

Episode step 21350, time diff 3.306118965148926, total time dif 2690.290949821472)
step: 21350 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(2.5477102), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062355416268110274), 'actor_loss': np.float64(-0.9721170365810394), 'hyper_actor_loss': np.float64(3.625474164437037e-05), 'behavior_loss': np.float64(0.514455109834671)}

Episode step 21360, time diff 3.3045506477355957, total time dif 2693.597068786621)
step: 21360 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(2.537365), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06090293936431408), 'actor_loss': np.float64(-0.9919301986694335), 'hyper_actor_loss': np.float64(2.7489466810948214e-05), 'behavior_loss': np.float64(0.4827155500650406)}

Episode step 21370, time diff 3.346163511276245, total time dif 2696.9016194343567)
step: 21370 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(2.620909), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07317261397838593), 'actor_loss': np.float64(-0.9941281139850616), 'hyper_actor_loss': np.float64(3.066780627705157e-05), 'behavior_loss': np.float64(0.5005059182643891)}

Episode step 21380, time diff 3.29463791847229, total time dif 2700.247782945633)
step: 21380 @ episode report: {'average_total_reward': np.float32(11.246668), 'reward_variance': np.float32(0.7536003), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(9.533334), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.066585348919034), 'actor_loss': np.float64(-0.9995415091514588), 'hyper_actor_loss': np.float64(2.9843394077033736e-05), 'behavior_loss': np.float64(0.4981272488832474)}

Episode step 21390, time diff 3.328812599182129, total time dif 2703.542420864105)
step: 21390 @ episode report: {'average_total_reward': np.float32(11.456668), 'reward_variance': np.float32(2.243468), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0700082678347826), 'actor_loss': np.float64(-0.9974079370498657), 'hyper_actor_loss': np.float64(4.0307751078216825e-05), 'behavior_loss': np.float64(0.46615849137306214)}

Episode step 21400, time diff 3.406507968902588, total time dif 2706.8712334632874)
step: 21400 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(1.5024257), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07025482840836048), 'actor_loss': np.float64(-1.0033529102802277), 'hyper_actor_loss': np.float64(2.7316900377627464e-05), 'behavior_loss': np.float64(0.4842903554439545)}

Episode step 21410, time diff 3.3516738414764404, total time dif 2710.27774143219)
step: 21410 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(4.4595933), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06406037621200085), 'actor_loss': np.float64(-1.0184142947196961), 'hyper_actor_loss': np.float64(3.079352154600201e-05), 'behavior_loss': np.float64(0.47697465121746063)}

Episode step 21420, time diff 3.371795415878296, total time dif 2713.6294152736664)
step: 21420 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(1.9543812), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061054838448762895), 'actor_loss': np.float64(-0.9877788245677948), 'hyper_actor_loss': np.float64(2.8537004618556237e-05), 'behavior_loss': np.float64(0.5187625825405121)}

Episode step 21430, time diff 3.568657398223877, total time dif 2717.0012106895447)
step: 21430 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(3.1846175), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06436732318252325), 'actor_loss': np.float64(-0.9860022902488709), 'hyper_actor_loss': np.float64(3.336825830047019e-05), 'behavior_loss': np.float64(0.48851287364959717)}

Episode step 21440, time diff 3.3902077674865723, total time dif 2720.5698680877686)
step: 21440 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(3.9149184), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07077187653630972), 'actor_loss': np.float64(-0.9865308940410614), 'hyper_actor_loss': np.float64(2.901536281569861e-05), 'behavior_loss': np.float64(0.521450987458229)}

Episode step 21450, time diff 3.4248592853546143, total time dif 2723.960075855255)
step: 21450 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(2.4105449), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06156084276735783), 'actor_loss': np.float64(-0.9849329590797424), 'hyper_actor_loss': np.float64(3.208777889085468e-05), 'behavior_loss': np.float64(0.5217343091964721)}

Episode step 21460, time diff 3.4135758876800537, total time dif 2727.3849351406097)
step: 21460 @ episode report: {'average_total_reward': np.float32(11.532224), 'reward_variance': np.float32(2.9943314), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060919050499796866), 'actor_loss': np.float64(-0.9865657389163971), 'hyper_actor_loss': np.float64(2.764323726296425e-05), 'behavior_loss': np.float64(0.471139258146286)}

Episode step 21470, time diff 3.417693853378296, total time dif 2730.79851102829)
step: 21470 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(4.3461237), 'max_total_reward': np.float32(16.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05664408728480339), 'actor_loss': np.float64(-0.9982960224151611), 'hyper_actor_loss': np.float64(2.810751338984119e-05), 'behavior_loss': np.float64(0.4524108856916428)}

Episode step 21480, time diff 3.3619205951690674, total time dif 2734.216204881668)
step: 21480 @ episode report: {'average_total_reward': np.float32(11.96889), 'reward_variance': np.float32(4.649083), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05414017997682095), 'actor_loss': np.float64(-0.9908486247062683), 'hyper_actor_loss': np.float64(3.364132480783155e-05), 'behavior_loss': np.float64(0.45353164374828336)}

Episode step 21490, time diff 3.4514832496643066, total time dif 2737.578125476837)
step: 21490 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(2.066032), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.047342149168252946), 'actor_loss': np.float64(-0.9758473873138428), 'hyper_actor_loss': np.float64(2.855188595276559e-05), 'behavior_loss': np.float64(0.4752455294132233)}

Episode step 21500, time diff 3.409883499145508, total time dif 2741.0296087265015)
step: 21500 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(2.0081744), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0566580580547452), 'actor_loss': np.float64(-0.9752565920352936), 'hyper_actor_loss': np.float64(2.753401604422834e-05), 'behavior_loss': np.float64(0.5076963722705841)}

Episode step 21510, time diff 3.3921313285827637, total time dif 2744.439492225647)
step: 21510 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(5.4008455), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.065880910679698), 'actor_loss': np.float64(-0.9891735732555389), 'hyper_actor_loss': np.float64(2.5658384220150766e-05), 'behavior_loss': np.float64(0.48912745118141177)}

Episode step 21520, time diff 3.4333584308624268, total time dif 2747.8316235542297)
step: 21520 @ episode report: {'average_total_reward': np.float32(11.42), 'reward_variance': np.float32(3.6740208), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0661583423614502), 'actor_loss': np.float64(-1.0124145984649657), 'hyper_actor_loss': np.float64(2.699135075090453e-05), 'behavior_loss': np.float64(0.476213738322258)}

Episode step 21530, time diff 3.4754645824432373, total time dif 2751.264981985092)
step: 21530 @ episode report: {'average_total_reward': np.float32(11.383334), 'reward_variance': np.float32(5.2175875), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06298041883856058), 'actor_loss': np.float64(-0.9836081266403198), 'hyper_actor_loss': np.float64(2.6549941867415328e-05), 'behavior_loss': np.float64(0.45057725310325625)}

Episode step 21540, time diff 3.4081990718841553, total time dif 2754.7404465675354)
step: 21540 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(2.9013333), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.050785012729465964), 'actor_loss': np.float64(-0.978088665008545), 'hyper_actor_loss': np.float64(3.3494093258923385e-05), 'behavior_loss': np.float64(0.4839008778333664)}

Episode step 21550, time diff 3.3794257640838623, total time dif 2758.1486456394196)
step: 21550 @ episode report: {'average_total_reward': np.float32(11.195556), 'reward_variance': np.float32(5.395758), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06970411352813244), 'actor_loss': np.float64(-0.9922483205795288), 'hyper_actor_loss': np.float64(2.966123902297113e-05), 'behavior_loss': np.float64(0.4888406753540039)}

Episode step 21560, time diff 3.4469213485717773, total time dif 2761.5280714035034)
step: 21560 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(1.338397), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06425024885684252), 'actor_loss': np.float64(-1.0070539355278014), 'hyper_actor_loss': np.float64(3.148286923533305e-05), 'behavior_loss': np.float64(0.4827775418758392)}

Episode step 21570, time diff 3.4946067333221436, total time dif 2764.974992752075)
step: 21570 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(0.8913142), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07457679118961096), 'actor_loss': np.float64(-0.9845595538616181), 'hyper_actor_loss': np.float64(2.9979503597132862e-05), 'behavior_loss': np.float64(0.4875009715557098)}

Episode step 21580, time diff 3.498643398284912, total time dif 2768.4695994853973)
step: 21580 @ episode report: {'average_total_reward': np.float32(11.371112), 'reward_variance': np.float32(1.4142023), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.655557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06219482235610485), 'actor_loss': np.float64(-1.0019118726253509), 'hyper_actor_loss': np.float64(2.7593434606387745e-05), 'behavior_loss': np.float64(0.5062670886516571)}

Episode step 21590, time diff 3.6084654331207275, total time dif 2771.9682428836823)
step: 21590 @ episode report: {'average_total_reward': np.float32(11.332224), 'reward_variance': np.float32(0.9492949), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06443026401102543), 'actor_loss': np.float64(-0.9891293227672577), 'hyper_actor_loss': np.float64(3.0039104058232623e-05), 'behavior_loss': np.float64(0.47628031075000765)}

Episode step 21600, time diff 3.425807237625122, total time dif 2775.576708316803)
step: 21600 @ episode report: {'average_total_reward': np.float32(11.46889), 'reward_variance': np.float32(1.9823158), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05797134134918451), 'actor_loss': np.float64(-0.9868809580802917), 'hyper_actor_loss': np.float64(2.8211755488882772e-05), 'behavior_loss': np.float64(0.48683043718338015)}

Episode step 21610, time diff 3.3989932537078857, total time dif 2779.002515554428)
step: 21610 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.9037983), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06964841075241565), 'actor_loss': np.float64(-1.0017692744731903), 'hyper_actor_loss': np.float64(4.028373077744618e-05), 'behavior_loss': np.float64(0.48763529360294344)}

Episode step 21620, time diff 3.4869048595428467, total time dif 2782.401508808136)
step: 21620 @ episode report: {'average_total_reward': np.float32(11.756668), 'reward_variance': np.float32(3.9773946), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.051976611092686656), 'actor_loss': np.float64(-0.9815458953380585), 'hyper_actor_loss': np.float64(2.8197079518577085e-05), 'behavior_loss': np.float64(0.49564285278320314)}

Episode step 21630, time diff 3.377436637878418, total time dif 2785.888413667679)
step: 21630 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(0.75487524), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056567158363759515), 'actor_loss': np.float64(-0.9785682678222656), 'hyper_actor_loss': np.float64(3.0252810938691253e-05), 'behavior_loss': np.float64(0.4654958039522171)}

Episode step 21640, time diff 3.4126644134521484, total time dif 2789.2658503055573)
step: 21640 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.931857), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06250326335430145), 'actor_loss': np.float64(-1.0000912845134735), 'hyper_actor_loss': np.float64(2.7558311921893618e-05), 'behavior_loss': np.float64(0.4819929271936417)}

Episode step 21650, time diff 3.4310176372528076, total time dif 2792.6785147190094)
step: 21650 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.2918465), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06237907819449902), 'actor_loss': np.float64(-1.004718792438507), 'hyper_actor_loss': np.float64(3.160949327138951e-05), 'behavior_loss': np.float64(0.4513373762369156)}

Episode step 21660, time diff 3.404978036880493, total time dif 2796.109532356262)
step: 21660 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.126252), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07677106000483036), 'actor_loss': np.float64(-1.002860140800476), 'hyper_actor_loss': np.float64(3.406284558877814e-05), 'behavior_loss': np.float64(0.502804371714592)}

Episode step 21670, time diff 3.421008825302124, total time dif 2799.5145103931427)
step: 21670 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(3.2226691), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06471345350146293), 'actor_loss': np.float64(-1.003621017932892), 'hyper_actor_loss': np.float64(3.164619683957426e-05), 'behavior_loss': np.float64(0.47290759682655337)}

Episode step 21680, time diff 3.383574962615967, total time dif 2802.935519218445)
step: 21680 @ episode report: {'average_total_reward': np.float32(10.671112), 'reward_variance': np.float32(5.3037577), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0744686596095562), 'actor_loss': np.float64(-1.0089458227157593), 'hyper_actor_loss': np.float64(2.807108321576379e-05), 'behavior_loss': np.float64(0.4908360630273819)}

Episode step 21690, time diff 3.4242241382598877, total time dif 2806.319094181061)
step: 21690 @ episode report: {'average_total_reward': np.float32(11.671112), 'reward_variance': np.float32(4.330646), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060567181650549175), 'actor_loss': np.float64(-0.9941427528858184), 'hyper_actor_loss': np.float64(3.055487341043772e-05), 'behavior_loss': np.float64(0.45431498885154725)}

Episode step 21700, time diff 3.411652088165283, total time dif 2809.7433183193207)
step: 21700 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(1.5398024), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05941594801843166), 'actor_loss': np.float64(-0.9903545558452607), 'hyper_actor_loss': np.float64(3.526729833538411e-05), 'behavior_loss': np.float64(0.4781989574432373)}

Episode step 21710, time diff 3.3627586364746094, total time dif 2813.154970407486)
step: 21710 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(0.55913126), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05385374929755926), 'actor_loss': np.float64(-0.9865685641765595), 'hyper_actor_loss': np.float64(3.089082092628814e-05), 'behavior_loss': np.float64(0.49031600952148435)}

Episode step 21720, time diff 3.327389717102051, total time dif 2816.5177290439606)
step: 21720 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.1622412), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06944819130003452), 'actor_loss': np.float64(-0.9761709809303284), 'hyper_actor_loss': np.float64(2.572903031250462e-05), 'behavior_loss': np.float64(0.5222836852073669)}

Episode step 21730, time diff 3.2967052459716797, total time dif 2819.8451187610626)
step: 21730 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(2.3747413), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053181614726781845), 'actor_loss': np.float64(-0.9935792863368988), 'hyper_actor_loss': np.float64(2.7513391614775173e-05), 'behavior_loss': np.float64(0.4605070620775223)}

Episode step 21740, time diff 3.384552478790283, total time dif 2823.1418240070343)
step: 21740 @ episode report: {'average_total_reward': np.float32(10.807779), 'reward_variance': np.float32(1.5238533), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0773785725235939), 'actor_loss': np.float64(-0.9893771886825562), 'hyper_actor_loss': np.float64(2.903705353674013e-05), 'behavior_loss': np.float64(0.4824782133102417)}

Episode step 21750, time diff 3.4603419303894043, total time dif 2826.5263764858246)
step: 21750 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(6.5021105), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05103481151163578), 'actor_loss': np.float64(-1.0062291026115417), 'hyper_actor_loss': np.float64(2.8280640253797174e-05), 'behavior_loss': np.float64(0.47642924785614016)}

Episode step 21760, time diff 3.3154356479644775, total time dif 2829.986718416214)
step: 21760 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(2.8534575), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06028096377849579), 'actor_loss': np.float64(-0.9735954284667969), 'hyper_actor_loss': np.float64(2.5299661501776427e-05), 'behavior_loss': np.float64(0.5011570751667023)}

Episode step 21770, time diff 3.3220341205596924, total time dif 2833.3021540641785)
step: 21770 @ episode report: {'average_total_reward': np.float32(11.820001), 'reward_variance': np.float32(2.3010077), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06375655811280012), 'actor_loss': np.float64(-0.9947452485561371), 'hyper_actor_loss': np.float64(2.7583679548115468e-05), 'behavior_loss': np.float64(0.49625896513462064)}

Episode step 21780, time diff 3.3222529888153076, total time dif 2836.624188184738)
step: 21780 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(6.728444), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06331569347530604), 'actor_loss': np.float64(-1.0074911713600159), 'hyper_actor_loss': np.float64(2.345809043617919e-05), 'behavior_loss': np.float64(0.45867644250392914)}

Episode step 21790, time diff 3.3526105880737305, total time dif 2839.9464411735535)
step: 21790 @ episode report: {'average_total_reward': np.float32(11.1466675), 'reward_variance': np.float32(1.1588348), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0555005244910717), 'actor_loss': np.float64(-0.9789693772792816), 'hyper_actor_loss': np.float64(2.3907421928015536e-05), 'behavior_loss': np.float64(0.5060988545417786)}

Episode step 21800, time diff 3.3688547611236572, total time dif 2843.299051761627)
step: 21800 @ episode report: {'average_total_reward': np.float32(10.585557), 'reward_variance': np.float32(5.2463484), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053437655605375764), 'actor_loss': np.float64(-0.9652225613594055), 'hyper_actor_loss': np.float64(2.860555687220767e-05), 'behavior_loss': np.float64(0.48046937882900237)}

Episode step 21810, time diff 3.3137874603271484, total time dif 2846.667906522751)
step: 21810 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(2.4032664), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07117927260696888), 'actor_loss': np.float64(-0.9893080353736877), 'hyper_actor_loss': np.float64(2.614830827951664e-05), 'behavior_loss': np.float64(0.44169354140758516)}

Episode step 21820, time diff 3.3758132457733154, total time dif 2849.981693983078)
step: 21820 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(0.6619802), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06336053311824799), 'actor_loss': np.float64(-1.0159255504608153), 'hyper_actor_loss': np.float64(2.5247236044378953e-05), 'behavior_loss': np.float64(0.47155387997627257)}

Episode step 21830, time diff 3.3555080890655518, total time dif 2853.3575072288513)
step: 21830 @ episode report: {'average_total_reward': np.float32(11.332223), 'reward_variance': np.float32(2.208678), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05896846726536751), 'actor_loss': np.float64(-0.9887789785861969), 'hyper_actor_loss': np.float64(2.6734790480986703e-05), 'behavior_loss': np.float64(0.43970443308353424)}

Episode step 21840, time diff 3.328411817550659, total time dif 2856.713015317917)
step: 21840 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(1.831681), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05293652173131704), 'actor_loss': np.float64(-0.9828779935836792), 'hyper_actor_loss': np.float64(2.6517338301346172e-05), 'behavior_loss': np.float64(0.44293138682842254)}

Episode step 21850, time diff 3.315859794616699, total time dif 2860.0414271354675)
step: 21850 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(2.0954318), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057528520748019216), 'actor_loss': np.float64(-0.9813713371753693), 'hyper_actor_loss': np.float64(2.439904728817055e-05), 'behavior_loss': np.float64(0.5174694448709488)}

Episode step 21860, time diff 3.3488376140594482, total time dif 2863.3572869300842)
step: 21860 @ episode report: {'average_total_reward': np.float32(11.283335), 'reward_variance': np.float32(3.5772908), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07115925587713719), 'actor_loss': np.float64(-0.985933142900467), 'hyper_actor_loss': np.float64(2.6827975852938833e-05), 'behavior_loss': np.float64(0.4918682396411896)}

Episode step 21870, time diff 3.3143272399902344, total time dif 2866.7061245441437)
step: 21870 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(5.3868017), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07161103822290897), 'actor_loss': np.float64(-1.0174033522605896), 'hyper_actor_loss': np.float64(2.2066780547902454e-05), 'behavior_loss': np.float64(0.4728775531053543)}

Episode step 21880, time diff 3.332663059234619, total time dif 2870.020451784134)
step: 21880 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.2860315), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06600405350327491), 'actor_loss': np.float64(-1.0143347203731536), 'hyper_actor_loss': np.float64(2.834260267263744e-05), 'behavior_loss': np.float64(0.45097612738609316)}

Episode step 21890, time diff 3.3027334213256836, total time dif 2873.3531148433685)
step: 21890 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(1.2759312), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06142921783030033), 'actor_loss': np.float64(-0.9853105247020721), 'hyper_actor_loss': np.float64(2.2263216305873356e-05), 'behavior_loss': np.float64(0.458196172118187)}

Episode step 21900, time diff 3.263690233230591, total time dif 2876.655848264694)
step: 21900 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(2.1208959), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07060216683894396), 'actor_loss': np.float64(-0.9918324828147889), 'hyper_actor_loss': np.float64(2.0963828683306927e-05), 'behavior_loss': np.float64(0.4515116274356842)}

Episode step 21910, time diff 3.2998600006103516, total time dif 2879.919538497925)
step: 21910 @ episode report: {'average_total_reward': np.float32(11.556668), 'reward_variance': np.float32(0.5353198), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06355022974312305), 'actor_loss': np.float64(-1.0111008167266846), 'hyper_actor_loss': np.float64(2.535871772124665e-05), 'behavior_loss': np.float64(0.4323701709508896)}

Episode step 21920, time diff 3.4097511768341064, total time dif 2883.219398498535)
step: 21920 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(3.2290912), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05935028642416), 'actor_loss': np.float64(-0.9962733566761017), 'hyper_actor_loss': np.float64(2.3400016289087944e-05), 'behavior_loss': np.float64(0.4750154435634613)}

Episode step 21930, time diff 3.206225872039795, total time dif 2886.6291496753693)
step: 21930 @ episode report: {'average_total_reward': np.float32(12.005556), 'reward_variance': np.float32(3.2381294), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06078117582947016), 'actor_loss': np.float64(-0.9818416655063629), 'hyper_actor_loss': np.float64(2.2972738770477007e-05), 'behavior_loss': np.float64(0.45722671747207644)}

Episode step 21940, time diff 3.187803030014038, total time dif 2889.835375547409)
step: 21940 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(1.3208201), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05087502915412188), 'actor_loss': np.float64(-0.9804234862327575), 'hyper_actor_loss': np.float64(3.294575708423508e-05), 'behavior_loss': np.float64(0.4391226977109909)}

Episode step 21950, time diff 3.228780508041382, total time dif 2893.023178577423)
step: 21950 @ episode report: {'average_total_reward': np.float32(10.934445), 'reward_variance': np.float32(1.414011), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06845784597098828), 'actor_loss': np.float64(-0.9900789558887482), 'hyper_actor_loss': np.float64(2.710910193854943e-05), 'behavior_loss': np.float64(0.4632286846637726)}

Episode step 21960, time diff 3.2706801891326904, total time dif 2896.2519590854645)
step: 21960 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(2.3802025), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06321150660514832), 'actor_loss': np.float64(-1.0115037918090821), 'hyper_actor_loss': np.float64(2.9496006573026533e-05), 'behavior_loss': np.float64(0.4277813047170639)}

Episode step 21970, time diff 3.2536978721618652, total time dif 2899.522639274597)
step: 21970 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(4.078619), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06185095645487308), 'actor_loss': np.float64(-0.9993588030338287), 'hyper_actor_loss': np.float64(1.9883305685652886e-05), 'behavior_loss': np.float64(0.40011430978775026)}

Episode step 21980, time diff 3.2481441497802734, total time dif 2902.776337146759)
step: 21980 @ episode report: {'average_total_reward': np.float32(11.832223), 'reward_variance': np.float32(2.6983323), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060897082462906836), 'actor_loss': np.float64(-0.9979598045349121), 'hyper_actor_loss': np.float64(2.323885382793378e-05), 'behavior_loss': np.float64(0.4524579018354416)}

Episode step 21990, time diff 3.2589783668518066, total time dif 2906.0244812965393)
step: 21990 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(2.8514616), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07327189482748508), 'actor_loss': np.float64(-0.9914595723152161), 'hyper_actor_loss': np.float64(2.2012773479218593e-05), 'behavior_loss': np.float64(0.4584654301404953)}

Episode step 22000, time diff 3.219163179397583, total time dif 2909.283459663391)
step: 22000 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(2.8989184), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07908215075731277), 'actor_loss': np.float64(-1.0183815002441405), 'hyper_actor_loss': np.float64(2.3170642180048162e-05), 'behavior_loss': np.float64(0.49370649755001067)}

Episode step 22010, time diff 3.240901470184326, total time dif 2912.5026228427887)
step: 22010 @ episode report: {'average_total_reward': np.float32(10.883333), 'reward_variance': np.float32(4.3877845), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06478177793323994), 'actor_loss': np.float64(-0.9972225844860076), 'hyper_actor_loss': np.float64(2.5280299996666145e-05), 'behavior_loss': np.float64(0.4595560818910599)}

Episode step 22020, time diff 3.2297091484069824, total time dif 2915.743524312973)
step: 22020 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(3.6814027), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06355114690959454), 'actor_loss': np.float64(-0.9771249294281006), 'hyper_actor_loss': np.float64(2.5691196424304507e-05), 'behavior_loss': np.float64(0.4691373139619827)}

Episode step 22030, time diff 3.2586286067962646, total time dif 2918.97323346138)
step: 22030 @ episode report: {'average_total_reward': np.float32(10.085557), 'reward_variance': np.float32(0.88353264), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07551098242402077), 'actor_loss': np.float64(-1.0001530289649962), 'hyper_actor_loss': np.float64(2.0498293451964855e-05), 'behavior_loss': np.float64(0.4653013378381729)}

Episode step 22040, time diff 3.3281290531158447, total time dif 2922.2318620681763)
step: 22040 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(3.2616413), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0730423767119646), 'actor_loss': np.float64(-1.0087196588516236), 'hyper_actor_loss': np.float64(2.392773258179659e-05), 'behavior_loss': np.float64(0.5209540605545044)}

Episode step 22050, time diff 3.292617082595825, total time dif 2925.559991121292)
step: 22050 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(1.3289253), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0865020040422678), 'actor_loss': np.float64(-0.9955862700939179), 'hyper_actor_loss': np.float64(2.4938871683843898e-05), 'behavior_loss': np.float64(0.45090535581111907)}

Episode step 22060, time diff 3.308323383331299, total time dif 2928.852608203888)
step: 22060 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(3.3906987), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06819034069776535), 'actor_loss': np.float64(-1.0221861600875854), 'hyper_actor_loss': np.float64(2.1977865799271967e-05), 'behavior_loss': np.float64(0.4302026450634003)}

Episode step 22070, time diff 3.3131015300750732, total time dif 2932.1609315872192)
step: 22070 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(0.9838089), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05729575268924236), 'actor_loss': np.float64(-0.9863958299160004), 'hyper_actor_loss': np.float64(2.6712431099440438e-05), 'behavior_loss': np.float64(0.45571622550487517)}

Episode step 22080, time diff 3.335930109024048, total time dif 2935.4740331172943)
step: 22080 @ episode report: {'average_total_reward': np.float32(12.330001), 'reward_variance': np.float32(3.3778527), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06261631362140178), 'actor_loss': np.float64(-0.9757770597934723), 'hyper_actor_loss': np.float64(2.1898832164879424e-05), 'behavior_loss': np.float64(0.4432436913251877)}

Episode step 22090, time diff 3.434748649597168, total time dif 2938.8099632263184)
step: 22090 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(2.4116652), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07374862544238567), 'actor_loss': np.float64(-0.9974988877773285), 'hyper_actor_loss': np.float64(2.5567393640812953e-05), 'behavior_loss': np.float64(0.47643281519412994)}

Episode step 22100, time diff 3.2382357120513916, total time dif 2942.2447118759155)
step: 22100 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(4.3359833), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06500643379986286), 'actor_loss': np.float64(-1.0045280933380127), 'hyper_actor_loss': np.float64(2.25562841478677e-05), 'behavior_loss': np.float64(0.43096317946910856)}

Episode step 22110, time diff 3.2592241764068604, total time dif 2945.482947587967)
step: 22110 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.5516789), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06972377263009548), 'actor_loss': np.float64(-1.0005742967128755), 'hyper_actor_loss': np.float64(2.0397263597260462e-05), 'behavior_loss': np.float64(0.4367502361536026)}

Episode step 22120, time diff 3.251321792602539, total time dif 2948.742171764374)
step: 22120 @ episode report: {'average_total_reward': np.float32(10.958889), 'reward_variance': np.float32(1.7079767), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06906277500092983), 'actor_loss': np.float64(-0.9974755704402923), 'hyper_actor_loss': np.float64(2.2775424986321015e-05), 'behavior_loss': np.float64(0.41315747797489166)}

Episode step 22130, time diff 3.2652461528778076, total time dif 2951.9934935569763)
step: 22130 @ episode report: {'average_total_reward': np.float32(11.42), 'reward_variance': np.float32(6.1877985), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05199089720845222), 'actor_loss': np.float64(-0.9973506093025207), 'hyper_actor_loss': np.float64(3.1133671654970385e-05), 'behavior_loss': np.float64(0.40296529829502103)}

Episode step 22140, time diff 3.233567237854004, total time dif 2955.258739709854)
step: 22140 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(1.767635), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06450464371591806), 'actor_loss': np.float64(-0.9859462857246399), 'hyper_actor_loss': np.float64(2.2834023002360482e-05), 'behavior_loss': np.float64(0.40004001557826996)}

Episode step 22150, time diff 3.2841858863830566, total time dif 2958.492306947708)
step: 22150 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(2.1999307), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06459550708532333), 'actor_loss': np.float64(-1.006191349029541), 'hyper_actor_loss': np.float64(2.561651544965571e-05), 'behavior_loss': np.float64(0.433550101518631)}

Episode step 22160, time diff 3.2650370597839355, total time dif 2961.776492834091)
step: 22160 @ episode report: {'average_total_reward': np.float32(11.520001), 'reward_variance': np.float32(1.5243168), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0653823260217905), 'actor_loss': np.float64(-0.996998518705368), 'hyper_actor_loss': np.float64(2.3490602052334e-05), 'behavior_loss': np.float64(0.4154073178768158)}

Episode step 22170, time diff 3.252793073654175, total time dif 2965.041529893875)
step: 22170 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(2.5304558), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06972053796052932), 'actor_loss': np.float64(-0.9996244370937347), 'hyper_actor_loss': np.float64(2.5971357899834402e-05), 'behavior_loss': np.float64(0.4212795555591583)}

Episode step 22180, time diff 3.2480573654174805, total time dif 2968.2943229675293)
step: 22180 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(1.1014923), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06588395275175571), 'actor_loss': np.float64(-1.0097779870033263), 'hyper_actor_loss': np.float64(2.3583083384437485e-05), 'behavior_loss': np.float64(0.42753905057907104)}

Episode step 22190, time diff 3.2317452430725098, total time dif 2971.542380332947)
step: 22190 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(2.0641093), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06419566795229911), 'actor_loss': np.float64(-0.9933747231960297), 'hyper_actor_loss': np.float64(2.462433685650467e-05), 'behavior_loss': np.float64(0.38719994127750396)}

Episode step 22200, time diff 3.2424309253692627, total time dif 2974.7741255760193)
step: 22200 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(4.806613), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059230881929397586), 'actor_loss': np.float64(-0.9979347109794616), 'hyper_actor_loss': np.float64(2.3969010180735494e-05), 'behavior_loss': np.float64(0.3933206617832184)}

Episode step 22210, time diff 3.234163761138916, total time dif 2978.0165565013885)
step: 22210 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(5.816743), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06473923046141863), 'actor_loss': np.float64(-0.9863308608531952), 'hyper_actor_loss': np.float64(2.471616662660381e-05), 'behavior_loss': np.float64(0.39804268479347227)}

Episode step 22220, time diff 3.2438321113586426, total time dif 2981.2507202625275)
step: 22220 @ episode report: {'average_total_reward': np.float32(11.132223), 'reward_variance': np.float32(4.228381), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05616985615342855), 'actor_loss': np.float64(-0.9982466399669647), 'hyper_actor_loss': np.float64(2.3343886095972265e-05), 'behavior_loss': np.float64(0.45380873382091524)}

Episode step 22230, time diff 3.243530750274658, total time dif 2984.494552373886)
step: 22230 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(4.519532), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05305328257381916), 'actor_loss': np.float64(-0.9587789058685303), 'hyper_actor_loss': np.float64(2.1465110330609603e-05), 'behavior_loss': np.float64(0.4240505874156952)}

Episode step 22240, time diff 3.2343411445617676, total time dif 2987.7380831241608)
step: 22240 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(3.1229892), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06992708593606949), 'actor_loss': np.float64(-0.9832296550273896), 'hyper_actor_loss': np.float64(2.3214915563585238e-05), 'behavior_loss': np.float64(0.4463441014289856)}

Episode step 22250, time diff 3.2249295711517334, total time dif 2990.9724242687225)
step: 22250 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(3.277533), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06100375410169363), 'actor_loss': np.float64(-1.0170747995376588), 'hyper_actor_loss': np.float64(2.4640697847644333e-05), 'behavior_loss': np.float64(0.38920022249221803)}

Episode step 22260, time diff 3.4165868759155273, total time dif 2994.1973538398743)
step: 22260 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(2.2853632), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05566766876727343), 'actor_loss': np.float64(-0.9791122555732727), 'hyper_actor_loss': np.float64(2.2035063921066467e-05), 'behavior_loss': np.float64(0.4085685282945633)}

Episode step 22270, time diff 3.241476058959961, total time dif 2997.61394071579)
step: 22270 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(2.1627364), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06673864237964153), 'actor_loss': np.float64(-0.9830179214477539), 'hyper_actor_loss': np.float64(2.1376484983193223e-05), 'behavior_loss': np.float64(0.42792252004146575)}

Episode step 22280, time diff 3.2444045543670654, total time dif 3000.8554167747498)
step: 22280 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(3.968697), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0625752042979002), 'actor_loss': np.float64(-1.002954912185669), 'hyper_actor_loss': np.float64(2.4996401134558254e-05), 'behavior_loss': np.float64(0.4277165800333023)}

Episode step 22290, time diff 3.253831386566162, total time dif 3004.099821329117)
step: 22290 @ episode report: {'average_total_reward': np.float32(11.495557), 'reward_variance': np.float32(3.1495852), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07359323836863041), 'actor_loss': np.float64(-0.9999170184135437), 'hyper_actor_loss': np.float64(2.7135098844155436e-05), 'behavior_loss': np.float64(0.434796941280365)}

Episode step 22300, time diff 3.221686601638794, total time dif 3007.353652715683)
step: 22300 @ episode report: {'average_total_reward': np.float32(11.595556), 'reward_variance': np.float32(5.077634), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05389166455715895), 'actor_loss': np.float64(-0.9890379428863525), 'hyper_actor_loss': np.float64(2.623647596919909e-05), 'behavior_loss': np.float64(0.41132027804851534)}

Episode step 22310, time diff 3.1621556282043457, total time dif 3010.575339317322)
step: 22310 @ episode report: {'average_total_reward': np.float32(12.217779), 'reward_variance': np.float32(2.5253386), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07396536320447922), 'actor_loss': np.float64(-0.9809565365314483), 'hyper_actor_loss': np.float64(2.6964941025653388e-05), 'behavior_loss': np.float64(0.41794186234474184)}

Episode step 22320, time diff 3.1568429470062256, total time dif 3013.737494945526)
step: 22320 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(3.9226868), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06565860398113728), 'actor_loss': np.float64(-1.015187954902649), 'hyper_actor_loss': np.float64(1.7561162258061815e-05), 'behavior_loss': np.float64(0.40655111968517305)}

Episode step 22330, time diff 3.1784262657165527, total time dif 3016.8943378925323)
step: 22330 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(5.034753), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053889207541942596), 'actor_loss': np.float64(-0.9821131885051727), 'hyper_actor_loss': np.float64(2.2903934677742654e-05), 'behavior_loss': np.float64(0.41043314933776853)}

Episode step 22340, time diff 3.1825106143951416, total time dif 3020.072764158249)
step: 22340 @ episode report: {'average_total_reward': np.float32(9.114446), 'reward_variance': np.float32(4.1967177), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06539335623383521), 'actor_loss': np.float64(-0.9778016805648804), 'hyper_actor_loss': np.float64(2.25461363697832e-05), 'behavior_loss': np.float64(0.4094068318605423)}

Episode step 22350, time diff 3.228210926055908, total time dif 3023.255274772644)
step: 22350 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(7.0391564), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06282498203217983), 'actor_loss': np.float64(-1.0039213001728058), 'hyper_actor_loss': np.float64(1.9126002553093712e-05), 'behavior_loss': np.float64(0.3951249659061432)}

Episode step 22360, time diff 3.205021858215332, total time dif 3026.4834856987)
step: 22360 @ episode report: {'average_total_reward': np.float32(11.2322235), 'reward_variance': np.float32(2.9192953), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.77778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053316831961274146), 'actor_loss': np.float64(-0.9888297557830811), 'hyper_actor_loss': np.float64(1.906150391732808e-05), 'behavior_loss': np.float64(0.3934851050376892)}

Episode step 22370, time diff 3.2049880027770996, total time dif 3029.6885075569153)
step: 22370 @ episode report: {'average_total_reward': np.float32(10.346667), 'reward_variance': np.float32(2.4860694), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060787492990493776), 'actor_loss': np.float64(-0.9795364499092102), 'hyper_actor_loss': np.float64(2.0507705903582975e-05), 'behavior_loss': np.float64(0.4192885190248489)}

Episode step 22380, time diff 3.1664440631866455, total time dif 3032.8934955596924)
step: 22380 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(3.430099), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06540727652609349), 'actor_loss': np.float64(-0.9918200612068176), 'hyper_actor_loss': np.float64(1.824063174353796e-05), 'behavior_loss': np.float64(0.40807066261768343)}

Episode step 22390, time diff 3.1761324405670166, total time dif 3036.059939622879)
step: 22390 @ episode report: {'average_total_reward': np.float32(11.0222225), 'reward_variance': np.float32(3.3412101), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06317312680184842), 'actor_loss': np.float64(-0.9974359810352326), 'hyper_actor_loss': np.float64(2.11488127206394e-05), 'behavior_loss': np.float64(0.3811381787061691)}

Episode step 22400, time diff 3.1981942653656006, total time dif 3039.236072063446)
step: 22400 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.5758338), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062174181640148166), 'actor_loss': np.float64(-0.9975236177444458), 'hyper_actor_loss': np.float64(2.0861685879935975e-05), 'behavior_loss': np.float64(0.40073588490486145)}

Episode step 22410, time diff 3.1514949798583984, total time dif 3042.4342663288116)
step: 22410 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(2.543724), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05411876328289509), 'actor_loss': np.float64(-0.9839512228965759), 'hyper_actor_loss': np.float64(2.5961776918848047e-05), 'behavior_loss': np.float64(0.40514886677265166)}

Episode step 22420, time diff 3.0829567909240723, total time dif 3045.58576130867)
step: 22420 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(1.4607608), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06382385920733213), 'actor_loss': np.float64(-0.9891095399856568), 'hyper_actor_loss': np.float64(2.0210515504004434e-05), 'behavior_loss': np.float64(0.36237773001194)}

Episode step 22430, time diff 3.270200729370117, total time dif 3048.668718099594)
step: 22430 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(4.472644), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06421689912676812), 'actor_loss': np.float64(-0.9968599617481232), 'hyper_actor_loss': np.float64(1.880092950159451e-05), 'behavior_loss': np.float64(0.3872550755739212)}

Episode step 22440, time diff 3.101206064224243, total time dif 3051.9389188289642)
step: 22440 @ episode report: {'average_total_reward': np.float32(11.495557), 'reward_variance': np.float32(3.454326), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05189180001616478), 'actor_loss': np.float64(-0.9918594837188721), 'hyper_actor_loss': np.float64(1.7003434550133534e-05), 'behavior_loss': np.float64(0.36106509864330294)}

Episode step 22450, time diff 3.0944957733154297, total time dif 3055.0401248931885)
step: 22450 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(2.3768213), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061799146421253684), 'actor_loss': np.float64(-0.991126811504364), 'hyper_actor_loss': np.float64(1.838426833273843e-05), 'behavior_loss': np.float64(0.39535232484340666)}

Episode step 22460, time diff 3.164996862411499, total time dif 3058.134620666504)
step: 22460 @ episode report: {'average_total_reward': np.float32(11.207778), 'reward_variance': np.float32(2.7015328), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05946738738566637), 'actor_loss': np.float64(-0.9972705483436585), 'hyper_actor_loss': np.float64(2.0381788908707675e-05), 'behavior_loss': np.float64(0.3534589111804962)}

Episode step 22470, time diff 3.138033390045166, total time dif 3061.2996175289154)
step: 22470 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(1.8147175), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05977918058633804), 'actor_loss': np.float64(-0.9910589456558228), 'hyper_actor_loss': np.float64(1.578650708324858e-05), 'behavior_loss': np.float64(0.38687032759189605)}

Episode step 22480, time diff 3.0858445167541504, total time dif 3064.4376509189606)
step: 22480 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.761385), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0803426917642355), 'actor_loss': np.float64(-1.0151311874389648), 'hyper_actor_loss': np.float64(1.7971036959352206e-05), 'behavior_loss': np.float64(0.37194169461727145)}

Episode step 22490, time diff 3.0958566665649414, total time dif 3067.5234954357147)
step: 22490 @ episode report: {'average_total_reward': np.float32(11.66889), 'reward_variance': np.float32(2.5741434), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05371978990733624), 'actor_loss': np.float64(-1.013629561662674), 'hyper_actor_loss': np.float64(1.7014439436024986e-05), 'behavior_loss': np.float64(0.3742528915405273)}

Episode step 22500, time diff 3.1277432441711426, total time dif 3070.6193521022797)
step: 22500 @ episode report: {'average_total_reward': np.float32(11.12), 'reward_variance': np.float32(4.2842913), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058815534599125384), 'actor_loss': np.float64(-0.973949533700943), 'hyper_actor_loss': np.float64(1.5217285726976115e-05), 'behavior_loss': np.float64(0.3546250879764557)}

Episode step 22510, time diff 3.1626665592193604, total time dif 3073.747095346451)
step: 22510 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.336431), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06214086683467031), 'actor_loss': np.float64(-0.9968266963958741), 'hyper_actor_loss': np.float64(1.8566188100521685e-05), 'behavior_loss': np.float64(0.39298535585403443)}

Episode step 22520, time diff 3.1140925884246826, total time dif 3076.90976190567)
step: 22520 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(1.5876547), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06891480125486851), 'actor_loss': np.float64(-0.9973270833492279), 'hyper_actor_loss': np.float64(1.717842424113769e-05), 'behavior_loss': np.float64(0.3972011566162109)}

Episode step 22530, time diff 3.1506540775299072, total time dif 3080.023854494095)
step: 22530 @ episode report: {'average_total_reward': np.float32(11.332224), 'reward_variance': np.float32(5.537938), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05861619766801596), 'actor_loss': np.float64(-0.9941792845726013), 'hyper_actor_loss': np.float64(1.8606347020977408e-05), 'behavior_loss': np.float64(0.3519893378019333)}

Episode step 22540, time diff 3.1288959980010986, total time dif 3083.1745085716248)
step: 22540 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(2.256006), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05741391461342573), 'actor_loss': np.float64(-0.9911227107048035), 'hyper_actor_loss': np.float64(1.5708941373304696e-05), 'behavior_loss': np.float64(0.38375707864761355)}

Episode step 22550, time diff 3.157971143722534, total time dif 3086.303404569626)
step: 22550 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(4.589536), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06326700448989868), 'actor_loss': np.float64(-0.9906124770641327), 'hyper_actor_loss': np.float64(1.7322940402664243e-05), 'behavior_loss': np.float64(0.389995139837265)}

Episode step 22560, time diff 3.105351686477661, total time dif 3089.4613757133484)
step: 22560 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(2.920752), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06430320665240288), 'actor_loss': np.float64(-0.9901934683322906), 'hyper_actor_loss': np.float64(1.7710342399368527e-05), 'behavior_loss': np.float64(0.40252100825309756)}

Episode step 22570, time diff 3.1291661262512207, total time dif 3092.566727399826)
step: 22570 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.110869), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06373149156570435), 'actor_loss': np.float64(-1.0004726946353912), 'hyper_actor_loss': np.float64(1.624942369744531e-05), 'behavior_loss': np.float64(0.372697651386261)}

Episode step 22580, time diff 3.0993001461029053, total time dif 3095.6958935260773)
step: 22580 @ episode report: {'average_total_reward': np.float32(10.646667), 'reward_variance': np.float32(1.0724396), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06672685854136944), 'actor_loss': np.float64(-0.9999197900295258), 'hyper_actor_loss': np.float64(1.542071368021425e-05), 'behavior_loss': np.float64(0.3974727302789688)}

Episode step 22590, time diff 3.293863296508789, total time dif 3098.79519367218)
step: 22590 @ episode report: {'average_total_reward': np.float32(11.42), 'reward_variance': np.float32(2.102908), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06256122440099716), 'actor_loss': np.float64(-1.0016869187355042), 'hyper_actor_loss': np.float64(1.748706436046632e-05), 'behavior_loss': np.float64(0.3574718564748764)}

Episode step 22600, time diff 3.1401469707489014, total time dif 3102.089056968689)
step: 22600 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(1.5203712), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06645838208496571), 'actor_loss': np.float64(-0.9993801832199096), 'hyper_actor_loss': np.float64(1.8628531142894646e-05), 'behavior_loss': np.float64(0.39412157237529755)}

Episode step 22610, time diff 3.086160659790039, total time dif 3105.229203939438)
step: 22610 @ episode report: {'average_total_reward': np.float32(9.451112), 'reward_variance': np.float32(1.6314373), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06242223642766476), 'actor_loss': np.float64(-0.9869353890419006), 'hyper_actor_loss': np.float64(1.4602599731006193e-05), 'behavior_loss': np.float64(0.362169885635376)}

Episode step 22620, time diff 3.193014144897461, total time dif 3108.315364599228)
step: 22620 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(2.2405677), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05418010391294956), 'actor_loss': np.float64(-0.9946443617343903), 'hyper_actor_loss': np.float64(1.5931865436868976e-05), 'behavior_loss': np.float64(0.36218803375959396)}

Episode step 22630, time diff 3.066025733947754, total time dif 3111.5083787441254)
step: 22630 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.9906125), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186594493687153), 'actor_loss': np.float64(-0.9913514375686645), 'hyper_actor_loss': np.float64(2.274318130730535e-05), 'behavior_loss': np.float64(0.37711222767829894)}

Episode step 22640, time diff 3.117020606994629, total time dif 3114.574404478073)
step: 22640 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(3.0159264), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0531254593282938), 'actor_loss': np.float64(-1.0011217296123505), 'hyper_actor_loss': np.float64(1.718823177725426e-05), 'behavior_loss': np.float64(0.3950486809015274)}

Episode step 22650, time diff 3.0537216663360596, total time dif 3117.6914250850677)
step: 22650 @ episode report: {'average_total_reward': np.float32(10.783335), 'reward_variance': np.float32(1.6224998), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06202226486057043), 'actor_loss': np.float64(-0.979322099685669), 'hyper_actor_loss': np.float64(1.3198712258599698e-05), 'behavior_loss': np.float64(0.37494719922542574)}

Episode step 22660, time diff 3.10094952583313, total time dif 3120.745146751404)
step: 22660 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.4176257), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05679631978273392), 'actor_loss': np.float64(-0.9877362012863159), 'hyper_actor_loss': np.float64(1.1494381578813772e-05), 'behavior_loss': np.float64(0.37459608912467957)}

Episode step 22670, time diff 3.1310129165649414, total time dif 3123.846096277237)
step: 22670 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(2.1696413), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07312306929379701), 'actor_loss': np.float64(-0.9962120413780212), 'hyper_actor_loss': np.float64(1.677212030699593e-05), 'behavior_loss': np.float64(0.3677399069070816)}

Episode step 22680, time diff 3.110983371734619, total time dif 3126.977109193802)
step: 22680 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(1.4756854), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05948335397988558), 'actor_loss': np.float64(-1.0016405940055848), 'hyper_actor_loss': np.float64(1.4768258097319631e-05), 'behavior_loss': np.float64(0.3728273957967758)}

Episode step 22690, time diff 3.069594383239746, total time dif 3130.0880925655365)
step: 22690 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(3.6492107), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06527311224490404), 'actor_loss': np.float64(-1.0038513362407684), 'hyper_actor_loss': np.float64(1.6904141011764297e-05), 'behavior_loss': np.float64(0.3471724480390549)}

Episode step 22700, time diff 3.091815233230591, total time dif 3133.1576869487762)
step: 22700 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.5529206), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06466117203235626), 'actor_loss': np.float64(-1.0123018860816955), 'hyper_actor_loss': np.float64(1.558517860758002e-05), 'behavior_loss': np.float64(0.3629178315401077)}

Episode step 22710, time diff 3.1042609214782715, total time dif 3136.249502182007)
step: 22710 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(2.64097), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06315138153731822), 'actor_loss': np.float64(-0.9858001351356507), 'hyper_actor_loss': np.float64(1.5054991763463477e-05), 'behavior_loss': np.float64(0.3765826165676117)}

Episode step 22720, time diff 3.103750467300415, total time dif 3139.353763103485)
step: 22720 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.9826715), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057401977851986884), 'actor_loss': np.float64(-0.9872181057929993), 'hyper_actor_loss': np.float64(1.3361518722376785e-05), 'behavior_loss': np.float64(0.33015764951705934)}

Episode step 22730, time diff 3.0990347862243652, total time dif 3142.4575135707855)
step: 22730 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.9134023), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0670442271977663), 'actor_loss': np.float64(-0.9949123919010162), 'hyper_actor_loss': np.float64(1.6793805843917654e-05), 'behavior_loss': np.float64(0.3780600011348724)}

Episode step 22740, time diff 3.125248432159424, total time dif 3145.55654835701)
step: 22740 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(1.4655077), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06175665371119976), 'actor_loss': np.float64(-1.0024994134902954), 'hyper_actor_loss': np.float64(1.4915710107743507e-05), 'behavior_loss': np.float64(0.37838122248649597)}

Episode step 22750, time diff 3.2848827838897705, total time dif 3148.6817967891693)
step: 22750 @ episode report: {'average_total_reward': np.float32(8.875557), 'reward_variance': np.float32(3.0797725), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07725012823939323), 'actor_loss': np.float64(-1.004219365119934), 'hyper_actor_loss': np.float64(1.870478427008493e-05), 'behavior_loss': np.float64(0.37489908635616304)}

Episode step 22760, time diff 3.0927932262420654, total time dif 3151.966679573059)
step: 22760 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(2.48045), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07135706283152103), 'actor_loss': np.float64(-1.016523015499115), 'hyper_actor_loss': np.float64(1.7521776680951006e-05), 'behavior_loss': np.float64(0.37787611186504366)}

Episode step 22770, time diff 3.108349323272705, total time dif 3155.059472799301)
step: 22770 @ episode report: {'average_total_reward': np.float32(8.963335), 'reward_variance': np.float32(2.6192362), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05479276590049267), 'actor_loss': np.float64(-0.9891229689121246), 'hyper_actor_loss': np.float64(1.5418520797538803e-05), 'behavior_loss': np.float64(0.4042215049266815)}

Episode step 22780, time diff 3.067446231842041, total time dif 3158.167822122574)
step: 22780 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(2.4944706), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06243033707141876), 'actor_loss': np.float64(-0.969398307800293), 'hyper_actor_loss': np.float64(1.2362447887426242e-05), 'behavior_loss': np.float64(0.3709929585456848)}

Episode step 22790, time diff 3.1021358966827393, total time dif 3161.235268354416)
step: 22790 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.331389), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0629411932080984), 'actor_loss': np.float64(-0.9943388104438782), 'hyper_actor_loss': np.float64(1.0736535750766052e-05), 'behavior_loss': np.float64(0.3502588778734207)}

Episode step 22800, time diff 3.0857396125793457, total time dif 3164.3374042510986)
step: 22800 @ episode report: {'average_total_reward': np.float32(9.236667), 'reward_variance': np.float32(3.4321256), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06133156660944224), 'actor_loss': np.float64(-1.0086115956306458), 'hyper_actor_loss': np.float64(1.2700712932200987e-05), 'behavior_loss': np.float64(0.35840758085250857)}

Episode step 22810, time diff 3.1009762287139893, total time dif 3167.423143863678)
step: 22810 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.238075), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06365665756165981), 'actor_loss': np.float64(-0.9839000582695008), 'hyper_actor_loss': np.float64(1.1210379943804582e-05), 'behavior_loss': np.float64(0.36651372015476225)}

Episode step 22820, time diff 3.0775442123413086, total time dif 3170.524120092392)
step: 22820 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(1.4740541), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055757921934127805), 'actor_loss': np.float64(-0.9879628002643586), 'hyper_actor_loss': np.float64(1.299336836382281e-05), 'behavior_loss': np.float64(0.3635092228651047)}

Episode step 22830, time diff 3.1059257984161377, total time dif 3173.6016643047333)
step: 22830 @ episode report: {'average_total_reward': np.float32(9.163333), 'reward_variance': np.float32(2.5109892), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06330520026385784), 'actor_loss': np.float64(-0.9856386005878448), 'hyper_actor_loss': np.float64(1.1952011436733301e-05), 'behavior_loss': np.float64(0.3647745668888092)}

Episode step 22840, time diff 3.0961177349090576, total time dif 3176.7075901031494)
step: 22840 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(3.366178), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.048620222695171834), 'actor_loss': np.float64(-0.9919827997684478), 'hyper_actor_loss': np.float64(1.0139441428691498e-05), 'behavior_loss': np.float64(0.3684812217950821)}

Episode step 22850, time diff 3.121068000793457, total time dif 3179.8037078380585)
step: 22850 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(3.9863725), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060004905611276624), 'actor_loss': np.float64(-0.9766907036304474), 'hyper_actor_loss': np.float64(1.3442928138829303e-05), 'behavior_loss': np.float64(0.33609871864318847)}

Episode step 22860, time diff 3.1613359451293945, total time dif 3182.924775838852)
step: 22860 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(1.5134925), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05263170786201954), 'actor_loss': np.float64(-0.9911531805992126), 'hyper_actor_loss': np.float64(1.4625669700762956e-05), 'behavior_loss': np.float64(0.36591036021709444)}

Episode step 22870, time diff 3.1432623863220215, total time dif 3186.0861117839813)
step: 22870 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(2.1062727), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05903891921043396), 'actor_loss': np.float64(-0.9798573732376099), 'hyper_actor_loss': np.float64(1.2965700170752825e-05), 'behavior_loss': np.float64(0.35090013444423673)}

Episode step 22880, time diff 3.145066738128662, total time dif 3189.2293741703033)
step: 22880 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(1.6384557), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06263852156698704), 'actor_loss': np.float64(-1.0053943872451783), 'hyper_actor_loss': np.float64(1.3758222212345573e-05), 'behavior_loss': np.float64(0.34576368033885957)}

Episode step 22890, time diff 3.1631789207458496, total time dif 3192.374440908432)
step: 22890 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(2.1199622), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07494840957224369), 'actor_loss': np.float64(-1.0033005475997925), 'hyper_actor_loss': np.float64(1.4638221091445303e-05), 'behavior_loss': np.float64(0.36602228581905366)}

Episode step 22900, time diff 3.122650623321533, total time dif 3195.537619829178)
step: 22900 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(1.4456314), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07062115669250488), 'actor_loss': np.float64(-1.011026632785797), 'hyper_actor_loss': np.float64(1.3117958496877691e-05), 'behavior_loss': np.float64(0.3781063765287399)}

Episode step 22910, time diff 3.257460594177246, total time dif 3198.6602704524994)
step: 22910 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(0.79611844), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06306040138006211), 'actor_loss': np.float64(-0.985663378238678), 'hyper_actor_loss': np.float64(1.2703857919404982e-05), 'behavior_loss': np.float64(0.36688523590564726)}

Episode step 22920, time diff 3.1466565132141113, total time dif 3201.9177310466766)
step: 22920 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.1986731), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0595749769359827), 'actor_loss': np.float64(-0.9905005693435669), 'hyper_actor_loss': np.float64(1.0873819928747252e-05), 'behavior_loss': np.float64(0.34468421936035154)}

Episode step 22930, time diff 3.125621795654297, total time dif 3205.0643875598907)
step: 22930 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(5.468465), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07219177484512329), 'actor_loss': np.float64(-0.995905476808548), 'hyper_actor_loss': np.float64(1.4792348338232841e-05), 'behavior_loss': np.float64(0.3721808761358261)}

Episode step 22940, time diff 3.168278694152832, total time dif 3208.190009355545)
step: 22940 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(2.2035432), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05363156124949455), 'actor_loss': np.float64(-0.9970685601234436), 'hyper_actor_loss': np.float64(1.1312184642520152e-05), 'behavior_loss': np.float64(0.367789489030838)}

Episode step 22950, time diff 3.1438496112823486, total time dif 3211.358288049698)
step: 22950 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(2.996795), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.052871670294553044), 'actor_loss': np.float64(-0.974762785434723), 'hyper_actor_loss': np.float64(1.2198791591799817e-05), 'behavior_loss': np.float64(0.33344775438308716)}

Episode step 22960, time diff 3.121654987335205, total time dif 3214.50213766098)
step: 22960 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(3.1923075), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056434862315654755), 'actor_loss': np.float64(-0.9902910709381103), 'hyper_actor_loss': np.float64(1.0324053664589884e-05), 'behavior_loss': np.float64(0.34980236291885375)}

Episode step 22970, time diff 3.105097770690918, total time dif 3217.6237926483154)
step: 22970 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.3288755), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06509334538131953), 'actor_loss': np.float64(-0.9955148518085479), 'hyper_actor_loss': np.float64(1.0724239973569639e-05), 'behavior_loss': np.float64(0.34344206303358077)}

Episode step 22980, time diff 3.1262311935424805, total time dif 3220.7288904190063)
step: 22980 @ episode report: {'average_total_reward': np.float32(9.975555), 'reward_variance': np.float32(3.3317478), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060318362712860105), 'actor_loss': np.float64(-1.001593142747879), 'hyper_actor_loss': np.float64(8.883956479621701e-06), 'behavior_loss': np.float64(0.3703017354011536)}

Episode step 22990, time diff 3.146566390991211, total time dif 3223.855121612549)
step: 22990 @ episode report: {'average_total_reward': np.float32(9.624446), 'reward_variance': np.float32(1.8295748), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05674099028110504), 'actor_loss': np.float64(-0.9807139813899994), 'hyper_actor_loss': np.float64(1.1505265001687804e-05), 'behavior_loss': np.float64(0.35934283435344694)}

Episode step 23000, time diff 3.1485793590545654, total time dif 3227.00168800354)
step: 23000 @ episode report: {'average_total_reward': np.float32(9.636668), 'reward_variance': np.float32(1.8561251), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05542959813028574), 'actor_loss': np.float64(-0.9811439990997315), 'hyper_actor_loss': np.float64(1.0307483626093017e-05), 'behavior_loss': np.float64(0.3450892508029938)}

Episode step 23010, time diff 3.1780409812927246, total time dif 3230.1502673625946)
step: 23010 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(0.83464307), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06804193258285522), 'actor_loss': np.float64(-0.986405223608017), 'hyper_actor_loss': np.float64(1.0645425572874955e-05), 'behavior_loss': np.float64(0.3831626892089844)}

Episode step 23020, time diff 3.14644193649292, total time dif 3233.3283083438873)
step: 23020 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(1.3861988), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07280625849962234), 'actor_loss': np.float64(-1.006531047821045), 'hyper_actor_loss': np.float64(1.22373414797039e-05), 'behavior_loss': np.float64(0.3740155726671219)}

Episode step 23030, time diff 3.1300034523010254, total time dif 3236.4747502803802)
step: 23030 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(3.1484451), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06143130399286747), 'actor_loss': np.float64(-0.9889808177947998), 'hyper_actor_loss': np.float64(1.2813862304028589e-05), 'behavior_loss': np.float64(0.3740816354751587)}

Episode step 23040, time diff 3.190941095352173, total time dif 3239.6047537326813)
step: 23040 @ episode report: {'average_total_reward': np.float32(9.414446), 'reward_variance': np.float32(3.465556), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07209069393575192), 'actor_loss': np.float64(-0.9879279375076294), 'hyper_actor_loss': np.float64(1.4661050863651326e-05), 'behavior_loss': np.float64(0.3655510425567627)}

Episode step 23050, time diff 3.1344826221466064, total time dif 3242.7956948280334)
step: 23050 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(2.1213636), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07129946798086166), 'actor_loss': np.float64(-1.0076945066452025), 'hyper_actor_loss': np.float64(1.3694229983229888e-05), 'behavior_loss': np.float64(0.34706578552722933)}

Episode step 23060, time diff 3.1842164993286133, total time dif 3245.93017745018)
step: 23060 @ episode report: {'average_total_reward': np.float32(9.612224), 'reward_variance': np.float32(2.68058), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0662333458662033), 'actor_loss': np.float64(-1.0068564057350158), 'hyper_actor_loss': np.float64(1.2395308840496e-05), 'behavior_loss': np.float64(0.35101467967033384)}

Episode step 23070, time diff 3.1844053268432617, total time dif 3249.1143939495087)
step: 23070 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(1.6743761), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08446327671408653), 'actor_loss': np.float64(-1.0057987809181212), 'hyper_actor_loss': np.float64(1.3396604890658636e-05), 'behavior_loss': np.float64(0.3670205116271973)}

Episode step 23080, time diff 3.302325963973999, total time dif 3252.298799276352)
step: 23080 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(1.168307), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06186516582965851), 'actor_loss': np.float64(-1.0006898581981658), 'hyper_actor_loss': np.float64(1.2504106553024031e-05), 'behavior_loss': np.float64(0.35274776816368103)}

Episode step 23090, time diff 3.1809468269348145, total time dif 3255.601125240326)
step: 23090 @ episode report: {'average_total_reward': np.float32(9.200001), 'reward_variance': np.float32(1.047926), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07456230521202087), 'actor_loss': np.float64(-0.9903841972351074), 'hyper_actor_loss': np.float64(1.2026405511278426e-05), 'behavior_loss': np.float64(0.350556218624115)}

Episode step 23100, time diff 3.209602117538452, total time dif 3258.7820720672607)
step: 23100 @ episode report: {'average_total_reward': np.float32(9.785556), 'reward_variance': np.float32(2.1849394), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07467282712459564), 'actor_loss': np.float64(-1.0048328995704652), 'hyper_actor_loss': np.float64(1.4376352100953228e-05), 'behavior_loss': np.float64(0.3365155622363091)}

Episode step 23110, time diff 3.192046642303467, total time dif 3261.991674184799)
step: 23110 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(5.0889826), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06285112202167512), 'actor_loss': np.float64(-1.019536316394806), 'hyper_actor_loss': np.float64(1.3866363678971538e-05), 'behavior_loss': np.float64(0.34980733692646027)}

Episode step 23120, time diff 3.2624568939208984, total time dif 3265.1837208271027)
step: 23120 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.3556743), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0684419184923172), 'actor_loss': np.float64(-0.9813684463500977), 'hyper_actor_loss': np.float64(1.6186696120712442e-05), 'behavior_loss': np.float64(0.36759836673736573)}

Episode step 23130, time diff 3.22625470161438, total time dif 3268.4461777210236)
step: 23130 @ episode report: {'average_total_reward': np.float32(8.914445), 'reward_variance': np.float32(3.418052), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05722322668880224), 'actor_loss': np.float64(-0.9820247650146484), 'hyper_actor_loss': np.float64(1.6154442892002408e-05), 'behavior_loss': np.float64(0.3800179988145828)}

Episode step 23140, time diff 3.2068095207214355, total time dif 3271.672432422638)
step: 23140 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(3.1030245), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0763554759323597), 'actor_loss': np.float64(-0.9930676579475403), 'hyper_actor_loss': np.float64(1.7329404818156037e-05), 'behavior_loss': np.float64(0.3587428033351898)}

Episode step 23150, time diff 3.185986042022705, total time dif 3274.8792419433594)
step: 23150 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(2.4399705), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060119161009788515), 'actor_loss': np.float64(-0.9965693593025208), 'hyper_actor_loss': np.float64(1.552334815642098e-05), 'behavior_loss': np.float64(0.37961632311344146)}

Episode step 23160, time diff 3.174435615539551, total time dif 3278.065227985382)
step: 23160 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.3543777), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07931476570665837), 'actor_loss': np.float64(-0.9893164694309234), 'hyper_actor_loss': np.float64(1.5753999832668342e-05), 'behavior_loss': np.float64(0.36107237040996554)}

Episode step 23170, time diff 3.1920783519744873, total time dif 3281.2396636009216)
step: 23170 @ episode report: {'average_total_reward': np.float32(8.887778), 'reward_variance': np.float32(2.9964306), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06456016823649406), 'actor_loss': np.float64(-1.0050086557865143), 'hyper_actor_loss': np.float64(1.575411170051666e-05), 'behavior_loss': np.float64(0.3701535612344742)}

Episode step 23180, time diff 3.2448058128356934, total time dif 3284.431741952896)
step: 23180 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.3208904), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05904543474316597), 'actor_loss': np.float64(-0.9809284746646881), 'hyper_actor_loss': np.float64(1.6203781888179948e-05), 'behavior_loss': np.float64(0.3556175231933594)}

Episode step 23190, time diff 3.310999631881714, total time dif 3287.676547765732)
step: 23190 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(2.0639765), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06876236870884896), 'actor_loss': np.float64(-0.9833463430404663), 'hyper_actor_loss': np.float64(1.8299731618753867e-05), 'behavior_loss': np.float64(0.37483649849891665)}

Episode step 23200, time diff 3.298250675201416, total time dif 3290.9875473976135)
step: 23200 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(3.3777237), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06853283680975437), 'actor_loss': np.float64(-0.9925942659378052), 'hyper_actor_loss': np.float64(1.8990790158568417e-05), 'behavior_loss': np.float64(0.3531552359461784)}

Episode step 23210, time diff 3.2839136123657227, total time dif 3294.285798072815)
step: 23210 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(0.8876358), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04947341345250607), 'actor_loss': np.float64(-0.9969319462776184), 'hyper_actor_loss': np.float64(2.3855680956330617e-05), 'behavior_loss': np.float64(0.35261318683624265)}

Episode step 23220, time diff 3.4234089851379395, total time dif 3297.5697116851807)
step: 23220 @ episode report: {'average_total_reward': np.float32(9.424446), 'reward_variance': np.float32(2.9053292), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.084012121707201), 'actor_loss': np.float64(-0.9818983793258667), 'hyper_actor_loss': np.float64(2.109152774210088e-05), 'behavior_loss': np.float64(0.35721873342990873)}

Episode step 23230, time diff 3.226036548614502, total time dif 3300.9931206703186)
step: 23230 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(2.7100062), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06635934244841338), 'actor_loss': np.float64(-1.008277416229248), 'hyper_actor_loss': np.float64(2.1350040333345534e-05), 'behavior_loss': np.float64(0.35980532467365267)}

Episode step 23240, time diff 3.188445568084717, total time dif 3304.219157218933)
step: 23240 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(2.3708415), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060544074326753614), 'actor_loss': np.float64(-0.9987591147422791), 'hyper_actor_loss': np.float64(1.8927302971860626e-05), 'behavior_loss': np.float64(0.33014768064022065)}

Episode step 23250, time diff 3.382025718688965, total time dif 3307.407602787018)
step: 23250 @ episode report: {'average_total_reward': np.float32(10.573334), 'reward_variance': np.float32(0.7296837), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06294676791876555), 'actor_loss': np.float64(-0.9963906466960907), 'hyper_actor_loss': np.float64(2.1519814254133962e-05), 'behavior_loss': np.float64(0.33914634585380554)}

Episode step 23260, time diff 3.158867597579956, total time dif 3310.789628505707)
step: 23260 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.0429585), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.051644795946776866), 'actor_loss': np.float64(-0.9722386837005615), 'hyper_actor_loss': np.float64(1.8614881355460966e-05), 'behavior_loss': np.float64(0.36166560649871826)}

Episode step 23270, time diff 3.1447861194610596, total time dif 3313.9484961032867)
step: 23270 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(1.5324208), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07450280971825123), 'actor_loss': np.float64(-0.9834397435188293), 'hyper_actor_loss': np.float64(1.971399542526342e-05), 'behavior_loss': np.float64(0.36361085772514345)}

Episode step 23280, time diff 3.308316946029663, total time dif 3317.093282222748)
step: 23280 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.3622427), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05976696778088808), 'actor_loss': np.float64(-1.0001483857631683), 'hyper_actor_loss': np.float64(3.0125312878226396e-05), 'behavior_loss': np.float64(0.3563130617141724)}

Episode step 23290, time diff 3.2125139236450195, total time dif 3320.4015991687775)
step: 23290 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(2.4353633), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05552146956324577), 'actor_loss': np.float64(-0.9771024823188782), 'hyper_actor_loss': np.float64(2.428389616397908e-05), 'behavior_loss': np.float64(0.3484018862247467)}

Episode step 23300, time diff 3.1718389987945557, total time dif 3323.6141130924225)
step: 23300 @ episode report: {'average_total_reward': np.float32(8.490001), 'reward_variance': np.float32(1.7448757), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0761770598590374), 'actor_loss': np.float64(-0.9874135792255402), 'hyper_actor_loss': np.float64(2.01825653675769e-05), 'behavior_loss': np.float64(0.34784320592880247)}

Episode step 23310, time diff 3.1522581577301025, total time dif 3326.785952091217)
step: 23310 @ episode report: {'average_total_reward': np.float32(9.151111), 'reward_variance': np.float32(1.535709), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07218494825065136), 'actor_loss': np.float64(-1.025837242603302), 'hyper_actor_loss': np.float64(1.7549101630720543e-05), 'behavior_loss': np.float64(0.3315798819065094)}

Episode step 23320, time diff 3.1432881355285645, total time dif 3329.938210248947)
step: 23320 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(1.2630819), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06505012027919292), 'actor_loss': np.float64(-1.006294697523117), 'hyper_actor_loss': np.float64(1.876820206234697e-05), 'behavior_loss': np.float64(0.34877489805221557)}

Episode step 23330, time diff 3.1347391605377197, total time dif 3333.0814983844757)
step: 23330 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.532376), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0652617171406746), 'actor_loss': np.float64(-0.9715457320213318), 'hyper_actor_loss': np.float64(1.559381671540905e-05), 'behavior_loss': np.float64(0.3467005714774132)}

Episode step 23340, time diff 3.1609299182891846, total time dif 3336.2162375450134)
step: 23340 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.3876548), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06867174059152603), 'actor_loss': np.float64(-0.9948058366775513), 'hyper_actor_loss': np.float64(1.6014801803976298e-05), 'behavior_loss': np.float64(0.34303652942180635)}

Episode step 23350, time diff 3.1732780933380127, total time dif 3339.3771674633026)
step: 23350 @ episode report: {'average_total_reward': np.float32(9.351111), 'reward_variance': np.float32(2.827215), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06462749987840652), 'actor_loss': np.float64(-1.0067990124225616), 'hyper_actor_loss': np.float64(1.720447944535408e-05), 'behavior_loss': np.float64(0.3415695607662201)}

Episode step 23360, time diff 3.1946167945861816, total time dif 3342.5504455566406)
step: 23360 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.5266223), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06352805737406016), 'actor_loss': np.float64(-0.9840279459953308), 'hyper_actor_loss': np.float64(1.753680871843244e-05), 'behavior_loss': np.float64(0.3466549664735794)}

Episode step 23370, time diff 3.1871132850646973, total time dif 3345.745062351227)
step: 23370 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(0.76122993), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060283970460295676), 'actor_loss': np.float64(-0.9806262731552124), 'hyper_actor_loss': np.float64(1.596429929122678e-05), 'behavior_loss': np.float64(0.35278962552547455)}

Episode step 23380, time diff 3.209953546524048, total time dif 3348.9321756362915)
step: 23380 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(1.3023708), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06496974565088749), 'actor_loss': np.float64(-0.9898503541946411), 'hyper_actor_loss': np.float64(1.64105186740926e-05), 'behavior_loss': np.float64(0.35558366775512695)}

Episode step 23390, time diff 3.2091944217681885, total time dif 3352.1421291828156)
step: 23390 @ episode report: {'average_total_reward': np.float32(10.24889), 'reward_variance': np.float32(0.80072105), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06216245740652084), 'actor_loss': np.float64(-0.9837540864944458), 'hyper_actor_loss': np.float64(1.569827736602747e-05), 'behavior_loss': np.float64(0.3563857555389404)}

Episode step 23400, time diff 3.1936991214752197, total time dif 3355.3513236045837)
step: 23400 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(4.7210245), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06176873538643122), 'actor_loss': np.float64(-0.9770849704742431), 'hyper_actor_loss': np.float64(1.7096774718083908e-05), 'behavior_loss': np.float64(0.34218534380197524)}

Episode step 23410, time diff 3.3968770503997803, total time dif 3358.545022726059)
step: 23410 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(3.4373832), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05714957658201456), 'actor_loss': np.float64(-0.9817976713180542), 'hyper_actor_loss': np.float64(1.7699650288705016e-05), 'behavior_loss': np.float64(0.3242032825946808)}

Episode step 23420, time diff 3.216982364654541, total time dif 3361.9418997764587)
step: 23420 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(4.151063), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05992171131074429), 'actor_loss': np.float64(-1.0064688324928284), 'hyper_actor_loss': np.float64(1.5867653110035464e-05), 'behavior_loss': np.float64(0.3321406960487366)}

Episode step 23430, time diff 3.2042641639709473, total time dif 3365.1588821411133)
step: 23430 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(0.771715), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06332479901611805), 'actor_loss': np.float64(-0.9820580601692199), 'hyper_actor_loss': np.float64(1.583563189342385e-05), 'behavior_loss': np.float64(0.35206660330295564)}

Episode step 23440, time diff 3.1485249996185303, total time dif 3368.3631463050842)
step: 23440 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(4.1349745), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06589178517460823), 'actor_loss': np.float64(-0.9878948748111724), 'hyper_actor_loss': np.float64(1.5620761769241653e-05), 'behavior_loss': np.float64(0.34015247523784636)}

Episode step 23450, time diff 3.1536295413970947, total time dif 3371.5116713047028)
step: 23450 @ episode report: {'average_total_reward': np.float32(11.15889), 'reward_variance': np.float32(3.919804), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06623460222035646), 'actor_loss': np.float64(-0.9872543931007385), 'hyper_actor_loss': np.float64(1.3587943158199778e-05), 'behavior_loss': np.float64(0.35881643295288085)}

Episode step 23460, time diff 3.1754872798919678, total time dif 3374.6653008461)
step: 23460 @ episode report: {'average_total_reward': np.float32(8.851112), 'reward_variance': np.float32(1.2355359), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0656295284628868), 'actor_loss': np.float64(-0.9966966450214386), 'hyper_actor_loss': np.float64(1.4003338765178342e-05), 'behavior_loss': np.float64(0.3167939156293869)}

Episode step 23470, time diff 3.2074623107910156, total time dif 3377.840788125992)
step: 23470 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(4.6918974), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0704201728105545), 'actor_loss': np.float64(-1.0066043972969054), 'hyper_actor_loss': np.float64(1.756425335770473e-05), 'behavior_loss': np.float64(0.3419839233160019)}

Episode step 23480, time diff 3.1803088188171387, total time dif 3381.048250436783)
step: 23480 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(3.6586425), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07967896796762944), 'actor_loss': np.float64(-1.001245665550232), 'hyper_actor_loss': np.float64(1.3919063349021599e-05), 'behavior_loss': np.float64(0.3277394026517868)}

Episode step 23490, time diff 3.1859071254730225, total time dif 3384.2285592556)
step: 23490 @ episode report: {'average_total_reward': np.float32(9.138889), 'reward_variance': np.float32(1.6913153), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06413331516087055), 'actor_loss': np.float64(-0.9957700669765472), 'hyper_actor_loss': np.float64(1.5091581553861034e-05), 'behavior_loss': np.float64(0.3481856808066368)}

Episode step 23500, time diff 3.199411153793335, total time dif 3387.414466381073)
step: 23500 @ episode report: {'average_total_reward': np.float32(10.797779), 'reward_variance': np.float32(4.9991064), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07309251315891743), 'actor_loss': np.float64(-0.9933700382709503), 'hyper_actor_loss': np.float64(1.988405138035887e-05), 'behavior_loss': np.float64(0.3454239010810852)}

Episode step 23510, time diff 3.1569247245788574, total time dif 3390.6138775348663)
step: 23510 @ episode report: {'average_total_reward': np.float32(10.285557), 'reward_variance': np.float32(0.5508405), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(9.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06782789565622807), 'actor_loss': np.float64(-1.0050875186920165), 'hyper_actor_loss': np.float64(1.2687142043432686e-05), 'behavior_loss': np.float64(0.35878625214099885)}

Episode step 23520, time diff 3.188108444213867, total time dif 3393.770802259445)
step: 23520 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(2.431091), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0631425891071558), 'actor_loss': np.float64(-0.9777370691299438), 'hyper_actor_loss': np.float64(1.4170059375828715e-05), 'behavior_loss': np.float64(0.3443309456110001)}

Episode step 23530, time diff 3.1744251251220703, total time dif 3396.958910703659)
step: 23530 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(1.8191849), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060490682721138), 'actor_loss': np.float64(-0.9711145162582397), 'hyper_actor_loss': np.float64(1.0918567340922891e-05), 'behavior_loss': np.float64(0.351797491312027)}

Episode step 23540, time diff 3.2175893783569336, total time dif 3400.133335828781)
step: 23540 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(2.633996), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057946118898689745), 'actor_loss': np.float64(-0.9834086537361145), 'hyper_actor_loss': np.float64(1.2726006934826728e-05), 'behavior_loss': np.float64(0.34672034680843355)}

Episode step 23550, time diff 3.204828977584839, total time dif 3403.350925207138)
step: 23550 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(2.8408437), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07357299327850342), 'actor_loss': np.float64(-0.9927006006240845), 'hyper_actor_loss': np.float64(1.2024623856632388e-05), 'behavior_loss': np.float64(0.31987124383449556)}

Episode step 23560, time diff 3.2001242637634277, total time dif 3406.555754184723)
step: 23560 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(2.1323164), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057530380040407184), 'actor_loss': np.float64(-1.0089966714382173), 'hyper_actor_loss': np.float64(1.2617533866432495e-05), 'behavior_loss': np.float64(0.31998776495456693)}

Episode step 23570, time diff 3.3189685344696045, total time dif 3409.7558784484863)
step: 23570 @ episode report: {'average_total_reward': np.float32(10.236667), 'reward_variance': np.float32(2.1417787), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06787993647158146), 'actor_loss': np.float64(-0.9865672647953033), 'hyper_actor_loss': np.float64(1.1584362891881028e-05), 'behavior_loss': np.float64(0.33229877054691315)}

Episode step 23580, time diff 3.1503477096557617, total time dif 3413.074846982956)
step: 23580 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.2231182), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.067941240593791), 'actor_loss': np.float64(-0.9963611125946045), 'hyper_actor_loss': np.float64(1.0920556042037788e-05), 'behavior_loss': np.float64(0.32549274563789365)}

Episode step 23590, time diff 3.1740150451660156, total time dif 3416.2251946926117)
step: 23590 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(5.0590672), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06781535632908345), 'actor_loss': np.float64(-1.006896162033081), 'hyper_actor_loss': np.float64(1.3861265142622869e-05), 'behavior_loss': np.float64(0.30809046179056165)}

Episode step 23600, time diff 3.198676109313965, total time dif 3419.3992097377777)
step: 23600 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(3.9662023), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0626757537946105), 'actor_loss': np.float64(-0.9863772511482238), 'hyper_actor_loss': np.float64(1.1559058248167275e-05), 'behavior_loss': np.float64(0.34272546172142027)}

Episode step 23610, time diff 3.187227249145508, total time dif 3422.5978858470917)
step: 23610 @ episode report: {'average_total_reward': np.float32(9.48778), 'reward_variance': np.float32(3.2894943), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07765095159411431), 'actor_loss': np.float64(-0.9930790662765503), 'hyper_actor_loss': np.float64(1.1715522759914165e-05), 'behavior_loss': np.float64(0.3615031003952026)}

Episode step 23620, time diff 3.1449716091156006, total time dif 3425.785113096237)
step: 23620 @ episode report: {'average_total_reward': np.float32(9.561111), 'reward_variance': np.float32(3.9640307), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06401863098144531), 'actor_loss': np.float64(-0.9981420159339904), 'hyper_actor_loss': np.float64(1.2739001886075129e-05), 'behavior_loss': np.float64(0.3298110544681549)}

Episode step 23630, time diff 3.122426748275757, total time dif 3428.930084705353)
step: 23630 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(4.659705), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06402887776494026), 'actor_loss': np.float64(-0.9864379465579987), 'hyper_actor_loss': np.float64(1.1522965542098973e-05), 'behavior_loss': np.float64(0.31811356693506243)}

Episode step 23640, time diff 3.2020504474639893, total time dif 3432.0525114536285)
step: 23640 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.198673), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05899619907140732), 'actor_loss': np.float64(-0.9886368691921235), 'hyper_actor_loss': np.float64(9.719051831780234e-06), 'behavior_loss': np.float64(0.3351691454648972)}

Episode step 23650, time diff 3.1476402282714844, total time dif 3435.2545619010925)
step: 23650 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(0.8066435), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0621007464826107), 'actor_loss': np.float64(-0.9808529078960418), 'hyper_actor_loss': np.float64(1.0815802215802251e-05), 'behavior_loss': np.float64(0.32789114117622375)}

Episode step 23660, time diff 3.1596016883850098, total time dif 3438.402202129364)
step: 23660 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(6.173237), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06625196486711502), 'actor_loss': np.float64(-0.9939542174339294), 'hyper_actor_loss': np.float64(9.486501267019775e-06), 'behavior_loss': np.float64(0.33461534082889555)}

Episode step 23670, time diff 3.1713709831237793, total time dif 3441.561803817749)
step: 23670 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(4.9034877), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06553391292691231), 'actor_loss': np.float64(-0.9935880243778229), 'hyper_actor_loss': np.float64(1.1270544655417325e-05), 'behavior_loss': np.float64(0.30063537508249283)}

Episode step 23680, time diff 3.1348440647125244, total time dif 3444.733174800873)
step: 23680 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(3.380135), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05964332073926926), 'actor_loss': np.float64(-1.0003207266330718), 'hyper_actor_loss': np.float64(1.1406668136260123e-05), 'behavior_loss': np.float64(0.3219611197710037)}

Episode step 23690, time diff 3.157996892929077, total time dif 3447.8680188655853)
step: 23690 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.8335559), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058939875662326814), 'actor_loss': np.float64(-0.9815296232700348), 'hyper_actor_loss': np.float64(1.0084156565426384e-05), 'behavior_loss': np.float64(0.3302656263113022)}

Episode step 23700, time diff 3.171642541885376, total time dif 3451.0260157585144)
step: 23700 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(1.082643), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0576445197686553), 'actor_loss': np.float64(-0.9712559878826141), 'hyper_actor_loss': np.float64(8.772267028689384e-06), 'behavior_loss': np.float64(0.3177612692117691)}

Episode step 23710, time diff 3.1246232986450195, total time dif 3454.1976583004)
step: 23710 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(3.8141685), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06341208051890135), 'actor_loss': np.float64(-0.9856220304965972), 'hyper_actor_loss': np.float64(9.152254051514319e-06), 'behavior_loss': np.float64(0.34239489436149595)}

Episode step 23720, time diff 3.132166624069214, total time dif 3457.322281599045)
step: 23720 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(2.5597477), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06407654955983162), 'actor_loss': np.float64(-0.9862494707107544), 'hyper_actor_loss': np.float64(9.547780882712686e-06), 'behavior_loss': np.float64(0.298791778087616)}

Episode step 23730, time diff 3.1131374835968018, total time dif 3460.454448223114)
step: 23730 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(2.1470237), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06492874585092068), 'actor_loss': np.float64(-0.995120519399643), 'hyper_actor_loss': np.float64(1.2523786290330463e-05), 'behavior_loss': np.float64(0.3536400765180588)}

Episode step 23740, time diff 3.3026959896087646, total time dif 3463.567585706711)
step: 23740 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(1.0594684), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06903386637568473), 'actor_loss': np.float64(-0.9774818003177643), 'hyper_actor_loss': np.float64(1.0687830535971442e-05), 'behavior_loss': np.float64(0.36346330046653746)}

Episode step 23750, time diff 3.1537601947784424, total time dif 3466.8702816963196)
step: 23750 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.5897839), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05453475769609213), 'actor_loss': np.float64(-0.9750834047794342), 'hyper_actor_loss': np.float64(9.877126285573467e-06), 'behavior_loss': np.float64(0.32517759799957274)}

Episode step 23760, time diff 3.170081377029419, total time dif 3470.024041891098)
step: 23760 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(4.928792), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07484267391264439), 'actor_loss': np.float64(-0.9897692441940308), 'hyper_actor_loss': np.float64(8.861708465701668e-06), 'behavior_loss': np.float64(0.3417151689529419)}

Episode step 23770, time diff 3.1659862995147705, total time dif 3473.1941232681274)
step: 23770 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.4180787), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07217985577881336), 'actor_loss': np.float64(-0.9977155387401581), 'hyper_actor_loss': np.float64(9.869119867289556e-06), 'behavior_loss': np.float64(0.3384545087814331)}

Episode step 23780, time diff 3.148094892501831, total time dif 3476.360109567642)
step: 23780 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(1.3017968), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07628822214901447), 'actor_loss': np.float64(-1.0076551675796508), 'hyper_actor_loss': np.float64(1.1671845231830958e-05), 'behavior_loss': np.float64(0.34116946160793304)}

Episode step 23790, time diff 3.196448802947998, total time dif 3479.508204460144)
step: 23790 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(2.3920796), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06713371612131595), 'actor_loss': np.float64(-1.0031285881996155), 'hyper_actor_loss': np.float64(9.080439758690772e-06), 'behavior_loss': np.float64(0.33070004284381865)}

Episode step 23800, time diff 3.1647226810455322, total time dif 3482.704653263092)
step: 23800 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.0829651), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07212196663022041), 'actor_loss': np.float64(-0.9894458949565887), 'hyper_actor_loss': np.float64(1.0756245637821849e-05), 'behavior_loss': np.float64(0.31072079837322236)}

Episode step 23810, time diff 3.1219451427459717, total time dif 3485.8693759441376)
step: 23810 @ episode report: {'average_total_reward': np.float32(8.426668), 'reward_variance': np.float32(4.5481305), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06373325958848), 'actor_loss': np.float64(-0.9920492053031922), 'hyper_actor_loss': np.float64(8.168263093466521e-06), 'behavior_loss': np.float64(0.3341688275337219)}

Episode step 23820, time diff 3.152461528778076, total time dif 3488.9913210868835)
step: 23820 @ episode report: {'average_total_reward': np.float32(9.924445), 'reward_variance': np.float32(1.8233774), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06230239532887936), 'actor_loss': np.float64(-0.9811950325965881), 'hyper_actor_loss': np.float64(1.136686373683915e-05), 'behavior_loss': np.float64(0.33646378517150877)}

Episode step 23830, time diff 3.1353724002838135, total time dif 3492.1437826156616)
step: 23830 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(3.6984794), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06543967016041279), 'actor_loss': np.float64(-0.9820619523525238), 'hyper_actor_loss': np.float64(9.52897273691633e-06), 'behavior_loss': np.float64(0.3165483668446541)}

Episode step 23840, time diff 3.149904727935791, total time dif 3495.2791550159454)
step: 23840 @ episode report: {'average_total_reward': np.float32(9.475557), 'reward_variance': np.float32(2.9964652), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06310557350516319), 'actor_loss': np.float64(-0.9970812737941742), 'hyper_actor_loss': np.float64(9.806619300434249e-06), 'behavior_loss': np.float64(0.32099032700061797)}

Episode step 23850, time diff 3.1469955444335938, total time dif 3498.429059743881)
step: 23850 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(2.4326184), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07700663283467293), 'actor_loss': np.float64(-1.0026845693588258), 'hyper_actor_loss': np.float64(1.080683164218499e-05), 'behavior_loss': np.float64(0.3541834831237793)}

Episode step 23860, time diff 3.1874773502349854, total time dif 3501.576055288315)
step: 23860 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(2.4233534), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06920075751841068), 'actor_loss': np.float64(-0.9875499844551087), 'hyper_actor_loss': np.float64(8.510486941304407e-06), 'behavior_loss': np.float64(0.3465653330087662)}

Episode step 23870, time diff 3.1681602001190186, total time dif 3504.76353263855)
step: 23870 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(1.3074088), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07053285762667656), 'actor_loss': np.float64(-0.9821040153503418), 'hyper_actor_loss': np.float64(7.208744591480354e-06), 'behavior_loss': np.float64(0.3093487173318863)}

Episode step 23880, time diff 3.1431808471679688, total time dif 3507.931692838669)
step: 23880 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(2.4281037), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07989406995475293), 'actor_loss': np.float64(-1.0165892839431763), 'hyper_actor_loss': np.float64(9.894713639369001e-06), 'behavior_loss': np.float64(0.3231192037463188)}

Episode step 23890, time diff 3.154299259185791, total time dif 3511.074873685837)
step: 23890 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(4.196748), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06802534237504006), 'actor_loss': np.float64(-1.0033562123775481), 'hyper_actor_loss': np.float64(8.030342132769875e-06), 'behavior_loss': np.float64(0.33048900961875916)}

Episode step 23900, time diff 3.295029640197754, total time dif 3514.2291729450226)
step: 23900 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(2.392816), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07407876178622246), 'actor_loss': np.float64(-0.9843994855880738), 'hyper_actor_loss': np.float64(9.137434062722605e-06), 'behavior_loss': np.float64(0.3035272553563118)}

Episode step 23910, time diff 3.1629526615142822, total time dif 3517.5242025852203)
step: 23910 @ episode report: {'average_total_reward': np.float32(8.177778), 'reward_variance': np.float32(2.592173), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0724521230906248), 'actor_loss': np.float64(-1.0025766491889954), 'hyper_actor_loss': np.float64(1.0793341198223061e-05), 'behavior_loss': np.float64(0.3366653025150299)}

Episode step 23920, time diff 3.130417585372925, total time dif 3520.6871552467346)
step: 23920 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(1.2330613), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05590538568794727), 'actor_loss': np.float64(-1.0007682919502259), 'hyper_actor_loss': np.float64(7.877231155362097e-06), 'behavior_loss': np.float64(0.30771385729312895)}

Episode step 23930, time diff 3.1620395183563232, total time dif 3523.8175728321075)
step: 23930 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.3333285), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05971500463783741), 'actor_loss': np.float64(-0.9635203778743744), 'hyper_actor_loss': np.float64(7.840481976018054e-06), 'behavior_loss': np.float64(0.3415160298347473)}

Episode step 23940, time diff 3.1666781902313232, total time dif 3526.979612350464)
step: 23940 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(2.421508), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06490672491490841), 'actor_loss': np.float64(-0.974104905128479), 'hyper_actor_loss': np.float64(1.0192118361374014e-05), 'behavior_loss': np.float64(0.3215492069721222)}

Episode step 23950, time diff 3.1612460613250732, total time dif 3530.146290540695)
step: 23950 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(3.0200448), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04691720129922032), 'actor_loss': np.float64(-0.9816762685775757), 'hyper_actor_loss': np.float64(1.0919167516476591e-05), 'behavior_loss': np.float64(0.348866218328476)}

Episode step 23960, time diff 3.1634938716888428, total time dif 3533.3075366020203)
step: 23960 @ episode report: {'average_total_reward': np.float32(9.700002), 'reward_variance': np.float32(2.1809878), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06763837151229382), 'actor_loss': np.float64(-0.9629120230674744), 'hyper_actor_loss': np.float64(9.76436131168157e-06), 'behavior_loss': np.float64(0.31799083650112153)}

Episode step 23970, time diff 3.1764440536499023, total time dif 3536.471030473709)
step: 23970 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(4.0555806), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05974185392260552), 'actor_loss': np.float64(-0.998413908481598), 'hyper_actor_loss': np.float64(9.62253570833127e-06), 'behavior_loss': np.float64(0.3186467409133911)}

Episode step 23980, time diff 3.1450934410095215, total time dif 3539.647474527359)
step: 23980 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(0.77622384), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06339423581957818), 'actor_loss': np.float64(-0.9801449835300445), 'hyper_actor_loss': np.float64(6.8172098963259485e-06), 'behavior_loss': np.float64(0.35475255846977233)}

Episode step 23990, time diff 3.1281650066375732, total time dif 3542.7925679683685)
step: 23990 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(3.5956166), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061863101460039614), 'actor_loss': np.float64(-0.969315767288208), 'hyper_actor_loss': np.float64(7.120528198356624e-06), 'behavior_loss': np.float64(0.3394070118665695)}

Episode step 24000, time diff 3.110119104385376, total time dif 3545.920732975006)
step: 24000 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(3.6190414), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07004636153578758), 'actor_loss': np.float64(-0.9830158174037933), 'hyper_actor_loss': np.float64(6.335691887215944e-06), 'behavior_loss': np.float64(0.32874599397182463)}

Episode step 24010, time diff 3.1201624870300293, total time dif 3549.0308520793915)
step: 24010 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.3678532), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0743820022791624), 'actor_loss': np.float64(-0.9992787837982178), 'hyper_actor_loss': np.float64(6.941211040611961e-06), 'behavior_loss': np.float64(0.3394630134105682)}

Episode step 24020, time diff 3.1763134002685547, total time dif 3552.1510145664215)
step: 24020 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.3023703), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05105983130633831), 'actor_loss': np.float64(-0.9787178337574005), 'hyper_actor_loss': np.float64(7.225859371828847e-06), 'behavior_loss': np.float64(0.34833433032035827)}

Episode step 24030, time diff 3.145080089569092, total time dif 3555.32732796669)
step: 24030 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(1.7194383), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06124364584684372), 'actor_loss': np.float64(-0.9609134912490844), 'hyper_actor_loss': np.float64(8.06494399512303e-06), 'behavior_loss': np.float64(0.32787107229232787)}

Episode step 24040, time diff 3.1273717880249023, total time dif 3558.472408056259)
step: 24040 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(3.1472297), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057223989814519885), 'actor_loss': np.float64(-0.9752624988555908), 'hyper_actor_loss': np.float64(9.364808738610008e-06), 'behavior_loss': np.float64(0.32692929804325105)}

Episode step 24050, time diff 3.179155111312866, total time dif 3561.599779844284)
step: 24050 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(1.9441988), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06744090206921101), 'actor_loss': np.float64(-0.9907841384410858), 'hyper_actor_loss': np.float64(9.15107443688612e-06), 'behavior_loss': np.float64(0.33386844396591187)}

Episode step 24060, time diff 3.1754794120788574, total time dif 3564.778934955597)
step: 24060 @ episode report: {'average_total_reward': np.float32(10.224444), 'reward_variance': np.float32(2.12844), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07207996547222137), 'actor_loss': np.float64(-1.0025829315185546), 'hyper_actor_loss': np.float64(9.23232437344268e-06), 'behavior_loss': np.float64(0.32437344193458556)}

Episode step 24070, time diff 3.3361032009124756, total time dif 3567.954414367676)
step: 24070 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(2.4984558), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05612930729985237), 'actor_loss': np.float64(-0.9857403397560119), 'hyper_actor_loss': np.float64(6.896001377754146e-06), 'behavior_loss': np.float64(0.3299350023269653)}

Episode step 24080, time diff 3.152970790863037, total time dif 3571.2905175685883)
step: 24080 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(2.3653839), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07310844920575618), 'actor_loss': np.float64(-0.9763709664344787), 'hyper_actor_loss': np.float64(1.059110181813594e-05), 'behavior_loss': np.float64(0.30671814680099485)}

Episode step 24090, time diff 3.152089834213257, total time dif 3574.4434883594513)
step: 24090 @ episode report: {'average_total_reward': np.float32(9.761112), 'reward_variance': np.float32(1.4505495), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07025045268237591), 'actor_loss': np.float64(-1.0088271260261537), 'hyper_actor_loss': np.float64(6.871004984532192e-06), 'behavior_loss': np.float64(0.3175900086760521)}

Episode step 24100, time diff 3.1983766555786133, total time dif 3577.5955781936646)
step: 24100 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.8994335), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06218956559896469), 'actor_loss': np.float64(-0.9885193705558777), 'hyper_actor_loss': np.float64(8.216321475629229e-06), 'behavior_loss': np.float64(0.3353851795196533)}

Episode step 24110, time diff 3.1573166847229004, total time dif 3580.793954849243)
step: 24110 @ episode report: {'average_total_reward': np.float32(9.548889), 'reward_variance': np.float32(3.1710422), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06041983552277088), 'actor_loss': np.float64(-0.9664950668811798), 'hyper_actor_loss': np.float64(7.47657468309626e-06), 'behavior_loss': np.float64(0.32761022746562957)}

Episode step 24120, time diff 3.219935178756714, total time dif 3583.951271533966)
step: 24120 @ episode report: {'average_total_reward': np.float32(8.526667), 'reward_variance': np.float32(3.6928697), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.049906693398952484), 'actor_loss': np.float64(-0.9770501971244812), 'hyper_actor_loss': np.float64(1.1475175188024878e-05), 'behavior_loss': np.float64(0.2944532364606857)}

Episode step 24130, time diff 3.2098848819732666, total time dif 3587.171206712723)
step: 24130 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(3.7339776), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06244070455431938), 'actor_loss': np.float64(-0.9792730271816253), 'hyper_actor_loss': np.float64(7.525303271904704e-06), 'behavior_loss': np.float64(0.3224331095814705)}

Episode step 24140, time diff 3.1931324005126953, total time dif 3590.381091594696)
step: 24140 @ episode report: {'average_total_reward': np.float32(9.74889), 'reward_variance': np.float32(1.8041284), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0584544200450182), 'actor_loss': np.float64(-0.9809529960155488), 'hyper_actor_loss': np.float64(8.163831762431072e-06), 'behavior_loss': np.float64(0.33672542572021485)}

Episode step 24150, time diff 3.17566180229187, total time dif 3593.5742239952087)
step: 24150 @ episode report: {'average_total_reward': np.float32(9.126668), 'reward_variance': np.float32(1.7014863), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0698094330728054), 'actor_loss': np.float64(-0.9831353724002838), 'hyper_actor_loss': np.float64(6.272917289606994e-06), 'behavior_loss': np.float64(0.33593629002571107)}

Episode step 24160, time diff 3.281327247619629, total time dif 3596.7498857975006)
step: 24160 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(3.2085693), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06849440708756446), 'actor_loss': np.float64(-1.0005435824394227), 'hyper_actor_loss': np.float64(6.587188590856386e-06), 'behavior_loss': np.float64(0.3065690129995346)}

Episode step 24170, time diff 3.360273838043213, total time dif 3600.0312130451202)
step: 24170 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(0.9387706), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06998186688870192), 'actor_loss': np.float64(-0.9897312641143798), 'hyper_actor_loss': np.float64(7.11838474671822e-06), 'behavior_loss': np.float64(0.313308385014534)}

Episode step 24180, time diff 3.29211163520813, total time dif 3603.3914868831635)
step: 24180 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(5.033412), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07144963815808296), 'actor_loss': np.float64(-0.9963756799697876), 'hyper_actor_loss': np.float64(7.496510897908593e-06), 'behavior_loss': np.float64(0.3022944539785385)}

Episode step 24190, time diff 3.378878355026245, total time dif 3606.6835985183716)
step: 24190 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(4.3887367), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06395218782126903), 'actor_loss': np.float64(-0.9979579031467438), 'hyper_actor_loss': np.float64(7.666658621019452e-06), 'behavior_loss': np.float64(0.3240046098828316)}

Episode step 24200, time diff 3.1826565265655518, total time dif 3610.062476873398)
step: 24200 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(1.3172398), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06083025261759758), 'actor_loss': np.float64(-0.9829560995101929), 'hyper_actor_loss': np.float64(9.683279949967983e-06), 'behavior_loss': np.float64(0.29788474291563033)}

Episode step 24210, time diff 3.246961832046509, total time dif 3613.2451333999634)
step: 24210 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.1806715), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.051459016464650634), 'actor_loss': np.float64(-0.9662449359893799), 'hyper_actor_loss': np.float64(7.021844021437574e-06), 'behavior_loss': np.float64(0.3210782468318939)}

Episode step 24220, time diff 3.2021982669830322, total time dif 3616.49209523201)
step: 24220 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.9695113), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06794524677097798), 'actor_loss': np.float64(-0.9727636575698853), 'hyper_actor_loss': np.float64(6.5814357867566285e-06), 'behavior_loss': np.float64(0.3018855810165405)}

Episode step 24230, time diff 3.3453640937805176, total time dif 3619.694293498993)
step: 24230 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(1.2676067), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06376785710453987), 'actor_loss': np.float64(-1.0110345721244811), 'hyper_actor_loss': np.float64(6.729357755830278e-06), 'behavior_loss': np.float64(0.3237054079771042)}

Episode step 24240, time diff 3.2015631198883057, total time dif 3623.0396575927734)
step: 24240 @ episode report: {'average_total_reward': np.float32(10.946667), 'reward_variance': np.float32(5.449526), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05710751637816429), 'actor_loss': np.float64(-0.9842797458171845), 'hyper_actor_loss': np.float64(5.7011880016943906e-06), 'behavior_loss': np.float64(0.2940380945801735)}

Episode step 24250, time diff 3.2145137786865234, total time dif 3626.2412207126617)
step: 24250 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(4.286035), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0583646897226572), 'actor_loss': np.float64(-0.9659364640712738), 'hyper_actor_loss': np.float64(5.657532710756641e-06), 'behavior_loss': np.float64(0.33246850669384004)}

Episode step 24260, time diff 3.1484992504119873, total time dif 3629.4557344913483)
step: 24260 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(1.591062), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06498117819428444), 'actor_loss': np.float64(-0.9796666204929352), 'hyper_actor_loss': np.float64(5.188696513869218e-06), 'behavior_loss': np.float64(0.2885729417204857)}

Episode step 24270, time diff 3.2725586891174316, total time dif 3632.6042337417603)
step: 24270 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(0.9173129), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06940856873989106), 'actor_loss': np.float64(-1.0041591584682465), 'hyper_actor_loss': np.float64(7.487590914934117e-06), 'behavior_loss': np.float64(0.33660497069358825)}

Episode step 24280, time diff 3.1896564960479736, total time dif 3635.8767924308777)
step: 24280 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(0.91395205), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07035695612430573), 'actor_loss': np.float64(-0.9881790101528167), 'hyper_actor_loss': np.float64(6.763171404600143e-06), 'behavior_loss': np.float64(0.3044074773788452)}

Episode step 24290, time diff 3.2625997066497803, total time dif 3639.0664489269257)
step: 24290 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(4.521901), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06782538034021854), 'actor_loss': np.float64(-0.9838616788387299), 'hyper_actor_loss': np.float64(6.4812888012966144e-06), 'behavior_loss': np.float64(0.32083619236946104)}

Episode step 24300, time diff 3.1734859943389893, total time dif 3642.3290486335754)
step: 24300 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(4.2921104), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06911324393004178), 'actor_loss': np.float64(-1.0054582834243775), 'hyper_actor_loss': np.float64(7.658793992959546e-06), 'behavior_loss': np.float64(0.293606773018837)}

Episode step 24310, time diff 3.178715229034424, total time dif 3645.5025346279144)
step: 24310 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(2.0031354), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07020294982939959), 'actor_loss': np.float64(-0.9786397039890289), 'hyper_actor_loss': np.float64(6.447715941249044e-06), 'behavior_loss': np.float64(0.3017463475465775)}

Episode step 24320, time diff 3.285566568374634, total time dif 3648.681249856949)
step: 24320 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(2.3353033), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05902058072388172), 'actor_loss': np.float64(-0.9846467733383178), 'hyper_actor_loss': np.float64(4.509023506216181e-06), 'behavior_loss': np.float64(0.3293412521481514)}

Episode step 24330, time diff 3.2047007083892822, total time dif 3651.9668164253235)
step: 24330 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(2.0554326), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04942083302885294), 'actor_loss': np.float64(-0.9578083693981171), 'hyper_actor_loss': np.float64(6.030124200151476e-06), 'behavior_loss': np.float64(0.3096994161605835)}

Episode step 24340, time diff 3.22446346282959, total time dif 3655.1715171337128)
step: 24340 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(3.7668004), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05452198497951031), 'actor_loss': np.float64(-0.9637244641780853), 'hyper_actor_loss': np.float64(7.498582249354513e-06), 'behavior_loss': np.float64(0.3216057181358337)}

Episode step 24350, time diff 3.209228754043579, total time dif 3658.3959805965424)
step: 24350 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(4.445019), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05226610042154789), 'actor_loss': np.float64(-0.9790284693241119), 'hyper_actor_loss': np.float64(6.61387559830473e-06), 'behavior_loss': np.float64(0.30942077934741974)}

Episode step 24360, time diff 3.1947855949401855, total time dif 3661.605209350586)
step: 24360 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(2.27844), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06201831102371216), 'actor_loss': np.float64(-0.9832011580467224), 'hyper_actor_loss': np.float64(6.401537962119619e-06), 'behavior_loss': np.float64(0.3053377866744995)}

Episode step 24370, time diff 3.204664945602417, total time dif 3664.799994945526)
step: 24370 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(5.0979953), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06217128187417984), 'actor_loss': np.float64(-0.9741764008998871), 'hyper_actor_loss': np.float64(5.0433137630534475e-06), 'behavior_loss': np.float64(0.3308633267879486)}

Episode step 24380, time diff 3.211559295654297, total time dif 3668.0046598911285)
step: 24380 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(1.2496165), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06842998526990414), 'actor_loss': np.float64(-0.9758816719055176), 'hyper_actor_loss': np.float64(5.104358024254907e-06), 'behavior_loss': np.float64(0.32560405135154724)}

Episode step 24390, time diff 3.164630651473999, total time dif 3671.216219186783)
step: 24390 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.1765978), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06802273578941823), 'actor_loss': np.float64(-0.9913645327091217), 'hyper_actor_loss': np.float64(5.457422071231122e-06), 'behavior_loss': np.float64(0.3021351143717766)}

Episode step 24400, time diff 3.400186777114868, total time dif 3674.380849838257)
step: 24400 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.0314832), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06962862312793731), 'actor_loss': np.float64(-0.9955212056636811), 'hyper_actor_loss': np.float64(5.769061726823566e-06), 'behavior_loss': np.float64(0.31254170536994935)}

Episode step 24410, time diff 3.183685779571533, total time dif 3677.7810366153717)
step: 24410 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(0.8420248), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0601207721978426), 'actor_loss': np.float64(-0.9802146255970001), 'hyper_actor_loss': np.float64(7.181750243034913e-06), 'behavior_loss': np.float64(0.3112132102251053)}

Episode step 24420, time diff 3.2255070209503174, total time dif 3680.9647223949432)
step: 24420 @ episode report: {'average_total_reward': np.float32(8.602222), 'reward_variance': np.float32(2.171699), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07044614776968956), 'actor_loss': np.float64(-0.9798873066902161), 'hyper_actor_loss': np.float64(6.3390879631697315e-06), 'behavior_loss': np.float64(0.31793407797813417)}

Episode step 24430, time diff 3.2695329189300537, total time dif 3684.1902294158936)
step: 24430 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(4.161409), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0623194282874465), 'actor_loss': np.float64(-0.9927038669586181), 'hyper_actor_loss': np.float64(8.240757142630174e-06), 'behavior_loss': np.float64(0.308258581161499)}

Episode step 24440, time diff 3.203739643096924, total time dif 3687.4597623348236)
step: 24440 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.4101179), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06681991443037986), 'actor_loss': np.float64(-0.9819723904132843), 'hyper_actor_loss': np.float64(5.381205005505762e-06), 'behavior_loss': np.float64(0.3084107175469398)}

Episode step 24450, time diff 3.227768659591675, total time dif 3690.6635019779205)
step: 24450 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(1.2834371), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07713819816708564), 'actor_loss': np.float64(-0.9939991712570191), 'hyper_actor_loss': np.float64(7.0224182763922725e-06), 'behavior_loss': np.float64(0.3239274859428406)}

Episode step 24460, time diff 3.205219030380249, total time dif 3693.891270637512)
step: 24460 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(1.3289245), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07111635357141495), 'actor_loss': np.float64(-1.0044482946395874), 'hyper_actor_loss': np.float64(4.769029760609555e-06), 'behavior_loss': np.float64(0.3055634588003159)}

Episode step 24470, time diff 3.222721576690674, total time dif 3697.0964896678925)
step: 24470 @ episode report: {'average_total_reward': np.float32(9.773335), 'reward_variance': np.float32(4.085782), 'max_total_reward': np.float32(14.388888), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06016195267438888), 'actor_loss': np.float64(-0.9826852381229401), 'hyper_actor_loss': np.float64(5.286125360726146e-06), 'behavior_loss': np.float64(0.3145225316286087)}

Episode step 24480, time diff 3.198385000228882, total time dif 3700.319211244583)
step: 24480 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(0.8738224), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06576733328402043), 'actor_loss': np.float64(-0.9695122361183166), 'hyper_actor_loss': np.float64(6.53481997687777e-06), 'behavior_loss': np.float64(0.2939462184906006)}

Episode step 24490, time diff 3.2293670177459717, total time dif 3703.517596244812)
step: 24490 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(1.0634812), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06286768317222595), 'actor_loss': np.float64(-0.9923965334892273), 'hyper_actor_loss': np.float64(5.722192031498707e-06), 'behavior_loss': np.float64(0.29693433046340945)}

Episode step 24500, time diff 3.2082467079162598, total time dif 3706.746963262558)
step: 24500 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(4.0181484), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06408560946583748), 'actor_loss': np.float64(-0.9896428048610687), 'hyper_actor_loss': np.float64(5.900159339944366e-06), 'behavior_loss': np.float64(0.2987689793109894)}

Episode step 24510, time diff 3.2456154823303223, total time dif 3709.9552099704742)
step: 24510 @ episode report: {'average_total_reward': np.float32(8.502222), 'reward_variance': np.float32(1.4071307), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0636013800278306), 'actor_loss': np.float64(-0.9680284202098847), 'hyper_actor_loss': np.float64(5.262372565084661e-06), 'behavior_loss': np.float64(0.31525480151176455)}

Episode step 24520, time diff 3.2633161544799805, total time dif 3713.2008254528046)
step: 24520 @ episode report: {'average_total_reward': np.float32(8.875556), 'reward_variance': np.float32(3.1700451), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060187983140349385), 'actor_loss': np.float64(-0.9730255424976348), 'hyper_actor_loss': np.float64(5.593762853095541e-06), 'behavior_loss': np.float64(0.3183907613158226)}

Episode step 24530, time diff 3.2633087635040283, total time dif 3716.4641416072845)
step: 24530 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(3.3100116), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05835569016635418), 'actor_loss': np.float64(-0.983989143371582), 'hyper_actor_loss': np.float64(4.249564608471701e-06), 'behavior_loss': np.float64(0.3060632929205894)}

Episode step 24540, time diff 3.2607758045196533, total time dif 3719.7274503707886)
step: 24540 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(1.5229496), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06799076721072198), 'actor_loss': np.float64(-0.9876229107379914), 'hyper_actor_loss': np.float64(5.036067273067602e-06), 'behavior_loss': np.float64(0.2851994395256042)}

Episode step 24550, time diff 3.238983154296875, total time dif 3722.9882261753082)
step: 24550 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.0888398), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06254745051264762), 'actor_loss': np.float64(-0.9936140537261963), 'hyper_actor_loss': np.float64(5.283617883833358e-06), 'behavior_loss': np.float64(0.3117628753185272)}

Episode step 24560, time diff 3.373591661453247, total time dif 3726.227209329605)
step: 24560 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(6.6841593), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07288977801799774), 'actor_loss': np.float64(-0.9832805633544922), 'hyper_actor_loss': np.float64(4.978323022442055e-06), 'behavior_loss': np.float64(0.30547957122325897)}

Episode step 24570, time diff 3.285123586654663, total time dif 3729.6008009910583)
step: 24570 @ episode report: {'average_total_reward': np.float32(9.126667), 'reward_variance': np.float32(2.1424), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07377144768834114), 'actor_loss': np.float64(-0.9927039682865143), 'hyper_actor_loss': np.float64(4.791540231963154e-06), 'behavior_loss': np.float64(0.31236920654773714)}

Episode step 24580, time diff 3.2659645080566406, total time dif 3732.885924577713)
step: 24580 @ episode report: {'average_total_reward': np.float32(8.69), 'reward_variance': np.float32(3.63201), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06716963108628989), 'actor_loss': np.float64(-0.9922872543334961), 'hyper_actor_loss': np.float64(5.3653028999178785e-06), 'behavior_loss': np.float64(0.325045582652092)}

Episode step 24590, time diff 3.247715950012207, total time dif 3736.1518890857697)
step: 24590 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(2.4264944), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059526846185326575), 'actor_loss': np.float64(-0.9698156237602233), 'hyper_actor_loss': np.float64(6.403906581908814e-06), 'behavior_loss': np.float64(0.3019249752163887)}

Episode step 24600, time diff 3.2345945835113525, total time dif 3739.399605035782)
step: 24600 @ episode report: {'average_total_reward': np.float32(8.953334), 'reward_variance': np.float32(1.0005876), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061763038858771324), 'actor_loss': np.float64(-0.9886477291584015), 'hyper_actor_loss': np.float64(5.1794930413961994e-06), 'behavior_loss': np.float64(0.28500905334949495)}

Episode step 24610, time diff 3.2957763671875, total time dif 3742.634199619293)
step: 24610 @ episode report: {'average_total_reward': np.float32(9.924445), 'reward_variance': np.float32(5.4713283), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06636675111949444), 'actor_loss': np.float64(-0.978266441822052), 'hyper_actor_loss': np.float64(5.657802148562041e-06), 'behavior_loss': np.float64(0.30130903273820875)}

Episode step 24620, time diff 3.2753775119781494, total time dif 3745.9299759864807)
step: 24620 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(3.059937), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06109615825116634), 'actor_loss': np.float64(-0.9838022530078888), 'hyper_actor_loss': np.float64(5.011341636418365e-06), 'behavior_loss': np.float64(0.30527222752571104)}

Episode step 24630, time diff 3.3095269203186035, total time dif 3749.205353498459)
step: 24630 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(0.9662026), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06497356295585632), 'actor_loss': np.float64(-0.9884670853614808), 'hyper_actor_loss': np.float64(5.986180599393265e-06), 'behavior_loss': np.float64(0.2886669263243675)}

Episode step 24640, time diff 3.328325033187866, total time dif 3752.5148804187775)
step: 24640 @ episode report: {'average_total_reward': np.float32(8.577779), 'reward_variance': np.float32(2.046815), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07652741000056267), 'actor_loss': np.float64(-0.9850498735904694), 'hyper_actor_loss': np.float64(5.498049131347216e-06), 'behavior_loss': np.float64(0.3203961431980133)}

Episode step 24650, time diff 3.3065850734710693, total time dif 3755.8432054519653)
step: 24650 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(2.51292), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07077400758862495), 'actor_loss': np.float64(-1.0004812717437743), 'hyper_actor_loss': np.float64(5.632606826111442e-06), 'behavior_loss': np.float64(0.3100493401288986)}

Episode step 24660, time diff 3.2659499645233154, total time dif 3759.1497905254364)
step: 24660 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.040051), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06357703097164631), 'actor_loss': np.float64(-0.989631462097168), 'hyper_actor_loss': np.float64(5.520113950296945e-06), 'behavior_loss': np.float64(0.30497820377349855)}

Episode step 24670, time diff 3.302384376525879, total time dif 3762.4157404899597)
step: 24670 @ episode report: {'average_total_reward': np.float32(9.53889), 'reward_variance': np.float32(3.752229), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06712869685143233), 'actor_loss': np.float64(-0.9620108187198639), 'hyper_actor_loss': np.float64(4.5958718374095046e-06), 'behavior_loss': np.float64(0.31396099776029585)}

Episode step 24680, time diff 3.292381525039673, total time dif 3765.7181248664856)
step: 24680 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(0.37907258), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053519082814455034), 'actor_loss': np.float64(-0.983169001340866), 'hyper_actor_loss': np.float64(6.319221733974701e-06), 'behavior_loss': np.float64(0.2900987401604652)}

Episode step 24690, time diff 3.3012750148773193, total time dif 3769.0105063915253)
step: 24690 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.4405142), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07245284356176854), 'actor_loss': np.float64(-0.9902033090591431), 'hyper_actor_loss': np.float64(5.362021283872309e-06), 'behavior_loss': np.float64(0.28873034864664077)}

Episode step 24700, time diff 3.3394293785095215, total time dif 3772.3117814064026)
step: 24700 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(1.1853096), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06787112727761269), 'actor_loss': np.float64(-1.0040667295455932), 'hyper_actor_loss': np.float64(5.703152032765501e-06), 'behavior_loss': np.float64(0.3140272855758667)}

Episode step 24710, time diff 3.3441972732543945, total time dif 3775.651210784912)
step: 24710 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(3.6925704), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054257795214653015), 'actor_loss': np.float64(-0.9807247877120971), 'hyper_actor_loss': np.float64(5.886603230464971e-06), 'behavior_loss': np.float64(0.29650501608848573)}

Episode step 24720, time diff 3.502723217010498, total time dif 3778.9954080581665)
step: 24720 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.0441065), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07203250266611576), 'actor_loss': np.float64(-0.9653623044490814), 'hyper_actor_loss': np.float64(6.077056082176569e-06), 'behavior_loss': np.float64(0.3070331484079361)}

Episode step 24730, time diff 3.3236985206604004, total time dif 3782.498131275177)
step: 24730 @ episode report: {'average_total_reward': np.float32(9.463334), 'reward_variance': np.float32(0.80284053), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06716536059975624), 'actor_loss': np.float64(-0.9877983272075653), 'hyper_actor_loss': np.float64(5.767561333414051e-06), 'behavior_loss': np.float64(0.3069385290145874)}

Episode step 24740, time diff 3.3224306106567383, total time dif 3785.8218297958374)
step: 24740 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.6864395), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05346907414495945), 'actor_loss': np.float64(-0.9831282436847687), 'hyper_actor_loss': np.float64(4.872985277870611e-06), 'behavior_loss': np.float64(0.2861863270401955)}

Episode step 24750, time diff 3.3171544075012207, total time dif 3789.144260406494)
step: 24750 @ episode report: {'average_total_reward': np.float32(9.626668), 'reward_variance': np.float32(1.2928693), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0749099962413311), 'actor_loss': np.float64(-0.9721583664417267), 'hyper_actor_loss': np.float64(4.839138250645192e-06), 'behavior_loss': np.float64(0.32853738963603973)}

Episode step 24760, time diff 3.350224733352661, total time dif 3792.4614148139954)
step: 24760 @ episode report: {'average_total_reward': np.float32(9.563334), 'reward_variance': np.float32(1.4779522), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06632903479039669), 'actor_loss': np.float64(-0.9910535633563995), 'hyper_actor_loss': np.float64(4.713625889962713e-06), 'behavior_loss': np.float64(0.2980662494897842)}

Episode step 24770, time diff 3.3035409450531006, total time dif 3795.811639547348)
step: 24770 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(3.3583317), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06106451898813248), 'actor_loss': np.float64(-0.9805511593818664), 'hyper_actor_loss': np.float64(5.138523056302801e-06), 'behavior_loss': np.float64(0.3056552141904831)}

Episode step 24780, time diff 3.3594772815704346, total time dif 3799.115180492401)
step: 24780 @ episode report: {'average_total_reward': np.float32(8.863333), 'reward_variance': np.float32(2.4455814), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060126278176903726), 'actor_loss': np.float64(-0.9714943051338196), 'hyper_actor_loss': np.float64(4.235968867760676e-06), 'behavior_loss': np.float64(0.30380443334579466)}

Episode step 24790, time diff 3.3313605785369873, total time dif 3802.4746577739716)
step: 24790 @ episode report: {'average_total_reward': np.float32(10.548889), 'reward_variance': np.float32(1.8108934), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05959982722997666), 'actor_loss': np.float64(-0.969262319803238), 'hyper_actor_loss': np.float64(3.7865986314500334e-06), 'behavior_loss': np.float64(0.2890544682741165)}

Episode step 24800, time diff 3.378065586090088, total time dif 3805.8060183525085)
step: 24800 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(3.0716069), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06508000791072846), 'actor_loss': np.float64(-0.9839344322681427), 'hyper_actor_loss': np.float64(6.146867985989957e-06), 'behavior_loss': np.float64(0.30322554111480715)}

Episode step 24810, time diff 3.3740150928497314, total time dif 3809.1840839385986)
step: 24810 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(3.1906188), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055102426558732986), 'actor_loss': np.float64(-0.9788247048854828), 'hyper_actor_loss': np.float64(4.654815143112501e-06), 'behavior_loss': np.float64(0.2971378579735756)}

Episode step 24820, time diff 3.3702287673950195, total time dif 3812.5580990314484)
step: 24820 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.1168449), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07513618301600218), 'actor_loss': np.float64(-0.9688245534896851), 'hyper_actor_loss': np.float64(5.171366888134799e-06), 'behavior_loss': np.float64(0.33013286888599397)}

Episode step 24830, time diff 3.380728244781494, total time dif 3815.9283277988434)
step: 24830 @ episode report: {'average_total_reward': np.float32(8.514444), 'reward_variance': np.float32(2.3738277), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07362963333725929), 'actor_loss': np.float64(-0.9958865404129028), 'hyper_actor_loss': np.float64(6.540844105984434e-06), 'behavior_loss': np.float64(0.31002218574285506)}

Episode step 24840, time diff 3.3770368099212646, total time dif 3819.309056043625)
step: 24840 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(2.477582), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05096008665859699), 'actor_loss': np.float64(-0.9845285296440125), 'hyper_actor_loss': np.float64(4.8797769977682036e-06), 'behavior_loss': np.float64(0.30750555694103243)}

Episode step 24850, time diff 3.373354196548462, total time dif 3822.686092853546)
step: 24850 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(1.9505678), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06726861372590065), 'actor_loss': np.float64(-0.9684588015079498), 'hyper_actor_loss': np.float64(4.733592913908069e-06), 'behavior_loss': np.float64(0.29719057083129885)}

Episode step 24860, time diff 3.4025161266326904, total time dif 3826.0594470500946)
step: 24860 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(0.61200505), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08071278929710388), 'actor_loss': np.float64(-0.9904223382472992), 'hyper_actor_loss': np.float64(4.128235980260797e-06), 'behavior_loss': np.float64(0.307553768157959)}

Episode step 24870, time diff 3.362664222717285, total time dif 3829.4619631767273)
step: 24870 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(3.4769828), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05611424930393696), 'actor_loss': np.float64(-0.9953356146812439), 'hyper_actor_loss': np.float64(5.162151728654862e-06), 'behavior_loss': np.float64(0.29624468386173247)}

Episode step 24880, time diff 3.3830106258392334, total time dif 3832.8246273994446)
step: 24880 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(0.84840024), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06820717081427574), 'actor_loss': np.float64(-0.9641866624355316), 'hyper_actor_loss': np.float64(5.0614491783562695e-06), 'behavior_loss': np.float64(0.3163294941186905)}

Episode step 24890, time diff 3.5532093048095703, total time dif 3836.207638025284)
step: 24890 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.1003649), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06268064081668853), 'actor_loss': np.float64(-0.9741428613662719), 'hyper_actor_loss': np.float64(4.139739019137778e-06), 'behavior_loss': np.float64(0.2733014538884163)}

Episode step 24900, time diff 3.3689353466033936, total time dif 3839.7608473300934)
step: 24900 @ episode report: {'average_total_reward': np.float32(8.926667), 'reward_variance': np.float32(2.0040295), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06347845327109099), 'actor_loss': np.float64(-0.9947147190570831), 'hyper_actor_loss': np.float64(4.45234279595752e-06), 'behavior_loss': np.float64(0.2963168442249298)}

Episode step 24910, time diff 3.3607988357543945, total time dif 3843.129782676697)
step: 24910 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(5.39336), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06178205497562885), 'actor_loss': np.float64(-0.9831075489521026), 'hyper_actor_loss': np.float64(4.403354591886455e-06), 'behavior_loss': np.float64(0.2974210798740387)}

Episode step 24920, time diff 3.394012689590454, total time dif 3846.490581512451)
step: 24920 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(1.6032298), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07657710537314415), 'actor_loss': np.float64(-0.9810390710830689), 'hyper_actor_loss': np.float64(5.186384987609926e-06), 'behavior_loss': np.float64(0.29163295328617095)}

Episode step 24930, time diff 3.3538830280303955, total time dif 3849.8845942020416)
step: 24930 @ episode report: {'average_total_reward': np.float32(8.477778), 'reward_variance': np.float32(2.0693824), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0711514674127102), 'actor_loss': np.float64(-1.0112579941749573), 'hyper_actor_loss': np.float64(5.6650098940735916e-06), 'behavior_loss': np.float64(0.2924507111310959)}

Episode step 24940, time diff 3.369619369506836, total time dif 3853.238477230072)
step: 24940 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(2.2203898), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0615726750344038), 'actor_loss': np.float64(-0.9822940051555633), 'hyper_actor_loss': np.float64(5.519866522263328e-06), 'behavior_loss': np.float64(0.29858916699886323)}

Episode step 24950, time diff 3.377748966217041, total time dif 3856.608096599579)
step: 24950 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(2.3902078), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06956081856042147), 'actor_loss': np.float64(-0.97890585064888), 'hyper_actor_loss': np.float64(5.381663640946499e-06), 'behavior_loss': np.float64(0.29221377670764925)}

Episode step 24960, time diff 3.3767783641815186, total time dif 3859.985845565796)
step: 24960 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(4.8007517), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.081178093329072), 'actor_loss': np.float64(-1.0063936233520507), 'hyper_actor_loss': np.float64(5.555760458264558e-06), 'behavior_loss': np.float64(0.3014396637678146)}

Episode step 24970, time diff 3.379753589630127, total time dif 3863.3626239299774)
step: 24970 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(1.321735), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057857639342546466), 'actor_loss': np.float64(-0.9951707124710083), 'hyper_actor_loss': np.float64(3.7938902096357196e-06), 'behavior_loss': np.float64(0.30371988713741305)}

Episode step 24980, time diff 3.379025459289551, total time dif 3866.7423775196075)
step: 24980 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(3.0788937), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059605265781283376), 'actor_loss': np.float64(-0.9477839708328247), 'hyper_actor_loss': np.float64(5.099437839817256e-06), 'behavior_loss': np.float64(0.29951035380363467)}

Episode step 24990, time diff 3.3756604194641113, total time dif 3870.121402978897)
step: 24990 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(4.930995), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06488465406000614), 'actor_loss': np.float64(-0.979002571105957), 'hyper_actor_loss': np.float64(5.326182531462109e-06), 'behavior_loss': np.float64(0.2987307995557785)}

Episode step 25000, time diff 3.3854525089263916, total time dif 3873.497063398361)
step: 25000 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.0396652), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07772721536457539), 'actor_loss': np.float64(-0.993774539232254), 'hyper_actor_loss': np.float64(5.814025507788756e-06), 'behavior_loss': np.float64(0.28580841422080994)}

Episode step 25010, time diff 3.3998773097991943, total time dif 3876.8825159072876)
step: 25010 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(3.2121747), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0746740810573101), 'actor_loss': np.float64(-0.994887912273407), 'hyper_actor_loss': np.float64(5.418705086412956e-06), 'behavior_loss': np.float64(0.3076640874147415)}

Episode step 25020, time diff 3.410057544708252, total time dif 3880.282393217087)
step: 25020 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(4.7552404), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0705486461520195), 'actor_loss': np.float64(-0.9950774669647217), 'hyper_actor_loss': np.float64(4.390879394122749e-06), 'behavior_loss': np.float64(0.29950724840164183)}

Episode step 25030, time diff 3.419346332550049, total time dif 3883.692450761795)
step: 25030 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.1657238), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058970732614398), 'actor_loss': np.float64(-0.9689676940441132), 'hyper_actor_loss': np.float64(5.710444997930608e-06), 'behavior_loss': np.float64(0.2962177276611328)}

Episode step 25040, time diff 3.430619239807129, total time dif 3887.111797094345)
step: 25040 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(1.1823077), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06774404086172581), 'actor_loss': np.float64(-0.9701396882534027), 'hyper_actor_loss': np.float64(6.0316751842037775e-06), 'behavior_loss': np.float64(0.28522860407829287)}

Episode step 25050, time diff 3.3797385692596436, total time dif 3890.542416334152)
step: 25050 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(1.8248692), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05483862310647965), 'actor_loss': np.float64(-0.9875075101852417), 'hyper_actor_loss': np.float64(4.7676240001237605e-06), 'behavior_loss': np.float64(0.2978767305612564)}

Episode step 25060, time diff 3.5702526569366455, total time dif 3893.922154903412)
step: 25060 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(2.002258), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06254600547254086), 'actor_loss': np.float64(-0.9642374455928803), 'hyper_actor_loss': np.float64(5.2236839565011906e-06), 'behavior_loss': np.float64(0.2792304754257202)}

Episode step 25070, time diff 3.396277666091919, total time dif 3897.4924075603485)
step: 25070 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(8.3401785), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06999381147325039), 'actor_loss': np.float64(-0.9840905666351318), 'hyper_actor_loss': np.float64(5.284656731419091e-06), 'behavior_loss': np.float64(0.29388095140457154)}

Episode step 25080, time diff 3.4380202293395996, total time dif 3900.8886852264404)
step: 25080 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(2.0366778), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05853163711726665), 'actor_loss': np.float64(-0.9890539288520813), 'hyper_actor_loss': np.float64(4.789957483808394e-06), 'behavior_loss': np.float64(0.28020207285881044)}

Episode step 25090, time diff 3.4243557453155518, total time dif 3904.32670545578)
step: 25090 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(3.7520993), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0729343805462122), 'actor_loss': np.float64(-0.9756789147853852), 'hyper_actor_loss': np.float64(4.795498898602091e-06), 'behavior_loss': np.float64(0.2888031035661697)}

Episode step 25100, time diff 3.616455554962158, total time dif 3907.7510612010956)
step: 25100 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.4501495), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05194885153323412), 'actor_loss': np.float64(-0.9766438245773316), 'hyper_actor_loss': np.float64(3.846990796319005e-06), 'behavior_loss': np.float64(0.2891663148999214)}

Episode step 25110, time diff 3.555135488510132, total time dif 3911.3675167560577)
step: 25110 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(1.8359709), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06870990358293057), 'actor_loss': np.float64(-0.9752335786819458), 'hyper_actor_loss': np.float64(4.100899889181164e-06), 'behavior_loss': np.float64(0.2901609271764755)}

Episode step 25120, time diff 3.5711076259613037, total time dif 3914.922652244568)
step: 25120 @ episode report: {'average_total_reward': np.float32(8.875556), 'reward_variance': np.float32(4.1775527), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05523309074342251), 'actor_loss': np.float64(-0.980699074268341), 'hyper_actor_loss': np.float64(4.048796472488902e-06), 'behavior_loss': np.float64(0.2915082097053528)}

Episode step 25130, time diff 3.507551670074463, total time dif 3918.493759870529)
step: 25130 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(5.6616), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07443691045045853), 'actor_loss': np.float64(-0.9676697373390197), 'hyper_actor_loss': np.float64(4.8570881290288526e-06), 'behavior_loss': np.float64(0.2962188541889191)}

Episode step 25140, time diff 3.6922237873077393, total time dif 3922.0013115406036)
step: 25140 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(4.0978765), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07022535279393197), 'actor_loss': np.float64(-0.9980380535125732), 'hyper_actor_loss': np.float64(4.637714437194518e-06), 'behavior_loss': np.float64(0.28405571579933164)}

Episode step 25150, time diff 3.5509328842163086, total time dif 3925.6935353279114)
step: 25150 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(2.6101744), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08139024078845977), 'actor_loss': np.float64(-1.0011460840702058), 'hyper_actor_loss': np.float64(4.224241661177075e-06), 'behavior_loss': np.float64(0.31570101976394654)}

Episode step 25160, time diff 3.701551914215088, total time dif 3929.2444682121277)
step: 25160 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(3.275908), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06762926541268825), 'actor_loss': np.float64(-0.9807777106761932), 'hyper_actor_loss': np.float64(4.163817061453301e-06), 'behavior_loss': np.float64(0.29219346344470976)}

Episode step 25170, time diff 3.6705751419067383, total time dif 3932.946020126343)
step: 25170 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(1.7949345), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07246049605309964), 'actor_loss': np.float64(-0.9885045647621155), 'hyper_actor_loss': np.float64(4.492815128287475e-06), 'behavior_loss': np.float64(0.29110147356987)}

Episode step 25180, time diff 3.673118829727173, total time dif 3936.6165952682495)
step: 25180 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(3.33648), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07575134783983231), 'actor_loss': np.float64(-0.9899200916290283), 'hyper_actor_loss': np.float64(3.7593747492792318e-06), 'behavior_loss': np.float64(0.2944514244794846)}

Episode step 25190, time diff 3.57025408744812, total time dif 3940.2897140979767)
step: 25190 @ episode report: {'average_total_reward': np.float32(9.561111), 'reward_variance': np.float32(5.119661), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06055856943130493), 'actor_loss': np.float64(-0.9903019189834594), 'hyper_actor_loss': np.float64(4.277730658941436e-06), 'behavior_loss': np.float64(0.28080043792724607)}

Episode step 25200, time diff 3.633469343185425, total time dif 3943.859968185425)
step: 25200 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(2.4805696), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07215653173625469), 'actor_loss': np.float64(-0.98561310172081), 'hyper_actor_loss': np.float64(3.98982590468222e-06), 'behavior_loss': np.float64(0.3094360888004303)}

Episode step 25210, time diff 3.5648281574249268, total time dif 3947.4934375286102)
step: 25210 @ episode report: {'average_total_reward': np.float32(8.202223), 'reward_variance': np.float32(1.4273535), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0688160628080368), 'actor_loss': np.float64(-0.9882481873035431), 'hyper_actor_loss': np.float64(3.938631220989919e-06), 'behavior_loss': np.float64(0.2935494750738144)}

Episode step 25220, time diff 3.794856548309326, total time dif 3951.058265686035)
step: 25220 @ episode report: {'average_total_reward': np.float32(9.551111), 'reward_variance': np.float32(2.1718323), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06671485900878907), 'actor_loss': np.float64(-0.9754664242267609), 'hyper_actor_loss': np.float64(3.3341062589897775e-06), 'behavior_loss': np.float64(0.3088818520307541)}

Episode step 25230, time diff 3.715620279312134, total time dif 3954.8531222343445)
step: 25230 @ episode report: {'average_total_reward': np.float32(10.422222), 'reward_variance': np.float32(2.312865), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06590423211455346), 'actor_loss': np.float64(-0.9816425323486329), 'hyper_actor_loss': np.float64(3.3522872399771584e-06), 'behavior_loss': np.float64(0.28803934454917907)}

Episode step 25240, time diff 4.012974500656128, total time dif 3958.5687425136566)
step: 25240 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.548811), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06639160737395286), 'actor_loss': np.float64(-0.9871066093444825), 'hyper_actor_loss': np.float64(3.3089182352341597e-06), 'behavior_loss': np.float64(0.2915449127554893)}

Episode step 25250, time diff 3.7440760135650635, total time dif 3962.5817170143127)
step: 25250 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(1.4664644), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06871104501187801), 'actor_loss': np.float64(-0.9778099238872529), 'hyper_actor_loss': np.float64(3.678597579437337e-06), 'behavior_loss': np.float64(0.3025376111268997)}

Episode step 25260, time diff 3.7459263801574707, total time dif 3966.325793027878)
step: 25260 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.8585389), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07546978518366813), 'actor_loss': np.float64(-0.9828101873397828), 'hyper_actor_loss': np.float64(3.852376312352135e-06), 'behavior_loss': np.float64(0.32158229053020476)}

Episode step 25270, time diff 3.719597816467285, total time dif 3970.0717194080353)
step: 25270 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.6909585), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06580748334527016), 'actor_loss': np.float64(-0.9895071744918823), 'hyper_actor_loss': np.float64(4.363618222669174e-06), 'behavior_loss': np.float64(0.28956047594547274)}

Episode step 25280, time diff 3.699000358581543, total time dif 3973.7913172245026)
step: 25280 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(4.059847), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07327215448021888), 'actor_loss': np.float64(-0.9834843635559082), 'hyper_actor_loss': np.float64(4.011982127849478e-06), 'behavior_loss': np.float64(0.3054278552532196)}

Episode step 25290, time diff 3.7617974281311035, total time dif 3977.490317583084)
step: 25290 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(2.365384), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07448561005294323), 'actor_loss': np.float64(-0.9865508139133453), 'hyper_actor_loss': np.float64(3.340913667670975e-06), 'behavior_loss': np.float64(0.30172319412231446)}

Episode step 25300, time diff 3.729912519454956, total time dif 3981.252115011215)
step: 25300 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(0.77790755), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06330233253538609), 'actor_loss': np.float64(-0.9835934102535248), 'hyper_actor_loss': np.float64(4.488457921070221e-06), 'behavior_loss': np.float64(0.28422847092151643)}

Episode step 25310, time diff 3.6785671710968018, total time dif 3984.98202753067)
step: 25310 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(1.080816), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07492787390947342), 'actor_loss': np.float64(-0.9833465576171875), 'hyper_actor_loss': np.float64(3.10729669763532e-06), 'behavior_loss': np.float64(0.3064552336931229)}

Episode step 25320, time diff 3.685981035232544, total time dif 3988.660594701767)
step: 25320 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(1.5681283), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05835666991770268), 'actor_loss': np.float64(-0.9729853749275208), 'hyper_actor_loss': np.float64(2.4742230948504584e-06), 'behavior_loss': np.float64(0.3083716094493866)}

Episode step 25330, time diff 3.710247039794922, total time dif 3992.3465757369995)
step: 25330 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(2.3515763), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07588496431708336), 'actor_loss': np.float64(-0.9676969826221467), 'hyper_actor_loss': np.float64(2.7181754376215395e-06), 'behavior_loss': np.float64(0.2921630278229713)}

Episode step 25340, time diff 3.658477544784546, total time dif 3996.0568227767944)
step: 25340 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(3.2544513), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06583336032927037), 'actor_loss': np.float64(-1.0045761168003082), 'hyper_actor_loss': np.float64(3.220551309368602e-06), 'behavior_loss': np.float64(0.29870463609695436)}

Episode step 25350, time diff 3.6438634395599365, total time dif 3999.715300321579)
step: 25350 @ episode report: {'average_total_reward': np.float32(9.873335), 'reward_variance': np.float32(1.6469431), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06879839934408664), 'actor_loss': np.float64(-0.9687653720378876), 'hyper_actor_loss': np.float64(2.8284613676987647e-06), 'behavior_loss': np.float64(0.30197018980979917)}

Episode step 25360, time diff 3.715144395828247, total time dif 4003.359163761139)
step: 25360 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(4.043576), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05584032889455557), 'actor_loss': np.float64(-0.9605562329292298), 'hyper_actor_loss': np.float64(2.526777836919791e-06), 'behavior_loss': np.float64(0.3120861917734146)}

Episode step 25370, time diff 3.631166696548462, total time dif 4007.074308156967)
step: 25370 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(3.3926125), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06556859016418456), 'actor_loss': np.float64(-0.9769693195819855), 'hyper_actor_loss': np.float64(2.87291793483746e-06), 'behavior_loss': np.float64(0.2954473078250885)}

Episode step 25380, time diff 3.669757604598999, total time dif 4010.7054748535156)
step: 25380 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(1.0009131), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.064918627217412), 'actor_loss': np.float64(-0.9761856734752655), 'hyper_actor_loss': np.float64(2.499680931578041e-06), 'behavior_loss': np.float64(0.30171950459480285)}

Episode step 25390, time diff 3.8351480960845947, total time dif 4014.3752324581146)
step: 25390 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(3.026104), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06860150471329689), 'actor_loss': np.float64(-0.9747761249542236), 'hyper_actor_loss': np.float64(2.3601143539053735e-06), 'behavior_loss': np.float64(0.2992907851934433)}

Episode step 25400, time diff 3.6748766899108887, total time dif 4018.210380554199)
step: 25400 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(1.4730628), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06851680390536785), 'actor_loss': np.float64(-0.9837081432342529), 'hyper_actor_loss': np.float64(2.7624113954516362e-06), 'behavior_loss': np.float64(0.2863219350576401)}

Episode step 25410, time diff 3.8833842277526855, total time dif 4021.88525724411)
step: 25410 @ episode report: {'average_total_reward': np.float32(8.851112), 'reward_variance': np.float32(3.3712401), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06934156864881516), 'actor_loss': np.float64(-0.9835584402084351), 'hyper_actor_loss': np.float64(2.4889483938750347e-06), 'behavior_loss': np.float64(0.2957996651530266)}

Episode step 25420, time diff 3.72746205329895, total time dif 4025.768641471863)
step: 25420 @ episode report: {'average_total_reward': np.float32(8.875556), 'reward_variance': np.float32(3.1151805), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07394571937620639), 'actor_loss': np.float64(-0.9950572848320007), 'hyper_actor_loss': np.float64(2.3146210992308625e-06), 'behavior_loss': np.float64(0.29319644570350645)}

Episode step 25430, time diff 3.734591245651245, total time dif 4029.4961035251617)
step: 25430 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(1.3168592), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060390134900808336), 'actor_loss': np.float64(-0.9904891729354859), 'hyper_actor_loss': np.float64(2.338996898743062e-06), 'behavior_loss': np.float64(0.294904363155365)}

Episode step 25440, time diff 3.510145902633667, total time dif 4033.230694770813)
step: 25440 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(2.171632), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061225568503141405), 'actor_loss': np.float64(-0.9760702729225159), 'hyper_actor_loss': np.float64(4.851545236306265e-06), 'behavior_loss': np.float64(0.29333305507898333)}

Episode step 25450, time diff 3.6725218296051025, total time dif 4036.7408406734467)
step: 25450 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.8547958), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0798923421651125), 'actor_loss': np.float64(-0.9690893888473511), 'hyper_actor_loss': np.float64(1.5136259207793046e-05), 'behavior_loss': np.float64(0.3126825660467148)}

Episode step 25460, time diff 3.596975326538086, total time dif 4040.4133625030518)
step: 25460 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(3.37285), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07192983776330948), 'actor_loss': np.float64(-0.9817975282669067), 'hyper_actor_loss': np.float64(2.2484534019895365e-05), 'behavior_loss': np.float64(0.30066782534122466)}

Episode step 25470, time diff 3.496769905090332, total time dif 4044.01033782959)
step: 25470 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(4.067444), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05588471181690693), 'actor_loss': np.float64(-0.9717385113239289), 'hyper_actor_loss': np.float64(1.0095572315549362e-05), 'behavior_loss': np.float64(0.30842943787574767)}

Episode step 25480, time diff 3.5547258853912354, total time dif 4047.50710773468)
step: 25480 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(3.261641), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05902542918920517), 'actor_loss': np.float64(-0.9564536869525909), 'hyper_actor_loss': np.float64(4.7432065002794845e-06), 'behavior_loss': np.float64(0.28547863811254504)}

Episode step 25490, time diff 3.558151960372925, total time dif 4051.0618336200714)
step: 25490 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(1.6034119), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06693985536694527), 'actor_loss': np.float64(-0.9910983741283417), 'hyper_actor_loss': np.float64(3.848962091979047e-06), 'behavior_loss': np.float64(0.29307176172733307)}

Episode step 25500, time diff 3.4999210834503174, total time dif 4054.6199855804443)
step: 25500 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(4.0904894), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06456370651721954), 'actor_loss': np.float64(-0.983402955532074), 'hyper_actor_loss': np.float64(4.983723010809627e-06), 'behavior_loss': np.float64(0.2998748540878296)}

Episode step 25510, time diff 3.499955892562866, total time dif 4058.1199066638947)
step: 25510 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(1.5928007), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08053560554981232), 'actor_loss': np.float64(-0.9748679816722869), 'hyper_actor_loss': np.float64(4.135660287829524e-06), 'behavior_loss': np.float64(0.29972225725650786)}

Episode step 25520, time diff 3.4944684505462646, total time dif 4061.6198625564575)
step: 25520 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.8455067), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05915095135569572), 'actor_loss': np.float64(-0.9894542336463928), 'hyper_actor_loss': np.float64(3.0964350798967644e-06), 'behavior_loss': np.float64(0.29825046062469485)}

Episode step 25530, time diff 3.41111421585083, total time dif 4065.114331007004)
step: 25530 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(1.9627358), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06254944950342178), 'actor_loss': np.float64(-0.9633719503879548), 'hyper_actor_loss': np.float64(3.3403771340090316e-06), 'behavior_loss': np.float64(0.3024334356188774)}

Episode step 25540, time diff 3.458613872528076, total time dif 4068.5254452228546)
step: 25540 @ episode report: {'average_total_reward': np.float32(10.485557), 'reward_variance': np.float32(2.2873344), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07061459571123123), 'actor_loss': np.float64(-0.9784338295459747), 'hyper_actor_loss': np.float64(2.956944854304311e-06), 'behavior_loss': np.float64(0.2948086142539978)}

Episode step 25550, time diff 3.4423880577087402, total time dif 4071.9840590953827)
step: 25550 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(3.2769928), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06994912922382354), 'actor_loss': np.float64(-1.001282423734665), 'hyper_actor_loss': np.float64(3.2001674981074757e-06), 'behavior_loss': np.float64(0.2923987299203873)}

Episode step 25560, time diff 3.574917793273926, total time dif 4075.4264471530914)
step: 25560 @ episode report: {'average_total_reward': np.float32(9.324446), 'reward_variance': np.float32(1.503674), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0709856878966093), 'actor_loss': np.float64(-0.9936259508132934), 'hyper_actor_loss': np.float64(3.1187251806841233e-06), 'behavior_loss': np.float64(0.29583223164081573)}

Episode step 25570, time diff 3.4368338584899902, total time dif 4079.0013649463654)
step: 25570 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(5.0615854), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0703007947653532), 'actor_loss': np.float64(-0.9650901913642883), 'hyper_actor_loss': np.float64(3.1122999871513456e-06), 'behavior_loss': np.float64(0.30161512196063994)}

Episode step 25580, time diff 3.4171416759490967, total time dif 4082.4381988048553)
step: 25580 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.722351), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07208900675177574), 'actor_loss': np.float64(-0.9878167510032654), 'hyper_actor_loss': np.float64(2.972649758703483e-06), 'behavior_loss': np.float64(0.27938721179962156)}

Episode step 25590, time diff 3.442883253097534, total time dif 4085.8553404808044)
step: 25590 @ episode report: {'average_total_reward': np.float32(9.451113), 'reward_variance': np.float32(4.790129), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.069462014362216), 'actor_loss': np.float64(-1.0012707710266113), 'hyper_actor_loss': np.float64(3.5529293199942914e-06), 'behavior_loss': np.float64(0.3130359411239624)}

Episode step 25600, time diff 3.4489235877990723, total time dif 4089.298223733902)
step: 25600 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(1.214983), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06295076925307512), 'actor_loss': np.float64(-0.9731238484382629), 'hyper_actor_loss': np.float64(3.4204930898340534e-06), 'behavior_loss': np.float64(0.28923421502113345)}

Episode step 25610, time diff 3.421206474304199, total time dif 4092.747147321701)
step: 25610 @ episode report: {'average_total_reward': np.float32(9.787779), 'reward_variance': np.float32(5.3029747), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07026577442884445), 'actor_loss': np.float64(-0.9794836163520813), 'hyper_actor_loss': np.float64(3.878450638694631e-06), 'behavior_loss': np.float64(0.3008430153131485)}

Episode step 25620, time diff 3.44982647895813, total time dif 4096.168353796005)
step: 25620 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(1.837457), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061164118349552155), 'actor_loss': np.float64(-0.9718569397926331), 'hyper_actor_loss': np.float64(3.75205081581953e-06), 'behavior_loss': np.float64(0.3013883769512177)}

Episode step 25630, time diff 3.4085850715637207, total time dif 4099.618180274963)
step: 25630 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(3.1429398), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06930992864072323), 'actor_loss': np.float64(-0.9772391378879547), 'hyper_actor_loss': np.float64(3.8681406749674355e-06), 'behavior_loss': np.float64(0.30175295621156695)}

Episode step 25640, time diff 3.461881399154663, total time dif 4103.026765346527)
step: 25640 @ episode report: {'average_total_reward': np.float32(9.387779), 'reward_variance': np.float32(2.5748258), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06926409862935543), 'actor_loss': np.float64(-0.9796808302402497), 'hyper_actor_loss': np.float64(4.104859931430837e-06), 'behavior_loss': np.float64(0.30434343218803406)}

Episode step 25650, time diff 3.445192813873291, total time dif 4106.488646745682)
step: 25650 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(1.143883), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07310837469995021), 'actor_loss': np.float64(-0.9916257798671723), 'hyper_actor_loss': np.float64(4.053787506563822e-06), 'behavior_loss': np.float64(0.30607170462608335)}

Episode step 25660, time diff 3.428283452987671, total time dif 4109.933839559555)
step: 25660 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(1.9743469), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07099457792937755), 'actor_loss': np.float64(-0.9909377336502075), 'hyper_actor_loss': np.float64(3.925923761016748e-06), 'behavior_loss': np.float64(0.2930021703243256)}

Episode step 25670, time diff 3.3994522094726562, total time dif 4113.362123012543)
step: 25670 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(1.942406), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07275154516100883), 'actor_loss': np.float64(-0.9898733913898468), 'hyper_actor_loss': np.float64(4.311604061513208e-06), 'behavior_loss': np.float64(0.29083352386951444)}

Episode step 25680, time diff 3.3948075771331787, total time dif 4116.761575222015)
step: 25680 @ episode report: {'average_total_reward': np.float32(10.112223), 'reward_variance': np.float32(1.2856414), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05726398807018995), 'actor_loss': np.float64(-0.9778534352779389), 'hyper_actor_loss': np.float64(4.127273314225022e-06), 'behavior_loss': np.float64(0.2905599355697632)}

Episode step 25690, time diff 3.44476056098938, total time dif 4120.156382799149)
step: 25690 @ episode report: {'average_total_reward': np.float32(10.61), 'reward_variance': np.float32(1.655542), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06907109692692756), 'actor_loss': np.float64(-0.9656985402107239), 'hyper_actor_loss': np.float64(3.65607418189029e-06), 'behavior_loss': np.float64(0.29361161291599275)}

Episode step 25700, time diff 3.3418943881988525, total time dif 4123.601143360138)
step: 25700 @ episode report: {'average_total_reward': np.float32(9.190001), 'reward_variance': np.float32(1.822579), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.166667), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0726998832076788), 'actor_loss': np.float64(-0.9950591027736664), 'hyper_actor_loss': np.float64(3.938256645596993e-06), 'behavior_loss': np.float64(0.2913791000843048)}

Episode step 25710, time diff 3.4067256450653076, total time dif 4126.943037748337)
step: 25710 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(1.9992707), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0694847060367465), 'actor_loss': np.float64(-0.990438061952591), 'hyper_actor_loss': np.float64(3.8915050481591605e-06), 'behavior_loss': np.float64(0.29996212720870974)}

Episode step 25720, time diff 3.5663645267486572, total time dif 4130.349763393402)
step: 25720 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(3.2401252), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06320326551795005), 'actor_loss': np.float64(-0.9715718448162078), 'hyper_actor_loss': np.float64(3.6024119708599756e-06), 'behavior_loss': np.float64(0.30603182315826416)}

Episode step 25730, time diff 3.3913116455078125, total time dif 4133.916127920151)
step: 25730 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(4.686494), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07758623585104943), 'actor_loss': np.float64(-0.9801759660243988), 'hyper_actor_loss': np.float64(3.790353866861551e-06), 'behavior_loss': np.float64(0.3185009270906448)}

Episode step 25740, time diff 3.429640769958496, total time dif 4137.307439565659)
step: 25740 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(6.424688), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07116653844714164), 'actor_loss': np.float64(-0.9764692664146424), 'hyper_actor_loss': np.float64(3.5975380342279094e-06), 'behavior_loss': np.float64(0.30959161520004275)}

Episode step 25750, time diff 3.479081869125366, total time dif 4140.737080335617)
step: 25750 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(0.76732004), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06930494271218776), 'actor_loss': np.float64(-0.9834476709365845), 'hyper_actor_loss': np.float64(3.671181639219867e-06), 'behavior_loss': np.float64(0.3030631512403488)}

Episode step 25760, time diff 3.3807852268218994, total time dif 4144.216162204742)
step: 25760 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(0.71861696), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059933296591043475), 'actor_loss': np.float64(-0.9777274906635285), 'hyper_actor_loss': np.float64(3.955490115004068e-06), 'behavior_loss': np.float64(0.2952534079551697)}

Episode step 25770, time diff 3.405728340148926, total time dif 4147.596947431564)
step: 25770 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(2.0680003), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06472360417246818), 'actor_loss': np.float64(-0.9672919809818268), 'hyper_actor_loss': np.float64(3.629279876804503e-06), 'behavior_loss': np.float64(0.3050662338733673)}

Episode step 25780, time diff 3.476093292236328, total time dif 4151.002675771713)
step: 25780 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(1.7606423), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06615143232047557), 'actor_loss': np.float64(-0.9722321331501007), 'hyper_actor_loss': np.float64(3.3798552976804787e-06), 'behavior_loss': np.float64(0.30177572667598723)}

Episode step 25790, time diff 3.3886265754699707, total time dif 4154.47876906395)
step: 25790 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(1.6923908), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07034696377813816), 'actor_loss': np.float64(-0.9957122206687927), 'hyper_actor_loss': np.float64(3.472374828561442e-06), 'behavior_loss': np.float64(0.2926725268363953)}

Episode step 25800, time diff 3.3860106468200684, total time dif 4157.86739563942)
step: 25800 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(2.8738961), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0630204152315855), 'actor_loss': np.float64(-0.9664125919342041), 'hyper_actor_loss': np.float64(4.034923790641187e-06), 'behavior_loss': np.float64(0.30980821549892423)}

Episode step 25810, time diff 3.4514713287353516, total time dif 4161.25340628624)
step: 25810 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(3.2187476), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06541674081236123), 'actor_loss': np.float64(-0.9657460153102875), 'hyper_actor_loss': np.float64(3.5345666901775983e-06), 'behavior_loss': np.float64(0.2953131109476089)}

Episode step 25820, time diff 3.418618679046631, total time dif 4164.704877614975)
step: 25820 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(4.9304004), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0690391194075346), 'actor_loss': np.float64(-0.987875509262085), 'hyper_actor_loss': np.float64(3.4615791946635e-06), 'behavior_loss': np.float64(0.303632116317749)}

Episode step 25830, time diff 3.4110870361328125, total time dif 4168.123496294022)
step: 25830 @ episode report: {'average_total_reward': np.float32(9.8977785), 'reward_variance': np.float32(4.114539), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08749643824994564), 'actor_loss': np.float64(-0.9882409155368805), 'hyper_actor_loss': np.float64(3.3795860417740186e-06), 'behavior_loss': np.float64(0.3023497611284256)}

Episode step 25840, time diff 3.4840128421783447, total time dif 4171.534583330154)
step: 25840 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(2.409042), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06438950560986996), 'actor_loss': np.float64(-1.0066636741161346), 'hyper_actor_loss': np.float64(3.2390956221206578e-06), 'behavior_loss': np.float64(0.2963493436574936)}

Episode step 25850, time diff 3.464289903640747, total time dif 4175.018596172333)
step: 25850 @ episode report: {'average_total_reward': np.float32(8.814445), 'reward_variance': np.float32(3.087952), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0505514744669199), 'actor_loss': np.float64(-0.9574227809906006), 'hyper_actor_loss': np.float64(3.7057948247820605e-06), 'behavior_loss': np.float64(0.295114329457283)}

Episode step 25860, time diff 3.4515180587768555, total time dif 4178.4828860759735)
step: 25860 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(1.4371865), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06696553081274033), 'actor_loss': np.float64(-0.9454280436038971), 'hyper_actor_loss': np.float64(3.398019862288493e-06), 'behavior_loss': np.float64(0.30192520618438723)}

Episode step 25870, time diff 3.42991304397583, total time dif 4181.93440413475)
step: 25870 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(1.998143), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06975743621587753), 'actor_loss': np.float64(-0.998241662979126), 'hyper_actor_loss': np.float64(3.1865303299127844e-06), 'behavior_loss': np.float64(0.2926362812519073)}

Episode step 25880, time diff 3.584805727005005, total time dif 4185.364317178726)
step: 25880 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(1.228604), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06437349058687687), 'actor_loss': np.float64(-0.9899130165576935), 'hyper_actor_loss': np.float64(2.8132389843449347e-06), 'behavior_loss': np.float64(0.30121115148067473)}

Episode step 25890, time diff 3.4961211681365967, total time dif 4188.949122905731)
step: 25890 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(2.066963), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06474589891731738), 'actor_loss': np.float64(-0.9607967078685761), 'hyper_actor_loss': np.float64(2.9256416610223822e-06), 'behavior_loss': np.float64(0.29858743250370023)}

Episode step 25900, time diff 3.455914258956909, total time dif 4192.445244073868)
step: 25900 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(1.5297339), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.047101786732673644), 'actor_loss': np.float64(-0.962966138124466), 'hyper_actor_loss': np.float64(2.925192484326544e-06), 'behavior_loss': np.float64(0.30533049404621126)}

Episode step 25910, time diff 3.4443869590759277, total time dif 4195.901158332825)
step: 25910 @ episode report: {'average_total_reward': np.float32(9.087779), 'reward_variance': np.float32(2.3508255), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0643832717090845), 'actor_loss': np.float64(-0.9516124665737152), 'hyper_actor_loss': np.float64(2.655079606483923e-06), 'behavior_loss': np.float64(0.30026118755340575)}

Episode step 25920, time diff 3.4809000492095947, total time dif 4199.345545291901)
step: 25920 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(3.3856254), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05930250473320484), 'actor_loss': np.float64(-0.9828203916549683), 'hyper_actor_loss': np.float64(2.43701476847491e-06), 'behavior_loss': np.float64(0.2949208915233612)}

Episode step 25930, time diff 3.4685795307159424, total time dif 4202.82644534111)
step: 25930 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(0.6431174), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058293039444833994), 'actor_loss': np.float64(-0.9761564135551453), 'hyper_actor_loss': np.float64(2.1369137584770214e-06), 'behavior_loss': np.float64(0.3003325194120407)}

Episode step 25940, time diff 3.462578773498535, total time dif 4206.295024871826)
step: 25940 @ episode report: {'average_total_reward': np.float32(9.5244465), 'reward_variance': np.float32(1.9183401), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06196042262017727), 'actor_loss': np.float64(-0.9606235086917877), 'hyper_actor_loss': np.float64(2.13634049259781e-06), 'behavior_loss': np.float64(0.2944694757461548)}

Episode step 25950, time diff 3.516038656234741, total time dif 4209.757603645325)
step: 25950 @ episode report: {'average_total_reward': np.float32(9.800001), 'reward_variance': np.float32(3.4830868), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05985889583826065), 'actor_loss': np.float64(-0.9715811371803283), 'hyper_actor_loss': np.float64(2.0183171727694573e-06), 'behavior_loss': np.float64(0.3041198402643204)}

Episode step 25960, time diff 3.5084455013275146, total time dif 4213.273642301559)
step: 25960 @ episode report: {'average_total_reward': np.float32(10.175556), 'reward_variance': np.float32(2.7901924), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08192569687962532), 'actor_loss': np.float64(-0.9913425624370575), 'hyper_actor_loss': np.float64(1.8042565216092043e-06), 'behavior_loss': np.float64(0.3133403331041336)}

Episode step 25970, time diff 3.524989604949951, total time dif 4216.782087802887)
step: 25970 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(1.6889089), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06366604194045067), 'actor_loss': np.float64(-0.9849925637245178), 'hyper_actor_loss': np.float64(1.9142411247230484e-06), 'behavior_loss': np.float64(0.3004777401685715)}

Episode step 25980, time diff 3.5486509799957275, total time dif 4220.307077407837)
step: 25980 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(5.354845), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08793924078345298), 'actor_loss': np.float64(-0.9876310169696808), 'hyper_actor_loss': np.float64(1.6299101616823464e-06), 'behavior_loss': np.float64(0.30510030686855316)}

Episode step 25990, time diff 3.518296957015991, total time dif 4223.855728387833)
step: 25990 @ episode report: {'average_total_reward': np.float32(8.99), 'reward_variance': np.float32(0.98487484), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.28889), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062471252679824826), 'actor_loss': np.float64(-1.0101490020751953), 'hyper_actor_loss': np.float64(1.6284705907310127e-06), 'behavior_loss': np.float64(0.29122706651687624)}

Episode step 26000, time diff 3.548078775405884, total time dif 4227.374025344849)
step: 26000 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(1.261617), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06541215144097805), 'actor_loss': np.float64(-0.9650191366672516), 'hyper_actor_loss': np.float64(1.657998927839799e-06), 'behavior_loss': np.float64(0.302779558300972)}

Episode step 26010, time diff 3.591904640197754, total time dif 4230.9221041202545)
step: 26010 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(2.0730143), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0753446340560913), 'actor_loss': np.float64(-0.9764793694019318), 'hyper_actor_loss': np.float64(1.67549251273158e-06), 'behavior_loss': np.float64(0.3159682810306549)}

Episode step 26020, time diff 3.5090749263763428, total time dif 4234.514008760452)
step: 26020 @ episode report: {'average_total_reward': np.float32(9.102223), 'reward_variance': np.float32(3.847674), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06963606029748917), 'actor_loss': np.float64(-0.9845002472400666), 'hyper_actor_loss': np.float64(1.6665642249336087e-06), 'behavior_loss': np.float64(0.29781513214111327)}

Episode step 26030, time diff 3.5694751739501953, total time dif 4238.023083686829)
step: 26030 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(3.3716736), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06065838262438774), 'actor_loss': np.float64(-0.9869860947132111), 'hyper_actor_loss': np.float64(1.4070852671466128e-06), 'behavior_loss': np.float64(0.3033968448638916)}

Episode step 26040, time diff 3.7260048389434814, total time dif 4241.592558860779)
step: 26040 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(1.1439126), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06350259482860565), 'actor_loss': np.float64(-0.9626236736774445), 'hyper_actor_loss': np.float64(1.459921168134315e-06), 'behavior_loss': np.float64(0.30964391976594924)}

Episode step 26050, time diff 3.499528646469116, total time dif 4245.318563699722)
step: 26050 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(1.2398074), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07544095553457737), 'actor_loss': np.float64(-0.9706693351268768), 'hyper_actor_loss': np.float64(1.398566672605739e-06), 'behavior_loss': np.float64(0.3001830965280533)}

Episode step 26060, time diff 3.5512142181396484, total time dif 4248.818092346191)
step: 26060 @ episode report: {'average_total_reward': np.float32(9.326667), 'reward_variance': np.float32(1.7618573), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06572094485163689), 'actor_loss': np.float64(-0.9933331191539765), 'hyper_actor_loss': np.float64(1.3726032193517312e-06), 'behavior_loss': np.float64(0.29647610187530515)}

Episode step 26070, time diff 3.5980987548828125, total time dif 4252.369306564331)
step: 26070 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(2.1541493), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06930729039013386), 'actor_loss': np.float64(-0.9937128305435181), 'hyper_actor_loss': np.float64(1.256456420151153e-06), 'behavior_loss': np.float64(0.30239234268665316)}

Episode step 26080, time diff 3.579087972640991, total time dif 4255.967405319214)
step: 26080 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(1.2385236), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054553257673978804), 'actor_loss': np.float64(-0.9672550439834595), 'hyper_actor_loss': np.float64(1.1213932793907588e-06), 'behavior_loss': np.float64(0.3013071835041046)}

Episode step 26090, time diff 3.599385976791382, total time dif 4259.546493291855)
step: 26090 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(4.2871237), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05994975119829178), 'actor_loss': np.float64(-0.9468774676322937), 'hyper_actor_loss': np.float64(1.1578226406072645e-06), 'behavior_loss': np.float64(0.3109581291675568)}

Episode step 26100, time diff 3.5124940872192383, total time dif 4263.145879268646)
step: 26100 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(1.1599615), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06123068034648895), 'actor_loss': np.float64(-0.9709456324577331), 'hyper_actor_loss': np.float64(1.1468907018752362e-06), 'behavior_loss': np.float64(0.3068790346384048)}

Episode step 26110, time diff 3.5579147338867188, total time dif 4266.6583733558655)
step: 26110 @ episode report: {'average_total_reward': np.float32(7.992223), 'reward_variance': np.float32(2.2455318), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05489268042147159), 'actor_loss': np.float64(-0.966293740272522), 'hyper_actor_loss': np.float64(1.1443187815984856e-06), 'behavior_loss': np.float64(0.3048669070005417)}

Episode step 26120, time diff 3.4970266819000244, total time dif 4270.216288089752)
step: 26120 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(1.1347021), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07011820636689663), 'actor_loss': np.float64(-0.9666714608669281), 'hyper_actor_loss': np.float64(1.239371789552024e-06), 'behavior_loss': np.float64(0.31277768313884735)}

Episode step 26130, time diff 3.5108840465545654, total time dif 4273.713314771652)
step: 26130 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(1.1628407), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07332448028028012), 'actor_loss': np.float64(-0.9856879472732544), 'hyper_actor_loss': np.float64(1.1431766324676573e-06), 'behavior_loss': np.float64(0.31016994416713717)}

Episode step 26140, time diff 3.5632078647613525, total time dif 4277.224198818207)
step: 26140 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(1.6742474), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06741792857646942), 'actor_loss': np.float64(-0.9843735516071319), 'hyper_actor_loss': np.float64(1.0869772097521491e-06), 'behavior_loss': np.float64(0.30672208070755)}

Episode step 26150, time diff 3.565826416015625, total time dif 4280.787406682968)
step: 26150 @ episode report: {'average_total_reward': np.float32(9.5244465), 'reward_variance': np.float32(1.6938969), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06006612963974476), 'actor_loss': np.float64(-0.9796564757823945), 'hyper_actor_loss': np.float64(1.1121731233743049e-06), 'behavior_loss': np.float64(0.31202573478221896)}

Episode step 26160, time diff 3.5826988220214844, total time dif 4284.353233098984)
step: 26160 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.1457734), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677381657063961), 'actor_loss': np.float64(-0.9632227778434753), 'hyper_actor_loss': np.float64(1.1044032078189047e-06), 'behavior_loss': np.float64(0.29863156378269196)}

Episode step 26170, time diff 3.5735862255096436, total time dif 4287.935931921005)
step: 26170 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.2210665), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07187890261411667), 'actor_loss': np.float64(-0.9820872366428375), 'hyper_actor_loss': np.float64(1.1590889585022524e-06), 'behavior_loss': np.float64(0.3099001169204712)}

Episode step 26180, time diff 3.5794947147369385, total time dif 4291.509518146515)
step: 26180 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(2.169458), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0539077440276742), 'actor_loss': np.float64(-0.9757224500179291), 'hyper_actor_loss': np.float64(1.1689836753703276e-06), 'behavior_loss': np.float64(0.30916148126125337)}

Episode step 26190, time diff 3.583030939102173, total time dif 4295.089012861252)
step: 26190 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(0.3667224), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0849157940596342), 'actor_loss': np.float64(-0.9608124911785125), 'hyper_actor_loss': np.float64(1.2259810603154619e-06), 'behavior_loss': np.float64(0.3180551499128342)}

Episode step 26200, time diff 3.5757603645324707, total time dif 4298.672043800354)
step: 26200 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(2.5108695), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07318317703902721), 'actor_loss': np.float64(-1.0045760810375213), 'hyper_actor_loss': np.float64(1.3174568152862775e-06), 'behavior_loss': np.float64(0.3028323620557785)}

Episode step 26210, time diff 3.719059944152832, total time dif 4302.2478041648865)
step: 26210 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(1.9941533), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07450001798570156), 'actor_loss': np.float64(-0.9952237427234649), 'hyper_actor_loss': np.float64(1.2003851566078083e-06), 'behavior_loss': np.float64(0.312466162443161)}

Episode step 26220, time diff 3.5764989852905273, total time dif 4305.966864109039)
step: 26220 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(2.6385684), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06434194184839725), 'actor_loss': np.float64(-0.9571353256702423), 'hyper_actor_loss': np.float64(1.2943829915457171e-06), 'behavior_loss': np.float64(0.30753080546855927)}

Episode step 26230, time diff 3.576385498046875, total time dif 4309.54336309433)
step: 26230 @ episode report: {'average_total_reward': np.float32(8.777779), 'reward_variance': np.float32(1.5506424), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07627819031476975), 'actor_loss': np.float64(-0.9784627437591553), 'hyper_actor_loss': np.float64(1.3553826079260034e-06), 'behavior_loss': np.float64(0.2985142856836319)}

Episode step 26240, time diff 3.575511932373047, total time dif 4313.119748592377)
step: 26240 @ episode report: {'average_total_reward': np.float32(10.012223), 'reward_variance': np.float32(4.7523336), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06721919886767865), 'actor_loss': np.float64(-1.0000806629657746), 'hyper_actor_loss': np.float64(1.3788242426926446e-06), 'behavior_loss': np.float64(0.30990349054336547)}

Episode step 26250, time diff 3.6417644023895264, total time dif 4316.69526052475)
step: 26250 @ episode report: {'average_total_reward': np.float32(9.163333), 'reward_variance': np.float32(2.5873108), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07362180929630995), 'actor_loss': np.float64(-0.9853082716464996), 'hyper_actor_loss': np.float64(1.3939678524366173e-06), 'behavior_loss': np.float64(0.30484165251255035)}

Episode step 26260, time diff 3.6682469844818115, total time dif 4320.337024927139)
step: 26260 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(2.694471), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05897659100592136), 'actor_loss': np.float64(-0.9753170609474182), 'hyper_actor_loss': np.float64(1.362741784305399e-06), 'behavior_loss': np.float64(0.29440472424030306)}

Episode step 26270, time diff 3.611006259918213, total time dif 4324.005271911621)
step: 26270 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.1552297), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05461802314966917), 'actor_loss': np.float64(-0.9620785713195801), 'hyper_actor_loss': np.float64(1.2692967231942021e-06), 'behavior_loss': np.float64(0.2952577084302902)}

Episode step 26280, time diff 3.7177722454071045, total time dif 4327.616278171539)
step: 26280 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(2.3029091), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06637836173176766), 'actor_loss': np.float64(-0.9771642088890076), 'hyper_actor_loss': np.float64(1.248416049293155e-06), 'behavior_loss': np.float64(0.3121621787548065)}

Episode step 26290, time diff 3.6705808639526367, total time dif 4331.334050416946)
step: 26290 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.195804), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07133741155266762), 'actor_loss': np.float64(-0.978827702999115), 'hyper_actor_loss': np.float64(1.4002793477629894e-06), 'behavior_loss': np.float64(0.30265736281871797)}

Episode step 26300, time diff 3.6997218132019043, total time dif 4335.004631280899)
step: 26300 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(2.6939454), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07094713933765888), 'actor_loss': np.float64(-0.9853057444095612), 'hyper_actor_loss': np.float64(1.1711824924987014e-06), 'behavior_loss': np.float64(0.3205209791660309)}

Episode step 26310, time diff 3.727747678756714, total time dif 4338.704353094101)
step: 26310 @ episode report: {'average_total_reward': np.float32(9.463334), 'reward_variance': np.float32(3.0118773), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06048589870333672), 'actor_loss': np.float64(-0.9676271319389343), 'hyper_actor_loss': np.float64(1.186461480529033e-06), 'behavior_loss': np.float64(0.30770512521266935)}

Episode step 26320, time diff 3.7736175060272217, total time dif 4342.432100772858)
step: 26320 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(3.265926), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06438673585653305), 'actor_loss': np.float64(-0.9690325736999512), 'hyper_actor_loss': np.float64(1.0840730851668922e-06), 'behavior_loss': np.float64(0.30420142114162446)}

Episode step 26330, time diff 3.750925302505493, total time dif 4346.205718278885)
step: 26330 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(2.3409388), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06501925811171531), 'actor_loss': np.float64(-0.9714758396148682), 'hyper_actor_loss': np.float64(1.051545484642702e-06), 'behavior_loss': np.float64(0.30091706812381747)}

Episode step 26340, time diff 3.8118391036987305, total time dif 4349.95664358139)
step: 26340 @ episode report: {'average_total_reward': np.float32(8.577778), 'reward_variance': np.float32(2.5670373), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07420331463217736), 'actor_loss': np.float64(-0.9900073409080505), 'hyper_actor_loss': np.float64(8.822778340800142e-07), 'behavior_loss': np.float64(0.2919874548912048)}

Episode step 26350, time diff 3.87015700340271, total time dif 4353.768482685089)
step: 26350 @ episode report: {'average_total_reward': np.float32(9.251111), 'reward_variance': np.float32(3.4483514), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06207464411854744), 'actor_loss': np.float64(-0.9922003030776978), 'hyper_actor_loss': np.float64(8.333173809660366e-07), 'behavior_loss': np.float64(0.3011800438165665)}

Episode step 26360, time diff 3.8850350379943848, total time dif 4357.638639688492)
step: 26360 @ episode report: {'average_total_reward': np.float32(9.187779), 'reward_variance': np.float32(2.261123), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06594917885959148), 'actor_loss': np.float64(-0.9602096736431122), 'hyper_actor_loss': np.float64(7.521365091633925e-07), 'behavior_loss': np.float64(0.30886968076229093)}

Episode step 26370, time diff 3.985025405883789, total time dif 4361.523674726486)
step: 26370 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(2.4774623), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06595590785145759), 'actor_loss': np.float64(-0.9796499013900757), 'hyper_actor_loss': np.float64(7.454442823018326e-07), 'behavior_loss': np.float64(0.3030727505683899)}

Episode step 26380, time diff 4.149656772613525, total time dif 4365.50870013237)
step: 26380 @ episode report: {'average_total_reward': np.float32(10.261112), 'reward_variance': np.float32(5.093908), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05604735873639584), 'actor_loss': np.float64(-0.9778449416160584), 'hyper_actor_loss': np.float64(5.852278832207958e-07), 'behavior_loss': np.float64(0.2979443848133087)}

Episode step 26390, time diff 4.056793928146362, total time dif 4369.6583569049835)
step: 26390 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(1.9667215), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07492895647883416), 'actor_loss': np.float64(-0.974960196018219), 'hyper_actor_loss': np.float64(6.797449600526307e-07), 'behavior_loss': np.float64(0.3186356246471405)}

Episode step 26400, time diff 4.118104457855225, total time dif 4373.71515083313)
step: 26400 @ episode report: {'average_total_reward': np.float32(9.336667), 'reward_variance': np.float32(3.7340999), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061447066627442835), 'actor_loss': np.float64(-0.9775698482990265), 'hyper_actor_loss': np.float64(5.024454992508253e-07), 'behavior_loss': np.float64(0.2936182379722595)}

Episode step 26410, time diff 4.1249706745147705, total time dif 4377.833255290985)
step: 26410 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(5.0678225), 'max_total_reward': np.float32(13.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07377213165163994), 'actor_loss': np.float64(-0.9799914002418518), 'hyper_actor_loss': np.float64(4.868626319876057e-07), 'behavior_loss': np.float64(0.3097390800714493)}

Episode step 26420, time diff 4.030326843261719, total time dif 4381.9582259655)
step: 26420 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(1.9657488), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06876101493835449), 'actor_loss': np.float64(-0.9873198091983795), 'hyper_actor_loss': np.float64(5.420470330363969e-07), 'behavior_loss': np.float64(0.30559240877628324)}

Episode step 26430, time diff 4.0776472091674805, total time dif 4385.988552808762)
step: 26430 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(3.6801097), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06346022374927998), 'actor_loss': np.float64(-0.9794129610061646), 'hyper_actor_loss': np.float64(4.691295657721639e-07), 'behavior_loss': np.float64(0.3086146593093872)}

Episode step 26440, time diff 4.168818712234497, total time dif 4390.066200017929)
step: 26440 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(1.5090923), 'max_total_reward': np.float32(11.1444435), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07008780315518379), 'actor_loss': np.float64(-0.9732846319675446), 'hyper_actor_loss': np.float64(4.510489560516362e-07), 'behavior_loss': np.float64(0.3100251525640488)}

Episode step 26450, time diff 4.164417266845703, total time dif 4394.235018730164)
step: 26450 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(1.5104498), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07278281152248382), 'actor_loss': np.float64(-0.9792523443698883), 'hyper_actor_loss': np.float64(5.406319871781307e-07), 'behavior_loss': np.float64(0.3112995266914368)}

Episode step 26460, time diff 4.175138235092163, total time dif 4398.399435997009)
step: 26460 @ episode report: {'average_total_reward': np.float32(9.973334), 'reward_variance': np.float32(1.7667208), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06739468090236186), 'actor_loss': np.float64(-0.9839406490325928), 'hyper_actor_loss': np.float64(4.676169822914744e-07), 'behavior_loss': np.float64(0.3062978327274323)}

Episode step 26470, time diff 4.146054267883301, total time dif 4402.574574232101)
step: 26470 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.015526), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07379473745822906), 'actor_loss': np.float64(-0.9878282010555267), 'hyper_actor_loss': np.float64(4.1991729062829106e-07), 'behavior_loss': np.float64(0.3035606235265732)}

Episode step 26480, time diff 4.1453094482421875, total time dif 4406.720628499985)
step: 26480 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(4.1607847), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07153102345764636), 'actor_loss': np.float64(-0.9797740936279297), 'hyper_actor_loss': np.float64(4.015768951148857e-07), 'behavior_loss': np.float64(0.31279856860637667)}

Episode step 26490, time diff 4.249569892883301, total time dif 4410.865937948227)
step: 26490 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(4.977551), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07507937140762806), 'actor_loss': np.float64(-0.9781687498092652), 'hyper_actor_loss': np.float64(4.490105425247748e-07), 'behavior_loss': np.float64(0.3100686103105545)}

Episode step 26500, time diff 4.278166055679321, total time dif 4415.11550784111)
step: 26500 @ episode report: {'average_total_reward': np.float32(9.800001), 'reward_variance': np.float32(0.8790371), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07360400930047035), 'actor_loss': np.float64(-0.9926078796386719), 'hyper_actor_loss': np.float64(4.84228499431083e-07), 'behavior_loss': np.float64(0.3029229313135147)}

Episode step 26510, time diff 4.250526189804077, total time dif 4419.39367389679)
step: 26510 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(3.3558888), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06868474408984185), 'actor_loss': np.float64(-0.9921370506286621), 'hyper_actor_loss': np.float64(5.661288639657869e-07), 'behavior_loss': np.float64(0.3099803298711777)}

Episode step 26520, time diff 4.265060186386108, total time dif 4423.644200086594)
step: 26520 @ episode report: {'average_total_reward': np.float32(9.463335), 'reward_variance': np.float32(2.3325696), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06876231022179127), 'actor_loss': np.float64(-0.9632894933223725), 'hyper_actor_loss': np.float64(5.579727542226465e-07), 'behavior_loss': np.float64(0.31239549815654755)}

Episode step 26530, time diff 4.2751007080078125, total time dif 4427.90926027298)
step: 26530 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(1.8185139), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06713297050446272), 'actor_loss': np.float64(-0.9798435032367706), 'hyper_actor_loss': np.float64(5.188742079553776e-07), 'behavior_loss': np.float64(0.3139187186956406)}

Episode step 26540, time diff 4.379304885864258, total time dif 4432.184360980988)
step: 26540 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(2.5562673), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08097709380090237), 'actor_loss': np.float64(-0.9816213130950928), 'hyper_actor_loss': np.float64(5.686353318878901e-07), 'behavior_loss': np.float64(0.30247251987457274)}

Episode step 26550, time diff 4.192765712738037, total time dif 4436.563665866852)
step: 26550 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.2809694), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06523226164281368), 'actor_loss': np.float64(-0.9920443534851074), 'hyper_actor_loss': np.float64(6.106182524945325e-07), 'behavior_loss': np.float64(0.3180448770523071)}

Episode step 26560, time diff 4.262263298034668, total time dif 4440.75643157959)
step: 26560 @ episode report: {'average_total_reward': np.float32(8.763334), 'reward_variance': np.float32(3.6120763), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06345138140022755), 'actor_loss': np.float64(-0.9677882969379425), 'hyper_actor_loss': np.float64(5.712399143931179e-07), 'behavior_loss': np.float64(0.30190915465354917)}

Episode step 26570, time diff 4.179893255233765, total time dif 4445.0186948776245)
step: 26570 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.0661052), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07594985291361808), 'actor_loss': np.float64(-0.98191539645195), 'hyper_actor_loss': np.float64(5.620912219228557e-07), 'behavior_loss': np.float64(0.30140538811683654)}

Episode step 26580, time diff 4.199761867523193, total time dif 4449.198588132858)
step: 26580 @ episode report: {'average_total_reward': np.float32(9.48778), 'reward_variance': np.float32(1.0969245), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07520493194460869), 'actor_loss': np.float64(-1.000615805387497), 'hyper_actor_loss': np.float64(5.060702847003995e-07), 'behavior_loss': np.float64(0.3048016518354416)}

Episode step 26590, time diff 4.2260260581970215, total time dif 4453.3983500003815)
step: 26590 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.0529401), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06914048679172993), 'actor_loss': np.float64(-0.9798782229423523), 'hyper_actor_loss': np.float64(4.54180016618011e-07), 'behavior_loss': np.float64(0.2987016439437866)}

Episode step 26600, time diff 4.186900854110718, total time dif 4457.6243760585785)
step: 26600 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(4.443236), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05297419875860214), 'actor_loss': np.float64(-0.9715110540390015), 'hyper_actor_loss': np.float64(4.807698530839843e-07), 'behavior_loss': np.float64(0.30307988822460175)}

Episode step 26610, time diff 4.226828098297119, total time dif 4461.811276912689)
step: 26610 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(3.0799456), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07460182160139084), 'actor_loss': np.float64(-0.9728781104087829), 'hyper_actor_loss': np.float64(3.805084219266064e-07), 'behavior_loss': np.float64(0.30920360088348386)}

Episode step 26620, time diff 4.288050889968872, total time dif 4466.038105010986)
step: 26620 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(1.4893138), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06961931549012661), 'actor_loss': np.float64(-0.9903125464916229), 'hyper_actor_loss': np.float64(3.5772622197782766e-07), 'behavior_loss': np.float64(0.29714749157428744)}

Episode step 26630, time diff 4.288614511489868, total time dif 4470.326155900955)
step: 26630 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.4231174), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06577350348234176), 'actor_loss': np.float64(-0.9909034192562103), 'hyper_actor_loss': np.float64(3.691380840109559e-07), 'behavior_loss': np.float64(0.3051452189683914)}

Episode step 26640, time diff 4.263339281082153, total time dif 4474.614770412445)
step: 26640 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(2.0205886), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06697833687067031), 'actor_loss': np.float64(-0.9743848502635956), 'hyper_actor_loss': np.float64(3.5809273697395837e-07), 'behavior_loss': np.float64(0.29373936355113983)}

Episode step 26650, time diff 4.265862941741943, total time dif 4478.878109693527)
step: 26650 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(2.0358527), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06822191029787064), 'actor_loss': np.float64(-0.9798733174800873), 'hyper_actor_loss': np.float64(2.8747320754973773e-07), 'behavior_loss': np.float64(0.3098683327436447)}

Episode step 26660, time diff 4.24738335609436, total time dif 4483.143972635269)
step: 26660 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(3.0246527), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07012270763516426), 'actor_loss': np.float64(-0.9812289297580719), 'hyper_actor_loss': np.float64(3.0849301424495934e-07), 'behavior_loss': np.float64(0.3018139749765396)}

Episode step 26670, time diff 4.247868776321411, total time dif 4487.3913559913635)
step: 26670 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(2.3928154), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06033039279282093), 'actor_loss': np.float64(-0.9825494110584259), 'hyper_actor_loss': np.float64(3.5811773813065884e-07), 'behavior_loss': np.float64(0.3149455666542053)}

Episode step 26680, time diff 4.227259397506714, total time dif 4491.639224767685)
step: 26680 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.780381), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06574490182101726), 'actor_loss': np.float64(-0.9687631309032441), 'hyper_actor_loss': np.float64(3.4111970705907877e-07), 'behavior_loss': np.float64(0.3034705489873886)}

Episode step 26690, time diff 4.252461671829224, total time dif 4495.866484165192)
step: 26690 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(1.6724743), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07009227219969034), 'actor_loss': np.float64(-0.9715854525566101), 'hyper_actor_loss': np.float64(4.021021823064075e-07), 'behavior_loss': np.float64(0.31292904317379)}

Episode step 26700, time diff 4.286562919616699, total time dif 4500.118945837021)
step: 26700 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(0.5061432), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07059335745871068), 'actor_loss': np.float64(-0.9985403299331665), 'hyper_actor_loss': np.float64(4.249444486958964e-07), 'behavior_loss': np.float64(0.30210645496845245)}

Episode step 26710, time diff 4.446717262268066, total time dif 4504.405508756638)
step: 26710 @ episode report: {'average_total_reward': np.float32(8.751111), 'reward_variance': np.float32(2.4928446), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06223968006670475), 'actor_loss': np.float64(-0.9785858273506165), 'hyper_actor_loss': np.float64(5.880776683397926e-07), 'behavior_loss': np.float64(0.2980184555053711)}

Episode step 26720, time diff 4.300943613052368, total time dif 4508.852226018906)
step: 26720 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.4525127), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06525681912899017), 'actor_loss': np.float64(-0.9714033365249634), 'hyper_actor_loss': np.float64(7.139881745388266e-07), 'behavior_loss': np.float64(0.3003552109003067)}

Episode step 26730, time diff 4.248921632766724, total time dif 4513.153169631958)
step: 26730 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.7294579), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07590326555073261), 'actor_loss': np.float64(-0.9975410521030426), 'hyper_actor_loss': np.float64(9.897555116822333e-07), 'behavior_loss': np.float64(0.30063217878341675)}

Episode step 26740, time diff 4.3031346797943115, total time dif 4517.402091264725)
step: 26740 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(1.4881927), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06535029932856559), 'actor_loss': np.float64(-0.9854807615280151), 'hyper_actor_loss': np.float64(1.0877551460453105e-06), 'behavior_loss': np.float64(0.3124153226613998)}

Episode step 26750, time diff 4.316432476043701, total time dif 4521.705225944519)
step: 26750 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(5.108736), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07705208621919155), 'actor_loss': np.float64(-0.9648781061172486), 'hyper_actor_loss': np.float64(1.2641878356589587e-06), 'behavior_loss': np.float64(0.3046366602182388)}

Episode step 26760, time diff 4.324084758758545, total time dif 4526.021658420563)
step: 26760 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(4.12687), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0568002488464117), 'actor_loss': np.float64(-0.9874019861221314), 'hyper_actor_loss': np.float64(1.6789040159892466e-06), 'behavior_loss': np.float64(0.30656168460845945)}

Episode step 26770, time diff 4.341135025024414, total time dif 4530.345743179321)
step: 26770 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.6834513), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05516950897872448), 'actor_loss': np.float64(-0.9624635338783264), 'hyper_actor_loss': np.float64(2.232815722891246e-06), 'behavior_loss': np.float64(0.3022389829158783)}

Episode step 26780, time diff 4.321086406707764, total time dif 4534.686878204346)
step: 26780 @ episode report: {'average_total_reward': np.float32(9.7733345), 'reward_variance': np.float32(1.759042), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05879228916019201), 'actor_loss': np.float64(-0.9626632511615754), 'hyper_actor_loss': np.float64(2.3076696720636393e-06), 'behavior_loss': np.float64(0.3077607899904251)}

Episode step 26790, time diff 4.4090635776519775, total time dif 4539.0079646110535)
step: 26790 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(2.4855678), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05866111535578966), 'actor_loss': np.float64(-0.9763830840587616), 'hyper_actor_loss': np.float64(3.361518110978068e-06), 'behavior_loss': np.float64(0.3068835616111755)}

Episode step 26800, time diff 4.43667459487915, total time dif 4543.417028188705)
step: 26800 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(2.420248), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07216423377394676), 'actor_loss': np.float64(-0.9723724305629731), 'hyper_actor_loss': np.float64(3.7714666632382434e-06), 'behavior_loss': np.float64(0.30586679875850675)}

Episode step 26810, time diff 4.406798839569092, total time dif 4547.853702783585)
step: 26810 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(3.3332705), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06216721758246422), 'actor_loss': np.float64(-0.9929286539554596), 'hyper_actor_loss': np.float64(4.15881493154302e-06), 'behavior_loss': np.float64(0.3034696877002716)}

Episode step 26820, time diff 4.466145277023315, total time dif 4552.260501623154)
step: 26820 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.2366776), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08088001422584057), 'actor_loss': np.float64(-0.9908269107341766), 'hyper_actor_loss': np.float64(5.131679154146696e-06), 'behavior_loss': np.float64(0.2953390866518021)}

Episode step 26830, time diff 4.4760613441467285, total time dif 4556.726646900177)
step: 26830 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(4.8458962), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06965146400034428), 'actor_loss': np.float64(-1.0041770160198211), 'hyper_actor_loss': np.float64(6.1812216245016314e-06), 'behavior_loss': np.float64(0.306181275844574)}

Episode step 26840, time diff 4.511422872543335, total time dif 4561.202708244324)
step: 26840 @ episode report: {'average_total_reward': np.float32(10.124446), 'reward_variance': np.float32(1.5611057), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0694784965366125), 'actor_loss': np.float64(-0.9832042694091797), 'hyper_actor_loss': np.float64(7.354706667683786e-06), 'behavior_loss': np.float64(0.3024551421403885)}

Episode step 26850, time diff 4.490522146224976, total time dif 4565.714131116867)
step: 26850 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(1.5683564), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06213374361395836), 'actor_loss': np.float64(-0.9740186035633087), 'hyper_actor_loss': np.float64(8.65635947775445e-06), 'behavior_loss': np.float64(0.3084750235080719)}

Episode step 26860, time diff 4.483866214752197, total time dif 4570.204653263092)
step: 26860 @ episode report: {'average_total_reward': np.float32(8.863335), 'reward_variance': np.float32(3.7189152), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07283035106956959), 'actor_loss': np.float64(-0.9657603859901428), 'hyper_actor_loss': np.float64(1.0312975791748614e-05), 'behavior_loss': np.float64(0.316565665602684)}

Episode step 26870, time diff 4.6276326179504395, total time dif 4574.688519477844)
step: 26870 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(0.7186175), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06725200824439526), 'actor_loss': np.float64(-0.9853310286998749), 'hyper_actor_loss': np.float64(1.1928997537324904e-05), 'behavior_loss': np.float64(0.30617223381996156)}

Episode step 26880, time diff 4.542221546173096, total time dif 4579.316152095795)
step: 26880 @ episode report: {'average_total_reward': np.float32(9.375557), 'reward_variance': np.float32(1.3117728), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0668342974036932), 'actor_loss': np.float64(-0.9714787900447845), 'hyper_actor_loss': np.float64(1.1973370419582352e-05), 'behavior_loss': np.float64(0.31048712134361267)}

Episode step 26890, time diff 4.504875421524048, total time dif 4583.858373641968)
step: 26890 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.8172462), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07499608658254146), 'actor_loss': np.float64(-0.9702949702739716), 'hyper_actor_loss': np.float64(1.2761613197653787e-05), 'behavior_loss': np.float64(0.30636121034622193)}

Episode step 26900, time diff 4.50545334815979, total time dif 4588.363249063492)
step: 26900 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(6.1478567), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06360313650220632), 'actor_loss': np.float64(-0.9905794024467468), 'hyper_actor_loss': np.float64(1.1995172189926962e-05), 'behavior_loss': np.float64(0.3063589870929718)}

Episode step 26910, time diff 4.501598119735718, total time dif 4592.868702411652)
step: 26910 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.274011), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06124223917722702), 'actor_loss': np.float64(-0.96758993268013), 'hyper_actor_loss': np.float64(1.1731609811249655e-05), 'behavior_loss': np.float64(0.30597578585147855)}

Episode step 26920, time diff 4.561061382293701, total time dif 4597.370300531387)
step: 26920 @ episode report: {'average_total_reward': np.float32(8.902223), 'reward_variance': np.float32(1.8536246), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07995584681630134), 'actor_loss': np.float64(-0.9819699943065643), 'hyper_actor_loss': np.float64(1.1411315790610388e-05), 'behavior_loss': np.float64(0.30777711272239683)}

Episode step 26930, time diff 4.6524059772491455, total time dif 4601.931361913681)
step: 26930 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(1.3533531), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07250425480306148), 'actor_loss': np.float64(-1.0040097534656525), 'hyper_actor_loss': np.float64(1.2769757358910282e-05), 'behavior_loss': np.float64(0.3014091432094574)}

Episode step 26940, time diff 4.629103899002075, total time dif 4606.58376789093)
step: 26940 @ episode report: {'average_total_reward': np.float32(9.451113), 'reward_variance': np.float32(1.646376), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06865450702607631), 'actor_loss': np.float64(-0.9710601389408111), 'hyper_actor_loss': np.float64(1.353469106106786e-05), 'behavior_loss': np.float64(0.3117039382457733)}

Episode step 26950, time diff 4.797800540924072, total time dif 4611.212871789932)
step: 26950 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.8585377), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07443525679409504), 'actor_loss': np.float64(-0.9832799971103668), 'hyper_actor_loss': np.float64(1.5029955739009892e-05), 'behavior_loss': np.float64(0.31847207248210907)}

Episode step 26960, time diff 4.899108409881592, total time dif 4616.010672330856)
step: 26960 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(1.3702224), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07375606708228588), 'actor_loss': np.float64(-0.9830559015274047), 'hyper_actor_loss': np.float64(1.5063045520946616e-05), 'behavior_loss': np.float64(0.3031067609786987)}

Episode step 26970, time diff 4.834916830062866, total time dif 4620.909780740738)
step: 26970 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(4.1408916), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07118386514484883), 'actor_loss': np.float64(-0.9960139334201813), 'hyper_actor_loss': np.float64(1.5342127790063385e-05), 'behavior_loss': np.float64(0.3105709344148636)}

Episode step 26980, time diff 4.761632204055786, total time dif 4625.744697570801)
step: 26980 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(4.116456), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06787178255617618), 'actor_loss': np.float64(-0.9686426937580108), 'hyper_actor_loss': np.float64(1.6519006840098883e-05), 'behavior_loss': np.float64(0.3250303387641907)}

Episode step 26990, time diff 4.705423593521118, total time dif 4630.506329774857)
step: 26990 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(1.488326), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05505222883075476), 'actor_loss': np.float64(-0.9544335722923278), 'hyper_actor_loss': np.float64(1.6645930827507982e-05), 'behavior_loss': np.float64(0.30125337541103364)}

Episode step 27000, time diff 4.726757526397705, total time dif 4635.211753368378)
step: 27000 @ episode report: {'average_total_reward': np.float32(9.138889), 'reward_variance': np.float32(1.8284748), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333347), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07386913038790226), 'actor_loss': np.float64(-0.9772803604602813), 'hyper_actor_loss': np.float64(1.613445319890161e-05), 'behavior_loss': np.float64(0.31202023327350614)}

Episode step 27010, time diff 4.726500511169434, total time dif 4639.938510894775)
step: 27010 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(2.4042468), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06491105109453202), 'actor_loss': np.float64(-0.9877498984336853), 'hyper_actor_loss': np.float64(1.5436469766427763e-05), 'behavior_loss': np.float64(0.3179016917943954)}

Episode step 27020, time diff 4.699037790298462, total time dif 4644.665011405945)
step: 27020 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.8450425), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0667894385755062), 'actor_loss': np.float64(-0.9628627359867096), 'hyper_actor_loss': np.float64(1.5399942822114098e-05), 'behavior_loss': np.float64(0.3078590273857117)}

Episode step 27030, time diff 4.706158638000488, total time dif 4649.364049196243)
step: 27030 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.6749941), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055152542516589166), 'actor_loss': np.float64(-0.9734750032424927), 'hyper_actor_loss': np.float64(1.3615586885862285e-05), 'behavior_loss': np.float64(0.3054631620645523)}

Episode step 27040, time diff 4.859114170074463, total time dif 4654.070207834244)
step: 27040 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.3602226), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06001640073955059), 'actor_loss': np.float64(-0.9643259167671203), 'hyper_actor_loss': np.float64(1.320887522524572e-05), 'behavior_loss': np.float64(0.30856889188289643)}

Episode step 27050, time diff 4.714498043060303, total time dif 4658.929322004318)
step: 27050 @ episode report: {'average_total_reward': np.float32(9.224444), 'reward_variance': np.float32(1.2815504), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06300204712897539), 'actor_loss': np.float64(-0.9678603112697601), 'hyper_actor_loss': np.float64(1.1836130215669983e-05), 'behavior_loss': np.float64(0.3130063205957413)}

Episode step 27060, time diff 4.765995502471924, total time dif 4663.6438200473785)
step: 27060 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(2.6495605), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06844751127064228), 'actor_loss': np.float64(-0.9745279431343079), 'hyper_actor_loss': np.float64(1.1351809553161729e-05), 'behavior_loss': np.float64(0.31179487705230713)}

Episode step 27070, time diff 4.726817607879639, total time dif 4668.4098155498505)
step: 27070 @ episode report: {'average_total_reward': np.float32(8.426667), 'reward_variance': np.float32(1.7939562), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06588916666805744), 'actor_loss': np.float64(-0.9796838283538818), 'hyper_actor_loss': np.float64(1.0294627281837166e-05), 'behavior_loss': np.float64(0.31930374503135683)}

Episode step 27080, time diff 4.7740638256073, total time dif 4673.13663315773)
step: 27080 @ episode report: {'average_total_reward': np.float32(8.663335), 'reward_variance': np.float32(0.9529891), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06198822390288115), 'actor_loss': np.float64(-0.9693790435791015), 'hyper_actor_loss': np.float64(1.033271410051384e-05), 'behavior_loss': np.float64(0.2991755783557892)}

Episode step 27090, time diff 4.771594524383545, total time dif 4677.910696983337)
step: 27090 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(2.0464559), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062102635949850084), 'actor_loss': np.float64(-0.9704425156116485), 'hyper_actor_loss': np.float64(9.599248915037607e-06), 'behavior_loss': np.float64(0.31113292276859283)}

Episode step 27100, time diff 4.769723415374756, total time dif 4682.682291507721)
step: 27100 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(1.0375359), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06149357408285141), 'actor_loss': np.float64(-0.9696475923061371), 'hyper_actor_loss': np.float64(8.316786943396436e-06), 'behavior_loss': np.float64(0.3118001461029053)}

Episode step 27110, time diff 4.755321264266968, total time dif 4687.452014923096)
step: 27110 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(4.066945), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06443103477358818), 'actor_loss': np.float64(-0.9756290853023529), 'hyper_actor_loss': np.float64(8.626105136499973e-06), 'behavior_loss': np.float64(0.2964377373456955)}

Episode step 27120, time diff 4.76056432723999, total time dif 4692.207336187363)
step: 27120 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.7762423), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06934188921004533), 'actor_loss': np.float64(-0.9896834552288055), 'hyper_actor_loss': np.float64(7.623241981491447e-06), 'behavior_loss': np.float64(0.301838344335556)}

Episode step 27130, time diff 4.709462404251099, total time dif 4696.967900514603)
step: 27130 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(0.6375355), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06272045690566301), 'actor_loss': np.float64(-0.9828033149242401), 'hyper_actor_loss': np.float64(6.695266893075313e-06), 'behavior_loss': np.float64(0.2937996745109558)}

Episode step 27140, time diff 4.790706396102905, total time dif 4701.677362918854)
step: 27140 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(1.4619706), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06353366281837225), 'actor_loss': np.float64(-0.9578306138515472), 'hyper_actor_loss': np.float64(6.182214065120206e-06), 'behavior_loss': np.float64(0.32656237483024597)}

Episode step 27150, time diff 4.742692470550537, total time dif 4706.468069314957)
step: 27150 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(1.4770482), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0630661791190505), 'actor_loss': np.float64(-0.9579926609992981), 'hyper_actor_loss': np.float64(5.5869849802547835e-06), 'behavior_loss': np.float64(0.31190999448299406)}

Episode step 27160, time diff 4.866318941116333, total time dif 4711.210761785507)
step: 27160 @ episode report: {'average_total_reward': np.float32(9.202223), 'reward_variance': np.float32(3.0817492), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06485411822795868), 'actor_loss': np.float64(-0.9860969424247742), 'hyper_actor_loss': np.float64(5.385383792599896e-06), 'behavior_loss': np.float64(0.3127558708190918)}

Episode step 27170, time diff 4.750020980834961, total time dif 4716.0770807266235)
step: 27170 @ episode report: {'average_total_reward': np.float32(8.216667), 'reward_variance': np.float32(1.2824261), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06937601119279861), 'actor_loss': np.float64(-0.9735321581363678), 'hyper_actor_loss': np.float64(5.216346426095697e-06), 'behavior_loss': np.float64(0.31178331971168516)}

Episode step 27180, time diff 4.63472843170166, total time dif 4720.8271017074585)
step: 27180 @ episode report: {'average_total_reward': np.float32(8.775556), 'reward_variance': np.float32(5.46718), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07627713121473789), 'actor_loss': np.float64(-0.9846144556999207), 'hyper_actor_loss': np.float64(4.847733657697973e-06), 'behavior_loss': np.float64(0.3071583598852158)}

Episode step 27190, time diff 4.654173374176025, total time dif 4725.46183013916)
step: 27190 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(4.262086), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07291328385472298), 'actor_loss': np.float64(-0.9970325410366059), 'hyper_actor_loss': np.float64(4.777976459990896e-06), 'behavior_loss': np.float64(0.30014961361885073)}

Episode step 27200, time diff 4.8174755573272705, total time dif 4730.116003513336)
step: 27200 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(7.290183), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.04432001542299986), 'actor_loss': np.float64(-0.9757982730865479), 'hyper_actor_loss': np.float64(4.376160291030828e-06), 'behavior_loss': np.float64(0.2965725749731064)}

Episode step 27210, time diff 4.686150550842285, total time dif 4734.933479070663)
step: 27210 @ episode report: {'average_total_reward': np.float32(9.475557), 'reward_variance': np.float32(1.3460441), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0628961432725191), 'actor_loss': np.float64(-0.9474983215332031), 'hyper_actor_loss': np.float64(4.449628727343225e-06), 'behavior_loss': np.float64(0.30358898639678955)}

Episode step 27220, time diff 4.9114768505096436, total time dif 4739.619629621506)
step: 27220 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(1.1526468), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06930786781013012), 'actor_loss': np.float64(-0.9837154030799866), 'hyper_actor_loss': np.float64(4.270178578735795e-06), 'behavior_loss': np.float64(0.3190619766712189)}

Episode step 27230, time diff 4.726770877838135, total time dif 4744.531106472015)
step: 27230 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.7138714), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06004972904920578), 'actor_loss': np.float64(-0.9712865650653839), 'hyper_actor_loss': np.float64(4.340913801570423e-06), 'behavior_loss': np.float64(0.3178111582994461)}

Episode step 27240, time diff 4.731868505477905, total time dif 4749.2578773498535)
step: 27240 @ episode report: {'average_total_reward': np.float32(8.763334), 'reward_variance': np.float32(2.0429645), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0750664658844471), 'actor_loss': np.float64(-0.9602496504783631), 'hyper_actor_loss': np.float64(3.896522230206756e-06), 'behavior_loss': np.float64(0.3124084800481796)}

Episode step 27250, time diff 4.756608486175537, total time dif 4753.989745855331)
step: 27250 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.7023892), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.533335), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07023327425122261), 'actor_loss': np.float64(-0.9892489731311798), 'hyper_actor_loss': np.float64(3.6707670233226962e-06), 'behavior_loss': np.float64(0.3079949736595154)}

Episode step 27260, time diff 4.726521253585815, total time dif 4758.746354341507)
step: 27260 @ episode report: {'average_total_reward': np.float32(9.214445), 'reward_variance': np.float32(2.6681983), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06472934558987617), 'actor_loss': np.float64(-0.9939095079898834), 'hyper_actor_loss': np.float64(3.6022591302753424e-06), 'behavior_loss': np.float64(0.30090887248516085)}

Episode step 27270, time diff 4.751766681671143, total time dif 4763.472875595093)
step: 27270 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(4.364588), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06601127833127976), 'actor_loss': np.float64(-0.973631227016449), 'hyper_actor_loss': np.float64(3.6139431585979764e-06), 'behavior_loss': np.float64(0.30951848030090334)}

Episode step 27280, time diff 4.775374174118042, total time dif 4768.224642276764)
step: 27280 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(2.72527), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06834276467561722), 'actor_loss': np.float64(-0.9726198673248291), 'hyper_actor_loss': np.float64(3.305642894702032e-06), 'behavior_loss': np.float64(0.3032496452331543)}

Episode step 27290, time diff 4.7871856689453125, total time dif 4773.000016450882)
step: 27290 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(2.8196106), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07533580511808395), 'actor_loss': np.float64(-0.9875702500343323), 'hyper_actor_loss': np.float64(3.6186948364047565e-06), 'behavior_loss': np.float64(0.3171411007642746)}

Episode step 27300, time diff 4.772206783294678, total time dif 4777.787202119827)
step: 27300 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(2.8267028), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05762735903263092), 'actor_loss': np.float64(-0.9790548086166382), 'hyper_actor_loss': np.float64(3.321135227452032e-06), 'behavior_loss': np.float64(0.3034092515707016)}

Episode step 27310, time diff 4.837023019790649, total time dif 4782.559408903122)
step: 27310 @ episode report: {'average_total_reward': np.float32(9.787779), 'reward_variance': np.float32(1.126826), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07235323637723923), 'actor_loss': np.float64(-0.9713517844676971), 'hyper_actor_loss': np.float64(3.1226138162310237e-06), 'behavior_loss': np.float64(0.2964211910963058)}

Episode step 27320, time diff 4.826437473297119, total time dif 4787.396431922913)
step: 27320 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(2.8387172), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07119292616844178), 'actor_loss': np.float64(-0.9942194819450378), 'hyper_actor_loss': np.float64(3.292850942671066e-06), 'behavior_loss': np.float64(0.29285666793584825)}

Episode step 27330, time diff 4.790003538131714, total time dif 4792.22286939621)
step: 27330 @ episode report: {'average_total_reward': np.float32(8.490001), 'reward_variance': np.float32(3.0167527), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07458808049559593), 'actor_loss': np.float64(-0.9843075335025787), 'hyper_actor_loss': np.float64(3.003997744599474e-06), 'behavior_loss': np.float64(0.32193478047847746)}

Episode step 27340, time diff 4.848677635192871, total time dif 4797.012872934341)
step: 27340 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(4.539378), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06446620635688305), 'actor_loss': np.float64(-0.9681929886341095), 'hyper_actor_loss': np.float64(3.055653246519796e-06), 'behavior_loss': np.float64(0.3091128826141357)}

Episode step 27350, time diff 4.850378751754761, total time dif 4801.861550569534)
step: 27350 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.6445745), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07694928422570228), 'actor_loss': np.float64(-0.9767544806003571), 'hyper_actor_loss': np.float64(2.886156767090142e-06), 'behavior_loss': np.float64(0.32343543469905855)}

Episode step 27360, time diff 4.973330497741699, total time dif 4806.711929321289)
step: 27360 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.4894273), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0678573228418827), 'actor_loss': np.float64(-0.9796674489974976), 'hyper_actor_loss': np.float64(2.6424454972584497e-06), 'behavior_loss': np.float64(0.3126135230064392)}

Episode step 27370, time diff 4.953819274902344, total time dif 4811.685259819031)
step: 27370 @ episode report: {'average_total_reward': np.float32(8.975556), 'reward_variance': np.float32(5.0859222), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06514836214482785), 'actor_loss': np.float64(-0.9810018718242646), 'hyper_actor_loss': np.float64(2.6403731908430927e-06), 'behavior_loss': np.float64(0.30262285768985747)}

Episode step 27380, time diff 4.899542570114136, total time dif 4816.639079093933)
step: 27380 @ episode report: {'average_total_reward': np.float32(9.924446), 'reward_variance': np.float32(1.5744884), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0694332929328084), 'actor_loss': np.float64(-0.9814129829406738), 'hyper_actor_loss': np.float64(2.651875774972723e-06), 'behavior_loss': np.float64(0.29325790107250216)}

Episode step 27390, time diff 4.849758625030518, total time dif 4821.538621664047)
step: 27390 @ episode report: {'average_total_reward': np.float32(8.4777775), 'reward_variance': np.float32(4.166691), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05810551792383194), 'actor_loss': np.float64(-0.9751154363155365), 'hyper_actor_loss': np.float64(2.3272176576938364e-06), 'behavior_loss': np.float64(0.30591467022895813)}

Episode step 27400, time diff 4.807878494262695, total time dif 4826.388380289078)
step: 27400 @ episode report: {'average_total_reward': np.float32(9.251111), 'reward_variance': np.float32(2.3585484), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07172863818705082), 'actor_loss': np.float64(-0.9599627017974853), 'hyper_actor_loss': np.float64(2.2592185246139707e-06), 'behavior_loss': np.float64(0.305398228764534)}

Episode step 27410, time diff 4.737226963043213, total time dif 4831.19625878334)
step: 27410 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(3.4195359), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07409132197499275), 'actor_loss': np.float64(-0.9871477484703064), 'hyper_actor_loss': np.float64(2.202403447881807e-06), 'behavior_loss': np.float64(0.3077354222536087)}

Episode step 27420, time diff 4.800433158874512, total time dif 4835.933485746384)
step: 27420 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(0.8235506), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07164406664669513), 'actor_loss': np.float64(-0.9990830481052398), 'hyper_actor_loss': np.float64(2.497563764336519e-06), 'behavior_loss': np.float64(0.3072757810354233)}

Episode step 27430, time diff 4.848534822463989, total time dif 4840.733918905258)
step: 27430 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(2.814258), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06562218070030212), 'actor_loss': np.float64(-0.9705065071582795), 'hyper_actor_loss': np.float64(2.2294000928013704e-06), 'behavior_loss': np.float64(0.3128171980381012)}

Episode step 27440, time diff 4.831890106201172, total time dif 4845.582453727722)
step: 27440 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(4.5119467), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08408163115382195), 'actor_loss': np.float64(-0.9782108426094055), 'hyper_actor_loss': np.float64(2.2324771407511436e-06), 'behavior_loss': np.float64(0.3129677474498749)}

Episode step 27450, time diff 4.775201320648193, total time dif 4850.414343833923)
step: 27450 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.3598536), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06671055778861046), 'actor_loss': np.float64(-1.003720909357071), 'hyper_actor_loss': np.float64(2.2399134309125655e-06), 'behavior_loss': np.float64(0.30625261962413786)}

Episode step 27460, time diff 4.860491514205933, total time dif 4855.1895451545715)
step: 27460 @ episode report: {'average_total_reward': np.float32(9.961113), 'reward_variance': np.float32(4.033759), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06988504007458687), 'actor_loss': np.float64(-0.968682861328125), 'hyper_actor_loss': np.float64(2.4131938175742108e-06), 'behavior_loss': np.float64(0.3122220873832703)}

Episode step 27470, time diff 4.761926174163818, total time dif 4860.0500366687775)
step: 27470 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.745797), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05285480916500092), 'actor_loss': np.float64(-0.9571484148502349), 'hyper_actor_loss': np.float64(2.060987480945187e-06), 'behavior_loss': np.float64(0.3055089294910431)}

Episode step 27480, time diff 4.8688271045684814, total time dif 4864.811962842941)
step: 27480 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(1.5653691), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06511319801211357), 'actor_loss': np.float64(-0.9645303845405578), 'hyper_actor_loss': np.float64(2.052870797797368e-06), 'behavior_loss': np.float64(0.30717428028583527)}

Episode step 27490, time diff 4.793074607849121, total time dif 4869.68078994751)
step: 27490 @ episode report: {'average_total_reward': np.float32(8.963335), 'reward_variance': np.float32(2.7643723), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.066143249720335), 'actor_loss': np.float64(-0.9746947526931763), 'hyper_actor_loss': np.float64(1.937313049893419e-06), 'behavior_loss': np.float64(0.31428536474704744)}

Episode step 27500, time diff 4.7948572635650635, total time dif 4874.473864555359)
step: 27500 @ episode report: {'average_total_reward': np.float32(9.5244465), 'reward_variance': np.float32(2.0799456), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07147255577147008), 'actor_loss': np.float64(-0.9775318622589111), 'hyper_actor_loss': np.float64(1.8462356138115864e-06), 'behavior_loss': np.float64(0.301164498925209)}

Episode step 27510, time diff 4.726984024047852, total time dif 4879.268721818924)
step: 27510 @ episode report: {'average_total_reward': np.float32(9.8), 'reward_variance': np.float32(2.208222), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06615125238895417), 'actor_loss': np.float64(-0.9912122905254364), 'hyper_actor_loss': np.float64(1.8451111259309982e-06), 'behavior_loss': np.float64(0.28979859352111814)}

Episode step 27520, time diff 4.87831449508667, total time dif 4883.995705842972)
step: 27520 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.5752406), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06578151173889638), 'actor_loss': np.float64(-0.986791443824768), 'hyper_actor_loss': np.float64(1.6673882896611758e-06), 'behavior_loss': np.float64(0.30832224190235136)}

Episode step 27530, time diff 4.962606191635132, total time dif 4888.8740203380585)
step: 27530 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(0.6928692), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06130765974521637), 'actor_loss': np.float64(-0.971480005979538), 'hyper_actor_loss': np.float64(1.5721787463007786e-06), 'behavior_loss': np.float64(0.3077832579612732)}

Episode step 27540, time diff 4.743529796600342, total time dif 4893.836626529694)
step: 27540 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(3.4296002), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06981350630521774), 'actor_loss': np.float64(-0.9715931653976441), 'hyper_actor_loss': np.float64(1.4192957110026327e-06), 'behavior_loss': np.float64(0.3075357526540756)}

Episode step 27550, time diff 4.74609637260437, total time dif 4898.580156326294)
step: 27550 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(2.6744752), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05944448243826628), 'actor_loss': np.float64(-0.9767713725566864), 'hyper_actor_loss': np.float64(2.945981407265208e-06), 'behavior_loss': np.float64(0.29823934435844424)}

Episode step 27560, time diff 4.820981502532959, total time dif 4903.326252698898)
step: 27560 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.6250825), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06655277386307716), 'actor_loss': np.float64(-0.9747286915779114), 'hyper_actor_loss': np.float64(1.7344796901852532e-06), 'behavior_loss': np.float64(0.3048681378364563)}

Episode step 27570, time diff 4.829134941101074, total time dif 4908.147234201431)
step: 27570 @ episode report: {'average_total_reward': np.float32(9.424445), 'reward_variance': np.float32(2.0674026), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07361591346561909), 'actor_loss': np.float64(-0.9880459487438202), 'hyper_actor_loss': np.float64(1.5251573927343998e-06), 'behavior_loss': np.float64(0.30198021829128263)}

Episode step 27580, time diff 4.7781102657318115, total time dif 4912.976369142532)
step: 27580 @ episode report: {'average_total_reward': np.float32(8.926666), 'reward_variance': np.float32(1.1736096), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06855779718607664), 'actor_loss': np.float64(-0.9889140844345092), 'hyper_actor_loss': np.float64(1.6311514400513261e-06), 'behavior_loss': np.float64(0.31674892604351046)}

Episode step 27590, time diff 4.781117677688599, total time dif 4917.754479408264)
step: 27590 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.1183908), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186288125813008), 'actor_loss': np.float64(-0.9691141545772552), 'hyper_actor_loss': np.float64(1.4480249888038088e-06), 'behavior_loss': np.float64(0.30297878980636594)}

Episode step 27600, time diff 4.767488718032837, total time dif 4922.535597085953)
step: 27600 @ episode report: {'average_total_reward': np.float32(9.0), 'reward_variance': np.float32(1.9080002), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05653808992356062), 'actor_loss': np.float64(-0.9789657175540925), 'hyper_actor_loss': np.float64(1.3966470646664675e-06), 'behavior_loss': np.float64(0.30408293604850767)}

Episode step 27610, time diff 4.758214235305786, total time dif 4927.303085803986)
step: 27610 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.0999217), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05680977813899517), 'actor_loss': np.float64(-0.96849125623703), 'hyper_actor_loss': np.float64(1.2628658339508547e-06), 'behavior_loss': np.float64(0.30537039041519165)}

Episode step 27620, time diff 4.901888370513916, total time dif 4932.061300039291)
step: 27620 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.1936343), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06004326418042183), 'actor_loss': np.float64(-0.9516701638698578), 'hyper_actor_loss': np.float64(1.1612097466695558e-06), 'behavior_loss': np.float64(0.30696230828762056)}

Episode step 27630, time diff 5.056530475616455, total time dif 4936.963188409805)
step: 27630 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(1.8221347), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0691708367317915), 'actor_loss': np.float64(-0.9702765882015228), 'hyper_actor_loss': np.float64(1.0900894721999066e-06), 'behavior_loss': np.float64(0.307663232088089)}

Episode step 27640, time diff 5.044732093811035, total time dif 4942.019718885422)
step: 27640 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(1.8202336), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06435334272682666), 'actor_loss': np.float64(-0.9748417735099792), 'hyper_actor_loss': np.float64(1.0057563713417039e-06), 'behavior_loss': np.float64(0.29818161129951476)}

Episode step 27650, time diff 5.021066665649414, total time dif 4947.064450979233)
step: 27650 @ episode report: {'average_total_reward': np.float32(9.451113), 'reward_variance': np.float32(4.09435), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07447434514760971), 'actor_loss': np.float64(-0.9861820936203003), 'hyper_actor_loss': np.float64(9.280119741106319e-07), 'behavior_loss': np.float64(0.30325068831443786)}

Episode step 27660, time diff 5.09588098526001, total time dif 4952.085517644882)
step: 27660 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(3.3558288), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059521736949682234), 'actor_loss': np.float64(-0.9875784277915954), 'hyper_actor_loss': np.float64(8.424638053838862e-07), 'behavior_loss': np.float64(0.30092349350452424)}

Episode step 27670, time diff 5.090804100036621, total time dif 4957.181398630142)
step: 27670 @ episode report: {'average_total_reward': np.float32(9.200001), 'reward_variance': np.float32(4.3771853), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06694082356989384), 'actor_loss': np.float64(-0.9777883946895599), 'hyper_actor_loss': np.float64(7.7526785844384e-07), 'behavior_loss': np.float64(0.30418441295623777)}

Episode step 27680, time diff 5.06018590927124, total time dif 4962.272202730179)
step: 27680 @ episode report: {'average_total_reward': np.float32(8.751111), 'reward_variance': np.float32(2.2958314), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06962739042937756), 'actor_loss': np.float64(-0.9728354036808013), 'hyper_actor_loss': np.float64(7.911266777682612e-07), 'behavior_loss': np.float64(0.31407935321331026)}

Episode step 27690, time diff 5.242582321166992, total time dif 4967.33238863945)
step: 27690 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(0.960866), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07899931259453297), 'actor_loss': np.float64(-0.9841182351112365), 'hyper_actor_loss': np.float64(8.727507804451306e-07), 'behavior_loss': np.float64(0.3106996864080429)}

Episode step 27700, time diff 5.073675632476807, total time dif 4972.574970960617)
step: 27700 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.8915317), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07097203619778156), 'actor_loss': np.float64(-0.9909971237182618), 'hyper_actor_loss': np.float64(8.292949132737704e-07), 'behavior_loss': np.float64(0.3088162958621979)}

Episode step 27710, time diff 5.158409357070923, total time dif 4977.648646593094)
step: 27710 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.7563164), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06530859135091305), 'actor_loss': np.float64(-0.966892546415329), 'hyper_actor_loss': np.float64(7.687901984354539e-07), 'behavior_loss': np.float64(0.3173098236322403)}

Episode step 27720, time diff 5.166240930557251, total time dif 4982.807055950165)
step: 27720 @ episode report: {'average_total_reward': np.float32(8.614446), 'reward_variance': np.float32(2.5544224), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07041010335087776), 'actor_loss': np.float64(-0.9701792955398559), 'hyper_actor_loss': np.float64(6.644770223829255e-07), 'behavior_loss': np.float64(0.30416665971279144)}

Episode step 27730, time diff 5.188077449798584, total time dif 4987.973296880722)
step: 27730 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(1.5979264), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058312018401920795), 'actor_loss': np.float64(-0.9836498022079467), 'hyper_actor_loss': np.float64(7.11984597501214e-07), 'behavior_loss': np.float64(0.311151972413063)}

Episode step 27740, time diff 5.228427171707153, total time dif 4993.161374330521)
step: 27740 @ episode report: {'average_total_reward': np.float32(9.138889), 'reward_variance': np.float32(1.9646479), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06713438332080841), 'actor_loss': np.float64(-0.9708688199520111), 'hyper_actor_loss': np.float64(6.596885555154586e-07), 'behavior_loss': np.float64(0.2980400413274765)}

Episode step 27750, time diff 5.2361955642700195, total time dif 4998.389801502228)
step: 27750 @ episode report: {'average_total_reward': np.float32(9.451112), 'reward_variance': np.float32(1.321708), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07387641984969377), 'actor_loss': np.float64(-0.9874864399433136), 'hyper_actor_loss': np.float64(6.697771766539517e-07), 'behavior_loss': np.float64(0.3115070998668671)}

Episode step 27760, time diff 5.257009744644165, total time dif 5003.625997066498)
step: 27760 @ episode report: {'average_total_reward': np.float32(8.5633335), 'reward_variance': np.float32(5.8990135), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08082150630652904), 'actor_loss': np.float64(-0.9957832932472229), 'hyper_actor_loss': np.float64(7.416043160901608e-07), 'behavior_loss': np.float64(0.2999682366847992)}

Episode step 27770, time diff 5.197741746902466, total time dif 5008.883006811142)
step: 27770 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(4.604816), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06797677297145129), 'actor_loss': np.float64(-0.9901774942874908), 'hyper_actor_loss': np.float64(7.508888074880815e-07), 'behavior_loss': np.float64(0.31239830553531645)}

Episode step 27780, time diff 5.118473291397095, total time dif 5014.080748558044)
step: 27780 @ episode report: {'average_total_reward': np.float32(8.763333), 'reward_variance': np.float32(1.5177543), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07061252705752849), 'actor_loss': np.float64(-0.971146559715271), 'hyper_actor_loss': np.float64(8.816527321187096e-06), 'behavior_loss': np.float64(0.30469744801521303)}

Episode step 27790, time diff 5.140877962112427, total time dif 5019.1992218494415)
step: 27790 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(3.2052853), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0678245984017849), 'actor_loss': np.float64(-0.9775284349918365), 'hyper_actor_loss': np.float64(8.977682909971918e-06), 'behavior_loss': np.float64(0.30985869765281676)}

Episode step 27800, time diff 5.053038835525513, total time dif 5024.340099811554)
step: 27800 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(3.0166733), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056923318281769754), 'actor_loss': np.float64(-0.9783835768699646), 'hyper_actor_loss': np.float64(2.5715312858665127e-06), 'behavior_loss': np.float64(0.29791470170021056)}

Episode step 27810, time diff 5.135239839553833, total time dif 5029.3931386470795)
step: 27810 @ episode report: {'average_total_reward': np.float32(8.726666), 'reward_variance': np.float32(1.8146222), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07859477549791336), 'actor_loss': np.float64(-0.9630811154842377), 'hyper_actor_loss': np.float64(2.8523035325633827e-06), 'behavior_loss': np.float64(0.30818929672241213)}

Episode step 27820, time diff 5.073189735412598, total time dif 5034.528378486633)
step: 27820 @ episode report: {'average_total_reward': np.float32(9.687779), 'reward_variance': np.float32(1.9500103), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06521506942808628), 'actor_loss': np.float64(-0.9947681784629822), 'hyper_actor_loss': np.float64(2.736948749770818e-06), 'behavior_loss': np.float64(0.29891309440135955)}

Episode step 27830, time diff 5.005993604660034, total time dif 5039.601568222046)
step: 27830 @ episode report: {'average_total_reward': np.float32(9.587779), 'reward_variance': np.float32(1.8279861), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057701876200735566), 'actor_loss': np.float64(-0.9751562952995301), 'hyper_actor_loss': np.float64(2.7428060093370733e-06), 'behavior_loss': np.float64(0.303885406255722)}

Episode step 27840, time diff 5.111961603164673, total time dif 5044.607561826706)
step: 27840 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(2.5334435), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06906564496457576), 'actor_loss': np.float64(-0.9537783980369567), 'hyper_actor_loss': np.float64(2.6543537842371735e-06), 'behavior_loss': np.float64(0.3099109917879105)}

Episode step 27850, time diff 5.16867470741272, total time dif 5049.719523429871)
step: 27850 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(3.134321), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07559942118823529), 'actor_loss': np.float64(-0.9856335520744324), 'hyper_actor_loss': np.float64(2.5487910306765116e-06), 'behavior_loss': np.float64(0.31796354353427886)}

Episode step 27860, time diff 5.014815807342529, total time dif 5054.888198137283)
step: 27860 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(1.5808015), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07537484001368285), 'actor_loss': np.float64(-0.9900236308574677), 'hyper_actor_loss': np.float64(2.4400707388849695e-06), 'behavior_loss': np.float64(0.3157207667827606)}

Episode step 27870, time diff 5.033077001571655, total time dif 5059.903013944626)
step: 27870 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(2.2431917), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06420253664255142), 'actor_loss': np.float64(-0.9686772346496582), 'hyper_actor_loss': np.float64(2.3456930534848653e-06), 'behavior_loss': np.float64(0.3109875828027725)}

Episode step 27880, time diff 5.093275547027588, total time dif 5064.9360909461975)
step: 27880 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.2187471), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06175169683992863), 'actor_loss': np.float64(-0.967909437417984), 'hyper_actor_loss': np.float64(2.1668261183549477e-06), 'behavior_loss': np.float64(0.3082818388938904)}

Episode step 27890, time diff 5.127684593200684, total time dif 5070.029366493225)
step: 27890 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(3.1015327), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06755641922354698), 'actor_loss': np.float64(-0.9923677623271943), 'hyper_actor_loss': np.float64(1.9579302374950203e-06), 'behavior_loss': np.float64(0.2924976617097855)}

Episode step 27900, time diff 5.115141868591309, total time dif 5075.157051086426)
step: 27900 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.3761232), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07091664858162403), 'actor_loss': np.float64(-0.9802089035511017), 'hyper_actor_loss': np.float64(1.952675643224211e-06), 'behavior_loss': np.float64(0.3058922618627548)}

Episode step 27910, time diff 5.123167037963867, total time dif 5080.272192955017)
step: 27910 @ episode report: {'average_total_reward': np.float32(9.548889), 'reward_variance': np.float32(5.8025227), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06108499765396118), 'actor_loss': np.float64(-0.9746731281280517), 'hyper_actor_loss': np.float64(1.800343886770861e-06), 'behavior_loss': np.float64(0.30012130439281465)}

Episode step 27920, time diff 5.170443296432495, total time dif 5085.395359992981)
step: 27920 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(3.3259761), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061490038968622686), 'actor_loss': np.float64(-0.9677285134792328), 'hyper_actor_loss': np.float64(1.7659246509538207e-06), 'behavior_loss': np.float64(0.31286725103855134)}

Episode step 27930, time diff 5.175342798233032, total time dif 5090.565803289413)
step: 27930 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(2.9892418), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07588538639247418), 'actor_loss': np.float64(-0.9811857640743256), 'hyper_actor_loss': np.float64(1.4155516055325279e-06), 'behavior_loss': np.float64(0.31332955658435824)}

Episode step 27940, time diff 5.1924004554748535, total time dif 5095.7411460876465)
step: 27940 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(2.923957), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0671009797602892), 'actor_loss': np.float64(-0.9802061796188355), 'hyper_actor_loss': np.float64(1.333942850578751e-06), 'behavior_loss': np.float64(0.31852753460407257)}

Episode step 27950, time diff 5.189121246337891, total time dif 5100.933546543121)
step: 27950 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.9288647), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07697637490928173), 'actor_loss': np.float64(-0.9829583823680877), 'hyper_actor_loss': np.float64(1.2804649259123834e-06), 'behavior_loss': np.float64(0.30587762892246245)}

Episode step 27960, time diff 5.230353116989136, total time dif 5106.122667789459)
step: 27960 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(2.657063), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0618748651817441), 'actor_loss': np.float64(-0.9871429443359375), 'hyper_actor_loss': np.float64(1.2457349271244312e-06), 'behavior_loss': np.float64(0.30508054941892626)}

Episode step 27970, time diff 5.179542779922485, total time dif 5111.353020906448)
step: 27970 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(4.0109587), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06376685351133346), 'actor_loss': np.float64(-0.9692471027374268), 'hyper_actor_loss': np.float64(1.2602730407706986e-06), 'behavior_loss': np.float64(0.30112160444259645)}

Episode step 27980, time diff 5.213255405426025, total time dif 5116.532563686371)
step: 27980 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(1.3361977), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08239055648446084), 'actor_loss': np.float64(-0.9920029580593109), 'hyper_actor_loss': np.float64(1.7491188486928877e-06), 'behavior_loss': np.float64(0.314702644944191)}

Episode step 27990, time diff 5.29698634147644, total time dif 5121.745819091797)
step: 27990 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(2.8580506), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06675271168351174), 'actor_loss': np.float64(-0.9848105430603027), 'hyper_actor_loss': np.float64(2.0686894117716292e-06), 'behavior_loss': np.float64(0.3103576749563217)}

Episode step 28000, time diff 5.184523105621338, total time dif 5127.042805433273)
step: 28000 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(1.7732464), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05632545370608568), 'actor_loss': np.float64(-0.9576773464679718), 'hyper_actor_loss': np.float64(2.0338814238129998e-06), 'behavior_loss': np.float64(0.3074118196964264)}

Episode step 28010, time diff 5.25376033782959, total time dif 5132.227328538895)
step: 28010 @ episode report: {'average_total_reward': np.float32(8.987779), 'reward_variance': np.float32(4.8874197), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06639760192483664), 'actor_loss': np.float64(-0.9600148975849152), 'hyper_actor_loss': np.float64(2.428418883937411e-06), 'behavior_loss': np.float64(0.30891690850257875)}

Episode step 28020, time diff 5.4014363288879395, total time dif 5137.481088876724)
step: 28020 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(0.68264323), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05527168791741133), 'actor_loss': np.float64(-0.9849585950374603), 'hyper_actor_loss': np.float64(2.236702141544811e-06), 'behavior_loss': np.float64(0.29044783413410186)}

Episode step 28030, time diff 5.304124116897583, total time dif 5142.882525205612)
step: 28030 @ episode report: {'average_total_reward': np.float32(9.561111), 'reward_variance': np.float32(3.7121549), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07448589950799941), 'actor_loss': np.float64(-0.980315363407135), 'hyper_actor_loss': np.float64(2.196023319811502e-06), 'behavior_loss': np.float64(0.3093614548444748)}

Episode step 28040, time diff 5.3628830909729, total time dif 5148.18664932251)
step: 28040 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(1.1366929), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0640734251588583), 'actor_loss': np.float64(-0.9775079965591431), 'hyper_actor_loss': np.float64(1.9913360802092937e-06), 'behavior_loss': np.float64(0.30022901892662046)}

Episode step 28050, time diff 5.349883556365967, total time dif 5153.549532413483)
step: 28050 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(3.7642722), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06218699887394905), 'actor_loss': np.float64(-0.9730797708034515), 'hyper_actor_loss': np.float64(1.7248618519261073e-06), 'behavior_loss': np.float64(0.3139231204986572)}

Episode step 28060, time diff 5.322009801864624, total time dif 5158.899415969849)
step: 28060 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(3.8801105), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0719387847930193), 'actor_loss': np.float64(-0.9775663256645203), 'hyper_actor_loss': np.float64(1.6730058405300952e-06), 'behavior_loss': np.float64(0.297139498591423)}

Episode step 28070, time diff 5.276699781417847, total time dif 5164.221425771713)
step: 28070 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(3.723357), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06852984614670277), 'actor_loss': np.float64(-0.9868973970413208), 'hyper_actor_loss': np.float64(1.4902724046805814e-06), 'behavior_loss': np.float64(0.3098346143960953)}

Episode step 28080, time diff 5.318376302719116, total time dif 5169.498125553131)
step: 28080 @ episode report: {'average_total_reward': np.float32(9.551112), 'reward_variance': np.float32(3.019709), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0754313649609685), 'actor_loss': np.float64(-0.9813468813896179), 'hyper_actor_loss': np.float64(1.362635100576881e-06), 'behavior_loss': np.float64(0.2974465489387512)}

Episode step 28090, time diff 5.2059104442596436, total time dif 5174.81650185585)
step: 28090 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(1.7469155), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05951213650405407), 'actor_loss': np.float64(-0.9821941077709198), 'hyper_actor_loss': np.float64(1.3391612128543783e-06), 'behavior_loss': np.float64(0.3015868246555328)}

Episode step 28100, time diff 5.25818133354187, total time dif 5180.02241230011)
step: 28100 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(1.7548664), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08185193724930287), 'actor_loss': np.float64(-0.979293018579483), 'hyper_actor_loss': np.float64(1.4044841009308585e-06), 'behavior_loss': np.float64(0.31078411638736725)}

Episode step 28110, time diff 5.243168830871582, total time dif 5185.280593633652)
step: 28110 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(5.395655), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07035541757941247), 'actor_loss': np.float64(-0.9875973463058472), 'hyper_actor_loss': np.float64(1.2934751225657237e-06), 'behavior_loss': np.float64(0.30711873471736906)}

Episode step 28120, time diff 5.253928184509277, total time dif 5190.523762464523)
step: 28120 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(4.8917775), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06994078606367111), 'actor_loss': np.float64(-0.9818251430988312), 'hyper_actor_loss': np.float64(1.3798947861687339e-06), 'behavior_loss': np.float64(0.3040739446878433)}

Episode step 28130, time diff 5.187941074371338, total time dif 5195.777690649033)
step: 28130 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(1.1481742), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07550901360809803), 'actor_loss': np.float64(-0.9812499761581421), 'hyper_actor_loss': np.float64(1.3586533668785706e-06), 'behavior_loss': np.float64(0.3183071345090866)}

Episode step 28140, time diff 5.179242849349976, total time dif 5200.965631723404)
step: 28140 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(1.1796743), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07016254160553217), 'actor_loss': np.float64(-0.9844341695308685), 'hyper_actor_loss': np.float64(1.6716423260731972e-06), 'behavior_loss': np.float64(0.30340097546577455)}

Episode step 28150, time diff 5.17191219329834, total time dif 5206.144874572754)
step: 28150 @ episode report: {'average_total_reward': np.float32(9.536668), 'reward_variance': np.float32(2.5667915), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06683476455509663), 'actor_loss': np.float64(-0.9742010116577149), 'hyper_actor_loss': np.float64(1.9492876845106367e-06), 'behavior_loss': np.float64(0.3005054175853729)}

Episode step 28160, time diff 5.178766489028931, total time dif 5211.316786766052)
step: 28160 @ episode report: {'average_total_reward': np.float32(8.775557), 'reward_variance': np.float32(1.5438964), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0658882912248373), 'actor_loss': np.float64(-0.9677183449268341), 'hyper_actor_loss': np.float64(1.864022442532587e-06), 'behavior_loss': np.float64(0.32032368183135984)}

Episode step 28170, time diff 5.2982518672943115, total time dif 5216.495553255081)
step: 28170 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(2.6849988), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07801338173449039), 'actor_loss': np.float64(-0.9816623508930207), 'hyper_actor_loss': np.float64(1.7312900581600843e-06), 'behavior_loss': np.float64(0.30014199316501616)}

Episode step 28180, time diff 5.271115064620972, total time dif 5221.7938051223755)
step: 28180 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.5603569), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06661272831261159), 'actor_loss': np.float64(-0.9934589564800262), 'hyper_actor_loss': np.float64(1.5789031522217556e-06), 'behavior_loss': np.float64(0.31022439897060394)}

Episode step 28190, time diff 5.475021123886108, total time dif 5227.0649201869965)
step: 28190 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(1.9299374), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07755925692617893), 'actor_loss': np.float64(-0.9792639553546906), 'hyper_actor_loss': np.float64(1.5046123053252814e-06), 'behavior_loss': np.float64(0.32366715371608734)}

Episode step 28200, time diff 5.26941704750061, total time dif 5232.539941310883)
step: 28200 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(1.1077726), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07669995427131653), 'actor_loss': np.float64(-0.9803557813167572), 'hyper_actor_loss': np.float64(1.4874814155518834e-06), 'behavior_loss': np.float64(0.3075458571314812)}

Episode step 28210, time diff 5.281178951263428, total time dif 5237.809358358383)
step: 28210 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.2560055), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07585602961480617), 'actor_loss': np.float64(-1.0028774440288544), 'hyper_actor_loss': np.float64(1.4115689054960967e-06), 'behavior_loss': np.float64(0.30553933680057527)}

Episode step 28220, time diff 5.314887762069702, total time dif 5243.090537309647)
step: 28220 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.8667703), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062136137299239634), 'actor_loss': np.float64(-0.9866850852966309), 'hyper_actor_loss': np.float64(1.2596593933267285e-06), 'behavior_loss': np.float64(0.29567588269710543)}

Episode step 28230, time diff 5.2871198654174805, total time dif 5248.405425071716)
step: 28230 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(1.9500068), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06348943337798119), 'actor_loss': np.float64(-0.9605855047702789), 'hyper_actor_loss': np.float64(1.2523802638497728e-06), 'behavior_loss': np.float64(0.30536951422691344)}

Episode step 28240, time diff 5.3461408615112305, total time dif 5253.692544937134)
step: 28240 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(5.0781603), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06068975105881691), 'actor_loss': np.float64(-0.9699253082275391), 'hyper_actor_loss': np.float64(1.1452473358986026e-06), 'behavior_loss': np.float64(0.3028248816728592)}

Episode step 28250, time diff 5.361853837966919, total time dif 5259.038685798645)
step: 28250 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(2.4581294), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06754486076533794), 'actor_loss': np.float64(-0.9733993947505951), 'hyper_actor_loss': np.float64(1.0987527218730975e-06), 'behavior_loss': np.float64(0.3193806201219559)}

Episode step 28260, time diff 5.403671979904175, total time dif 5264.400539636612)
step: 28260 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(5.670081), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061923154070973395), 'actor_loss': np.float64(-0.9712807059288024), 'hyper_actor_loss': np.float64(1.093920019457073e-06), 'behavior_loss': np.float64(0.31057028770446776)}

Episode step 28270, time diff 5.437367677688599, total time dif 5269.804211616516)
step: 28270 @ episode report: {'average_total_reward': np.float32(9.214444), 'reward_variance': np.float32(2.8697295), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061488892324268817), 'actor_loss': np.float64(-0.9750387609004975), 'hyper_actor_loss': np.float64(1.0650195747530233e-06), 'behavior_loss': np.float64(0.3078874260187149)}

Episode step 28280, time diff 5.438084602355957, total time dif 5275.241579294205)
step: 28280 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(4.8023214), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07869086898863316), 'actor_loss': np.float64(-0.9696669578552246), 'hyper_actor_loss': np.float64(1.0144410452994635e-06), 'behavior_loss': np.float64(0.2970708966255188)}

Episode step 28290, time diff 5.455688714981079, total time dif 5280.679663896561)
step: 28290 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(3.0604463), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07650768719613552), 'actor_loss': np.float64(-0.992967814207077), 'hyper_actor_loss': np.float64(9.941862174400739e-07), 'behavior_loss': np.float64(0.320054417848587)}

Episode step 28300, time diff 5.475851058959961, total time dif 5286.135352611542)
step: 28300 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(0.86388123), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07911422401666642), 'actor_loss': np.float64(-0.9926533043384552), 'hyper_actor_loss': np.float64(9.982013011722302e-07), 'behavior_loss': np.float64(0.3072157084941864)}

Episode step 28310, time diff 5.4852776527404785, total time dif 5291.611203670502)
step: 28310 @ episode report: {'average_total_reward': np.float32(10.099999), 'reward_variance': np.float32(1.4891849), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06322313025593758), 'actor_loss': np.float64(-0.984866452217102), 'hyper_actor_loss': np.float64(9.53739333908743e-07), 'behavior_loss': np.float64(0.3018992990255356)}

Episode step 28320, time diff 5.477403163909912, total time dif 5297.096481323242)
step: 28320 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(0.9168442), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07005355581641197), 'actor_loss': np.float64(-0.9807519376277923), 'hyper_actor_loss': np.float64(9.787788201265358e-07), 'behavior_loss': np.float64(0.3010737538337708)}

Episode step 28330, time diff 5.53462290763855, total time dif 5302.573884487152)
step: 28330 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(1.5229499), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06376408897340298), 'actor_loss': np.float64(-0.9754275977611542), 'hyper_actor_loss': np.float64(9.44749854170368e-07), 'behavior_loss': np.float64(0.2925559729337692)}

Episode step 28340, time diff 5.483928442001343, total time dif 5308.108507394791)
step: 28340 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(1.6391618), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07226352170109748), 'actor_loss': np.float64(-0.9830873489379883), 'hyper_actor_loss': np.float64(9.400529108916089e-07), 'behavior_loss': np.float64(0.294178718328476)}

Episode step 28350, time diff 5.658846378326416, total time dif 5313.592435836792)
step: 28350 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(3.1104152), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057463721558451655), 'actor_loss': np.float64(-0.9792070865631104), 'hyper_actor_loss': np.float64(8.761656999922707e-07), 'behavior_loss': np.float64(0.29259476959705355)}

Episode step 28360, time diff 5.533145904541016, total time dif 5319.251282215118)
step: 28360 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(3.1812153), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07782028131186962), 'actor_loss': np.float64(-0.9748722434043884), 'hyper_actor_loss': np.float64(7.71362488194427e-07), 'behavior_loss': np.float64(0.29867608547210694)}

Episode step 28370, time diff 5.516421794891357, total time dif 5324.784428119659)
step: 28370 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(5.3982334), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07093065418303013), 'actor_loss': np.float64(-0.9998972237110137), 'hyper_actor_loss': np.float64(7.442951357461425e-07), 'behavior_loss': np.float64(0.3078885316848755)}

Episode step 28380, time diff 5.594380617141724, total time dif 5330.300849914551)
step: 28380 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(1.9443945), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08282783776521682), 'actor_loss': np.float64(-0.9933857381343841), 'hyper_actor_loss': np.float64(7.293184296486288e-07), 'behavior_loss': np.float64(0.3185407221317291)}

Episode step 28390, time diff 5.595031261444092, total time dif 5335.8952305316925)
step: 28390 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.1632147), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05983789814636111), 'actor_loss': np.float64(-0.9807794749736786), 'hyper_actor_loss': np.float64(6.933670817943493e-07), 'behavior_loss': np.float64(0.30112434476614)}

Episode step 28400, time diff 5.6039416790008545, total time dif 5341.490261793137)
step: 28400 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(4.0510926), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07374599389731884), 'actor_loss': np.float64(-0.9693334519863128), 'hyper_actor_loss': np.float64(7.532043468927441e-07), 'behavior_loss': np.float64(0.2986637979745865)}

Episode step 28410, time diff 5.619687557220459, total time dif 5347.094203472137)
step: 28410 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(4.2811484), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06354534924030304), 'actor_loss': np.float64(-0.9907213747501373), 'hyper_actor_loss': np.float64(7.742695515844389e-07), 'behavior_loss': np.float64(0.292034375667572)}

Episode step 28420, time diff 5.632718801498413, total time dif 5352.713891029358)
step: 28420 @ episode report: {'average_total_reward': np.float32(9.724446), 'reward_variance': np.float32(3.8221188), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0722982119768858), 'actor_loss': np.float64(-0.9755832314491272), 'hyper_actor_loss': np.float64(7.101091512140556e-07), 'behavior_loss': np.float64(0.3164981544017792)}

Episode step 28430, time diff 5.587602853775024, total time dif 5358.346609830856)
step: 28430 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.3552295), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.052191452495753764), 'actor_loss': np.float64(-0.9644055545330048), 'hyper_actor_loss': np.float64(6.936781119293301e-07), 'behavior_loss': np.float64(0.2988672971725464)}

Episode step 28440, time diff 5.6401965618133545, total time dif 5363.934212684631)
step: 28440 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(4.1664305), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07452620938420296), 'actor_loss': np.float64(-0.9734540700912475), 'hyper_actor_loss': np.float64(6.797848186579358e-07), 'behavior_loss': np.float64(0.3133760571479797)}

Episode step 28450, time diff 5.689146518707275, total time dif 5369.574409246445)
step: 28450 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(4.3938627), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059982335194945335), 'actor_loss': np.float64(-0.9817009270191193), 'hyper_actor_loss': np.float64(6.22783312564934e-07), 'behavior_loss': np.float64(0.30165574252605437)}

Episode step 28460, time diff 5.63070821762085, total time dif 5375.263555765152)
step: 28460 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(1.3014934), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06332413218915463), 'actor_loss': np.float64(-0.9696710586547852), 'hyper_actor_loss': np.float64(6.381557682288985e-07), 'behavior_loss': np.float64(0.31174922585487364)}

Episode step 28470, time diff 5.6683430671691895, total time dif 5380.894263982773)
step: 28470 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(1.549167), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05876538101583719), 'actor_loss': np.float64(-0.9601049780845642), 'hyper_actor_loss': np.float64(5.919528518916195e-07), 'behavior_loss': np.float64(0.30299913585186006)}

Episode step 28480, time diff 5.723607301712036, total time dif 5386.562607049942)
step: 28480 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(3.4883704), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0660570167005062), 'actor_loss': np.float64(-0.9690139055252075), 'hyper_actor_loss': np.float64(5.848406885888835e-07), 'behavior_loss': np.float64(0.3032835453748703)}

Episode step 28490, time diff 5.731388807296753, total time dif 5392.286214351654)
step: 28490 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(5.2640495), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07670494485646487), 'actor_loss': np.float64(-0.9939984858036042), 'hyper_actor_loss': np.float64(5.663286572143988e-07), 'behavior_loss': np.float64(0.31183391213417055)}

Episode step 28500, time diff 5.675873517990112, total time dif 5398.017603158951)
step: 28500 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(1.5066619), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08614791035652161), 'actor_loss': np.float64(-0.9910773932933807), 'hyper_actor_loss': np.float64(5.565452511291369e-07), 'behavior_loss': np.float64(0.31402183175086973)}

Episode step 28510, time diff 5.697315454483032, total time dif 5403.693476676941)
step: 28510 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(1.257773), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06966755511239171), 'actor_loss': np.float64(-0.9852412879467011), 'hyper_actor_loss': np.float64(5.099063628222211e-07), 'behavior_loss': np.float64(0.2880349919199944)}

Episode step 28520, time diff 5.878226280212402, total time dif 5409.390792131424)
step: 28520 @ episode report: {'average_total_reward': np.float32(9.551111), 'reward_variance': np.float32(1.2132146), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07682666704058647), 'actor_loss': np.float64(-0.980864804983139), 'hyper_actor_loss': np.float64(5.296505179330779e-07), 'behavior_loss': np.float64(0.3254499465227127)}

Episode step 28530, time diff 5.653941869735718, total time dif 5415.269018411636)
step: 28530 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(4.0894866), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07569636851549148), 'actor_loss': np.float64(-0.9919204592704773), 'hyper_actor_loss': np.float64(5.314389170507639e-07), 'behavior_loss': np.float64(0.3224960893392563)}

Episode step 28540, time diff 5.591918230056763, total time dif 5420.922960281372)
step: 28540 @ episode report: {'average_total_reward': np.float32(9.863334), 'reward_variance': np.float32(2.2730136), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05998865775763988), 'actor_loss': np.float64(-0.9730328798294068), 'hyper_actor_loss': np.float64(5.158219607892534e-07), 'behavior_loss': np.float64(0.30929396748542787)}

Episode step 28550, time diff 5.606501579284668, total time dif 5426.514878511429)
step: 28550 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(2.6106226), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059652078151702884), 'actor_loss': np.float64(-0.9552932798862457), 'hyper_actor_loss': np.float64(5.184181588901992e-07), 'behavior_loss': np.float64(0.2922314047813416)}

Episode step 28560, time diff 5.588097810745239, total time dif 5432.1213800907135)
step: 28560 @ episode report: {'average_total_reward': np.float32(10.200002), 'reward_variance': np.float32(1.4489142), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07562582530081272), 'actor_loss': np.float64(-0.9819721877574921), 'hyper_actor_loss': np.float64(5.074446789876674e-07), 'behavior_loss': np.float64(0.3010370761156082)}

Episode step 28570, time diff 5.5970869064331055, total time dif 5437.709477901459)
step: 28570 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.8788936), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06611494421958923), 'actor_loss': np.float64(-0.9982062101364135), 'hyper_actor_loss': np.float64(4.7207948625782594e-07), 'behavior_loss': np.float64(0.2964618533849716)}

Episode step 28580, time diff 5.553771018981934, total time dif 5443.306564807892)
step: 28580 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.9806426), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06720702536404133), 'actor_loss': np.float64(-0.9833188116550445), 'hyper_actor_loss': np.float64(4.630279022421746e-07), 'behavior_loss': np.float64(0.2977925598621368)}

Episode step 28590, time diff 5.609005689620972, total time dif 5448.860335826874)
step: 28590 @ episode report: {'average_total_reward': np.float32(9.900001), 'reward_variance': np.float32(5.2140737), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06450914740562438), 'actor_loss': np.float64(-0.9884709477424621), 'hyper_actor_loss': np.float64(4.3984078104131184e-07), 'behavior_loss': np.float64(0.3210469126701355)}

Episode step 28600, time diff 5.527216911315918, total time dif 5454.469341516495)
step: 28600 @ episode report: {'average_total_reward': np.float32(9.075557), 'reward_variance': np.float32(1.4071063), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06977180279791355), 'actor_loss': np.float64(-0.9666183829307556), 'hyper_actor_loss': np.float64(7.135135291491679e-07), 'behavior_loss': np.float64(0.3146406590938568)}

Episode step 28610, time diff 5.4414873123168945, total time dif 5459.996558427811)
step: 28610 @ episode report: {'average_total_reward': np.float32(9.8), 'reward_variance': np.float32(3.4910614), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07305791899561882), 'actor_loss': np.float64(-0.9809745490550995), 'hyper_actor_loss': np.float64(5.606824004189548e-07), 'behavior_loss': np.float64(0.3090255334973335)}

Episode step 28620, time diff 5.4637131690979, total time dif 5465.438045740128)
step: 28620 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(2.076252), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06129081733524799), 'actor_loss': np.float64(-0.9942254722118378), 'hyper_actor_loss': np.float64(5.007328127248912e-07), 'behavior_loss': np.float64(0.2995500713586807)}

Episode step 28630, time diff 5.437851667404175, total time dif 5470.9017589092255)
step: 28630 @ episode report: {'average_total_reward': np.float32(8.514444), 'reward_variance': np.float32(1.9708418), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07257366217672825), 'actor_loss': np.float64(-0.9723878741264343), 'hyper_actor_loss': np.float64(4.911691803499707e-07), 'behavior_loss': np.float64(0.29993508756160736)}

Episode step 28640, time diff 5.460449695587158, total time dif 5476.33961057663)
step: 28640 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(2.249567), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06826751939952373), 'actor_loss': np.float64(-0.9703707039356232), 'hyper_actor_loss': np.float64(4.577509542968983e-07), 'behavior_loss': np.float64(0.3088477522134781)}

Episode step 28650, time diff 5.42535662651062, total time dif 5481.800060272217)
step: 28650 @ episode report: {'average_total_reward': np.float32(10.012222), 'reward_variance': np.float32(5.6196904), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06655367985367774), 'actor_loss': np.float64(-0.9756996989250183), 'hyper_actor_loss': np.float64(4.60656929135439e-07), 'behavior_loss': np.float64(0.3087391793727875)}

Episode step 28660, time diff 5.350651264190674, total time dif 5487.225416898727)
step: 28660 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(6.9617805), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06375275012105704), 'actor_loss': np.float64(-0.9830241024494171), 'hyper_actor_loss': np.float64(4.530411189307415e-07), 'behavior_loss': np.float64(0.3003260552883148)}

Episode step 28670, time diff 5.340844631195068, total time dif 5492.576068162918)
step: 28670 @ episode report: {'average_total_reward': np.float32(9.126667), 'reward_variance': np.float32(1.4406459), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06854206845164298), 'actor_loss': np.float64(-0.9777476012706756), 'hyper_actor_loss': np.float64(4.481412929635553e-07), 'behavior_loss': np.float64(0.3085764467716217)}

Episode step 28680, time diff 5.579855442047119, total time dif 5497.916912794113)
step: 28680 @ episode report: {'average_total_reward': np.float32(10.012223), 'reward_variance': np.float32(3.9338639), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06745868362486362), 'actor_loss': np.float64(-0.9775002419948577), 'hyper_actor_loss': np.float64(3.837656237237752e-07), 'behavior_loss': np.float64(0.3115214616060257)}

Episode step 28690, time diff 5.416101932525635, total time dif 5503.49676823616)
step: 28690 @ episode report: {'average_total_reward': np.float32(10.0611105), 'reward_variance': np.float32(2.206253), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07213680893182754), 'actor_loss': np.float64(-0.9765120625495911), 'hyper_actor_loss': np.float64(3.7294074957117117e-07), 'behavior_loss': np.float64(0.301818411052227)}

Episode step 28700, time diff 5.456936359405518, total time dif 5508.912870168686)
step: 28700 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(2.9258466), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06825791709125043), 'actor_loss': np.float64(-0.9899420738220215), 'hyper_actor_loss': np.float64(3.639972618429965e-07), 'behavior_loss': np.float64(0.3120881259441376)}

Episode step 28710, time diff 5.462364435195923, total time dif 5514.369806528091)
step: 28710 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(2.6631463), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06301660314202309), 'actor_loss': np.float64(-0.9721955537796021), 'hyper_actor_loss': np.float64(3.275244893075069e-07), 'behavior_loss': np.float64(0.29711903929710387)}

Episode step 28720, time diff 5.497888565063477, total time dif 5519.832170963287)
step: 28720 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(1.850654), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07872807793319225), 'actor_loss': np.float64(-0.9769472897052764), 'hyper_actor_loss': np.float64(3.4674615960739176e-07), 'behavior_loss': np.float64(0.3034542828798294)}

Episode step 28730, time diff 5.501969814300537, total time dif 5525.330059528351)
step: 28730 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.1802232), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07174733616411685), 'actor_loss': np.float64(-0.9976216673851013), 'hyper_actor_loss': np.float64(3.1667678683788837e-07), 'behavior_loss': np.float64(0.3183716356754303)}

Episode step 28740, time diff 5.438173055648804, total time dif 5530.832029342651)
step: 28740 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(2.8109183), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07397928908467293), 'actor_loss': np.float64(-0.9910657823085784), 'hyper_actor_loss': np.float64(3.147458755847765e-07), 'behavior_loss': np.float64(0.3017358064651489)}

Episode step 28750, time diff 5.376680374145508, total time dif 5536.2702023983)
step: 28750 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(1.3488013), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06122646555304527), 'actor_loss': np.float64(-0.9664200127124787), 'hyper_actor_loss': np.float64(3.119368884085816e-07), 'behavior_loss': np.float64(0.3199920117855072)}

Episode step 28760, time diff 5.385788679122925, total time dif 5541.646882772446)
step: 28760 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(2.2335553), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06595785776153207), 'actor_loss': np.float64(-0.958936458826065), 'hyper_actor_loss': np.float64(2.969528736684879e-07), 'behavior_loss': np.float64(0.30884331166744233)}

Episode step 28770, time diff 5.573967218399048, total time dif 5547.032671451569)
step: 28770 @ episode report: {'average_total_reward': np.float32(9.338888), 'reward_variance': np.float32(2.7572901), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06617950387299061), 'actor_loss': np.float64(-0.976049417257309), 'hyper_actor_loss': np.float64(2.8771714255526604e-07), 'behavior_loss': np.float64(0.2997890278697014)}

Episode step 28780, time diff 5.513262987136841, total time dif 5552.606638669968)
step: 28780 @ episode report: {'average_total_reward': np.float32(9.238889), 'reward_variance': np.float32(1.3365247), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05905267298221588), 'actor_loss': np.float64(-0.9829480051994324), 'hyper_actor_loss': np.float64(2.4996141121391704e-07), 'behavior_loss': np.float64(0.30281582474708557)}

Episode step 28790, time diff 5.522097826004028, total time dif 5558.1199016571045)
step: 28790 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(1.6073593), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0725990666076541), 'actor_loss': np.float64(-0.9721413016319275), 'hyper_actor_loss': np.float64(2.398883850673883e-07), 'behavior_loss': np.float64(0.31512993276119233)}

Episode step 28800, time diff 5.5163867473602295, total time dif 5563.6419994831085)
step: 28800 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(2.273212), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07073185294866562), 'actor_loss': np.float64(-0.9897915899753571), 'hyper_actor_loss': np.float64(2.388605381042908e-07), 'behavior_loss': np.float64(0.30257411003112794)}

Episode step 28810, time diff 5.582435369491577, total time dif 5569.158386230469)
step: 28810 @ episode report: {'average_total_reward': np.float32(9.675555), 'reward_variance': np.float32(1.5660943), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06606247685849667), 'actor_loss': np.float64(-0.9932992100715637), 'hyper_actor_loss': np.float64(2.2590133568201055e-07), 'behavior_loss': np.float64(0.3040514290332794)}

Episode step 28820, time diff 5.508632183074951, total time dif 5574.74082159996)
step: 28820 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.802919), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06313509121537209), 'actor_loss': np.float64(-0.9629811227321625), 'hyper_actor_loss': np.float64(2.3700403914972412e-07), 'behavior_loss': np.float64(0.3189800351858139)}

Episode step 28830, time diff 5.517957925796509, total time dif 5580.249453783035)
step: 28830 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(3.4292207), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06506868451833725), 'actor_loss': np.float64(-0.9590763688087464), 'hyper_actor_loss': np.float64(2.0841070664801008e-07), 'behavior_loss': np.float64(0.3069490700960159)}

Episode step 28840, time diff 5.4480884075164795, total time dif 5585.767411708832)
step: 28840 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(2.657567), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06334562078118325), 'actor_loss': np.float64(-0.9793546795845032), 'hyper_actor_loss': np.float64(2.0128192801394107e-07), 'behavior_loss': np.float64(0.31279079914093016)}

Episode step 28850, time diff 5.74300479888916, total time dif 5591.215500116348)
step: 28850 @ episode report: {'average_total_reward': np.float32(8.938889), 'reward_variance': np.float32(1.3408949), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07008382454514503), 'actor_loss': np.float64(-0.9779764950275421), 'hyper_actor_loss': np.float64(1.9859523519016876e-07), 'behavior_loss': np.float64(0.3029442936182022)}

Episode step 28860, time diff 5.58018159866333, total time dif 5596.958504915237)
step: 28860 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(1.9357086), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07222395278513431), 'actor_loss': np.float64(-0.980594527721405), 'hyper_actor_loss': np.float64(1.7495585069582376e-07), 'behavior_loss': np.float64(0.30060902535915374)}

Episode step 28870, time diff 5.590362310409546, total time dif 5602.538686513901)
step: 28870 @ episode report: {'average_total_reward': np.float32(9.126667), 'reward_variance': np.float32(3.2282271), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08319194279611111), 'actor_loss': np.float64(-1.0084046959877013), 'hyper_actor_loss': np.float64(1.8426475492105965e-07), 'behavior_loss': np.float64(0.30998470485210416)}

Episode step 28880, time diff 5.576833248138428, total time dif 5608.12904882431)
step: 28880 @ episode report: {'average_total_reward': np.float32(9.673334), 'reward_variance': np.float32(4.7120543), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06673015020787716), 'actor_loss': np.float64(-0.9973969161510468), 'hyper_actor_loss': np.float64(1.7710974447027185e-07), 'behavior_loss': np.float64(0.3077548652887344)}

Episode step 28890, time diff 5.567680358886719, total time dif 5613.705882072449)
step: 28890 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(1.9078716), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07775440774857997), 'actor_loss': np.float64(-0.9728210151195527), 'hyper_actor_loss': np.float64(1.6089405505681497e-07), 'behavior_loss': np.float64(0.2979272872209549)}

Episode step 28900, time diff 5.55477499961853, total time dif 5619.273562431335)
step: 28900 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(2.1377974), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07169348113238812), 'actor_loss': np.float64(-0.990966135263443), 'hyper_actor_loss': np.float64(1.604082868311707e-07), 'behavior_loss': np.float64(0.3093504160642624)}

Episode step 28910, time diff 5.579463958740234, total time dif 5624.828337430954)
step: 28910 @ episode report: {'average_total_reward': np.float32(9.675555), 'reward_variance': np.float32(2.174588), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0733948815613985), 'actor_loss': np.float64(-0.9855114221572876), 'hyper_actor_loss': np.float64(1.450734941954579e-07), 'behavior_loss': np.float64(0.32015972435474394)}

Episode step 28920, time diff 5.611660957336426, total time dif 5630.407801389694)
step: 28920 @ episode report: {'average_total_reward': np.float32(9.463334), 'reward_variance': np.float32(1.3280509), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06761030629277229), 'actor_loss': np.float64(-0.9758946537971497), 'hyper_actor_loss': np.float64(1.3059450836294674e-07), 'behavior_loss': np.float64(0.3083977162837982)}

Episode step 28930, time diff 5.662818193435669, total time dif 5636.019462347031)
step: 28930 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(1.6720064), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0738007877022028), 'actor_loss': np.float64(-0.9933464348316192), 'hyper_actor_loss': np.float64(1.3930642310810982e-07), 'behavior_loss': np.float64(0.29486226439476015)}

Episode step 28940, time diff 5.6394970417022705, total time dif 5641.682280540466)
step: 28940 @ episode report: {'average_total_reward': np.float32(10.3122225), 'reward_variance': np.float32(2.4615169), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07390719130635262), 'actor_loss': np.float64(-1.0037968754768372), 'hyper_actor_loss': np.float64(1.1547094445063521e-07), 'behavior_loss': np.float64(0.2993750929832458)}

Episode step 28950, time diff 5.638522148132324, total time dif 5647.321777582169)
step: 28950 @ episode report: {'average_total_reward': np.float32(9.675557), 'reward_variance': np.float32(1.7660933), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06891904678195715), 'actor_loss': np.float64(-0.9834769666194916), 'hyper_actor_loss': np.float64(1.1964150559151676e-07), 'behavior_loss': np.float64(0.30981292724609377)}

Episode step 28960, time diff 5.6107611656188965, total time dif 5652.960299730301)
step: 28960 @ episode report: {'average_total_reward': np.float32(9.212223), 'reward_variance': np.float32(1.2359619), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07602078542113304), 'actor_loss': np.float64(-0.9778757810592651), 'hyper_actor_loss': np.float64(1.1883067188023234e-07), 'behavior_loss': np.float64(0.3018907725811005)}

Episode step 28970, time diff 5.588428735733032, total time dif 5658.57106089592)
step: 28970 @ episode report: {'average_total_reward': np.float32(9.114446), 'reward_variance': np.float32(1.8091373), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06797166317701339), 'actor_loss': np.float64(-0.9885946631431579), 'hyper_actor_loss': np.float64(1.0615613774689336e-07), 'behavior_loss': np.float64(0.3085616141557693)}

Episode step 28980, time diff 5.676468133926392, total time dif 5664.159489631653)
step: 28980 @ episode report: {'average_total_reward': np.float32(8.887779), 'reward_variance': np.float32(3.6727529), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07334510684013366), 'actor_loss': np.float64(-0.9973778188228607), 'hyper_actor_loss': np.float64(1.042646694315863e-07), 'behavior_loss': np.float64(0.2900388091802597)}

Episode step 28990, time diff 5.761319398880005, total time dif 5669.835957765579)
step: 28990 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.489427), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05779302194714546), 'actor_loss': np.float64(-0.9808684527873993), 'hyper_actor_loss': np.float64(1.2250328964569235e-07), 'behavior_loss': np.float64(0.3062795788049698)}

Episode step 29000, time diff 5.709532976150513, total time dif 5675.597277164459)
step: 29000 @ episode report: {'average_total_reward': np.float32(9.875555), 'reward_variance': np.float32(1.2098958), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05767397247254848), 'actor_loss': np.float64(-0.9579633116722107), 'hyper_actor_loss': np.float64(1.1502943877417238e-07), 'behavior_loss': np.float64(0.3000132292509079)}

Episode step 29010, time diff 5.865303993225098, total time dif 5681.30681014061)
step: 29010 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.482968), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06149098575115204), 'actor_loss': np.float64(-0.9704615235328674), 'hyper_actor_loss': np.float64(1.0638552936370615e-07), 'behavior_loss': np.float64(0.30493625402450564)}

Episode step 29020, time diff 5.847469091415405, total time dif 5687.172114133835)
step: 29020 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(2.6127517), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0681580051779747), 'actor_loss': np.float64(-0.9760137438774109), 'hyper_actor_loss': np.float64(1.1568611881784819e-07), 'behavior_loss': np.float64(0.30121816098690035)}

Episode step 29030, time diff 5.8539838790893555, total time dif 5693.01958322525)
step: 29030 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(1.755551), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07031498700380326), 'actor_loss': np.float64(-0.9852829933166504), 'hyper_actor_loss': np.float64(1.1165328999140911e-07), 'behavior_loss': np.float64(0.31096294820308684)}

Episode step 29040, time diff 5.8044753074646, total time dif 5698.87356710434)
step: 29040 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.2958226), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06396667249500751), 'actor_loss': np.float64(-0.9895075380802154), 'hyper_actor_loss': np.float64(1.0785619579678496e-07), 'behavior_loss': np.float64(0.3079711407423019)}

Episode step 29050, time diff 5.811936616897583, total time dif 5704.678042411804)
step: 29050 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.9642768), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08406225740909576), 'actor_loss': np.float64(-0.9803675889968873), 'hyper_actor_loss': np.float64(1.0714830764868565e-07), 'behavior_loss': np.float64(0.30964507460594176)}

Episode step 29060, time diff 5.800392150878906, total time dif 5710.489979028702)
step: 29060 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(4.8859262), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07105543948709965), 'actor_loss': np.float64(-0.9928384363651276), 'hyper_actor_loss': np.float64(1.0059064479150947e-07), 'behavior_loss': np.float64(0.30751965641975404)}

Episode step 29070, time diff 5.741021156311035, total time dif 5716.290371179581)
step: 29070 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(3.5778534), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058804810605943204), 'actor_loss': np.float64(-0.9765184938907623), 'hyper_actor_loss': np.float64(9.813078065690206e-08), 'behavior_loss': np.float64(0.3109788686037064)}

Episode step 29080, time diff 5.694151878356934, total time dif 5722.031392335892)
step: 29080 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(2.2664986), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06601412072777749), 'actor_loss': np.float64(-0.9612798869609833), 'hyper_actor_loss': np.float64(9.063969770295444e-08), 'behavior_loss': np.float64(0.2955319404602051)}

Episode step 29090, time diff 5.698254346847534, total time dif 5727.725544214249)
step: 29090 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(3.4680305), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07412519715726376), 'actor_loss': np.float64(-0.9926495492458344), 'hyper_actor_loss': np.float64(7.13896739057418e-08), 'behavior_loss': np.float64(0.31231583952903746)}

Episode step 29100, time diff 5.756510972976685, total time dif 5733.423798561096)
step: 29100 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(1.9196249), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06989026814699173), 'actor_loss': np.float64(-0.994497787952423), 'hyper_actor_loss': np.float64(7.662754484272227e-08), 'behavior_loss': np.float64(0.30487356185913084)}

Episode step 29110, time diff 5.851538181304932, total time dif 5739.180309534073)
step: 29110 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(4.422944), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07273410446941853), 'actor_loss': np.float64(-0.9801223158836365), 'hyper_actor_loss': np.float64(8.288504460551849e-08), 'behavior_loss': np.float64(0.3139009356498718)}

Episode step 29120, time diff 5.89924693107605, total time dif 5745.031847715378)
step: 29120 @ episode report: {'average_total_reward': np.float32(9.214445), 'reward_variance': np.float32(1.442766), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06499221473932267), 'actor_loss': np.float64(-0.9778847873210907), 'hyper_actor_loss': np.float64(8.384276029005377e-08), 'behavior_loss': np.float64(0.3064565062522888)}

Episode step 29130, time diff 5.980358839035034, total time dif 5750.931094646454)
step: 29130 @ episode report: {'average_total_reward': np.float32(8.514444), 'reward_variance': np.float32(3.1509147), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06353119695559144), 'actor_loss': np.float64(-0.9633236348628997), 'hyper_actor_loss': np.float64(1.5949748686239217e-07), 'behavior_loss': np.float64(0.30702372789382937)}

Episode step 29140, time diff 5.965798854827881, total time dif 5756.911453485489)
step: 29140 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(3.040693), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07382096350193024), 'actor_loss': np.float64(-0.9826257407665253), 'hyper_actor_loss': np.float64(2.4708600037115503e-07), 'behavior_loss': np.float64(0.30397692024707795)}

Episode step 29150, time diff 5.8453309535980225, total time dif 5762.877252340317)
step: 29150 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.9025047), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06814380642026663), 'actor_loss': np.float64(-0.9994439899921417), 'hyper_actor_loss': np.float64(1.8304420521531028e-07), 'behavior_loss': np.float64(0.30540340542793276)}

Episode step 29160, time diff 5.9887144565582275, total time dif 5768.722583293915)
step: 29160 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(4.8462243), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06791089177131653), 'actor_loss': np.float64(-0.9788201510906219), 'hyper_actor_loss': np.float64(8.94832005826629e-08), 'behavior_loss': np.float64(0.29685090482234955)}

Episode step 29170, time diff 6.080684423446655, total time dif 5774.711297750473)
step: 29170 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(4.1055427), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06899027209728956), 'actor_loss': np.float64(-0.9744679450988769), 'hyper_actor_loss': np.float64(8.8490462246682e-08), 'behavior_loss': np.float64(0.30868653357028963)}

Episode step 29180, time diff 6.228510141372681, total time dif 5780.79198217392)
step: 29180 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.9734865), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06372982542961836), 'actor_loss': np.float64(-0.9779139280319213), 'hyper_actor_loss': np.float64(8.11421060120665e-08), 'behavior_loss': np.float64(0.3248144954442978)}

Episode step 29190, time diff 6.118124961853027, total time dif 5787.020492315292)
step: 29190 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(1.6529185), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07381628677248955), 'actor_loss': np.float64(-0.975213223695755), 'hyper_actor_loss': np.float64(7.790589471312614e-08), 'behavior_loss': np.float64(0.3100853770971298)}

Episode step 29200, time diff 6.1559507846832275, total time dif 5793.138617277145)
step: 29200 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(3.4958568), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05814106278121471), 'actor_loss': np.float64(-0.9808568835258484), 'hyper_actor_loss': np.float64(8.009887224602607e-08), 'behavior_loss': np.float64(0.29601470828056337)}

Episode step 29210, time diff 6.179906368255615, total time dif 5799.294568061829)
step: 29210 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(2.725131), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06956134662032128), 'actor_loss': np.float64(-0.9796287715435028), 'hyper_actor_loss': np.float64(8.70165770550102e-08), 'behavior_loss': np.float64(0.31814135015010836)}

Episode step 29220, time diff 6.198361873626709, total time dif 5805.474474430084)
step: 29220 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.5925927), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06910339184105396), 'actor_loss': np.float64(-0.9757735550403595), 'hyper_actor_loss': np.float64(9.969848093760448e-08), 'behavior_loss': np.float64(0.30953982174396516)}

Episode step 29230, time diff 6.166752576828003, total time dif 5811.672836303711)
step: 29230 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(3.2325883), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07376824095845222), 'actor_loss': np.float64(-0.9871836721897125), 'hyper_actor_loss': np.float64(9.548399191317002e-08), 'behavior_loss': np.float64(0.30610205233097076)}

Episode step 29240, time diff 6.175761461257935, total time dif 5817.839588880539)
step: 29240 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(1.2021288), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07119401022791863), 'actor_loss': np.float64(-0.9916724801063538), 'hyper_actor_loss': np.float64(1.1157421297980363e-07), 'behavior_loss': np.float64(0.30434337109327314)}

Episode step 29250, time diff 6.18887734413147, total time dif 5824.015350341797)
step: 29250 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(3.2256565), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0825779926031828), 'actor_loss': np.float64(-0.9989167928695679), 'hyper_actor_loss': np.float64(1.0696633410134381e-07), 'behavior_loss': np.float64(0.30375619828701017)}

Episode step 29260, time diff 6.165153503417969, total time dif 5830.204227685928)
step: 29260 @ episode report: {'average_total_reward': np.float32(9.74889), 'reward_variance': np.float32(3.7478065), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0756559669971466), 'actor_loss': np.float64(-1.0018924474716187), 'hyper_actor_loss': np.float64(2.3783092331086664e-07), 'behavior_loss': np.float64(0.30738246738910674)}

Episode step 29270, time diff 6.035983085632324, total time dif 5836.369381189346)
step: 29270 @ episode report: {'average_total_reward': np.float32(9.351111), 'reward_variance': np.float32(1.5129675), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05948474816977978), 'actor_loss': np.float64(-0.9843030452728272), 'hyper_actor_loss': np.float64(2.4450837088352273e-07), 'behavior_loss': np.float64(0.30044244825839994)}

Episode step 29280, time diff 6.0171058177948, total time dif 5842.405364274979)
step: 29280 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(3.0874934), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06178134363144636), 'actor_loss': np.float64(-0.960941481590271), 'hyper_actor_loss': np.float64(1.4597846984543139e-07), 'behavior_loss': np.float64(0.31251104176044464)}

Episode step 29290, time diff 6.076035499572754, total time dif 5848.422470092773)
step: 29290 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(1.3187472), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07893457151949405), 'actor_loss': np.float64(-0.9742993831634521), 'hyper_actor_loss': np.float64(1.5078177142413552e-07), 'behavior_loss': np.float64(0.3176420956850052)}

Episode step 29300, time diff 6.100526809692383, total time dif 5854.498505592346)
step: 29300 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.7522513), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07603982388973236), 'actor_loss': np.float64(-0.9967925488948822), 'hyper_actor_loss': np.float64(1.778590387857548e-07), 'behavior_loss': np.float64(0.3018882691860199)}

Episode step 29310, time diff 6.2221856117248535, total time dif 5860.599032402039)
step: 29310 @ episode report: {'average_total_reward': np.float32(8.602222), 'reward_variance': np.float32(2.3362918), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06637513116002083), 'actor_loss': np.float64(-0.9912087321281433), 'hyper_actor_loss': np.float64(1.7154154505760743e-07), 'behavior_loss': np.float64(0.3077243357896805)}

Episode step 29320, time diff 6.205468654632568, total time dif 5866.821218013763)
step: 29320 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(2.6288846), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07843978703022003), 'actor_loss': np.float64(-0.9834569990634918), 'hyper_actor_loss': np.float64(1.8129530445776255e-07), 'behavior_loss': np.float64(0.31274614930152894)}

Episode step 29330, time diff 6.188793420791626, total time dif 5873.026686668396)
step: 29330 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.7213788), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06520433910191059), 'actor_loss': np.float64(-0.9814552187919616), 'hyper_actor_loss': np.float64(2.2729733899495842e-07), 'behavior_loss': np.float64(0.32116046249866487)}

Episode step 29340, time diff 6.221777677536011, total time dif 5879.215480089188)
step: 29340 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.513976), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06715603768825532), 'actor_loss': np.float64(-0.9666537642478943), 'hyper_actor_loss': np.float64(2.599757010557369e-07), 'behavior_loss': np.float64(0.3133968114852905)}

Episode step 29350, time diff 6.357485055923462, total time dif 5885.437257766724)
step: 29350 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(3.5285048), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07294816710054874), 'actor_loss': np.float64(-0.9674611151218414), 'hyper_actor_loss': np.float64(3.0294573321043573e-07), 'behavior_loss': np.float64(0.31441743671894073)}

Episode step 29360, time diff 6.146877765655518, total time dif 5891.794742822647)
step: 29360 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(0.7349197), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07576153352856636), 'actor_loss': np.float64(-0.9827430248260498), 'hyper_actor_loss': np.float64(3.22308943623284e-07), 'behavior_loss': np.float64(0.31929991841316224)}

Episode step 29370, time diff 6.28949236869812, total time dif 5897.941620588303)
step: 29370 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(1.0846484), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0640765182673931), 'actor_loss': np.float64(-0.9867784559726716), 'hyper_actor_loss': np.float64(3.2917395174081323e-07), 'behavior_loss': np.float64(0.3089634656906128)}

Episode step 29380, time diff 6.246798515319824, total time dif 5904.231112957001)
step: 29380 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(2.1039946), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08526398055255413), 'actor_loss': np.float64(-0.976485151052475), 'hyper_actor_loss': np.float64(4.14467481846259e-07), 'behavior_loss': np.float64(0.31290046870708466)}

Episode step 29390, time diff 6.3072428703308105, total time dif 5910.477911472321)
step: 29390 @ episode report: {'average_total_reward': np.float32(9.700001), 'reward_variance': np.float32(6.819505), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06560035571455955), 'actor_loss': np.float64(-1.000968623161316), 'hyper_actor_loss': np.float64(4.646323816359654e-07), 'behavior_loss': np.float64(0.2975247144699097)}

Episode step 29400, time diff 6.3114378452301025, total time dif 5916.785154342651)
step: 29400 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(2.2811966), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05990304686129093), 'actor_loss': np.float64(-0.9808786571025848), 'hyper_actor_loss': np.float64(5.434529924741583e-07), 'behavior_loss': np.float64(0.29915951788425443)}

Episode step 29410, time diff 6.209305763244629, total time dif 5923.0965921878815)
step: 29410 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(8.55253), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060559422709047794), 'actor_loss': np.float64(-0.9622967004776001), 'hyper_actor_loss': np.float64(5.85820060905462e-07), 'behavior_loss': np.float64(0.30684579610824586)}

Episode step 29420, time diff 6.238608360290527, total time dif 5929.305897951126)
step: 29420 @ episode report: {'average_total_reward': np.float32(8.651113), 'reward_variance': np.float32(0.8936844), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08485049232840539), 'actor_loss': np.float64(-0.9706669390201569), 'hyper_actor_loss': np.float64(7.164046508023603e-07), 'behavior_loss': np.float64(0.3223557144403458)}

Episode step 29430, time diff 6.26585841178894, total time dif 5935.544506311417)
step: 29430 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(2.7115016), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07067755423486233), 'actor_loss': np.float64(-0.9913265347480774), 'hyper_actor_loss': np.float64(7.84583068025313e-07), 'behavior_loss': np.float64(0.3065772280097008)}

Episode step 29440, time diff 6.282486915588379, total time dif 5941.810364723206)
step: 29440 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.5194389), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06687355227768421), 'actor_loss': np.float64(-0.9855630338191986), 'hyper_actor_loss': np.float64(8.403137030654761e-07), 'behavior_loss': np.float64(0.30659427046775817)}

Episode step 29450, time diff 6.25265097618103, total time dif 5948.092851638794)
step: 29450 @ episode report: {'average_total_reward': np.float32(9.138889), 'reward_variance': np.float32(3.1936116), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07151017859578132), 'actor_loss': np.float64(-0.974760764837265), 'hyper_actor_loss': np.float64(8.361180448446248e-07), 'behavior_loss': np.float64(0.3096547842025757)}

Episode step 29460, time diff 6.237412214279175, total time dif 5954.345502614975)
step: 29460 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(1.5243163), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07291447818279266), 'actor_loss': np.float64(-0.99065962433815), 'hyper_actor_loss': np.float64(9.613537770292169e-07), 'behavior_loss': np.float64(0.29011282324790955)}

Episode step 29470, time diff 6.233882665634155, total time dif 5960.582914829254)
step: 29470 @ episode report: {'average_total_reward': np.float32(10.122223), 'reward_variance': np.float32(3.0850875), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07420750856399536), 'actor_loss': np.float64(-0.9978631556034088), 'hyper_actor_loss': np.float64(1.065064054728282e-06), 'behavior_loss': np.float64(0.29977122843265536)}

Episode step 29480, time diff 6.148805618286133, total time dif 5966.816797494888)
step: 29480 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(1.61203), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06700521931052209), 'actor_loss': np.float64(-0.978211271762848), 'hyper_actor_loss': np.float64(9.104092839606892e-07), 'behavior_loss': np.float64(0.3125578314065933)}

Episode step 29490, time diff 6.241281270980835, total time dif 5972.965603113174)
step: 29490 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(5.951385), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677543930709362), 'actor_loss': np.float64(-0.9682078123092651), 'hyper_actor_loss': np.float64(8.782287295616697e-07), 'behavior_loss': np.float64(0.3098581165075302)}

Episode step 29500, time diff 6.26637077331543, total time dif 5979.206884384155)
step: 29500 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.8889084), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07495266683399678), 'actor_loss': np.float64(-0.989819872379303), 'hyper_actor_loss': np.float64(9.811463428377465e-07), 'behavior_loss': np.float64(0.30557728707790377)}

Episode step 29510, time diff 6.408025503158569, total time dif 5985.473255157471)
step: 29510 @ episode report: {'average_total_reward': np.float32(9.5244465), 'reward_variance': np.float32(2.8904397), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06662128884345293), 'actor_loss': np.float64(-0.983212822675705), 'hyper_actor_loss': np.float64(1.09531800944751e-06), 'behavior_loss': np.float64(0.302423483133316)}

Episode step 29520, time diff 6.321293830871582, total time dif 5991.881280660629)
step: 29520 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.5633457), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06208055801689625), 'actor_loss': np.float64(-0.9668270587921143), 'hyper_actor_loss': np.float64(1.4855762856313958e-06), 'behavior_loss': np.float64(0.3032067269086838)}

Episode step 29530, time diff 6.26878547668457, total time dif 5998.202574491501)
step: 29530 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.1941187), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08092099577188491), 'actor_loss': np.float64(-0.98616943359375), 'hyper_actor_loss': np.float64(1.803826819468668e-06), 'behavior_loss': np.float64(0.29626721143722534)}

Episode step 29540, time diff 6.300789833068848, total time dif 6004.471359968185)
step: 29540 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.7431905), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05998214092105627), 'actor_loss': np.float64(-0.989426213502884), 'hyper_actor_loss': np.float64(2.1065142391307744e-06), 'behavior_loss': np.float64(0.30759612619876864)}

Episode step 29550, time diff 6.2297892570495605, total time dif 6010.772149801254)
step: 29550 @ episode report: {'average_total_reward': np.float32(9.787778), 'reward_variance': np.float32(3.6211467), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056853257864713666), 'actor_loss': np.float64(-0.9573003828525544), 'hyper_actor_loss': np.float64(2.6127945375264972e-06), 'behavior_loss': np.float64(0.2982136934995651)}

Episode step 29560, time diff 6.28148889541626, total time dif 6017.001939058304)
step: 29560 @ episode report: {'average_total_reward': np.float32(8.865557), 'reward_variance': np.float32(2.339221), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06592610739171505), 'actor_loss': np.float64(-0.9578318417072296), 'hyper_actor_loss': np.float64(2.9286755079738213e-06), 'behavior_loss': np.float64(0.3072646349668503)}

Episode step 29570, time diff 6.386194944381714, total time dif 6023.28342795372)
step: 29570 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.2723708), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07152394875884056), 'actor_loss': np.float64(-0.9844172894954681), 'hyper_actor_loss': np.float64(4.532082721198094e-06), 'behavior_loss': np.float64(0.3063169926404953)}

Episode step 29580, time diff 6.516830921173096, total time dif 6029.669622898102)
step: 29580 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(2.8733947), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06769360415637493), 'actor_loss': np.float64(-0.9893531024456024), 'hyper_actor_loss': np.float64(6.445644112318405e-06), 'behavior_loss': np.float64(0.315366193652153)}

Episode step 29590, time diff 6.539490222930908, total time dif 6036.186453819275)
step: 29590 @ episode report: {'average_total_reward': np.float32(8.390001), 'reward_variance': np.float32(1.6860609), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07769193463027477), 'actor_loss': np.float64(-0.9688868641853332), 'hyper_actor_loss': np.float64(1.0256397126795492e-05), 'behavior_loss': np.float64(0.3106801509857178)}

Episode step 29600, time diff 6.484056234359741, total time dif 6042.725944042206)
step: 29600 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(3.4860864), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061849542520940305), 'actor_loss': np.float64(-0.979242330789566), 'hyper_actor_loss': np.float64(1.5334323961724294e-05), 'behavior_loss': np.float64(0.29988364279270174)}

Episode step 29610, time diff 6.486692667007446, total time dif 6049.210000276566)
step: 29610 @ episode report: {'average_total_reward': np.float32(10.24889), 'reward_variance': np.float32(2.3578827), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06562080383300781), 'actor_loss': np.float64(-0.9867440819740295), 'hyper_actor_loss': np.float64(2.0951070291630457e-05), 'behavior_loss': np.float64(0.3098404496908188)}

Episode step 29620, time diff 6.466735124588013, total time dif 6055.696692943573)
step: 29620 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.4325484), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.072343098372221), 'actor_loss': np.float64(-0.9780957341194153), 'hyper_actor_loss': np.float64(2.4572456823079847e-05), 'behavior_loss': np.float64(0.3165089160203934)}

Episode step 29630, time diff 6.2619781494140625, total time dif 6062.163428068161)
step: 29630 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(2.123987), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05561448335647583), 'actor_loss': np.float64(-0.9639231383800506), 'hyper_actor_loss': np.float64(2.9221761542430615e-05), 'behavior_loss': np.float64(0.31926266849040985)}

Episode step 29640, time diff 6.165342092514038, total time dif 6068.425406217575)
step: 29640 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(5.5239224), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06355019453912973), 'actor_loss': np.float64(-0.9684702277183532), 'hyper_actor_loss': np.float64(3.7113521830178795e-05), 'behavior_loss': np.float64(0.3143827497959137)}

Episode step 29650, time diff 6.12083888053894, total time dif 6074.590748310089)
step: 29650 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.5249393), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06757591068744659), 'actor_loss': np.float64(-0.9830097556114197), 'hyper_actor_loss': np.float64(4.048738010169473e-05), 'behavior_loss': np.float64(0.3161706417798996)}

Episode step 29660, time diff 5.838003635406494, total time dif 6080.711587190628)
step: 29660 @ episode report: {'average_total_reward': np.float32(10.285555), 'reward_variance': np.float32(5.0482244), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07086518928408622), 'actor_loss': np.float64(-0.9912916243076324), 'hyper_actor_loss': np.float64(5.309709886205383e-05), 'behavior_loss': np.float64(0.30800156891345976)}

Episode step 29670, time diff 5.780644178390503, total time dif 6086.549590826035)
step: 29670 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(3.11094), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06988323889672757), 'actor_loss': np.float64(-0.976739251613617), 'hyper_actor_loss': np.float64(6.51976530207321e-05), 'behavior_loss': np.float64(0.3274927526712418)}

Episode step 29680, time diff 5.533754348754883, total time dif 6092.330235004425)
step: 29680 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.3711221), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07839201427996159), 'actor_loss': np.float64(-0.9723747789859771), 'hyper_actor_loss': np.float64(5.620274059765506e-05), 'behavior_loss': np.float64(0.31984578669071195)}

Episode step 29690, time diff 5.114878177642822, total time dif 6097.86398935318)
step: 29690 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(1.0047271), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05345509443432093), 'actor_loss': np.float64(-0.9793988704681397), 'hyper_actor_loss': np.float64(6.540723661601078e-05), 'behavior_loss': np.float64(0.30085929930210115)}

Episode step 29700, time diff 5.121557712554932, total time dif 6102.978867530823)
step: 29700 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.024723), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07222175300121307), 'actor_loss': np.float64(-0.9745835185050964), 'hyper_actor_loss': np.float64(6.238286041480024e-05), 'behavior_loss': np.float64(0.30612885057926176)}

Episode step 29710, time diff 5.002504348754883, total time dif 6108.100425243378)
step: 29710 @ episode report: {'average_total_reward': np.float32(10.410002), 'reward_variance': np.float32(3.079616), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0669815257191658), 'actor_loss': np.float64(-0.9921902477741241), 'hyper_actor_loss': np.float64(5.987754739180673e-05), 'behavior_loss': np.float64(0.3240624874830246)}

Episode step 29720, time diff 4.905197620391846, total time dif 6113.102929592133)
step: 29720 @ episode report: {'average_total_reward': np.float32(11.568891), 'reward_variance': np.float32(3.655848), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06398396976292134), 'actor_loss': np.float64(-0.9707958400249481), 'hyper_actor_loss': np.float64(6.468499523180072e-05), 'behavior_loss': np.float64(0.33695421516895296)}

Episode step 29730, time diff 4.975977897644043, total time dif 6118.008127212524)
step: 29730 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(2.9842026), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06959128957241774), 'actor_loss': np.float64(-0.9628766775131226), 'hyper_actor_loss': np.float64(6.242320123419631e-05), 'behavior_loss': np.float64(0.3184599429368973)}

Episode step 29740, time diff 5.009198904037476, total time dif 6122.9841051101685)
step: 29740 @ episode report: {'average_total_reward': np.float32(10.285557), 'reward_variance': np.float32(3.036199), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06792402751743794), 'actor_loss': np.float64(-0.9814145982265472), 'hyper_actor_loss': np.float64(6.124996652943083e-05), 'behavior_loss': np.float64(0.3164184629917145)}

Episode step 29750, time diff 4.986011743545532, total time dif 6127.993304014206)
step: 29750 @ episode report: {'average_total_reward': np.float32(9.214445), 'reward_variance': np.float32(4.3470387), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06588702872395516), 'actor_loss': np.float64(-0.9843349695205689), 'hyper_actor_loss': np.float64(5.66383565455908e-05), 'behavior_loss': np.float64(0.3098992645740509)}

Episode step 29760, time diff 4.961514234542847, total time dif 6132.9793157577515)
step: 29760 @ episode report: {'average_total_reward': np.float32(11.171111), 'reward_variance': np.float32(3.670919), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0640319149941206), 'actor_loss': np.float64(-0.9751244783401489), 'hyper_actor_loss': np.float64(5.720948392990976e-05), 'behavior_loss': np.float64(0.3134234338998795)}

Episode step 29770, time diff 4.932027101516724, total time dif 6137.940829992294)
step: 29770 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(3.4908345), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07592974603176117), 'actor_loss': np.float64(-0.9812345087528229), 'hyper_actor_loss': np.float64(5.981903013889678e-05), 'behavior_loss': np.float64(0.32907111942768097)}

Episode step 29780, time diff 4.896843194961548, total time dif 6142.872857093811)
step: 29780 @ episode report: {'average_total_reward': np.float32(10.197778), 'reward_variance': np.float32(3.2251065), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07011237107217312), 'actor_loss': np.float64(-0.9918241798877716), 'hyper_actor_loss': np.float64(6.541652473970316e-05), 'behavior_loss': np.float64(0.33222249448299407)}

Episode step 29790, time diff 4.857853889465332, total time dif 6147.769700288773)
step: 29790 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(4.414469), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07031797245144844), 'actor_loss': np.float64(-0.9712596356868743), 'hyper_actor_loss': np.float64(6.961280705581885e-05), 'behavior_loss': np.float64(0.3252818763256073)}

Episode step 29800, time diff 4.818436145782471, total time dif 6152.627554178238)
step: 29800 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(3.4706528), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0657479364424944), 'actor_loss': np.float64(-0.9796791195869445), 'hyper_actor_loss': np.float64(7.040421369310934e-05), 'behavior_loss': np.float64(0.3212440848350525)}

Episode step 29810, time diff 4.854160785675049, total time dif 6157.44599032402)
step: 29810 @ episode report: {'average_total_reward': np.float32(10.012222), 'reward_variance': np.float32(2.085443), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06704104393720627), 'actor_loss': np.float64(-0.9807244420051575), 'hyper_actor_loss': np.float64(7.859510878915899e-05), 'behavior_loss': np.float64(0.3231094151735306)}

Episode step 29820, time diff 4.852865695953369, total time dif 6162.300151109695)
step: 29820 @ episode report: {'average_total_reward': np.float32(8.502222), 'reward_variance': np.float32(2.721378), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0606363482773304), 'actor_loss': np.float64(-0.9667169332504273), 'hyper_actor_loss': np.float64(7.970320948516018e-05), 'behavior_loss': np.float64(0.3288328558206558)}

Episode step 29830, time diff 4.893827676773071, total time dif 6167.153016805649)
step: 29830 @ episode report: {'average_total_reward': np.float32(8.216667), 'reward_variance': np.float32(3.141809), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05617180056869984), 'actor_loss': np.float64(-0.9530853867530823), 'hyper_actor_loss': np.float64(6.34042800811585e-05), 'behavior_loss': np.float64(0.3288710415363312)}

Episode step 29840, time diff 4.645747661590576, total time dif 6172.046844482422)
step: 29840 @ episode report: {'average_total_reward': np.float32(8.902224), 'reward_variance': np.float32(3.1404407), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081182077527046), 'actor_loss': np.float64(-0.9664218842983245), 'hyper_actor_loss': np.float64(6.132740745670162e-05), 'behavior_loss': np.float64(0.32181714475154877)}

Episode step 29850, time diff 4.770589590072632, total time dif 6176.692592144012)
step: 29850 @ episode report: {'average_total_reward': np.float32(7.304445), 'reward_variance': np.float32(2.3879807), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06425448805093766), 'actor_loss': np.float64(-0.9893102467060089), 'hyper_actor_loss': np.float64(6.36257063888479e-05), 'behavior_loss': np.float64(0.3349457323551178)}

Episode step 29860, time diff 4.602444410324097, total time dif 6181.463181734085)
step: 29860 @ episode report: {'average_total_reward': np.float32(7.604445), 'reward_variance': np.float32(5.270549), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07207395769655704), 'actor_loss': np.float64(-0.9688170075416564), 'hyper_actor_loss': np.float64(6.56214171613101e-05), 'behavior_loss': np.float64(0.3249316871166229)}

Episode step 29870, time diff 4.579786777496338, total time dif 6186.065626144409)
step: 29870 @ episode report: {'average_total_reward': np.float32(7.1800003), 'reward_variance': np.float32(3.7572045), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06668033562600613), 'actor_loss': np.float64(-0.9891520380973816), 'hyper_actor_loss': np.float64(6.0086415396654046e-05), 'behavior_loss': np.float64(0.32448223531246184)}

Episode step 29880, time diff 4.613462209701538, total time dif 6190.6454129219055)
step: 29880 @ episode report: {'average_total_reward': np.float32(7.504445), 'reward_variance': np.float32(4.1183753), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06589102298021317), 'actor_loss': np.float64(-0.9775679290294648), 'hyper_actor_loss': np.float64(6.225679280760232e-05), 'behavior_loss': np.float64(0.3294074088335037)}

Episode step 29890, time diff 4.659469127655029, total time dif 6195.258875131607)
step: 29890 @ episode report: {'average_total_reward': np.float32(8.165556), 'reward_variance': np.float32(3.6980863), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07574877925217152), 'actor_loss': np.float64(-0.9747493743896485), 'hyper_actor_loss': np.float64(6.288935983320699e-05), 'behavior_loss': np.float64(0.33534599244594576)}

Episode step 29900, time diff 4.435124397277832, total time dif 6199.918344259262)
step: 29900 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(1.7836593), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07528844512999058), 'actor_loss': np.float64(-0.9897118985652924), 'hyper_actor_loss': np.float64(5.7569239652366376e-05), 'behavior_loss': np.float64(0.3120276272296906)}

Episode step 29910, time diff 4.3031721115112305, total time dif 6204.35346865654)
step: 29910 @ episode report: {'average_total_reward': np.float32(7.6800003), 'reward_variance': np.float32(1.0542667), 'max_total_reward': np.float32(9.655556), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06145864799618721), 'actor_loss': np.float64(-0.9873380243778229), 'hyper_actor_loss': np.float64(5.4437028302345426e-05), 'behavior_loss': np.float64(0.3207658350467682)}

Episode step 29920, time diff 4.400987863540649, total time dif 6208.656640768051)
step: 29920 @ episode report: {'average_total_reward': np.float32(7.804445), 'reward_variance': np.float32(2.532227), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06770700551569461), 'actor_loss': np.float64(-0.9727998435497284), 'hyper_actor_loss': np.float64(5.435389575723093e-05), 'behavior_loss': np.float64(0.3167538851499557)}

Episode step 29930, time diff 4.329224586486816, total time dif 6213.057628631592)
step: 29930 @ episode report: {'average_total_reward': np.float32(7.5800004), 'reward_variance': np.float32(0.91374874), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06531266905367375), 'actor_loss': np.float64(-0.9747272789478302), 'hyper_actor_loss': np.float64(5.076045308669563e-05), 'behavior_loss': np.float64(0.33931970298290254)}

Episode step 29940, time diff 4.393359184265137, total time dif 6217.386853218079)
step: 29940 @ episode report: {'average_total_reward': np.float32(7.118889), 'reward_variance': np.float32(1.245433), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06408767346292735), 'actor_loss': np.float64(-0.9648436844348908), 'hyper_actor_loss': np.float64(4.7430679114768284e-05), 'behavior_loss': np.float64(0.31452277302742004)}

Episode step 29950, time diff 4.420374155044556, total time dif 6221.780212402344)
step: 29950 @ episode report: {'average_total_reward': np.float32(7.1922226), 'reward_variance': np.float32(2.4145455), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06494242958724498), 'actor_loss': np.float64(-0.9796143770217896), 'hyper_actor_loss': np.float64(4.538850917015225e-05), 'behavior_loss': np.float64(0.31718423366546633)}

Episode step 29960, time diff 4.354418992996216, total time dif 6226.200586557388)
step: 29960 @ episode report: {'average_total_reward': np.float32(7.8411117), 'reward_variance': np.float32(1.059088), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06727548576891422), 'actor_loss': np.float64(-0.9863084018230438), 'hyper_actor_loss': np.float64(4.2004045099020004e-05), 'behavior_loss': np.float64(0.3439720392227173)}

Episode step 29970, time diff 4.2991321086883545, total time dif 6230.5550055503845)
step: 29970 @ episode report: {'average_total_reward': np.float32(8.016667), 'reward_variance': np.float32(2.9038086), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06985444873571396), 'actor_loss': np.float64(-0.9762898504734039), 'hyper_actor_loss': np.float64(4.094108335266356e-05), 'behavior_loss': np.float64(0.32674362063407897)}

Episode step 29980, time diff 4.345771551132202, total time dif 6234.854137659073)
step: 29980 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(2.4344401), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07130437567830086), 'actor_loss': np.float64(-0.9735338807106018), 'hyper_actor_loss': np.float64(3.799960795731749e-05), 'behavior_loss': np.float64(0.3382011353969574)}

Episode step 29990, time diff 4.205559730529785, total time dif 6239.199909210205)
step: 29990 @ episode report: {'average_total_reward': np.float32(8.602222), 'reward_variance': np.float32(1.6100941), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06656677387654782), 'actor_loss': np.float64(-0.9746399044990539), 'hyper_actor_loss': np.float64(3.485596171231009e-05), 'behavior_loss': np.float64(0.3150981903076172)}

Episode step 30000, time diff 4.394889831542969, total time dif 6243.405468940735)
step: 30000 @ episode report: {'average_total_reward': np.float32(8.541112), 'reward_variance': np.float32(1.6078529), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0702213816344738), 'actor_loss': np.float64(-0.9905161619186401), 'hyper_actor_loss': np.float64(3.297289986221585e-05), 'behavior_loss': np.float64(0.3328461140394211)}

Episode step 30010, time diff 4.32331109046936, total time dif 6247.800358772278)
step: 30010 @ episode report: {'average_total_reward': np.float32(7.8411117), 'reward_variance': np.float32(2.712495), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07055866122245788), 'actor_loss': np.float64(-0.9805984854698181), 'hyper_actor_loss': np.float64(3.3852072374429556e-05), 'behavior_loss': np.float64(0.3332137674093246)}

Episode step 30020, time diff 4.331981658935547, total time dif 6252.123669862747)
step: 30020 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(1.3012953), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06325908824801445), 'actor_loss': np.float64(-0.9632878720760345), 'hyper_actor_loss': np.float64(3.0574241827707736e-05), 'behavior_loss': np.float64(0.33050051629543303)}

Episode step 30030, time diff 4.264192819595337, total time dif 6256.455651521683)
step: 30030 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(3.5135179), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06243983767926693), 'actor_loss': np.float64(-0.9659106850624084), 'hyper_actor_loss': np.float64(3.0231455457396806e-05), 'behavior_loss': np.float64(0.33356868028640746)}

Episode step 30040, time diff 4.189054727554321, total time dif 6260.719844341278)
step: 30040 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.2092707), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07131353747099638), 'actor_loss': np.float64(-0.9797624707221985), 'hyper_actor_loss': np.float64(2.9705205633945297e-05), 'behavior_loss': np.float64(0.3310560256242752)}

Episode step 30050, time diff 4.115931987762451, total time dif 6264.908899068832)
step: 30050 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(1.3273036), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07200189158320427), 'actor_loss': np.float64(-0.998835825920105), 'hyper_actor_loss': np.float64(2.6376676578365733e-05), 'behavior_loss': np.float64(0.324186310172081)}

Episode step 30060, time diff 4.237226724624634, total time dif 6269.024831056595)
step: 30060 @ episode report: {'average_total_reward': np.float32(8.190001), 'reward_variance': np.float32(2.525888), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06237294301390648), 'actor_loss': np.float64(-0.9767949938774109), 'hyper_actor_loss': np.float64(3.0198294734873343e-05), 'behavior_loss': np.float64(0.32679944932460786)}

Episode step 30070, time diff 4.219338655471802, total time dif 6273.2620577812195)
step: 30070 @ episode report: {'average_total_reward': np.float32(7.11889), 'reward_variance': np.float32(1.5277299), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06091020964086056), 'actor_loss': np.float64(-0.9603582262992859), 'hyper_actor_loss': np.float64(3.112580361630535e-05), 'behavior_loss': np.float64(0.33859069347381593)}

Episode step 30080, time diff 4.050470352172852, total time dif 6277.481396436691)
step: 30080 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(2.970721), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05959468148648739), 'actor_loss': np.float64(-0.966504055261612), 'hyper_actor_loss': np.float64(2.7224619771004654e-05), 'behavior_loss': np.float64(0.3163906067609787)}

Episode step 30090, time diff 4.156494855880737, total time dif 6281.531866788864)
step: 30090 @ episode report: {'average_total_reward': np.float32(8.641111), 'reward_variance': np.float32(0.9022729), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06409157365560532), 'actor_loss': np.float64(-0.9733364403247833), 'hyper_actor_loss': np.float64(2.5018712040036917e-05), 'behavior_loss': np.float64(0.3242879331111908)}

Episode step 30100, time diff 4.127346992492676, total time dif 6285.688361644745)
step: 30100 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(1.5212891), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06538004279136658), 'actor_loss': np.float64(-0.9691034197807312), 'hyper_actor_loss': np.float64(2.6061476637551097e-05), 'behavior_loss': np.float64(0.32909516990184784)}

Episode step 30110, time diff 4.205146074295044, total time dif 6289.815708637238)
step: 30110 @ episode report: {'average_total_reward': np.float32(7.6800003), 'reward_variance': np.float32(1.6921924), 'max_total_reward': np.float32(9.777777), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06555068474262953), 'actor_loss': np.float64(-0.9713052093982697), 'hyper_actor_loss': np.float64(2.5521749194012955e-05), 'behavior_loss': np.float64(0.31660468876361847)}

Episode step 30120, time diff 4.171613693237305, total time dif 6294.020854711533)
step: 30120 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(3.392178), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08239339925348758), 'actor_loss': np.float64(-0.9919640183448791), 'hyper_actor_loss': np.float64(2.7147805303684437e-05), 'behavior_loss': np.float64(0.3322332054376602)}

Episode step 30130, time diff 4.215024709701538, total time dif 6298.19246840477)
step: 30130 @ episode report: {'average_total_reward': np.float32(7.8922224), 'reward_variance': np.float32(2.9116063), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07243704721331597), 'actor_loss': np.float64(-1.0084894239902495), 'hyper_actor_loss': np.float64(2.7561881506699137e-05), 'behavior_loss': np.float64(0.32672205865383147)}

Episode step 30140, time diff 4.064424991607666, total time dif 6302.407493114471)
step: 30140 @ episode report: {'average_total_reward': np.float32(8.004445), 'reward_variance': np.float32(1.814203), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06534538865089416), 'actor_loss': np.float64(-0.9738711833953857), 'hyper_actor_loss': np.float64(2.4379426395171323e-05), 'behavior_loss': np.float64(0.31618150770664216)}

Episode step 30150, time diff 4.045997858047485, total time dif 6306.471918106079)
step: 30150 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(2.7293696), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06358913071453572), 'actor_loss': np.float64(-0.9634678184986114), 'hyper_actor_loss': np.float64(2.4656508321641014e-05), 'behavior_loss': np.float64(0.32682582437992097)}

Episode step 30160, time diff 4.226734638214111, total time dif 6310.517915964127)
step: 30160 @ episode report: {'average_total_reward': np.float32(7.916667), 'reward_variance': np.float32(3.3966732), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07397940345108509), 'actor_loss': np.float64(-0.9885666012763977), 'hyper_actor_loss': np.float64(2.4063676573859993e-05), 'behavior_loss': np.float64(0.3215246170759201)}

Episode step 30170, time diff 4.070854902267456, total time dif 6314.744650602341)
step: 30170 @ episode report: {'average_total_reward': np.float32(7.0800004), 'reward_variance': np.float32(3.0692544), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07779198549687863), 'actor_loss': np.float64(-0.9848739206790924), 'hyper_actor_loss': np.float64(2.3620057800144422e-05), 'behavior_loss': np.float64(0.3286585122346878)}

Episode step 30180, time diff 4.0067360401153564, total time dif 6318.815505504608)
step: 30180 @ episode report: {'average_total_reward': np.float32(7.4555564), 'reward_variance': np.float32(1.0697281), 'max_total_reward': np.float32(9.655555), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06902461871504784), 'actor_loss': np.float64(-0.982966560125351), 'hyper_actor_loss': np.float64(2.166554386349162e-05), 'behavior_loss': np.float64(0.3281231611967087)}

Episode step 30190, time diff 4.010720252990723, total time dif 6322.8222415447235)
step: 30190 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(0.9612298), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06518037393689155), 'actor_loss': np.float64(-0.9728282392024994), 'hyper_actor_loss': np.float64(2.112648289767094e-05), 'behavior_loss': np.float64(0.32504093945026397)}

Episode step 30200, time diff 4.034468173980713, total time dif 6326.832961797714)
step: 30200 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(3.4775321), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.069331219419837), 'actor_loss': np.float64(-0.9831952571868896), 'hyper_actor_loss': np.float64(1.8933677711174824e-05), 'behavior_loss': np.float64(0.32285445332527163)}

Episode step 30210, time diff 4.037659168243408, total time dif 6330.867429971695)
step: 30210 @ episode report: {'average_total_reward': np.float32(7.6922226), 'reward_variance': np.float32(1.3891866), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08523792587220669), 'actor_loss': np.float64(-0.9847215414047241), 'hyper_actor_loss': np.float64(1.88008825716679e-05), 'behavior_loss': np.float64(0.3205524951219559)}

Episode step 30220, time diff 4.020012617111206, total time dif 6334.905089139938)
step: 30220 @ episode report: {'average_total_reward': np.float32(7.4433336), 'reward_variance': np.float32(0.9454924), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06801571398973465), 'actor_loss': np.float64(-0.9983803033828735), 'hyper_actor_loss': np.float64(1.8827493659046012e-05), 'behavior_loss': np.float64(0.3281365126371384)}

Episode step 30230, time diff 4.001441240310669, total time dif 6338.92510175705)
step: 30230 @ episode report: {'average_total_reward': np.float32(8.49), 'reward_variance': np.float32(3.5404305), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06584677286446095), 'actor_loss': np.float64(-0.9663480460643769), 'hyper_actor_loss': np.float64(1.9442482334852683e-05), 'behavior_loss': np.float64(0.3249691277742386)}

Episode step 30240, time diff 4.070501804351807, total time dif 6342.92654299736)
step: 30240 @ episode report: {'average_total_reward': np.float32(8.228889), 'reward_variance': np.float32(1.8501284), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06405612044036388), 'actor_loss': np.float64(-0.9654992401599884), 'hyper_actor_loss': np.float64(1.778102805474191e-05), 'behavior_loss': np.float64(0.3267915904521942)}

Episode step 30250, time diff 4.064328670501709, total time dif 6346.997044801712)
step: 30250 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(4.3105235), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07816560417413712), 'actor_loss': np.float64(-0.9847973585128784), 'hyper_actor_loss': np.float64(1.7860036496131216e-05), 'behavior_loss': np.float64(0.3230093330144882)}

Episode step 30260, time diff 4.026628732681274, total time dif 6351.061373472214)
step: 30260 @ episode report: {'average_total_reward': np.float32(8.116667), 'reward_variance': np.float32(1.678673), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05282435081899166), 'actor_loss': np.float64(-0.9746819138526917), 'hyper_actor_loss': np.float64(1.7262776964344084e-05), 'behavior_loss': np.float64(0.33168624341487885)}

Episode step 30270, time diff 4.006945371627808, total time dif 6355.088002204895)
step: 30270 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(2.4131355), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.073582798615098), 'actor_loss': np.float64(-0.9542226195335388), 'hyper_actor_loss': np.float64(1.7161896812467604e-05), 'behavior_loss': np.float64(0.3277811765670776)}

Episode step 30280, time diff 4.0326197147369385, total time dif 6359.094947576523)
step: 30280 @ episode report: {'average_total_reward': np.float32(6.518889), 'reward_variance': np.float32(1.7237298), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06409140639007091), 'actor_loss': np.float64(-0.9836395502090454), 'hyper_actor_loss': np.float64(1.8971346071339214e-05), 'behavior_loss': np.float64(0.31780588030815127)}

Episode step 30290, time diff 4.073641538619995, total time dif 6363.12756729126)
step: 30290 @ episode report: {'average_total_reward': np.float32(7.7922225), 'reward_variance': np.float32(1.2140014), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06827960722148418), 'actor_loss': np.float64(-0.9809472501277924), 'hyper_actor_loss': np.float64(1.6875117034942376e-05), 'behavior_loss': np.float64(0.3227110534906387)}

Episode step 30300, time diff 4.076675653457642, total time dif 6367.20120882988)
step: 30300 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(4.3029337), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07363184094429016), 'actor_loss': np.float64(-0.9677952945232391), 'hyper_actor_loss': np.float64(1.462365071347449e-05), 'behavior_loss': np.float64(0.3470457047224045)}

Episode step 30310, time diff 4.107588052749634, total time dif 6371.277884483337)
step: 30310 @ episode report: {'average_total_reward': np.float32(8.63889), 'reward_variance': np.float32(2.0415866), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06212321743369102), 'actor_loss': np.float64(-0.9683454275131226), 'hyper_actor_loss': np.float64(1.64814043273509e-05), 'behavior_loss': np.float64(0.3294981628656387)}

Episode step 30320, time diff 4.311943292617798, total time dif 6375.385472536087)
step: 30320 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(2.742347), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06023211404681206), 'actor_loss': np.float64(-0.9603706061840057), 'hyper_actor_loss': np.float64(2.0511369984888007e-05), 'behavior_loss': np.float64(0.32750768065452573)}

Episode step 30330, time diff 4.227808237075806, total time dif 6379.697415828705)
step: 30330 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(3.7225296), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07058846168220043), 'actor_loss': np.float64(-0.9494973361492157), 'hyper_actor_loss': np.float64(3.891139112965902e-05), 'behavior_loss': np.float64(0.32930259108543397)}

Episode step 30340, time diff 3.954257011413574, total time dif 6383.925224065781)
step: 30340 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.0694513), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06929557546973228), 'actor_loss': np.float64(-0.9937591075897216), 'hyper_actor_loss': np.float64(4.2373161340947264e-05), 'behavior_loss': np.float64(0.3200795590877533)}

Episode step 30350, time diff 3.8772354125976562, total time dif 6387.879481077194)
step: 30350 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(3.4077644), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06753708869218826), 'actor_loss': np.float64(-0.9891706645488739), 'hyper_actor_loss': np.float64(2.599727868073387e-05), 'behavior_loss': np.float64(0.3338019996881485)}

Episode step 30360, time diff 4.106582164764404, total time dif 6391.756716489792)
step: 30360 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(1.7173831), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06475565396249294), 'actor_loss': np.float64(-0.9565372407436371), 'hyper_actor_loss': np.float64(1.937111755978549e-05), 'behavior_loss': np.float64(0.3277668863534927)}

Episode step 30370, time diff 4.155766725540161, total time dif 6395.863298654556)
step: 30370 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(1.6041186), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06064161360263824), 'actor_loss': np.float64(-0.9614775776863098), 'hyper_actor_loss': np.float64(2.018113409576472e-05), 'behavior_loss': np.float64(0.3207837849855423)}

Episode step 30380, time diff 4.183594703674316, total time dif 6400.019065380096)
step: 30380 @ episode report: {'average_total_reward': np.float32(10.722222), 'reward_variance': np.float32(5.505382), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07602110393345356), 'actor_loss': np.float64(-0.9782595455646514), 'hyper_actor_loss': np.float64(2.8576421391335317e-05), 'behavior_loss': np.float64(0.3324312448501587)}

Episode step 30390, time diff 4.171167850494385, total time dif 6404.202660083771)
step: 30390 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(1.9817343), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.070084298402071), 'actor_loss': np.float64(-0.9843518555164337), 'hyper_actor_loss': np.float64(3.693436683533946e-05), 'behavior_loss': np.float64(0.32047985792160033)}

Episode step 30400, time diff 4.163210391998291, total time dif 6408.373827934265)
step: 30400 @ episode report: {'average_total_reward': np.float32(10.185555), 'reward_variance': np.float32(5.998397), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07151637114584446), 'actor_loss': np.float64(-0.9799231469631196), 'hyper_actor_loss': np.float64(3.400262230570661e-05), 'behavior_loss': np.float64(0.3324540168046951)}

Episode step 30410, time diff 4.140244245529175, total time dif 6412.537038326263)
step: 30410 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(1.588958), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060333941504359244), 'actor_loss': np.float64(-0.9749085009098053), 'hyper_actor_loss': np.float64(3.184382730978541e-05), 'behavior_loss': np.float64(0.3167035669088364)}

Episode step 30420, time diff 4.118612051010132, total time dif 6416.677282571793)
step: 30420 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.524366), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06475258031859994), 'actor_loss': np.float64(-0.9769887924194336), 'hyper_actor_loss': np.float64(2.5777417431527282e-05), 'behavior_loss': np.float64(0.324136683344841)}

Episode step 30430, time diff 4.038050651550293, total time dif 6420.795894622803)
step: 30430 @ episode report: {'average_total_reward': np.float32(8.302222), 'reward_variance': np.float32(2.5162914), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08132509626448155), 'actor_loss': np.float64(-0.9864784300327301), 'hyper_actor_loss': np.float64(2.201491079176776e-05), 'behavior_loss': np.float64(0.3268286794424057)}

Episode step 30440, time diff 4.082396745681763, total time dif 6424.833945274353)
step: 30440 @ episode report: {'average_total_reward': np.float32(10.397779), 'reward_variance': np.float32(1.1901429), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0798982385545969), 'actor_loss': np.float64(-1.0003160178661346), 'hyper_actor_loss': np.float64(2.3758002862450665e-05), 'behavior_loss': np.float64(0.33016002774238584)}

Episode step 30450, time diff 4.164421558380127, total time dif 6428.916342020035)
step: 30450 @ episode report: {'average_total_reward': np.float32(8.59), 'reward_variance': np.float32(1.1552708), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06659590043127536), 'actor_loss': np.float64(-0.9938784003257751), 'hyper_actor_loss': np.float64(2.346918608964188e-05), 'behavior_loss': np.float64(0.32936638593673706)}

Episode step 30460, time diff 4.15959620475769, total time dif 6433.080763578415)
step: 30460 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(0.46010002), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06431389562785625), 'actor_loss': np.float64(-0.9597880601882934), 'hyper_actor_loss': np.float64(2.5490953521511984e-05), 'behavior_loss': np.float64(0.3271947234869003)}

Episode step 30470, time diff 4.138148307800293, total time dif 6437.240359783173)
step: 30470 @ episode report: {'average_total_reward': np.float32(8.651113), 'reward_variance': np.float32(1.5700055), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0787123255431652), 'actor_loss': np.float64(-0.9694869160652161), 'hyper_actor_loss': np.float64(2.4186070913856383e-05), 'behavior_loss': np.float64(0.31839999854564666)}

Episode step 30480, time diff 4.347157716751099, total time dif 6441.378508090973)
step: 30480 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(1.0756652), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06387097910046577), 'actor_loss': np.float64(-0.9955643296241761), 'hyper_actor_loss': np.float64(2.3618833438376896e-05), 'behavior_loss': np.float64(0.33969166576862336)}

Episode step 30490, time diff 4.14763617515564, total time dif 6445.725665807724)
step: 30490 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(5.09658), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07317729108035564), 'actor_loss': np.float64(-0.9715935111045837), 'hyper_actor_loss': np.float64(2.395332430751296e-05), 'behavior_loss': np.float64(0.3319851905107498)}

Episode step 30500, time diff 4.179994821548462, total time dif 6449.87330198288)
step: 30500 @ episode report: {'average_total_reward': np.float32(10.685556), 'reward_variance': np.float32(2.0610142), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07814627550542355), 'actor_loss': np.float64(-0.9845901131629944), 'hyper_actor_loss': np.float64(2.4881438184820583e-05), 'behavior_loss': np.float64(0.33168584704399107)}

Episode step 30510, time diff 4.2371132373809814, total time dif 6454.053296804428)
step: 30510 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(1.390178), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05447550937533378), 'actor_loss': np.float64(-0.9760432958602905), 'hyper_actor_loss': np.float64(2.5383835782122334e-05), 'behavior_loss': np.float64(0.33927229046821594)}

Episode step 30520, time diff 4.153506755828857, total time dif 6458.290410041809)
step: 30520 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(0.8187025), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056915453635156155), 'actor_loss': np.float64(-0.9462925374507904), 'hyper_actor_loss': np.float64(2.3321703156398144e-05), 'behavior_loss': np.float64(0.3305341303348541)}

Episode step 30530, time diff 4.171968936920166, total time dif 6462.443916797638)
step: 30530 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(3.1558337), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061524085327982904), 'actor_loss': np.float64(-0.9547757148742676), 'hyper_actor_loss': np.float64(2.3046212663757615e-05), 'behavior_loss': np.float64(0.3324334442615509)}

Episode step 30540, time diff 4.205766439437866, total time dif 6466.615885734558)
step: 30540 @ episode report: {'average_total_reward': np.float32(10.385557), 'reward_variance': np.float32(3.052076), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06458463072776795), 'actor_loss': np.float64(-0.975398862361908), 'hyper_actor_loss': np.float64(2.3682510436628947e-05), 'behavior_loss': np.float64(0.32761685848236083)}

Episode step 30550, time diff 4.156243801116943, total time dif 6470.821652173996)
step: 30550 @ episode report: {'average_total_reward': np.float32(8.59), 'reward_variance': np.float32(0.97971505), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677220094949007), 'actor_loss': np.float64(-0.9735249817371369), 'hyper_actor_loss': np.float64(2.5176499002554918e-05), 'behavior_loss': np.float64(0.3453153192996979)}

Episode step 30560, time diff 4.196328639984131, total time dif 6474.977895975113)
step: 30560 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(2.4924445), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06494939997792244), 'actor_loss': np.float64(-0.9672944128513337), 'hyper_actor_loss': np.float64(2.4455787934130057e-05), 'behavior_loss': np.float64(0.33006838262081145)}

Episode step 30570, time diff 4.147753953933716, total time dif 6479.174224615097)
step: 30570 @ episode report: {'average_total_reward': np.float32(10.124446), 'reward_variance': np.float32(2.5017974), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061007778719067575), 'actor_loss': np.float64(-0.9755085349082947), 'hyper_actor_loss': np.float64(2.381265767326113e-05), 'behavior_loss': np.float64(0.32727986872196196)}

Episode step 30580, time diff 4.124154806137085, total time dif 6483.321978569031)
step: 30580 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.0649679), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06376354303210974), 'actor_loss': np.float64(-0.9669687986373902), 'hyper_actor_loss': np.float64(2.4407652381341906e-05), 'behavior_loss': np.float64(0.33502785563468934)}

Episode step 30590, time diff 4.207511901855469, total time dif 6487.446133375168)
step: 30590 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.9727764), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06790263913571834), 'actor_loss': np.float64(-0.9842208027839661), 'hyper_actor_loss': np.float64(2.6841981525649317e-05), 'behavior_loss': np.float64(0.3384167790412903)}

Episode step 30600, time diff 4.250124931335449, total time dif 6491.653645277023)
step: 30600 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(1.6878383), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06824403628706932), 'actor_loss': np.float64(-0.9736252367496491), 'hyper_actor_loss': np.float64(2.804932137223659e-05), 'behavior_loss': np.float64(0.3389158010482788)}

Episode step 30610, time diff 4.27506160736084, total time dif 6495.903770208359)
step: 30610 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(1.6385233), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06481083296239376), 'actor_loss': np.float64(-0.9747337877750397), 'hyper_actor_loss': np.float64(3.073345033044461e-05), 'behavior_loss': np.float64(0.3306350201368332)}

Episode step 30620, time diff 4.297187805175781, total time dif 6500.17883181572)
step: 30620 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(2.6116352), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05802023634314537), 'actor_loss': np.float64(-0.9644643843173981), 'hyper_actor_loss': np.float64(3.123095393675612e-05), 'behavior_loss': np.float64(0.3366631031036377)}

Episode step 30630, time diff 4.317033052444458, total time dif 6504.476019620895)
step: 30630 @ episode report: {'average_total_reward': np.float32(9.126668), 'reward_variance': np.float32(2.751906), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07333271279931068), 'actor_loss': np.float64(-0.9676960110664368), 'hyper_actor_loss': np.float64(3.294798079878092e-05), 'behavior_loss': np.float64(0.34895962178707124)}

Episode step 30640, time diff 4.25358510017395, total time dif 6508.79305267334)
step: 30640 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(2.470027), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07028301618993282), 'actor_loss': np.float64(-0.9792888700962067), 'hyper_actor_loss': np.float64(4.047542970511131e-05), 'behavior_loss': np.float64(0.3475722700357437)}

Episode step 30650, time diff 4.4476165771484375, total time dif 6513.046637773514)
step: 30650 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.4874127), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.063410016708076), 'actor_loss': np.float64(-0.9695224702358246), 'hyper_actor_loss': np.float64(5.580467513937038e-05), 'behavior_loss': np.float64(0.3505849301815033)}

Episode step 30660, time diff 4.37616229057312, total time dif 6517.494254350662)
step: 30660 @ episode report: {'average_total_reward': np.float32(9.451113), 'reward_variance': np.float32(3.3262272), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06621238086372613), 'actor_loss': np.float64(-0.9669154107570648), 'hyper_actor_loss': np.float64(7.844593783374876e-05), 'behavior_loss': np.float64(0.3418478578329086)}

Episode step 30670, time diff 4.427436590194702, total time dif 6521.870416641235)
step: 30670 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(2.611649), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06791063733398914), 'actor_loss': np.float64(-0.983219838142395), 'hyper_actor_loss': np.float64(9.145306466962211e-05), 'behavior_loss': np.float64(0.3452083349227905)}

Episode step 30680, time diff 4.530103921890259, total time dif 6526.29785323143)
step: 30680 @ episode report: {'average_total_reward': np.float32(9.6877775), 'reward_variance': np.float32(2.1195908), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08059660978615284), 'actor_loss': np.float64(-0.9830789983272552), 'hyper_actor_loss': np.float64(0.00014632482343586163), 'behavior_loss': np.float64(0.3624385118484497)}

Episode step 30690, time diff 4.463602781295776, total time dif 6530.82795715332)
step: 30690 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(2.1538136), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07451851405203343), 'actor_loss': np.float64(-0.9812008142471313), 'hyper_actor_loss': np.float64(0.0001224047220603097), 'behavior_loss': np.float64(0.35908295810222624)}

Episode step 30700, time diff 4.45019006729126, total time dif 6535.291559934616)
step: 30700 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.975291), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07684813123196363), 'actor_loss': np.float64(-0.9829448759555817), 'hyper_actor_loss': np.float64(9.546622313791887e-05), 'behavior_loss': np.float64(0.34172719419002534)}

Episode step 30710, time diff 4.476663827896118, total time dif 6539.741750001907)
step: 30710 @ episode report: {'average_total_reward': np.float32(8.926667), 'reward_variance': np.float32(1.340203), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0740008432418108), 'actor_loss': np.float64(-0.9993289113044739), 'hyper_actor_loss': np.float64(7.966712873894722e-05), 'behavior_loss': np.float64(0.35352833569049835)}

Episode step 30720, time diff 4.424002408981323, total time dif 6544.2184138298035)
step: 30720 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(1.2345384), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062233468517661095), 'actor_loss': np.float64(-0.9771181166172027), 'hyper_actor_loss': np.float64(7.6334472396411e-05), 'behavior_loss': np.float64(0.3487456440925598)}

Episode step 30730, time diff 4.424771308898926, total time dif 6548.642416238785)
step: 30730 @ episode report: {'average_total_reward': np.float32(8.502222), 'reward_variance': np.float32(1.4619951), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0687945669516921), 'actor_loss': np.float64(-0.9632360875606537), 'hyper_actor_loss': np.float64(6.75024333759211e-05), 'behavior_loss': np.float64(0.3541883409023285)}

Episode step 30740, time diff 4.401062726974487, total time dif 6553.067187547684)
step: 30740 @ episode report: {'average_total_reward': np.float32(7.7777786), 'reward_variance': np.float32(2.6943216), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06552478969097138), 'actor_loss': np.float64(-0.9704856216907501), 'hyper_actor_loss': np.float64(6.712937611155211e-05), 'behavior_loss': np.float64(0.3464778125286102)}

Episode step 30750, time diff 4.374697685241699, total time dif 6557.468250274658)
step: 30750 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(3.164178), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07439886480569839), 'actor_loss': np.float64(-0.988058203458786), 'hyper_actor_loss': np.float64(6.548708624904975e-05), 'behavior_loss': np.float64(0.353303924202919)}

Episode step 30760, time diff 4.391294240951538, total time dif 6561.8429479599)
step: 30760 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(3.2342086), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07573064826428891), 'actor_loss': np.float64(-0.9918996512889862), 'hyper_actor_loss': np.float64(6.581846682820469e-05), 'behavior_loss': np.float64(0.3476578414440155)}

Episode step 30770, time diff 4.472042083740234, total time dif 6566.234242200851)
step: 30770 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(1.283468), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07640533111989498), 'actor_loss': np.float64(-0.995945292711258), 'hyper_actor_loss': np.float64(6.875423277961091e-05), 'behavior_loss': np.float64(0.3529266625642776)}

Episode step 30780, time diff 4.388716459274292, total time dif 6570.706284284592)
step: 30780 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(1.45279), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06798224300146102), 'actor_loss': np.float64(-0.9777512311935425), 'hyper_actor_loss': np.float64(7.021514611551538e-05), 'behavior_loss': np.float64(0.35198868811130524)}

Episode step 30790, time diff 4.4472596645355225, total time dif 6575.095000743866)
step: 30790 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(0.6375359), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07229930497705936), 'actor_loss': np.float64(-0.9696129500865937), 'hyper_actor_loss': np.float64(6.870317738503218e-05), 'behavior_loss': np.float64(0.3566868305206299)}

Episode step 30800, time diff 4.466052293777466, total time dif 6579.5422604084015)
step: 30800 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(3.9564202), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0625713013112545), 'actor_loss': np.float64(-0.9780767619609833), 'hyper_actor_loss': np.float64(6.80792385537643e-05), 'behavior_loss': np.float64(0.35046371519565583)}

Episode step 30810, time diff 4.502937316894531, total time dif 6584.008312702179)
step: 30810 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.9428353), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07664583399891853), 'actor_loss': np.float64(-0.972225946187973), 'hyper_actor_loss': np.float64(6.495785019069444e-05), 'behavior_loss': np.float64(0.35810098946094515)}

Episode step 30820, time diff 4.6764702796936035, total time dif 6588.5112500190735)
step: 30820 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.7966931), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06888556480407715), 'actor_loss': np.float64(-0.9878548920154572), 'hyper_actor_loss': np.float64(6.545630349137354e-05), 'behavior_loss': np.float64(0.3475642651319504)}

Episode step 30830, time diff 4.545872688293457, total time dif 6593.187720298767)
step: 30830 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.6116252), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.7777777), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07214613445103168), 'actor_loss': np.float64(-0.9907240450382233), 'hyper_actor_loss': np.float64(6.823491276009008e-05), 'behavior_loss': np.float64(0.3498577207326889)}

Episode step 30840, time diff 4.577739238739014, total time dif 6597.733592987061)
step: 30840 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(1.9627364), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07736920602619649), 'actor_loss': np.float64(-0.990581476688385), 'hyper_actor_loss': np.float64(6.885902694193646e-05), 'behavior_loss': np.float64(0.35785486698150637)}

Episode step 30850, time diff 4.608551740646362, total time dif 6602.3113322258)
step: 30850 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(3.2929382), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06705877259373665), 'actor_loss': np.float64(-0.9899944722652435), 'hyper_actor_loss': np.float64(7.343200268223881e-05), 'behavior_loss': np.float64(0.3495854586362839)}

Episode step 30860, time diff 4.62854790687561, total time dif 6606.919883966446)
step: 30860 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.1252542), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07610287107527255), 'actor_loss': np.float64(-0.9793660581111908), 'hyper_actor_loss': np.float64(7.468771073035896e-05), 'behavior_loss': np.float64(0.3632442504167557)}

Episode step 30870, time diff 4.635295867919922, total time dif 6611.5484318733215)
step: 30870 @ episode report: {'average_total_reward': np.float32(8.802222), 'reward_variance': np.float32(2.116637), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07130932696163654), 'actor_loss': np.float64(-0.9664196610450745), 'hyper_actor_loss': np.float64(7.281204452738165e-05), 'behavior_loss': np.float64(0.35903525948524473)}

Episode step 30880, time diff 4.639113903045654, total time dif 6616.1837277412415)
step: 30880 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.7014523), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06639491282403469), 'actor_loss': np.float64(-0.979314535856247), 'hyper_actor_loss': np.float64(6.976287077122833e-05), 'behavior_loss': np.float64(0.3531475454568863)}

Episode step 30890, time diff 4.650109767913818, total time dif 6620.822841644287)
step: 30890 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(2.8778977), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06548761315643788), 'actor_loss': np.float64(-0.9791908204555512), 'hyper_actor_loss': np.float64(7.092997257132083e-05), 'behavior_loss': np.float64(0.3582246541976929)}

Episode step 30900, time diff 4.634451389312744, total time dif 6625.472951412201)
step: 30900 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(4.09139), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07548442780971527), 'actor_loss': np.float64(-0.9765120267868042), 'hyper_actor_loss': np.float64(7.158655134844593e-05), 'behavior_loss': np.float64(0.3527597814798355)}

Episode step 30910, time diff 4.667024374008179, total time dif 6630.107402801514)
step: 30910 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(2.2310917), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059149665758013725), 'actor_loss': np.float64(-0.9771127223968505), 'hyper_actor_loss': np.float64(6.992272537900134e-05), 'behavior_loss': np.float64(0.3570019483566284)}

Episode step 30920, time diff 4.666723012924194, total time dif 6634.774427175522)
step: 30920 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.4619946), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06773680374026299), 'actor_loss': np.float64(-0.9704905807971954), 'hyper_actor_loss': np.float64(7.039922493277118e-05), 'behavior_loss': np.float64(0.3521557331085205)}

Episode step 30930, time diff 4.673191785812378, total time dif 6639.441150188446)
step: 30930 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.2704742), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0700142189860344), 'actor_loss': np.float64(-0.9832964479923249), 'hyper_actor_loss': np.float64(6.852796323073563e-05), 'behavior_loss': np.float64(0.35705587863922117)}

Episode step 30940, time diff 4.671854257583618, total time dif 6644.114341974258)
step: 30940 @ episode report: {'average_total_reward': np.float32(9.924445), 'reward_variance': np.float32(1.4951804), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07378179244697095), 'actor_loss': np.float64(-0.9808085799217224), 'hyper_actor_loss': np.float64(6.878671338199638e-05), 'behavior_loss': np.float64(0.3625465601682663)}

Episode step 30950, time diff 4.658682823181152, total time dif 6648.786196231842)
step: 30950 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(0.8484891), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07390298694372177), 'actor_loss': np.float64(-0.9766424179077149), 'hyper_actor_loss': np.float64(6.360038350976538e-05), 'behavior_loss': np.float64(0.3532312333583832)}

Episode step 30960, time diff 4.583312034606934, total time dif 6653.444879055023)
step: 30960 @ episode report: {'average_total_reward': np.float32(9.461111), 'reward_variance': np.float32(1.8261055), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07524334192276001), 'actor_loss': np.float64(-0.9908787906169891), 'hyper_actor_loss': np.float64(6.542343799083028e-05), 'behavior_loss': np.float64(0.3597062945365906)}

Episode step 30970, time diff 4.530731439590454, total time dif 6658.02819108963)
step: 30970 @ episode report: {'average_total_reward': np.float32(9.351112), 'reward_variance': np.float32(2.7723503), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07523393854498864), 'actor_loss': np.float64(-0.9938462376594543), 'hyper_actor_loss': np.float64(6.418175435101148e-05), 'behavior_loss': np.float64(0.36443363428115844)}

Episode step 30980, time diff 4.552440643310547, total time dif 6662.558922529221)
step: 30980 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(4.065946), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0804725505411625), 'actor_loss': np.float64(-0.9831778645515442), 'hyper_actor_loss': np.float64(6.34095547866309e-05), 'behavior_loss': np.float64(0.3617596834897995)}

Episode step 30990, time diff 4.794817209243774, total time dif 6667.111363172531)
step: 30990 @ episode report: {'average_total_reward': np.float32(9.126668), 'reward_variance': np.float32(2.7962766), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0638684906065464), 'actor_loss': np.float64(-0.9843661904335022), 'hyper_actor_loss': np.float64(5.7117259348160585e-05), 'behavior_loss': np.float64(0.35146135091781616)}

Episode step 31000, time diff 4.685375928878784, total time dif 6671.906180381775)
step: 31000 @ episode report: {'average_total_reward': np.float32(9.863334), 'reward_variance': np.float32(4.065039), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06451852340251207), 'actor_loss': np.float64(-0.9662154912948608), 'hyper_actor_loss': np.float64(5.6537238560849803e-05), 'behavior_loss': np.float64(0.3576323688030243)}

Episode step 31010, time diff 4.740931272506714, total time dif 6676.591556310654)
step: 31010 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(2.871487), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07166916690766811), 'actor_loss': np.float64(-0.9657768249511719), 'hyper_actor_loss': np.float64(5.7552772705093955e-05), 'behavior_loss': np.float64(0.3546459972858429)}

Episode step 31020, time diff 4.7507710456848145, total time dif 6681.33248758316)
step: 31020 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(4.1803966), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07190641164779663), 'actor_loss': np.float64(-1.000036495923996), 'hyper_actor_loss': np.float64(5.6695442253840156e-05), 'behavior_loss': np.float64(0.3573713004589081)}

Episode step 31030, time diff 4.892582654953003, total time dif 6686.083258628845)
step: 31030 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(1.7927723), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06487831100821495), 'actor_loss': np.float64(-0.9825977861881257), 'hyper_actor_loss': np.float64(5.600187942036428e-05), 'behavior_loss': np.float64(0.34998233020305636)}

Episode step 31040, time diff 4.932958364486694, total time dif 6690.975841283798)
step: 31040 @ episode report: {'average_total_reward': np.float32(9.351111), 'reward_variance': np.float32(2.5813134), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08680209591984749), 'actor_loss': np.float64(-0.985667484998703), 'hyper_actor_loss': np.float64(5.255320320429746e-05), 'behavior_loss': np.float64(0.3595579594373703)}

Episode step 31050, time diff 4.908514738082886, total time dif 6695.908799648285)
step: 31050 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.8455627), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08000730723142624), 'actor_loss': np.float64(-0.9899918138980865), 'hyper_actor_loss': np.float64(7.797492507961578e-05), 'behavior_loss': np.float64(0.3627756297588348)}

Episode step 31060, time diff 4.89410138130188, total time dif 6700.817314386368)
step: 31060 @ episode report: {'average_total_reward': np.float32(10.061112), 'reward_variance': np.float32(3.1863277), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06798030324280262), 'actor_loss': np.float64(-0.989908879995346), 'hyper_actor_loss': np.float64(9.298589138779789e-05), 'behavior_loss': np.float64(0.356648251414299)}

Episode step 31070, time diff 4.924592733383179, total time dif 6705.71141576767)
step: 31070 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(1.3183819), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058741646073758605), 'actor_loss': np.float64(-0.9593041777610779), 'hyper_actor_loss': np.float64(7.725282630417496e-05), 'behavior_loss': np.float64(0.3677001088857651)}

Episode step 31080, time diff 4.954809904098511, total time dif 6710.636008501053)
step: 31080 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(1.4506285), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05825822297483683), 'actor_loss': np.float64(-0.9578453779220581), 'hyper_actor_loss': np.float64(6.705319028696977e-05), 'behavior_loss': np.float64(0.3566918820142746)}

Episode step 31090, time diff 4.922567844390869, total time dif 6715.590818405151)
step: 31090 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(1.9627774), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0739897744730115), 'actor_loss': np.float64(-0.9672237277030945), 'hyper_actor_loss': np.float64(6.400974525604397e-05), 'behavior_loss': np.float64(0.3634653568267822)}

Episode step 31100, time diff 4.97784948348999, total time dif 6720.513386249542)
step: 31100 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.5896056), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06762563046067953), 'actor_loss': np.float64(-0.9894660294055939), 'hyper_actor_loss': np.float64(9.861409853328951e-05), 'behavior_loss': np.float64(0.36942960917949674)}

Episode step 31110, time diff 5.052913427352905, total time dif 6725.491235733032)
step: 31110 @ episode report: {'average_total_reward': np.float32(9.463335), 'reward_variance': np.float32(1.7220752), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0693474356085062), 'actor_loss': np.float64(-0.9606776118278504), 'hyper_actor_loss': np.float64(0.0001277456889511086), 'behavior_loss': np.float64(0.36586320102214814)}

Episode step 31120, time diff 4.798180341720581, total time dif 6730.544149160385)
step: 31120 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(0.522296), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06462277360260486), 'actor_loss': np.float64(-0.9629253685474396), 'hyper_actor_loss': np.float64(0.00010177664444199763), 'behavior_loss': np.float64(0.37004140615463255)}

Episode step 31130, time diff 4.8686363697052, total time dif 6735.342329502106)
step: 31130 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(0.93343365), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06795781627297401), 'actor_loss': np.float64(-0.9752990543842316), 'hyper_actor_loss': np.float64(7.93933060776908e-05), 'behavior_loss': np.float64(0.36536754965782164)}

Episode step 31140, time diff 4.925139904022217, total time dif 6740.210965871811)
step: 31140 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.1986169), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06756135560572148), 'actor_loss': np.float64(-0.9741196572780609), 'hyper_actor_loss': np.float64(7.433233404299244e-05), 'behavior_loss': np.float64(0.3620115667581558)}

Episode step 31150, time diff 5.037209987640381, total time dif 6745.136105775833)
step: 31150 @ episode report: {'average_total_reward': np.float32(9.338889), 'reward_variance': np.float32(3.0365992), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06676587983965873), 'actor_loss': np.float64(-0.9800205945968627), 'hyper_actor_loss': np.float64(7.037824325379915e-05), 'behavior_loss': np.float64(0.36082897782325746)}

Episode step 31160, time diff 5.161244869232178, total time dif 6750.1733157634735)
step: 31160 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(4.8154774), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.078884806483984), 'actor_loss': np.float64(-0.9844773769378662), 'hyper_actor_loss': np.float64(7.188015733845532e-05), 'behavior_loss': np.float64(0.36212811172008513)}

Episode step 31170, time diff 5.016169548034668, total time dif 6755.334560632706)
step: 31170 @ episode report: {'average_total_reward': np.float32(9.100001), 'reward_variance': np.float32(2.068272), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06632668152451515), 'actor_loss': np.float64(-0.968095576763153), 'hyper_actor_loss': np.float64(7.16519592970144e-05), 'behavior_loss': np.float64(0.36788186728954314)}

Episode step 31180, time diff 4.956942081451416, total time dif 6760.35073018074)
step: 31180 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.0754511), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07389161176979542), 'actor_loss': np.float64(-0.9664439499378205), 'hyper_actor_loss': np.float64(7.100235525285825e-05), 'behavior_loss': np.float64(0.36815930604934693)}

Episode step 31190, time diff 4.954838991165161, total time dif 6765.307672262192)
step: 31190 @ episode report: {'average_total_reward': np.float32(9.424445), 'reward_variance': np.float32(1.2843416), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06932651996612549), 'actor_loss': np.float64(-0.9915354430675507), 'hyper_actor_loss': np.float64(6.76317409670446e-05), 'behavior_loss': np.float64(0.36524654030799864)}

Episode step 31200, time diff 4.991229057312012, total time dif 6770.262511253357)
step: 31200 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(3.5002167), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07640585415065289), 'actor_loss': np.float64(-0.9871458351612091), 'hyper_actor_loss': np.float64(7.237901736516506e-05), 'behavior_loss': np.float64(0.3627275049686432)}

Episode step 31210, time diff 4.962153911590576, total time dif 6775.253740310669)
step: 31210 @ episode report: {'average_total_reward': np.float32(9.000001), 'reward_variance': np.float32(3.8132844), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06578513197600841), 'actor_loss': np.float64(-0.9719603478908538), 'hyper_actor_loss': np.float64(7.143155598896556e-05), 'behavior_loss': np.float64(0.3710775703191757)}

Episode step 31220, time diff 5.0081467628479, total time dif 6780.2158942222595)
step: 31220 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.1901684), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08097829595208168), 'actor_loss': np.float64(-0.9729933917522431), 'hyper_actor_loss': np.float64(6.678653298877179e-05), 'behavior_loss': np.float64(0.3614707350730896)}

Episode step 31230, time diff 5.0480992794036865, total time dif 6785.224040985107)
step: 31230 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(1.4808446), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0555987723171711), 'actor_loss': np.float64(-0.9810714423656464), 'hyper_actor_loss': np.float64(7.2760467446642e-05), 'behavior_loss': np.float64(0.35672065913677214)}

Episode step 31240, time diff 5.0191969871521, total time dif 6790.272140264511)
step: 31240 @ episode report: {'average_total_reward': np.float32(9.324446), 'reward_variance': np.float32(2.144588), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07748435139656067), 'actor_loss': np.float64(-0.9659732341766357), 'hyper_actor_loss': np.float64(6.849165620224085e-05), 'behavior_loss': np.float64(0.3619709998369217)}

Episode step 31250, time diff 5.025728464126587, total time dif 6795.291337251663)
step: 31250 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(1.474869), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07677818909287452), 'actor_loss': np.float64(-1.0111307799816132), 'hyper_actor_loss': np.float64(6.68313314236002e-05), 'behavior_loss': np.float64(0.36236833333969115)}

Episode step 31260, time diff 5.097223281860352, total time dif 6800.31706571579)
step: 31260 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.7299362), 'max_total_reward': np.float32(12.1444435), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06552671529352665), 'actor_loss': np.float64(-0.9817870438098908), 'hyper_actor_loss': np.float64(6.89670407155063e-05), 'behavior_loss': np.float64(0.3629330962896347)}

Episode step 31270, time diff 5.0321009159088135, total time dif 6805.41428899765)
step: 31270 @ episode report: {'average_total_reward': np.float32(10.012223), 'reward_variance': np.float32(0.9048255), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06263195984065532), 'actor_loss': np.float64(-0.9484164237976074), 'hyper_actor_loss': np.float64(6.511016908916645e-05), 'behavior_loss': np.float64(0.3692678064107895)}

Episode step 31280, time diff 5.054947137832642, total time dif 6810.446389913559)
step: 31280 @ episode report: {'average_total_reward': np.float32(9.075557), 'reward_variance': np.float32(3.5592797), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06140685584396124), 'actor_loss': np.float64(-0.9478650212287902), 'hyper_actor_loss': np.float64(6.660057479166426e-05), 'behavior_loss': np.float64(0.37258065640926363)}

Episode step 31290, time diff 4.9856650829315186, total time dif 6815.501337051392)
step: 31290 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(2.4843469), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057120774500072004), 'actor_loss': np.float64(-0.9677827835083008), 'hyper_actor_loss': np.float64(6.204839228303171e-05), 'behavior_loss': np.float64(0.3587107390165329)}

Episode step 31300, time diff 5.052190542221069, total time dif 6820.487002134323)
step: 31300 @ episode report: {'average_total_reward': np.float32(8.638889), 'reward_variance': np.float32(5.001266), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07249750271439552), 'actor_loss': np.float64(-0.9839296817779541), 'hyper_actor_loss': np.float64(6.549416975758504e-05), 'behavior_loss': np.float64(0.36562162041664126)}

Episode step 31310, time diff 5.035014629364014, total time dif 6825.539192676544)
step: 31310 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.8247218), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06751340478658677), 'actor_loss': np.float64(-0.9716015875339508), 'hyper_actor_loss': np.float64(6.321609616861678e-05), 'behavior_loss': np.float64(0.3680806905031204)}

Episode step 31320, time diff 5.223496913909912, total time dif 6830.574207305908)
step: 31320 @ episode report: {'average_total_reward': np.float32(9.536666), 'reward_variance': np.float32(3.294989), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06919772028923035), 'actor_loss': np.float64(-0.9743755757808685), 'hyper_actor_loss': np.float64(6.374057120410726e-05), 'behavior_loss': np.float64(0.3646136909723282)}

Episode step 31330, time diff 5.0056610107421875, total time dif 6835.797704219818)
step: 31330 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.9543774), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07257986925542355), 'actor_loss': np.float64(-0.9822877705097198), 'hyper_actor_loss': np.float64(6.406272841559257e-05), 'behavior_loss': np.float64(0.3574611485004425)}

Episode step 31340, time diff 5.063058614730835, total time dif 6840.80336523056)
step: 31340 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(3.1566958), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06671802774071693), 'actor_loss': np.float64(-0.9886654913425446), 'hyper_actor_loss': np.float64(6.257700988498982e-05), 'behavior_loss': np.float64(0.36047435998916627)}

Episode step 31350, time diff 5.122040510177612, total time dif 6845.866423845291)
step: 31350 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(1.7161283), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06426416300237178), 'actor_loss': np.float64(-0.9735004425048828), 'hyper_actor_loss': np.float64(6.237790366867558e-05), 'behavior_loss': np.float64(0.3650956779718399)}

Episode step 31360, time diff 4.859639406204224, total time dif 6850.988464355469)
step: 31360 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.7189636), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.073169407248497), 'actor_loss': np.float64(-0.9661043763160706), 'hyper_actor_loss': np.float64(6.162980025692377e-05), 'behavior_loss': np.float64(0.37417625486850736)}

Episode step 31370, time diff 4.727941989898682, total time dif 6855.848103761673)
step: 31370 @ episode report: {'average_total_reward': np.float32(8.451112), 'reward_variance': np.float32(2.4349678), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08269607909023761), 'actor_loss': np.float64(-0.9883887052536011), 'hyper_actor_loss': np.float64(5.966885109955911e-05), 'behavior_loss': np.float64(0.36573341190814973)}

Episode step 31380, time diff 4.867712497711182, total time dif 6860.576045751572)
step: 31380 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(2.208647), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06838223710656166), 'actor_loss': np.float64(-0.9936741113662719), 'hyper_actor_loss': np.float64(5.612161876342725e-05), 'behavior_loss': np.float64(0.3627704232931137)}

Episode step 31390, time diff 5.053028106689453, total time dif 6865.443758249283)
step: 31390 @ episode report: {'average_total_reward': np.float32(8.89), 'reward_variance': np.float32(1.4719616), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07227137051522732), 'actor_loss': np.float64(-0.9793060719966888), 'hyper_actor_loss': np.float64(5.6098433560691774e-05), 'behavior_loss': np.float64(0.3588596791028976)}

Episode step 31400, time diff 5.100136756896973, total time dif 6870.496786355972)
step: 31400 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(0.9365632), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08238716199994087), 'actor_loss': np.float64(-0.9928141176700592), 'hyper_actor_loss': np.float64(5.567188418353908e-05), 'behavior_loss': np.float64(0.3691595792770386)}

Episode step 31410, time diff 5.078994274139404, total time dif 6875.596923112869)
step: 31410 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(3.9014277), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08510496169328689), 'actor_loss': np.float64(-1.0008639752864839), 'hyper_actor_loss': np.float64(5.8720732704387045e-05), 'behavior_loss': np.float64(0.3605435073375702)}

Episode step 31420, time diff 5.083214044570923, total time dif 6880.675917387009)
step: 31420 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(3.3165245), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07651242054998875), 'actor_loss': np.float64(-0.9982729732990265), 'hyper_actor_loss': np.float64(5.5153613357106226e-05), 'behavior_loss': np.float64(0.3707632213830948)}

Episode step 31430, time diff 5.018632650375366, total time dif 6885.75913143158)
step: 31430 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(5.228177), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06560705471783876), 'actor_loss': np.float64(-0.966998291015625), 'hyper_actor_loss': np.float64(5.2971052355133e-05), 'behavior_loss': np.float64(0.3667802006006241)}

Episode step 31440, time diff 4.973792314529419, total time dif 6890.777764081955)
step: 31440 @ episode report: {'average_total_reward': np.float32(9.8977785), 'reward_variance': np.float32(2.3514025), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0778927568346262), 'actor_loss': np.float64(-0.9651924550533295), 'hyper_actor_loss': np.float64(5.328162296791561e-05), 'behavior_loss': np.float64(0.3691200494766235)}

Episode step 31450, time diff 4.912421941757202, total time dif 6895.751556396484)
step: 31450 @ episode report: {'average_total_reward': np.float32(9.536668), 'reward_variance': np.float32(3.2431133), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06339305080473423), 'actor_loss': np.float64(-0.9765197038650513), 'hyper_actor_loss': np.float64(5.418856053438503e-05), 'behavior_loss': np.float64(0.3668885827064514)}

Episode step 31460, time diff 4.855777263641357, total time dif 6900.663978338242)
step: 31460 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(3.0106435), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060394825786352156), 'actor_loss': np.float64(-0.966853666305542), 'hyper_actor_loss': np.float64(5.0185062718810514e-05), 'behavior_loss': np.float64(0.36926273703575135)}

Episode step 31470, time diff 4.945587158203125, total time dif 6905.519755601883)
step: 31470 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(2.7946973), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06942849569022655), 'actor_loss': np.float64(-0.9640913903713226), 'hyper_actor_loss': np.float64(5.071375053375959e-05), 'behavior_loss': np.float64(0.3650208294391632)}

Episode step 31480, time diff 5.1165385246276855, total time dif 6910.465342760086)
step: 31480 @ episode report: {'average_total_reward': np.float32(9.500002), 'reward_variance': np.float32(3.2056549), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06800504121929407), 'actor_loss': np.float64(-0.9776174545288085), 'hyper_actor_loss': np.float64(4.621256885002367e-05), 'behavior_loss': np.float64(0.3721408754587173)}

Episode step 31490, time diff 5.0821733474731445, total time dif 6915.581881284714)
step: 31490 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(2.7887177), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06903623677790165), 'actor_loss': np.float64(-0.9768822133541107), 'hyper_actor_loss': np.float64(4.551987731247209e-05), 'behavior_loss': np.float64(0.37250658571720124)}

Episode step 31500, time diff 5.065502882003784, total time dif 6920.664054632187)
step: 31500 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(1.8036245), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06811200939118862), 'actor_loss': np.float64(-0.9726825177669525), 'hyper_actor_loss': np.float64(4.672348331951071e-05), 'behavior_loss': np.float64(0.3683614045381546)}

Episode step 31510, time diff 5.0613110065460205, total time dif 6925.729557514191)
step: 31510 @ episode report: {'average_total_reward': np.float32(8.863334), 'reward_variance': np.float32(2.721903), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06659477427601815), 'actor_loss': np.float64(-0.9691838920116425), 'hyper_actor_loss': np.float64(4.6459869554382746e-05), 'behavior_loss': np.float64(0.3666741341352463)}

Episode step 31520, time diff 4.886154651641846, total time dif 6930.790868520737)
step: 31520 @ episode report: {'average_total_reward': np.float32(8.214445), 'reward_variance': np.float32(2.2472847), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07436518222093583), 'actor_loss': np.float64(-0.9811347484588623), 'hyper_actor_loss': np.float64(4.6404886961681766e-05), 'behavior_loss': np.float64(0.37177749574184416)}

Episode step 31530, time diff 4.882023811340332, total time dif 6935.6770231723785)
step: 31530 @ episode report: {'average_total_reward': np.float32(9.336667), 'reward_variance': np.float32(2.8059025), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07075113542377949), 'actor_loss': np.float64(-0.9881231367588044), 'hyper_actor_loss': np.float64(4.6499069867422804e-05), 'behavior_loss': np.float64(0.3693024665117264)}

Episode step 31540, time diff 4.946013689041138, total time dif 6940.559046983719)
step: 31540 @ episode report: {'average_total_reward': np.float32(10.012223), 'reward_variance': np.float32(1.3572452), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05973319411277771), 'actor_loss': np.float64(-0.9785151660442353), 'hyper_actor_loss': np.float64(4.392864684632514e-05), 'behavior_loss': np.float64(0.3615403711795807)}

Episode step 31550, time diff 5.0192084312438965, total time dif 6945.50506067276)
step: 31550 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.22087), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06487540490925311), 'actor_loss': np.float64(-0.9677675426006317), 'hyper_actor_loss': np.float64(4.386286054796074e-05), 'behavior_loss': np.float64(0.3695394337177277)}

Episode step 31560, time diff 5.055511713027954, total time dif 6950.524269104004)
step: 31560 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(2.4181504), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06737364511936902), 'actor_loss': np.float64(-0.9720824539661408), 'hyper_actor_loss': np.float64(4.3007713611586953e-05), 'behavior_loss': np.float64(0.36211042404174804)}

Episode step 31570, time diff 5.075170516967773, total time dif 6955.579780817032)
step: 31570 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.911659), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06600985303521156), 'actor_loss': np.float64(-0.9860038578510284), 'hyper_actor_loss': np.float64(4.2864911665674296e-05), 'behavior_loss': np.float64(0.37328662574291227)}

Episode step 31580, time diff 5.091737747192383, total time dif 6960.654951334)
step: 31580 @ episode report: {'average_total_reward': np.float32(9.787778), 'reward_variance': np.float32(1.5482832), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07542504779994488), 'actor_loss': np.float64(-0.9668490886688232), 'hyper_actor_loss': np.float64(4.1401813359698283e-05), 'behavior_loss': np.float64(0.37079827189445497)}

Episode step 31590, time diff 5.045218229293823, total time dif 6965.746689081192)
step: 31590 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(5.494645), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06350677665323019), 'actor_loss': np.float64(-0.9628357827663422), 'hyper_actor_loss': np.float64(4.0418978096568024e-05), 'behavior_loss': np.float64(0.37274200916290284)}

Episode step 31600, time diff 4.871455430984497, total time dif 6970.791907310486)
step: 31600 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(1.7947029), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05766089037060738), 'actor_loss': np.float64(-0.9577171266078949), 'hyper_actor_loss': np.float64(4.075573597219773e-05), 'behavior_loss': np.float64(0.3805070996284485)}

Episode step 31610, time diff 4.929247140884399, total time dif 6975.66336274147)
step: 31610 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(2.5977798), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07154165599495173), 'actor_loss': np.float64(-0.9637083351612091), 'hyper_actor_loss': np.float64(3.878724892274477e-05), 'behavior_loss': np.float64(0.3646581441164017)}

Episode step 31620, time diff 5.198431015014648, total time dif 6980.592609882355)
step: 31620 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.693468), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058131405338644984), 'actor_loss': np.float64(-0.9841176927089691), 'hyper_actor_loss': np.float64(3.848916239803657e-05), 'behavior_loss': np.float64(0.3781851977109909)}

Episode step 31630, time diff 5.267808198928833, total time dif 6985.791040897369)
step: 31630 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.7143961), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06554954685270786), 'actor_loss': np.float64(-0.9600187540054321), 'hyper_actor_loss': np.float64(3.847873813356273e-05), 'behavior_loss': np.float64(0.3633974939584732)}

Episode step 31640, time diff 5.016588449478149, total time dif 6991.058849096298)
step: 31640 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(1.2345381), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06406628973782062), 'actor_loss': np.float64(-0.9653166770935059), 'hyper_actor_loss': np.float64(3.5832698267768136e-05), 'behavior_loss': np.float64(0.3669815182685852)}

Episode step 31650, time diff 5.090985536575317, total time dif 6996.075437545776)
step: 31650 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(1.6246866), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05868231002241373), 'actor_loss': np.float64(-0.9780325651168823), 'hyper_actor_loss': np.float64(3.48276689692284e-05), 'behavior_loss': np.float64(0.3588185101747513)}

Episode step 31660, time diff 4.9020562171936035, total time dif 7001.166423082352)
step: 31660 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.1427855), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05946946162730456), 'actor_loss': np.float64(-0.9681826233863831), 'hyper_actor_loss': np.float64(3.344128272146918e-05), 'behavior_loss': np.float64(0.3741353988647461)}

Episode step 31670, time diff 4.939592123031616, total time dif 7006.068479299545)
step: 31670 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(2.4225984), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0701403085142374), 'actor_loss': np.float64(-0.9529929101467133), 'hyper_actor_loss': np.float64(3.331252719362965e-05), 'behavior_loss': np.float64(0.3738102614879608)}

Episode step 31680, time diff 4.986377239227295, total time dif 7011.008071422577)
step: 31680 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(3.8816063), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05744103901088238), 'actor_loss': np.float64(-0.9735572159290313), 'hyper_actor_loss': np.float64(3.3926941978279504e-05), 'behavior_loss': np.float64(0.3721929997205734)}

Episode step 31690, time diff 4.9627845287323, total time dif 7015.994448661804)
step: 31690 @ episode report: {'average_total_reward': np.float32(9.6877775), 'reward_variance': np.float32(1.9255669), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07889074273407459), 'actor_loss': np.float64(-0.9773631811141967), 'hyper_actor_loss': np.float64(3.115080126008252e-05), 'behavior_loss': np.float64(0.36652655303478243)}

Episode step 31700, time diff 4.934089183807373, total time dif 7020.9572331905365)
step: 31700 @ episode report: {'average_total_reward': np.float32(8.902224), 'reward_variance': np.float32(2.4760697), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07665274925529957), 'actor_loss': np.float64(-0.9951026797294616), 'hyper_actor_loss': np.float64(2.9539828938140998e-05), 'behavior_loss': np.float64(0.3732870042324066)}

Episode step 31710, time diff 4.917388916015625, total time dif 7025.891322374344)
step: 31710 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.8296897), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06320353560149669), 'actor_loss': np.float64(-0.9838730812072753), 'hyper_actor_loss': np.float64(2.978372631332604e-05), 'behavior_loss': np.float64(0.36373646557331085)}

Episode step 31720, time diff 4.977009534835815, total time dif 7030.8087112903595)
step: 31720 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(3.2265058), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08773911893367767), 'actor_loss': np.float64(-0.9651151835918427), 'hyper_actor_loss': np.float64(2.9625838578795083e-05), 'behavior_loss': np.float64(0.37888666093349455)}

Episode step 31730, time diff 4.9657697677612305, total time dif 7035.785720825195)
step: 31730 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(2.3678539), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0629480293020606), 'actor_loss': np.float64(-0.9947108387947082), 'hyper_actor_loss': np.float64(3.037216665688902e-05), 'behavior_loss': np.float64(0.3685881108045578)}

Episode step 31740, time diff 5.000736713409424, total time dif 7040.7514905929565)
step: 31740 @ episode report: {'average_total_reward': np.float32(9.412224), 'reward_variance': np.float32(2.2541347), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0638391550630331), 'actor_loss': np.float64(-0.9796344459056854), 'hyper_actor_loss': np.float64(3.1233347544912246e-05), 'behavior_loss': np.float64(0.36638265252113345)}

Episode step 31750, time diff 5.004707336425781, total time dif 7045.752227306366)
step: 31750 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.738203), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07231745086610317), 'actor_loss': np.float64(-0.9686514019966126), 'hyper_actor_loss': np.float64(3.057472786167636e-05), 'behavior_loss': np.float64(0.3741060942411423)}

Episode step 31760, time diff 5.033719778060913, total time dif 7050.756934642792)
step: 31760 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(4.6537156), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060257836058735845), 'actor_loss': np.float64(-0.9689726114273072), 'hyper_actor_loss': np.float64(3.0799853448115755e-05), 'behavior_loss': np.float64(0.36569774746894834)}

Episode step 31770, time diff 4.98006534576416, total time dif 7055.790654420853)
step: 31770 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(2.2305684), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07912325412034989), 'actor_loss': np.float64(-0.9819084942340851), 'hyper_actor_loss': np.float64(3.2289727096213026e-05), 'behavior_loss': np.float64(0.3811733067035675)}

Episode step 31780, time diff 5.0272057056427, total time dif 7060.770719766617)
step: 31780 @ episode report: {'average_total_reward': np.float32(8.402224), 'reward_variance': np.float32(2.2584894), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06368701718747616), 'actor_loss': np.float64(-0.9727335214614868), 'hyper_actor_loss': np.float64(3.215620836272137e-05), 'behavior_loss': np.float64(0.3711377114057541)}

Episode step 31790, time diff 4.910942077636719, total time dif 7065.7979254722595)
step: 31790 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(1.467606), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057788581028580664), 'actor_loss': np.float64(-0.957896602153778), 'hyper_actor_loss': np.float64(3.113097063760506e-05), 'behavior_loss': np.float64(0.36729376912117007)}

Episode step 31800, time diff 4.947387456893921, total time dif 7070.708867549896)
step: 31800 @ episode report: {'average_total_reward': np.float32(8.651111), 'reward_variance': np.float32(3.9466221), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05870061367750168), 'actor_loss': np.float64(-0.9603629291057587), 'hyper_actor_loss': np.float64(3.1706582558399535e-05), 'behavior_loss': np.float64(0.3724502921104431)}

Episode step 31810, time diff 5.10944128036499, total time dif 7075.65625500679)
step: 31810 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(3.0320787), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0737901870161295), 'actor_loss': np.float64(-0.9795112431049346), 'hyper_actor_loss': np.float64(3.0217584208003245e-05), 'behavior_loss': np.float64(0.37382308542728426)}

Episode step 31820, time diff 4.957638740539551, total time dif 7080.765696287155)
step: 31820 @ episode report: {'average_total_reward': np.float32(9.33889), 'reward_variance': np.float32(1.8684756), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07645744271576405), 'actor_loss': np.float64(-0.9958698093891144), 'hyper_actor_loss': np.float64(2.9504209487640763e-05), 'behavior_loss': np.float64(0.37172738611698153)}

Episode step 31830, time diff 4.999087810516357, total time dif 7085.723335027695)
step: 31830 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(3.1307719), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06485876590013503), 'actor_loss': np.float64(-0.9732449114322662), 'hyper_actor_loss': np.float64(3.06850057313568e-05), 'behavior_loss': np.float64(0.368665537238121)}

Episode step 31840, time diff 4.976675271987915, total time dif 7090.722422838211)
step: 31840 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(3.0141194), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06103164181113243), 'actor_loss': np.float64(-0.9612052500247955), 'hyper_actor_loss': np.float64(2.8925428705406375e-05), 'behavior_loss': np.float64(0.3693986803293228)}

Episode step 31850, time diff 4.87363338470459, total time dif 7095.699098110199)
step: 31850 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.0690924), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06477605402469636), 'actor_loss': np.float64(-0.9712675988674164), 'hyper_actor_loss': np.float64(2.9075197198835666e-05), 'behavior_loss': np.float64(0.3633425712585449)}

Episode step 31860, time diff 4.899402856826782, total time dif 7100.572731494904)
step: 31860 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(2.0954323), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06276814229786395), 'actor_loss': np.float64(-0.981597501039505), 'hyper_actor_loss': np.float64(2.8528698749141768e-05), 'behavior_loss': np.float64(0.3668085187673569)}

Episode step 31870, time diff 4.882475137710571, total time dif 7105.47213435173)
step: 31870 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.6670872), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06703780069947243), 'actor_loss': np.float64(-0.9717256367206574), 'hyper_actor_loss': np.float64(2.791186525428202e-05), 'behavior_loss': np.float64(0.3732499986886978)}

Episode step 31880, time diff 4.910741090774536, total time dif 7110.354609489441)
step: 31880 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(5.4451175), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07966708280146122), 'actor_loss': np.float64(-0.9714015066623688), 'hyper_actor_loss': np.float64(2.7406819208408705e-05), 'behavior_loss': np.float64(0.37634292542934417)}

Episode step 31890, time diff 4.905730724334717, total time dif 7115.265350580215)
step: 31890 @ episode report: {'average_total_reward': np.float32(8.926667), 'reward_variance': np.float32(0.86388147), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0575899874791503), 'actor_loss': np.float64(-0.9806578874588012), 'hyper_actor_loss': np.float64(2.77824994554976e-05), 'behavior_loss': np.float64(0.359238064289093)}

Episode step 31900, time diff 4.93441104888916, total time dif 7120.17108130455)
step: 31900 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(3.1926122), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07221419475972653), 'actor_loss': np.float64(-0.9789683878421783), 'hyper_actor_loss': np.float64(2.8184680559206754e-05), 'behavior_loss': np.float64(0.37586373686790464)}

Episode step 31910, time diff 4.9079062938690186, total time dif 7125.105492353439)
step: 31910 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(4.1791716), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05936448499560356), 'actor_loss': np.float64(-0.9713845312595367), 'hyper_actor_loss': np.float64(2.723829420574475e-05), 'behavior_loss': np.float64(0.36391817927360537)}

Episode step 31920, time diff 4.972076177597046, total time dif 7130.013398647308)
step: 31920 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(2.575862), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06389494687318802), 'actor_loss': np.float64(-0.9673215985298157), 'hyper_actor_loss': np.float64(2.7382721418689472e-05), 'behavior_loss': np.float64(0.3727389693260193)}

Episode step 31930, time diff 4.924656867980957, total time dif 7134.985474824905)
step: 31930 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(2.330865), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0676764726638794), 'actor_loss': np.float64(-0.9720533668994904), 'hyper_actor_loss': np.float64(2.7248519472777843e-05), 'behavior_loss': np.float64(0.3707426875829697)}

Episode step 31940, time diff 4.96614146232605, total time dif 7139.910131692886)
step: 31940 @ episode report: {'average_total_reward': np.float32(9.126668), 'reward_variance': np.float32(1.4466224), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06423963028937578), 'actor_loss': np.float64(-0.98426473736763), 'hyper_actor_loss': np.float64(2.5482717683189547e-05), 'behavior_loss': np.float64(0.3741940349340439)}

Episode step 31950, time diff 4.92749547958374, total time dif 7144.876273155212)
step: 31950 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(2.8615117), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06313807480037212), 'actor_loss': np.float64(-0.9629861116409302), 'hyper_actor_loss': np.float64(2.4817635858198627e-05), 'behavior_loss': np.float64(0.36886830925941466)}

Episode step 31960, time diff 4.912998199462891, total time dif 7149.803768634796)
step: 31960 @ episode report: {'average_total_reward': np.float32(9.675556), 'reward_variance': np.float32(1.2788103), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05694406591355801), 'actor_loss': np.float64(-0.9585905611515045), 'hyper_actor_loss': np.float64(2.4944411052274518e-05), 'behavior_loss': np.float64(0.3766312271356583)}

Episode step 31970, time diff 5.06518816947937, total time dif 7154.716766834259)
step: 31970 @ episode report: {'average_total_reward': np.float32(8.302223), 'reward_variance': np.float32(1.508785), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06929044760763645), 'actor_loss': np.float64(-0.9668987452983856), 'hyper_actor_loss': np.float64(2.4523274259991012e-05), 'behavior_loss': np.float64(0.37185016870498655)}

Episode step 31980, time diff 4.910199165344238, total time dif 7159.781955003738)
step: 31980 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(4.3797526), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06238376591354609), 'actor_loss': np.float64(-0.9770489752292633), 'hyper_actor_loss': np.float64(2.517790508136386e-05), 'behavior_loss': np.float64(0.3650992900133133)}

Episode step 31990, time diff 4.919987440109253, total time dif 7164.692154169083)
step: 31990 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(3.9878387), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06228210665285587), 'actor_loss': np.float64(-0.9705256819725037), 'hyper_actor_loss': np.float64(2.4189426949305925e-05), 'behavior_loss': np.float64(0.3710616320371628)}

Episode step 32000, time diff 4.954859256744385, total time dif 7169.612141609192)
step: 32000 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(3.2570317), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08914694152772426), 'actor_loss': np.float64(-0.9845747768878936), 'hyper_actor_loss': np.float64(2.4943030257418287e-05), 'behavior_loss': np.float64(0.373407319188118)}

Episode step 32010, time diff 4.9562599658966064, total time dif 7174.567000865936)
step: 32010 @ episode report: {'average_total_reward': np.float32(9.6877775), 'reward_variance': np.float32(2.4018874), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07304706051945686), 'actor_loss': np.float64(-1.005338728427887), 'hyper_actor_loss': np.float64(2.494804994057631e-05), 'behavior_loss': np.float64(0.3698063552379608)}

Episode step 32020, time diff 5.0973474979400635, total time dif 7179.523260831833)
step: 32020 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(1.7299364), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07375276535749435), 'actor_loss': np.float64(-0.9844064533710479), 'hyper_actor_loss': np.float64(2.5088180154853035e-05), 'behavior_loss': np.float64(0.37630623281002046)}

Episode step 32030, time diff 5.079450368881226, total time dif 7184.620608329773)
step: 32030 @ episode report: {'average_total_reward': np.float32(8.614446), 'reward_variance': np.float32(3.0661504), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06437504664063454), 'actor_loss': np.float64(-0.9699272155761719), 'hyper_actor_loss': np.float64(2.537177988415351e-05), 'behavior_loss': np.float64(0.3717374861240387)}

Episode step 32040, time diff 5.086130619049072, total time dif 7189.700058698654)
step: 32040 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(1.3175814), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06777735017240047), 'actor_loss': np.float64(-0.9569925546646119), 'hyper_actor_loss': np.float64(2.5588468997739255e-05), 'behavior_loss': np.float64(0.3749307543039322)}

Episode step 32050, time diff 5.167290210723877, total time dif 7194.786189317703)
step: 32050 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(2.8744006), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06723688691854476), 'actor_loss': np.float64(-0.9790982306003571), 'hyper_actor_loss': np.float64(2.7188073545403313e-05), 'behavior_loss': np.float64(0.3655552834272385)}

Episode step 32060, time diff 5.092736482620239, total time dif 7199.953479528427)
step: 32060 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(1.8082335), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06697462014853954), 'actor_loss': np.float64(-0.9852315425872803), 'hyper_actor_loss': np.float64(3.1015197237138636e-05), 'behavior_loss': np.float64(0.37695867717266085)}

Episode step 32070, time diff 5.028648376464844, total time dif 7205.046216011047)
step: 32070 @ episode report: {'average_total_reward': np.float32(8.765556), 'reward_variance': np.float32(1.6766531), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.28889), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07563436478376388), 'actor_loss': np.float64(-0.9706798911094665), 'hyper_actor_loss': np.float64(2.9090199859638233e-05), 'behavior_loss': np.float64(0.3671418070793152)}

Episode step 32080, time diff 4.975875377655029, total time dif 7210.074864387512)
step: 32080 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.1497792), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07681829929351806), 'actor_loss': np.float64(-0.987568074464798), 'hyper_actor_loss': np.float64(3.227409197279485e-05), 'behavior_loss': np.float64(0.3689564257860184)}

Episode step 32090, time diff 4.918888568878174, total time dif 7215.050739765167)
step: 32090 @ episode report: {'average_total_reward': np.float32(9.114446), 'reward_variance': np.float32(1.4201001), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07173731923103333), 'actor_loss': np.float64(-0.9964106857776642), 'hyper_actor_loss': np.float64(3.1010653219709636e-05), 'behavior_loss': np.float64(0.36388438642024995)}

Episode step 32100, time diff 4.739396810531616, total time dif 7219.969628334045)
step: 32100 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.5708508), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05702382735908031), 'actor_loss': np.float64(-0.9711863577365876), 'hyper_actor_loss': np.float64(2.6893383437709418e-05), 'behavior_loss': np.float64(0.3710086613893509)}

Episode step 32110, time diff 4.74896502494812, total time dif 7224.709025144577)
step: 32110 @ episode report: {'average_total_reward': np.float32(8.802222), 'reward_variance': np.float32(2.7331061), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07961208149790763), 'actor_loss': np.float64(-0.9692226886749268), 'hyper_actor_loss': np.float64(2.531499540054938e-05), 'behavior_loss': np.float64(0.36578781008720396)}

Episode step 32120, time diff 4.79735803604126, total time dif 7229.457990169525)
step: 32120 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(1.3764694), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06922071352601052), 'actor_loss': np.float64(-0.9852698266506195), 'hyper_actor_loss': np.float64(2.4717164706089534e-05), 'behavior_loss': np.float64(0.36372349262237547)}

Episode step 32130, time diff 4.691319227218628, total time dif 7234.255348205566)
step: 32130 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(3.6216252), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06295483149588108), 'actor_loss': np.float64(-0.9822681665420532), 'hyper_actor_loss': np.float64(2.3471056374546605e-05), 'behavior_loss': np.float64(0.3651554822921753)}

Episode step 32140, time diff 4.853962659835815, total time dif 7238.946667432785)
step: 32140 @ episode report: {'average_total_reward': np.float32(8.614446), 'reward_variance': np.float32(1.6002481), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07967358194291592), 'actor_loss': np.float64(-0.973585331439972), 'hyper_actor_loss': np.float64(2.2630477178609e-05), 'behavior_loss': np.float64(0.3672570735216141)}

Episode step 32150, time diff 4.737781763076782, total time dif 7243.800630092621)
step: 32150 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(1.3691154), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06490473300218583), 'actor_loss': np.float64(-0.9782715260982513), 'hyper_actor_loss': np.float64(2.3790054365235845e-05), 'behavior_loss': np.float64(0.36595221757888796)}

Episode step 32160, time diff 4.657540798187256, total time dif 7248.538411855698)
step: 32160 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.0839056), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07154776565730572), 'actor_loss': np.float64(-0.9798393368721008), 'hyper_actor_loss': np.float64(2.309116789547261e-05), 'behavior_loss': np.float64(0.371686252951622)}

Episode step 32170, time diff 4.62852931022644, total time dif 7253.195952653885)
step: 32170 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(2.9029088), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06445902399718761), 'actor_loss': np.float64(-0.9671670436859131), 'hyper_actor_loss': np.float64(2.3610448442923372e-05), 'behavior_loss': np.float64(0.369126558303833)}

Episode step 32180, time diff 4.589128732681274, total time dif 7257.824481964111)
step: 32180 @ episode report: {'average_total_reward': np.float32(9.436668), 'reward_variance': np.float32(3.1427422), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059956418350338936), 'actor_loss': np.float64(-0.9758298575878144), 'hyper_actor_loss': np.float64(2.4016780116653536e-05), 'behavior_loss': np.float64(0.356750950217247)}

Episode step 32190, time diff 4.634318113327026, total time dif 7262.413610696793)
step: 32190 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(1.3324212), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06332528963685036), 'actor_loss': np.float64(-0.976622712612152), 'hyper_actor_loss': np.float64(2.179008224629797e-05), 'behavior_loss': np.float64(0.3703369140625)}

Episode step 32200, time diff 4.6054394245147705, total time dif 7267.04792881012)
step: 32200 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(1.6908348), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06853376068174839), 'actor_loss': np.float64(-0.9720857560634613), 'hyper_actor_loss': np.float64(2.2463700588559732e-05), 'behavior_loss': np.float64(0.37493695616722106)}

Episode step 32210, time diff 4.59917140007019, total time dif 7271.653368234634)
step: 32210 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(3.7424388), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07674658820033073), 'actor_loss': np.float64(-0.965801191329956), 'hyper_actor_loss': np.float64(2.199201571784215e-05), 'behavior_loss': np.float64(0.3750394284725189)}

Episode step 32220, time diff 4.576103925704956, total time dif 7276.252539634705)
step: 32220 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(2.951798), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06206807978451252), 'actor_loss': np.float64(-0.9777034878730774), 'hyper_actor_loss': np.float64(2.072251918434631e-05), 'behavior_loss': np.float64(0.36209923923015597)}

Episode step 32230, time diff 4.567798137664795, total time dif 7280.82864356041)
step: 32230 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.5602785), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06816524341702461), 'actor_loss': np.float64(-0.9747953832149505), 'hyper_actor_loss': np.float64(2.0384039999044035e-05), 'behavior_loss': np.float64(0.3748409688472748)}

Episode step 32240, time diff 4.527242422103882, total time dif 7285.396441698074)
step: 32240 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(1.0865195), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.075881252810359), 'actor_loss': np.float64(-0.9680009007453918), 'hyper_actor_loss': np.float64(1.8729677503870334e-05), 'behavior_loss': np.float64(0.36274964213371275)}

Episode step 32250, time diff 4.553028106689453, total time dif 7289.923684120178)
step: 32250 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(1.0598959), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07134512923657894), 'actor_loss': np.float64(-0.9918824553489685), 'hyper_actor_loss': np.float64(1.8876711510529277e-05), 'behavior_loss': np.float64(0.3669148921966553)}

Episode step 32260, time diff 4.578348875045776, total time dif 7294.476712226868)
step: 32260 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(0.8865198), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06423143520951272), 'actor_loss': np.float64(-0.9778389155864715), 'hyper_actor_loss': np.float64(1.800501559046097e-05), 'behavior_loss': np.float64(0.3607627719640732)}

Episode step 32270, time diff 4.474738597869873, total time dif 7299.055061101913)
step: 32270 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(1.994667), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06703773736953736), 'actor_loss': np.float64(-0.9647466599941253), 'hyper_actor_loss': np.float64(1.725464990158798e-05), 'behavior_loss': np.float64(0.36738846004009246)}

Episode step 32280, time diff 4.507057189941406, total time dif 7303.529799699783)
step: 32280 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(3.9548507), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.071037333086133), 'actor_loss': np.float64(-0.9758623600006103), 'hyper_actor_loss': np.float64(1.6094183047243858e-05), 'behavior_loss': np.float64(0.37361471354961395)}

Episode step 32290, time diff 4.511313438415527, total time dif 7308.036856889725)
step: 32290 @ episode report: {'average_total_reward': np.float32(8.190001), 'reward_variance': np.float32(3.7668018), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06500760689377785), 'actor_loss': np.float64(-0.9722574472427368), 'hyper_actor_loss': np.float64(1.6586379115324234e-05), 'behavior_loss': np.float64(0.35997467637062075)}

Episode step 32300, time diff 4.551447153091431, total time dif 7312.54817032814)
step: 32300 @ episode report: {'average_total_reward': np.float32(8.465556), 'reward_variance': np.float32(3.1442344), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06084055956453085), 'actor_loss': np.float64(-0.9708309412002564), 'hyper_actor_loss': np.float64(1.6000113191694254e-05), 'behavior_loss': np.float64(0.37472738027572633)}

Episode step 32310, time diff 4.665416717529297, total time dif 7317.099617481232)
step: 32310 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(3.2689145), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06093985829502344), 'actor_loss': np.float64(-0.9611946880817414), 'hyper_actor_loss': np.float64(1.562238612677902e-05), 'behavior_loss': np.float64(0.3626274406909943)}

Episode step 32320, time diff 4.482836723327637, total time dif 7321.765034198761)
step: 32320 @ episode report: {'average_total_reward': np.float32(9.33889), 'reward_variance': np.float32(2.004648), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06249728389084339), 'actor_loss': np.float64(-0.9660208404064179), 'hyper_actor_loss': np.float64(1.612352152733365e-05), 'behavior_loss': np.float64(0.3596650332212448)}

Episode step 32330, time diff 4.563512563705444, total time dif 7326.247870922089)
step: 32330 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(2.2416914), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06611440032720566), 'actor_loss': np.float64(-0.9715810418128967), 'hyper_actor_loss': np.float64(1.691742036200594e-05), 'behavior_loss': np.float64(0.3678899943828583)}

Episode step 32340, time diff 4.571705102920532, total time dif 7330.811383485794)
step: 32340 @ episode report: {'average_total_reward': np.float32(9.548891), 'reward_variance': np.float32(1.4597832), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053858683072030546), 'actor_loss': np.float64(-0.9687132060527801), 'hyper_actor_loss': np.float64(1.601314079380245e-05), 'behavior_loss': np.float64(0.3640462338924408)}

Episode step 32350, time diff 4.550891876220703, total time dif 7335.383088588715)
step: 32350 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(3.3876367), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05991462729871273), 'actor_loss': np.float64(-0.95922572016716), 'hyper_actor_loss': np.float64(1.5922555485303745e-05), 'behavior_loss': np.float64(0.3631940335035324)}

Episode step 32360, time diff 4.580560684204102, total time dif 7339.933980464935)
step: 32360 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(5.42397), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07192531861364841), 'actor_loss': np.float64(-0.970851331949234), 'hyper_actor_loss': np.float64(1.6429219976998864e-05), 'behavior_loss': np.float64(0.3710398465394974)}

Episode step 32370, time diff 4.644667148590088, total time dif 7344.514541149139)
step: 32370 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.4703906), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06967943236231804), 'actor_loss': np.float64(-0.9838504493236542), 'hyper_actor_loss': np.float64(2.6339380565332248e-05), 'behavior_loss': np.float64(0.36436641216278076)}

Episode step 32380, time diff 4.54724907875061, total time dif 7349.1592082977295)
step: 32380 @ episode report: {'average_total_reward': np.float32(10.0), 'reward_variance': np.float32(2.3324454), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0742567665874958), 'actor_loss': np.float64(-0.9793008923530578), 'hyper_actor_loss': np.float64(1.934292995429132e-05), 'behavior_loss': np.float64(0.3683719128370285)}

Episode step 32390, time diff 4.517264366149902, total time dif 7353.70645737648)
step: 32390 @ episode report: {'average_total_reward': np.float32(10.322224), 'reward_variance': np.float32(4.1088395), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06596632860600948), 'actor_loss': np.float64(-0.9725108623504639), 'hyper_actor_loss': np.float64(1.526535543234786e-05), 'behavior_loss': np.float64(0.3648476183414459)}

Episode step 32400, time diff 4.568439722061157, total time dif 7358.22372174263)
step: 32400 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(2.9588954), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0828576423227787), 'actor_loss': np.float64(-0.9901356160640716), 'hyper_actor_loss': np.float64(1.4977811952121556e-05), 'behavior_loss': np.float64(0.3627794086933136)}

Episode step 32410, time diff 4.517607927322388, total time dif 7362.792161464691)
step: 32410 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(2.039665), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06573717892169953), 'actor_loss': np.float64(-0.9983267605304718), 'hyper_actor_loss': np.float64(1.3607749679067638e-05), 'behavior_loss': np.float64(0.35989857017993926)}

Episode step 32420, time diff 4.511548280715942, total time dif 7367.309769392014)
step: 32420 @ episode report: {'average_total_reward': np.float32(9.912223), 'reward_variance': np.float32(2.5519865), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07505585663020611), 'actor_loss': np.float64(-0.9758983731269837), 'hyper_actor_loss': np.float64(1.3458207558869616e-05), 'behavior_loss': np.float64(0.3593696475028992)}

Episode step 32430, time diff 4.450815439224243, total time dif 7371.8213176727295)
step: 32430 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(1.5791806), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05868957284837961), 'actor_loss': np.float64(-0.9693107008934021), 'hyper_actor_loss': np.float64(1.3334733012015931e-05), 'behavior_loss': np.float64(0.3576452672481537)}

Episode step 32440, time diff 4.47548508644104, total time dif 7376.272133111954)
step: 32440 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(2.7672648), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07158726826310158), 'actor_loss': np.float64(-0.9723711490631104), 'hyper_actor_loss': np.float64(1.355062431684928e-05), 'behavior_loss': np.float64(0.36605662703514097)}

Episode step 32450, time diff 4.55959677696228, total time dif 7380.747618198395)
step: 32450 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(1.1387706), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06346782706677914), 'actor_loss': np.float64(-0.9765396118164062), 'hyper_actor_loss': np.float64(1.2830975447286618e-05), 'behavior_loss': np.float64(0.3635393977165222)}

Episode step 32460, time diff 4.585163354873657, total time dif 7385.307214975357)
step: 32460 @ episode report: {'average_total_reward': np.float32(9.6488905), 'reward_variance': np.float32(4.0270424), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07358124144375325), 'actor_loss': np.float64(-0.9821930527687073), 'hyper_actor_loss': np.float64(1.2642859655898064e-05), 'behavior_loss': np.float64(0.35872283577919006)}

Episode step 32470, time diff 4.612494707107544, total time dif 7389.892378330231)
step: 32470 @ episode report: {'average_total_reward': np.float32(9.548889), 'reward_variance': np.float32(3.135635), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0693349827080965), 'actor_loss': np.float64(-0.9904653251171112), 'hyper_actor_loss': np.float64(1.2555495959531982e-05), 'behavior_loss': np.float64(0.35218057334423064)}

Episode step 32480, time diff 4.489660978317261, total time dif 7394.504873037338)
step: 32480 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(1.3041103), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06566183716058731), 'actor_loss': np.float64(-0.9800712108612061), 'hyper_actor_loss': np.float64(1.219290879816981e-05), 'behavior_loss': np.float64(0.3557101637125015)}

Episode step 32490, time diff 4.479940176010132, total time dif 7398.9945340156555)
step: 32490 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(1.4676061), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0683230709284544), 'actor_loss': np.float64(-0.982669073343277), 'hyper_actor_loss': np.float64(1.2087870345567353e-05), 'behavior_loss': np.float64(0.3561155676841736)}

Episode step 32500, time diff 4.49483585357666, total time dif 7403.474474191666)
step: 32500 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(3.3582027), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05588749423623085), 'actor_loss': np.float64(-0.9761554419994354), 'hyper_actor_loss': np.float64(1.201032000608393e-05), 'behavior_loss': np.float64(0.35830944776535034)}

Episode step 32510, time diff 4.377438068389893, total time dif 7407.969310045242)
step: 32510 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(3.114801), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07990126721560956), 'actor_loss': np.float64(-0.9654143989086151), 'hyper_actor_loss': np.float64(1.1725649437721586e-05), 'behavior_loss': np.float64(0.37101206481456755)}

Episode step 32520, time diff 4.453183174133301, total time dif 7412.346748113632)
step: 32520 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(1.6078956), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06544686518609524), 'actor_loss': np.float64(-0.9740452706813812), 'hyper_actor_loss': np.float64(1.1139880916744005e-05), 'behavior_loss': np.float64(0.3643945574760437)}

Episode step 32530, time diff 4.477488279342651, total time dif 7416.7999312877655)
step: 32530 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.5123997), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06987675242125987), 'actor_loss': np.float64(-0.979002159833908), 'hyper_actor_loss': np.float64(1.073948506018496e-05), 'behavior_loss': np.float64(0.3724541187286377)}

Episode step 32540, time diff 4.454528570175171, total time dif 7421.277419567108)
step: 32540 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.4495256), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05935999006032944), 'actor_loss': np.float64(-0.9511033654212951), 'hyper_actor_loss': np.float64(1.110098919525626e-05), 'behavior_loss': np.float64(0.3597797691822052)}

Episode step 32550, time diff 4.398396253585815, total time dif 7425.731948137283)
step: 32550 @ episode report: {'average_total_reward': np.float32(9.014444), 'reward_variance': np.float32(0.63076687), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06871036067605019), 'actor_loss': np.float64(-0.9828591108322143), 'hyper_actor_loss': np.float64(1.0875115276576252e-05), 'behavior_loss': np.float64(0.35551999509334564)}

Episode step 32560, time diff 4.403623819351196, total time dif 7430.130344390869)
step: 32560 @ episode report: {'average_total_reward': np.float32(9.387779), 'reward_variance': np.float32(3.6342087), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06411584578454495), 'actor_loss': np.float64(-0.9850429356098175), 'hyper_actor_loss': np.float64(9.971728468372021e-06), 'behavior_loss': np.float64(0.37005381286144257)}

Episode step 32570, time diff 4.457534074783325, total time dif 7434.53396821022)
step: 32570 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(2.737309), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07365476563572884), 'actor_loss': np.float64(-0.9536604344844818), 'hyper_actor_loss': np.float64(9.496073653281201e-06), 'behavior_loss': np.float64(0.3673593789339066)}

Episode step 32580, time diff 4.4270920753479, total time dif 7438.991502285004)
step: 32580 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(2.273575), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06619832925498485), 'actor_loss': np.float64(-0.9697676777839661), 'hyper_actor_loss': np.float64(9.544429394736653e-06), 'behavior_loss': np.float64(0.36858636140823364)}

Episode step 32590, time diff 4.417369604110718, total time dif 7443.418594360352)
step: 32590 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.6855617), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07337157651782036), 'actor_loss': np.float64(-0.9857093155384063), 'hyper_actor_loss': np.float64(9.576183856552234e-06), 'behavior_loss': np.float64(0.3727760761976242)}

Episode step 32600, time diff 4.492146730422974, total time dif 7447.835963964462)
step: 32600 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(1.736216), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061973197385668755), 'actor_loss': np.float64(-0.9760391891002655), 'hyper_actor_loss': np.float64(8.887946933100466e-06), 'behavior_loss': np.float64(0.3691909432411194)}

Episode step 32610, time diff 4.500408887863159, total time dif 7452.328110694885)
step: 32610 @ episode report: {'average_total_reward': np.float32(10.373335), 'reward_variance': np.float32(4.1216846), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07166557423770428), 'actor_loss': np.float64(-0.9639751613140106), 'hyper_actor_loss': np.float64(9.087810940400232e-06), 'behavior_loss': np.float64(0.3765201926231384)}

Episode step 32620, time diff 4.434687852859497, total time dif 7456.828519582748)
step: 32620 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(3.3234138), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07777364030480385), 'actor_loss': np.float64(-0.9733201742172242), 'hyper_actor_loss': np.float64(9.124827056439244e-06), 'behavior_loss': np.float64(0.37404038310050963)}

Episode step 32630, time diff 4.47035813331604, total time dif 7461.263207435608)
step: 32630 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(3.854549), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06324151642620564), 'actor_loss': np.float64(-0.9817714929580689), 'hyper_actor_loss': np.float64(1.422956156602595e-05), 'behavior_loss': np.float64(0.3654048353433609)}

Episode step 32640, time diff 4.509420871734619, total time dif 7465.733565568924)
step: 32640 @ episode report: {'average_total_reward': np.float32(9.436667), 'reward_variance': np.float32(1.8559275), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06526962779462338), 'actor_loss': np.float64(-0.9711542725563049), 'hyper_actor_loss': np.float64(9.187893465423257e-06), 'behavior_loss': np.float64(0.3651380926370621)}

Episode step 32650, time diff 4.417144060134888, total time dif 7470.242986440659)
step: 32650 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.2046484), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07174341045320035), 'actor_loss': np.float64(-0.9689614474773407), 'hyper_actor_loss': np.float64(8.429208173765801e-06), 'behavior_loss': np.float64(0.36853860318660736)}

Episode step 32660, time diff 4.497889518737793, total time dif 7474.6601305007935)
step: 32660 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(1.8134029), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07718704044818878), 'actor_loss': np.float64(-0.9871957659721374), 'hyper_actor_loss': np.float64(1.090816131181782e-05), 'behavior_loss': np.float64(0.37293029129505156)}

Episode step 32670, time diff 4.441646575927734, total time dif 7479.158020019531)
step: 32670 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(1.3864495), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07478177212178708), 'actor_loss': np.float64(-0.98785799741745), 'hyper_actor_loss': np.float64(1.1158749566675397e-05), 'behavior_loss': np.float64(0.36932912170886995)}

Episode step 32680, time diff 4.429312467575073, total time dif 7483.599666595459)
step: 32680 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(2.4853625), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.077555251121521), 'actor_loss': np.float64(-0.9889211058616638), 'hyper_actor_loss': np.float64(9.91221095318906e-06), 'behavior_loss': np.float64(0.3697377532720566)}

Episode step 32690, time diff 4.455200433731079, total time dif 7488.028979063034)
step: 32690 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(1.8339622), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0703873660415411), 'actor_loss': np.float64(-0.9791078507900238), 'hyper_actor_loss': np.float64(9.115788316194084e-06), 'behavior_loss': np.float64(0.3612006515264511)}

Episode step 32700, time diff 4.377058267593384, total time dif 7492.484179496765)
step: 32700 @ episode report: {'average_total_reward': np.float32(9.4388895), 'reward_variance': np.float32(1.3150681), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186601124703884), 'actor_loss': np.float64(-0.975065141916275), 'hyper_actor_loss': np.float64(8.42624294818961e-06), 'behavior_loss': np.float64(0.36610974073410035)}

Episode step 32710, time diff 4.385331869125366, total time dif 7496.8612377643585)
step: 32710 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(3.5148156), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0676329780369997), 'actor_loss': np.float64(-0.9726599276065826), 'hyper_actor_loss': np.float64(8.421896245636162e-06), 'behavior_loss': np.float64(0.37096282839775085)}

Episode step 32720, time diff 4.410111427307129, total time dif 7501.246569633484)
step: 32720 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(2.6740997), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07411840930581093), 'actor_loss': np.float64(-0.9786873281002044), 'hyper_actor_loss': np.float64(1.0283126812282717e-05), 'behavior_loss': np.float64(0.3706905335187912)}

Episode step 32730, time diff 4.41045880317688, total time dif 7505.656681060791)
step: 32730 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.9907179), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0680657982826233), 'actor_loss': np.float64(-0.988166344165802), 'hyper_actor_loss': np.float64(8.94760460141697e-06), 'behavior_loss': np.float64(0.3614296793937683)}

Episode step 32740, time diff 4.41292667388916, total time dif 7510.067139863968)
step: 32740 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(3.076883), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06937022395431995), 'actor_loss': np.float64(-0.9832962214946747), 'hyper_actor_loss': np.float64(8.015033563424367e-06), 'behavior_loss': np.float64(0.3618489742279053)}

Episode step 32750, time diff 4.458187580108643, total time dif 7514.480066537857)
step: 32750 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(2.7866178), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05716625768691301), 'actor_loss': np.float64(-0.9681267857551574), 'hyper_actor_loss': np.float64(9.449521348869893e-06), 'behavior_loss': np.float64(0.35637701153755186)}

Episode step 32760, time diff 4.463888883590698, total time dif 7518.938254117966)
step: 32760 @ episode report: {'average_total_reward': np.float32(8.302223), 'reward_variance': np.float32(1.0598962), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06490733213722706), 'actor_loss': np.float64(-0.9640961587429047), 'hyper_actor_loss': np.float64(1.0951962667604676e-05), 'behavior_loss': np.float64(0.3682646006345749)}

Episode step 32770, time diff 4.402346134185791, total time dif 7523.402143001556)
step: 32770 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.2713783), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06711735166609287), 'actor_loss': np.float64(-0.9761212348937989), 'hyper_actor_loss': np.float64(8.962287984104477e-06), 'behavior_loss': np.float64(0.3690858483314514)}

Episode step 32780, time diff 4.414029121398926, total time dif 7527.804489135742)
step: 32780 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(1.1150681), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053955715522170065), 'actor_loss': np.float64(-0.9685500502586365), 'hyper_actor_loss': np.float64(8.014538479983457e-06), 'behavior_loss': np.float64(0.3726435512304306)}

Episode step 32790, time diff 4.368698835372925, total time dif 7532.218518257141)
step: 32790 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(1.1521289), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06746703609824181), 'actor_loss': np.float64(-0.9477759063243866), 'hyper_actor_loss': np.float64(1.4272419593908126e-05), 'behavior_loss': np.float64(0.36973555386066437)}

Episode step 32800, time diff 4.382540941238403, total time dif 7536.587217092514)
step: 32800 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(1.3566616), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07107067666947842), 'actor_loss': np.float64(-0.985893851518631), 'hyper_actor_loss': np.float64(9.640726284487755e-06), 'behavior_loss': np.float64(0.37172519564628603)}

Episode step 32810, time diff 4.569432497024536, total time dif 7540.969758033752)
step: 32810 @ episode report: {'average_total_reward': np.float32(8.226667), 'reward_variance': np.float32(3.6973393), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06624824218451977), 'actor_loss': np.float64(-0.9767866849899292), 'hyper_actor_loss': np.float64(1.0449573028381565e-05), 'behavior_loss': np.float64(0.37131309509277344)}

Episode step 32820, time diff 4.404743671417236, total time dif 7545.539190530777)
step: 32820 @ episode report: {'average_total_reward': np.float32(9.338889), 'reward_variance': np.float32(2.2360556), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06979444548487664), 'actor_loss': np.float64(-0.9728955030441284), 'hyper_actor_loss': np.float64(7.743515607216977e-06), 'behavior_loss': np.float64(0.3762259781360626)}

Episode step 32830, time diff 4.436977386474609, total time dif 7549.943934202194)
step: 32830 @ episode report: {'average_total_reward': np.float32(9.263333), 'reward_variance': np.float32(3.163804), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08344992958009242), 'actor_loss': np.float64(-0.9992528080940246), 'hyper_actor_loss': np.float64(1.1267467471043346e-05), 'behavior_loss': np.float64(0.36519291400909426)}

Episode step 32840, time diff 4.541137218475342, total time dif 7554.380911588669)
step: 32840 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(3.6345246), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07951072193682193), 'actor_loss': np.float64(-1.0180979251861573), 'hyper_actor_loss': np.float64(2.0310288346081506e-05), 'behavior_loss': np.float64(0.3685826539993286)}

Episode step 32850, time diff 4.399500846862793, total time dif 7558.922048807144)
step: 32850 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(1.7521747), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060316961258649826), 'actor_loss': np.float64(-0.9735596537590027), 'hyper_actor_loss': np.float64(9.19381454878021e-06), 'behavior_loss': np.float64(0.3666141003370285)}

Episode step 32860, time diff 4.4529407024383545, total time dif 7563.321549654007)
step: 32860 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(1.5901788), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06794901527464389), 'actor_loss': np.float64(-0.9658943116664886), 'hyper_actor_loss': np.float64(7.0383565798692874e-06), 'behavior_loss': np.float64(0.3679422289133072)}

Episode step 32870, time diff 4.36348557472229, total time dif 7567.774490356445)
step: 32870 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(3.313518), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06368524692952633), 'actor_loss': np.float64(-0.9733683586120605), 'hyper_actor_loss': np.float64(8.496603322782903e-06), 'behavior_loss': np.float64(0.36986750960350034)}

Episode step 32880, time diff 4.409741163253784, total time dif 7572.137975931168)
step: 32880 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.5762417), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07787117511034011), 'actor_loss': np.float64(-0.9545840501785279), 'hyper_actor_loss': np.float64(1.035793275150354e-05), 'behavior_loss': np.float64(0.3795930862426758)}

Episode step 32890, time diff 4.421783924102783, total time dif 7576.547717094421)
step: 32890 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(1.8940258), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05899444483220577), 'actor_loss': np.float64(-0.9696326494216919), 'hyper_actor_loss': np.float64(7.345722451645997e-06), 'behavior_loss': np.float64(0.36914273500442507)}

Episode step 32900, time diff 4.375372409820557, total time dif 7580.969501018524)
step: 32900 @ episode report: {'average_total_reward': np.float32(9.5633335), 'reward_variance': np.float32(1.9053841), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06254624612629414), 'actor_loss': np.float64(-0.9675573289394379), 'hyper_actor_loss': np.float64(6.740862818332971e-06), 'behavior_loss': np.float64(0.36779527366161346)}

Episode step 32910, time diff 4.351590871810913, total time dif 7585.344873428345)
step: 32910 @ episode report: {'average_total_reward': np.float32(8.565557), 'reward_variance': np.float32(3.0457895), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06485550329089165), 'actor_loss': np.float64(-0.960874205827713), 'hyper_actor_loss': np.float64(6.360500992741436e-06), 'behavior_loss': np.float64(0.3709327608346939)}

Episode step 32920, time diff 4.4015796184539795, total time dif 7589.696464300156)
step: 32920 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(2.3115067), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062290341779589656), 'actor_loss': np.float64(-0.9679418087005616), 'hyper_actor_loss': np.float64(6.610950549656991e-06), 'behavior_loss': np.float64(0.36707297563552854)}

Episode step 32930, time diff 4.398163080215454, total time dif 7594.09804391861)
step: 32930 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(1.1577233), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08076678477227688), 'actor_loss': np.float64(-0.9807111740112304), 'hyper_actor_loss': np.float64(7.283709510375047e-06), 'behavior_loss': np.float64(0.3708419591188431)}

Episode step 32940, time diff 4.35870361328125, total time dif 7598.496206998825)
step: 32940 @ episode report: {'average_total_reward': np.float32(9.200002), 'reward_variance': np.float32(3.8843951), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0741291493177414), 'actor_loss': np.float64(-0.9936495244503021), 'hyper_actor_loss': np.float64(6.561224017787026e-06), 'behavior_loss': np.float64(0.37198270857334137)}

Episode step 32950, time diff 4.47171950340271, total time dif 7602.854910612106)
step: 32950 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(2.8746045), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08310043588280677), 'actor_loss': np.float64(-0.9835309505462646), 'hyper_actor_loss': np.float64(6.114359803177649e-06), 'behavior_loss': np.float64(0.37768165171146395)}

Episode step 32960, time diff 4.4284827709198, total time dif 7607.326630115509)
step: 32960 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(3.1509154), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07283334247767925), 'actor_loss': np.float64(-0.9721572399139404), 'hyper_actor_loss': np.float64(6.236389845071244e-06), 'behavior_loss': np.float64(0.37393573522567747)}

Episode step 32970, time diff 4.586271524429321, total time dif 7611.755112886429)
step: 32970 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(5.5816994), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07778273448348046), 'actor_loss': np.float64(-0.9772104322910309), 'hyper_actor_loss': np.float64(6.7330379806662675e-06), 'behavior_loss': np.float64(0.3645882159471512)}

Episode step 32980, time diff 4.429502964019775, total time dif 7616.341384410858)
step: 32980 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(3.5876317), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07377645671367646), 'actor_loss': np.float64(-0.9982687950134277), 'hyper_actor_loss': np.float64(6.415994312192197e-06), 'behavior_loss': np.float64(0.37959170043468476)}

Episode step 32990, time diff 4.382766246795654, total time dif 7620.770887374878)
step: 32990 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(5.0360556), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0741315882652998), 'actor_loss': np.float64(-0.9839209616184235), 'hyper_actor_loss': np.float64(6.2040252032602435e-06), 'behavior_loss': np.float64(0.3706616789102554)}

Episode step 33000, time diff 4.411856651306152, total time dif 7625.153653621674)
step: 33000 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(2.5262532), 'max_total_reward': np.float32(13.1444435), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06220474224537611), 'actor_loss': np.float64(-0.9698642134666443), 'hyper_actor_loss': np.float64(5.796221148557379e-06), 'behavior_loss': np.float64(0.3678156226873398)}

Episode step 33010, time diff 4.369773864746094, total time dif 7629.56551027298)
step: 33010 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(3.5504212), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0663852084428072), 'actor_loss': np.float64(-0.9651293277740478), 'hyper_actor_loss': np.float64(6.375076873155195e-06), 'behavior_loss': np.float64(0.3669711917638779)}

Episode step 33020, time diff 4.434905767440796, total time dif 7633.935284137726)
step: 33020 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(1.4758537), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07402170076966286), 'actor_loss': np.float64(-0.9801665902137756), 'hyper_actor_loss': np.float64(5.800906865260913e-06), 'behavior_loss': np.float64(0.37594365477561953)}

Episode step 33030, time diff 4.453187704086304, total time dif 7638.370189905167)
step: 33030 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(2.214613), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056861044466495515), 'actor_loss': np.float64(-0.9677790880203248), 'hyper_actor_loss': np.float64(5.747737441197387e-06), 'behavior_loss': np.float64(0.37546921968460084)}

Episode step 33040, time diff 4.426089525222778, total time dif 7642.823377609253)
step: 33040 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.331823), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07503396533429622), 'actor_loss': np.float64(-0.9726469576358795), 'hyper_actor_loss': np.float64(5.61237598049047e-06), 'behavior_loss': np.float64(0.3657159060239792)}

Episode step 33050, time diff 4.417717695236206, total time dif 7647.249467134476)
step: 33050 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(3.5928109), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06759490929543972), 'actor_loss': np.float64(-0.9910150527954101), 'hyper_actor_loss': np.float64(4.7440220441785644e-06), 'behavior_loss': np.float64(0.3807569295167923)}

Episode step 33060, time diff 4.395004034042358, total time dif 7651.667184829712)
step: 33060 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(1.9597893), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05897770915180445), 'actor_loss': np.float64(-0.9639058709144592), 'hyper_actor_loss': np.float64(4.461737398742116e-06), 'behavior_loss': np.float64(0.38358200192451475)}

Episode step 33070, time diff 4.434566020965576, total time dif 7656.062188863754)
step: 33070 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(3.275882), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07463394552469253), 'actor_loss': np.float64(-0.9633866250514984), 'hyper_actor_loss': np.float64(4.365274958217924e-06), 'behavior_loss': np.float64(0.3679336369037628)}

Episode step 33080, time diff 4.403708219528198, total time dif 7660.49675488472)
step: 33080 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(2.573271), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07275072075426578), 'actor_loss': np.float64(-0.9946910440921783), 'hyper_actor_loss': np.float64(3.892731228916091e-06), 'behavior_loss': np.float64(0.37721491158008574)}

Episode step 33090, time diff 4.396594524383545, total time dif 7664.900463104248)
step: 33090 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(3.8027225), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06519779972732068), 'actor_loss': np.float64(-0.9812036216259002), 'hyper_actor_loss': np.float64(3.7769945492982513e-06), 'behavior_loss': np.float64(0.3776493400335312)}

Episode step 33100, time diff 4.409930467605591, total time dif 7669.297057628632)
step: 33100 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(2.2266915), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07739240638911724), 'actor_loss': np.float64(-0.9688472092151642), 'hyper_actor_loss': np.float64(3.973057755501941e-06), 'behavior_loss': np.float64(0.38674933910369874)}

Episode step 33110, time diff 4.423490524291992, total time dif 7673.706988096237)
step: 33110 @ episode report: {'average_total_reward': np.float32(9.112223), 'reward_variance': np.float32(1.6372954), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06781423464417458), 'actor_loss': np.float64(-0.9741357624530792), 'hyper_actor_loss': np.float64(4.436115796124796e-06), 'behavior_loss': np.float64(0.3756969690322876)}

Episode step 33120, time diff 4.479785442352295, total time dif 7678.130478620529)
step: 33120 @ episode report: {'average_total_reward': np.float32(9.712224), 'reward_variance': np.float32(4.200235), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07684706561267377), 'actor_loss': np.float64(-0.9864914536476135), 'hyper_actor_loss': np.float64(4.72099682156113e-06), 'behavior_loss': np.float64(0.3795949250459671)}

Episode step 33130, time diff 4.373780250549316, total time dif 7682.6102640628815)
step: 33130 @ episode report: {'average_total_reward': np.float32(9.424445), 'reward_variance': np.float32(5.711378), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0606853436678648), 'actor_loss': np.float64(-0.9720360338687897), 'hyper_actor_loss': np.float64(4.442931526682514e-06), 'behavior_loss': np.float64(0.3780345916748047)}

Episode step 33140, time diff 4.595804929733276, total time dif 7686.984044313431)
step: 33140 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(3.1030233), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06740690283477306), 'actor_loss': np.float64(-0.9584514617919921), 'hyper_actor_loss': np.float64(4.797275641976739e-06), 'behavior_loss': np.float64(0.37664265632629396)}

Episode step 33150, time diff 4.424729824066162, total time dif 7691.579849243164)
step: 33150 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(2.1694589), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07106801606714726), 'actor_loss': np.float64(-0.9832644402980805), 'hyper_actor_loss': np.float64(4.351633765509177e-06), 'behavior_loss': np.float64(0.38547848761081693)}

Episode step 33160, time diff 4.4106669425964355, total time dif 7696.00457906723)
step: 33160 @ episode report: {'average_total_reward': np.float32(9.400002), 'reward_variance': np.float32(1.6495314), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06683519557118416), 'actor_loss': np.float64(-0.9763145208358764), 'hyper_actor_loss': np.float64(4.383133477858791e-06), 'behavior_loss': np.float64(0.38192591071128845)}

Episode step 33170, time diff 4.447891473770142, total time dif 7700.415246009827)
step: 33170 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(2.1920767), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06310816090553999), 'actor_loss': np.float64(-0.9751936793327332), 'hyper_actor_loss': np.float64(3.8037641843402527e-06), 'behavior_loss': np.float64(0.3796120256185532)}

Episode step 33180, time diff 4.367605686187744, total time dif 7704.863137483597)
step: 33180 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(1.935854), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06618151254951954), 'actor_loss': np.float64(-0.9697408318519593), 'hyper_actor_loss': np.float64(4.245721379447787e-06), 'behavior_loss': np.float64(0.3735977470874786)}

Episode step 33190, time diff 4.415778875350952, total time dif 7709.230743169785)
step: 33190 @ episode report: {'average_total_reward': np.float32(9.33889), 'reward_variance': np.float32(4.1812654), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06008012518286705), 'actor_loss': np.float64(-0.9792535543441773), 'hyper_actor_loss': np.float64(4.136559368816961e-06), 'behavior_loss': np.float64(0.38656330704689024)}

Episode step 33200, time diff 4.360710144042969, total time dif 7713.6465220451355)
step: 33200 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(2.5172896), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06879647150635719), 'actor_loss': np.float64(-0.9701763331890106), 'hyper_actor_loss': np.float64(4.018116351289791e-06), 'behavior_loss': np.float64(0.37440750896930697)}

Episode step 33210, time diff 4.40946626663208, total time dif 7718.0072321891785)
step: 33210 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(4.039716), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07310853004455567), 'actor_loss': np.float64(-0.9848116517066956), 'hyper_actor_loss': np.float64(4.301710509935219e-06), 'behavior_loss': np.float64(0.38443793952465055)}

Episode step 33220, time diff 4.397167682647705, total time dif 7722.416698455811)
step: 33220 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(2.2635431), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06940875574946404), 'actor_loss': np.float64(-0.9900953233242035), 'hyper_actor_loss': np.float64(4.435023424775864e-06), 'behavior_loss': np.float64(0.3767962157726288)}

Episode step 33230, time diff 4.446319818496704, total time dif 7726.813866138458)
step: 33230 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(2.6215174), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06539336591959), 'actor_loss': np.float64(-0.9800570845603943), 'hyper_actor_loss': np.float64(4.318619266996393e-06), 'behavior_loss': np.float64(0.37818703055381775)}

Episode step 33240, time diff 4.412282943725586, total time dif 7731.260185956955)
step: 33240 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.9816097), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06614781953394414), 'actor_loss': np.float64(-0.9737048387527466), 'hyper_actor_loss': np.float64(4.527220835370827e-06), 'behavior_loss': np.float64(0.3770958870649338)}

Episode step 33250, time diff 4.41472053527832, total time dif 7735.6724689006805)
step: 33250 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(0.55913097), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07100985944271088), 'actor_loss': np.float64(-0.9707880020141602), 'hyper_actor_loss': np.float64(4.620933577825781e-06), 'behavior_loss': np.float64(0.37607074677944186)}

Episode step 33260, time diff 4.472966909408569, total time dif 7740.087189435959)
step: 33260 @ episode report: {'average_total_reward': np.float32(8.551111), 'reward_variance': np.float32(0.9857826), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06315866559743881), 'actor_loss': np.float64(-0.9827960193157196), 'hyper_actor_loss': np.float64(4.554162842396181e-06), 'behavior_loss': np.float64(0.3693362772464752)}

Episode step 33270, time diff 4.4091691970825195, total time dif 7744.560156345367)
step: 33270 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.7057972), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07781483605504036), 'actor_loss': np.float64(-0.9967399835586548), 'hyper_actor_loss': np.float64(4.602161288858042e-06), 'behavior_loss': np.float64(0.37927488684654237)}

Episode step 33280, time diff 4.445964574813843, total time dif 7748.96932554245)
step: 33280 @ episode report: {'average_total_reward': np.float32(8.751111), 'reward_variance': np.float32(1.1706221), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08166131041944028), 'actor_loss': np.float64(-0.9980129897594452), 'hyper_actor_loss': np.float64(5.193136667003273e-06), 'behavior_loss': np.float64(0.38443718254566195)}

Episode step 33290, time diff 4.435739994049072, total time dif 7753.415290117264)
step: 33290 @ episode report: {'average_total_reward': np.float32(10.4366665), 'reward_variance': np.float32(1.8982975), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06905489191412925), 'actor_loss': np.float64(-0.9780174136161804), 'hyper_actor_loss': np.float64(4.680157053371658e-06), 'behavior_loss': np.float64(0.3705872595310211)}

Episode step 33300, time diff 4.455393314361572, total time dif 7757.851030111313)
step: 33300 @ episode report: {'average_total_reward': np.float32(9.748891), 'reward_variance': np.float32(1.8041284), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06229616124182939), 'actor_loss': np.float64(-0.9721316158771515), 'hyper_actor_loss': np.float64(5.515488692253712e-06), 'behavior_loss': np.float64(0.3786196380853653)}

Episode step 33310, time diff 4.598848819732666, total time dif 7762.306423425674)
step: 33310 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.590697), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07336153760552407), 'actor_loss': np.float64(-0.9710600554943085), 'hyper_actor_loss': np.float64(5.8408744735061194e-06), 'behavior_loss': np.float64(0.378926157951355)}

Episode step 33320, time diff 4.464773654937744, total time dif 7766.905272245407)
step: 33320 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(6.1538525), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08039207868278027), 'actor_loss': np.float64(-0.9956900715827942), 'hyper_actor_loss': np.float64(5.928746031713672e-06), 'behavior_loss': np.float64(0.37465048730373385)}

Episode step 33330, time diff 4.494965553283691, total time dif 7771.370045900345)
step: 33330 @ episode report: {'average_total_reward': np.float32(9.4388895), 'reward_variance': np.float32(1.6756852), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06545510701835155), 'actor_loss': np.float64(-0.9845745205879212), 'hyper_actor_loss': np.float64(5.256741906123352e-06), 'behavior_loss': np.float64(0.3737136036157608)}

Episode step 33340, time diff 4.4626710414886475, total time dif 7775.8650114536285)
step: 33340 @ episode report: {'average_total_reward': np.float32(9.351112), 'reward_variance': np.float32(2.6027706), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07054176405072213), 'actor_loss': np.float64(-0.9723714530467987), 'hyper_actor_loss': np.float64(5.098662313685054e-06), 'behavior_loss': np.float64(0.3718647390604019)}

Episode step 33350, time diff 4.48019003868103, total time dif 7780.327682495117)
step: 33350 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.1247207), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05831364877521992), 'actor_loss': np.float64(-0.9662657380104065), 'hyper_actor_loss': np.float64(4.827103884963435e-06), 'behavior_loss': np.float64(0.37211892902851107)}

Episode step 33360, time diff 4.487334728240967, total time dif 7784.807872533798)
step: 33360 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(0.60721135), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0690792765468359), 'actor_loss': np.float64(-0.9702993333339691), 'hyper_actor_loss': np.float64(4.510737585405877e-06), 'behavior_loss': np.float64(0.3622175186872482)}

Episode step 33370, time diff 4.435277462005615, total time dif 7789.295207262039)
step: 33370 @ episode report: {'average_total_reward': np.float32(9.54889), 'reward_variance': np.float32(1.5420798), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08087949752807617), 'actor_loss': np.float64(-0.9907591819763184), 'hyper_actor_loss': np.float64(4.537199970400252e-06), 'behavior_loss': np.float64(0.37286877930164336)}

Episode step 33380, time diff 4.4383509159088135, total time dif 7793.730484724045)
step: 33380 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.1377974), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0723019890487194), 'actor_loss': np.float64(-0.9952007114887238), 'hyper_actor_loss': np.float64(4.520019365372718e-06), 'behavior_loss': np.float64(0.36562753319740293)}

Episode step 33390, time diff 4.434695482254028, total time dif 7798.168835639954)
step: 33390 @ episode report: {'average_total_reward': np.float32(9.102223), 'reward_variance': np.float32(3.3409336), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05998115111142397), 'actor_loss': np.float64(-0.9845782518386841), 'hyper_actor_loss': np.float64(4.534644153864064e-06), 'behavior_loss': np.float64(0.36250672340393064)}

Episode step 33400, time diff 4.486073732376099, total time dif 7802.603531122208)
step: 33400 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(2.0206127), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06263873744755984), 'actor_loss': np.float64(-0.9587187230587005), 'hyper_actor_loss': np.float64(4.618896946340101e-06), 'behavior_loss': np.float64(0.3752301573753357)}

Episode step 33410, time diff 4.409737825393677, total time dif 7807.089604854584)
step: 33410 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(3.5089498), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06092334985733032), 'actor_loss': np.float64(-0.9682184994220734), 'hyper_actor_loss': np.float64(4.399646900310472e-06), 'behavior_loss': np.float64(0.37164833545684817)}

Episode step 33420, time diff 4.428227424621582, total time dif 7811.499342679977)
step: 33420 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.3782039), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.288889), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06466438286006451), 'actor_loss': np.float64(-0.9705207407474518), 'hyper_actor_loss': np.float64(4.28856267262745e-06), 'behavior_loss': np.float64(0.37315106987953184)}

Episode step 33430, time diff 4.428294897079468, total time dif 7815.927570104599)
step: 33430 @ episode report: {'average_total_reward': np.float32(9.336668), 'reward_variance': np.float32(1.3220755), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06621306128799916), 'actor_loss': np.float64(-0.9800085008144379), 'hyper_actor_loss': np.float64(4.159591276220453e-06), 'behavior_loss': np.float64(0.37070130109786986)}

Episode step 33440, time diff 4.415661811828613, total time dif 7820.3558650016785)
step: 33440 @ episode report: {'average_total_reward': np.float32(9.48778), 'reward_variance': np.float32(1.7163818), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0671944733709097), 'actor_loss': np.float64(-0.9788167834281921), 'hyper_actor_loss': np.float64(4.177473579147773e-06), 'behavior_loss': np.float64(0.37466790080070494)}

Episode step 33450, time diff 4.467271566390991, total time dif 7824.771526813507)
step: 33450 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(2.3975759), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06347455717623234), 'actor_loss': np.float64(-0.9871791243553162), 'hyper_actor_loss': np.float64(4.162411482866446e-06), 'behavior_loss': np.float64(0.36674211025238035)}

Episode step 33460, time diff 4.407512903213501, total time dif 7829.238798379898)
step: 33460 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(2.6541355), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07307238914072514), 'actor_loss': np.float64(-0.9799617886543274), 'hyper_actor_loss': np.float64(4.325051622799947e-06), 'behavior_loss': np.float64(0.3768960416316986)}

Episode step 33470, time diff 4.520229339599609, total time dif 7833.646311283112)
step: 33470 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(1.8894272), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07223412021994591), 'actor_loss': np.float64(-0.9766440510749816), 'hyper_actor_loss': np.float64(4.283311363906251e-06), 'behavior_loss': np.float64(0.37556865215301516)}

Episode step 33480, time diff 4.405715465545654, total time dif 7838.166540622711)
step: 33480 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(1.2920059), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06645551286637782), 'actor_loss': np.float64(-0.9808248162269593), 'hyper_actor_loss': np.float64(4.385629972603056e-06), 'behavior_loss': np.float64(0.36605221033096313)}

Episode step 33490, time diff 4.451484680175781, total time dif 7842.572256088257)
step: 33490 @ episode report: {'average_total_reward': np.float32(8.763333), 'reward_variance': np.float32(1.3147666), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07553094550967217), 'actor_loss': np.float64(-0.9832381308078766), 'hyper_actor_loss': np.float64(4.626967438525753e-06), 'behavior_loss': np.float64(0.3645118325948715)}

Episode step 33500, time diff 4.458720684051514, total time dif 7847.023740768433)
step: 33500 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(0.90588796), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06735262535512447), 'actor_loss': np.float64(-1.0002841770648956), 'hyper_actor_loss': np.float64(4.81847018818371e-06), 'behavior_loss': np.float64(0.3726924300193787)}

Episode step 33510, time diff 4.422531366348267, total time dif 7851.482461452484)
step: 33510 @ episode report: {'average_total_reward': np.float32(9.351111), 'reward_variance': np.float32(2.641166), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061555422469973566), 'actor_loss': np.float64(-0.9854519546031952), 'hyper_actor_loss': np.float64(0.00019320475666972925), 'behavior_loss': np.float64(0.37363027930259707)}

Episode step 33520, time diff 4.367417097091675, total time dif 7855.904992818832)
step: 33520 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(3.455666), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08923331722617149), 'actor_loss': np.float64(-0.9483769774436951), 'hyper_actor_loss': np.float64(0.0006477002636529506), 'behavior_loss': np.float64(0.3890294134616852)}

Episode step 33530, time diff 4.319621801376343, total time dif 7860.272409915924)
step: 33530 @ episode report: {'average_total_reward': np.float32(9.663335), 'reward_variance': np.float32(2.1390378), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06448190994560718), 'actor_loss': np.float64(-0.9829878270626068), 'hyper_actor_loss': np.float64(0.0003315424197353423), 'behavior_loss': np.float64(0.3832204669713974)}

Episode step 33540, time diff 4.302568435668945, total time dif 7864.5920317173)
step: 33540 @ episode report: {'average_total_reward': np.float32(9.33889), 'reward_variance': np.float32(4.789241), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0675757173448801), 'actor_loss': np.float64(-0.9853471457958222), 'hyper_actor_loss': np.float64(0.00011702585834427737), 'behavior_loss': np.float64(0.39298763275146487)}

Episode step 33550, time diff 4.303318023681641, total time dif 7868.894600152969)
step: 33550 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.3298023), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07097434885799885), 'actor_loss': np.float64(-0.9668093740940094), 'hyper_actor_loss': np.float64(4.1715780753293075e-05), 'behavior_loss': np.float64(0.3819422423839569)}

Episode step 33560, time diff 4.302587270736694, total time dif 7873.197918176651)
step: 33560 @ episode report: {'average_total_reward': np.float32(8.875555), 'reward_variance': np.float32(1.4862174), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059207860566675666), 'actor_loss': np.float64(-0.982523500919342), 'hyper_actor_loss': np.float64(1.5183481536951149e-05), 'behavior_loss': np.float64(0.38373923897743223)}

Episode step 33570, time diff 4.352528810501099, total time dif 7877.500505447388)
step: 33570 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(1.8636059), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07827674113214016), 'actor_loss': np.float64(-0.9855006098747253), 'hyper_actor_loss': np.float64(1.144583084169426e-05), 'behavior_loss': np.float64(0.38318978250026703)}

Episode step 33580, time diff 4.2866363525390625, total time dif 7881.853034257889)
step: 33580 @ episode report: {'average_total_reward': np.float32(9.138889), 'reward_variance': np.float32(3.405562), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07835674397647381), 'actor_loss': np.float64(-0.9844147384166717), 'hyper_actor_loss': np.float64(1.1111687308584806e-05), 'behavior_loss': np.float64(0.3823722958564758)}

Episode step 33590, time diff 4.302922248840332, total time dif 7886.139670610428)
step: 33590 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(2.1722476), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06571167968213558), 'actor_loss': np.float64(-0.9879654586315155), 'hyper_actor_loss': np.float64(1.0904512873821658e-05), 'behavior_loss': np.float64(0.38363243639469147)}

Episode step 33600, time diff 4.336961984634399, total time dif 7890.442592859268)
step: 33600 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(2.099314), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07522284723818302), 'actor_loss': np.float64(-0.9737766027450562), 'hyper_actor_loss': np.float64(9.506147216598038e-06), 'behavior_loss': np.float64(0.37793395221233367)}

Episode step 33610, time diff 4.352022409439087, total time dif 7894.779554843903)
step: 33610 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(1.0280997), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07181192561984062), 'actor_loss': np.float64(-0.9827492952346801), 'hyper_actor_loss': np.float64(8.491171729474446e-06), 'behavior_loss': np.float64(0.38381720185279844)}

Episode step 33620, time diff 4.316479444503784, total time dif 7899.131577253342)
step: 33620 @ episode report: {'average_total_reward': np.float32(9.187779), 'reward_variance': np.float32(0.39722088), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07968200482428074), 'actor_loss': np.float64(-0.998132872581482), 'hyper_actor_loss': np.float64(8.961498497228604e-06), 'behavior_loss': np.float64(0.37796962857246397)}

Episode step 33630, time diff 4.3164284229278564, total time dif 7903.4480566978455)
step: 33630 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(4.668949), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07591472752392292), 'actor_loss': np.float64(-0.9967960894107819), 'hyper_actor_loss': np.float64(1.0292312617821154e-05), 'behavior_loss': np.float64(0.3814909070730209)}

Episode step 33640, time diff 4.520162582397461, total time dif 7907.764485120773)
step: 33640 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(2.5332463), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06022190675139427), 'actor_loss': np.float64(-0.9774251997470855), 'hyper_actor_loss': np.float64(7.86634741416492e-06), 'behavior_loss': np.float64(0.3811191558837891)}

Episode step 33650, time diff 4.337950229644775, total time dif 7912.284647703171)
step: 33650 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.121962), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06447526011615992), 'actor_loss': np.float64(-0.9652799248695374), 'hyper_actor_loss': np.float64(6.769954779883847e-06), 'behavior_loss': np.float64(0.37959952354431153)}

Episode step 33660, time diff 4.337836980819702, total time dif 7916.622597932816)
step: 33660 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.6750733), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06424769423902035), 'actor_loss': np.float64(-0.9650019407272339), 'hyper_actor_loss': np.float64(6.1878823544248e-06), 'behavior_loss': np.float64(0.3764551132917404)}

Episode step 33670, time diff 4.417428731918335, total time dif 7920.960434913635)
step: 33670 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(0.7170483), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07414926961064339), 'actor_loss': np.float64(-0.9865208685398101), 'hyper_actor_loss': np.float64(6.259889187276713e-06), 'behavior_loss': np.float64(0.38192748129367826)}

Episode step 33680, time diff 4.3389997482299805, total time dif 7925.377863645554)
step: 33680 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(2.5910974), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0684714050963521), 'actor_loss': np.float64(-0.9974506616592407), 'hyper_actor_loss': np.float64(7.947235371830174e-06), 'behavior_loss': np.float64(0.3752544105052948)}

Episode step 33690, time diff 4.373072147369385, total time dif 7929.716863393784)
step: 33690 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(2.6076345), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0655605137348175), 'actor_loss': np.float64(-0.9755351662635803), 'hyper_actor_loss': np.float64(7.71859322412638e-06), 'behavior_loss': np.float64(0.3738854259252548)}

Episode step 33700, time diff 4.301629304885864, total time dif 7934.089935541153)
step: 33700 @ episode report: {'average_total_reward': np.float32(9.212223), 'reward_variance': np.float32(3.8724313), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059851429611444476), 'actor_loss': np.float64(-0.9623113393783569), 'hyper_actor_loss': np.float64(4.9752631412047775e-06), 'behavior_loss': np.float64(0.37275377511978147)}

Episode step 33710, time diff 4.318665266036987, total time dif 7938.391564846039)
step: 33710 @ episode report: {'average_total_reward': np.float32(9.075557), 'reward_variance': np.float32(1.6589825), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07179933600127697), 'actor_loss': np.float64(-0.9770608723163605), 'hyper_actor_loss': np.float64(3.763530480682675e-06), 'behavior_loss': np.float64(0.37561326026916503)}

Episode step 33720, time diff 4.321673631668091, total time dif 7942.710230112076)
step: 33720 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(2.002029), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0650859672576189), 'actor_loss': np.float64(-0.9896556496620178), 'hyper_actor_loss': np.float64(3.5628508157969917e-06), 'behavior_loss': np.float64(0.3782055705785751)}

Episode step 33730, time diff 4.366340637207031, total time dif 7947.031903743744)
step: 33730 @ episode report: {'average_total_reward': np.float32(10.236668), 'reward_variance': np.float32(3.0325708), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06810224205255508), 'actor_loss': np.float64(-0.9733463823795319), 'hyper_actor_loss': np.float64(3.4901454682767507e-06), 'behavior_loss': np.float64(0.37909485697746276)}

Episode step 33740, time diff 4.316356897354126, total time dif 7951.398244380951)
step: 33740 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.8838708), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06626661084592342), 'actor_loss': np.float64(-0.9684701800346375), 'hyper_actor_loss': np.float64(3.499293802633474e-06), 'behavior_loss': np.float64(0.38012863099575045)}

Episode step 33750, time diff 4.380875825881958, total time dif 7955.714601278305)
step: 33750 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(2.0721102), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08006977662444115), 'actor_loss': np.float64(-0.9842419981956482), 'hyper_actor_loss': np.float64(3.3608476542212886e-06), 'behavior_loss': np.float64(0.37486196160316465)}

Episode step 33760, time diff 4.3352272510528564, total time dif 7960.095477104187)
step: 33760 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(1.7845786), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06946097761392593), 'actor_loss': np.float64(-0.9963907599449158), 'hyper_actor_loss': np.float64(3.383758030395256e-06), 'behavior_loss': np.float64(0.375919246673584)}

Episode step 33770, time diff 4.305058240890503, total time dif 7964.43070435524)
step: 33770 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(1.096924), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06696619279682636), 'actor_loss': np.float64(-0.9704819262027741), 'hyper_actor_loss': np.float64(3.3573115388207954e-06), 'behavior_loss': np.float64(0.3785808175802231)}

Episode step 33780, time diff 4.308494806289673, total time dif 7968.73576259613)
step: 33780 @ episode report: {'average_total_reward': np.float32(9.624446), 'reward_variance': np.float32(2.5488105), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06404197737574577), 'actor_loss': np.float64(-0.9542895019054413), 'hyper_actor_loss': np.float64(3.825889371000812e-06), 'behavior_loss': np.float64(0.37879597544670107)}

Episode step 33790, time diff 4.358242988586426, total time dif 7973.04425740242)
step: 33790 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(4.7339077), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07111988700926304), 'actor_loss': np.float64(-0.9713187932968139), 'hyper_actor_loss': np.float64(3.945004755223635e-06), 'behavior_loss': np.float64(0.37703036069869994)}

Episode step 33800, time diff 4.4467809200286865, total time dif 7977.4025003910065)
step: 33800 @ episode report: {'average_total_reward': np.float32(8.453334), 'reward_variance': np.float32(2.7638469), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05674591083079576), 'actor_loss': np.float64(-0.9931411206722259), 'hyper_actor_loss': np.float64(3.492631367407739e-06), 'behavior_loss': np.float64(0.3656029999256134)}

Episode step 33810, time diff 4.32021689414978, total time dif 7981.849281311035)
step: 33810 @ episode report: {'average_total_reward': np.float32(8.502222), 'reward_variance': np.float32(3.8465886), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07527368180453778), 'actor_loss': np.float64(-0.9803061962127686), 'hyper_actor_loss': np.float64(3.5511449596015153e-06), 'behavior_loss': np.float64(0.379947629570961)}

Episode step 33820, time diff 4.3539650440216064, total time dif 7986.169498205185)
step: 33820 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.5533689), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07379135563969612), 'actor_loss': np.float64(-0.9932599008083344), 'hyper_actor_loss': np.float64(3.4542885487098827e-06), 'behavior_loss': np.float64(0.38404859602451324)}

Episode step 33830, time diff 4.319628953933716, total time dif 7990.5234632492065)
step: 33830 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(2.3364513), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06816517133265734), 'actor_loss': np.float64(-0.9844599306583405), 'hyper_actor_loss': np.float64(3.244798176638142e-06), 'behavior_loss': np.float64(0.37446306049823763)}

Episode step 33840, time diff 4.335481882095337, total time dif 7994.84309220314)
step: 33840 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(3.2838867), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06880450770258903), 'actor_loss': np.float64(-0.9655851781368255), 'hyper_actor_loss': np.float64(3.3131190093627083e-06), 'behavior_loss': np.float64(0.3818089008331299)}

Episode step 33850, time diff 4.333600759506226, total time dif 7999.178574085236)
step: 33850 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(3.9016614), 'max_total_reward': np.float32(12.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06584881842136384), 'actor_loss': np.float64(-0.9725085377693177), 'hyper_actor_loss': np.float64(3.2978844274111906e-06), 'behavior_loss': np.float64(0.3786171466112137)}

Episode step 33860, time diff 4.343831777572632, total time dif 8003.512174844742)
step: 33860 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(2.1424065), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07427948378026486), 'actor_loss': np.float64(-0.9770562648773193), 'hyper_actor_loss': np.float64(3.043463220819831e-06), 'behavior_loss': np.float64(0.37467844784259796)}

Episode step 33870, time diff 4.391270399093628, total time dif 8007.856006622314)
step: 33870 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(3.6365209), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06604704484343529), 'actor_loss': np.float64(-0.9843557417392731), 'hyper_actor_loss': np.float64(3.1007539064376034e-06), 'behavior_loss': np.float64(0.3829401046037674)}

Episode step 33880, time diff 4.2966461181640625, total time dif 8012.247277021408)
step: 33880 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(2.1414084), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05531059056520462), 'actor_loss': np.float64(-0.9666350781917572), 'hyper_actor_loss': np.float64(3.0898844897819798e-06), 'behavior_loss': np.float64(0.37724749445915223)}

Episode step 33890, time diff 4.314253807067871, total time dif 8016.543923139572)
step: 33890 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(1.8489338), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060815613716840744), 'actor_loss': np.float64(-0.9584435939788818), 'hyper_actor_loss': np.float64(3.6746018167832515e-06), 'behavior_loss': np.float64(0.37119846045970917)}

Episode step 33900, time diff 4.332718133926392, total time dif 8020.85817694664)
step: 33900 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(4.473171), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05737873502075672), 'actor_loss': np.float64(-0.9672906756401062), 'hyper_actor_loss': np.float64(3.7788887766510015e-06), 'behavior_loss': np.float64(0.379312801361084)}

Episode step 33910, time diff 4.423135042190552, total time dif 8025.190895080566)
step: 33910 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(1.1412886), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06281728819012641), 'actor_loss': np.float64(-0.9762296617031098), 'hyper_actor_loss': np.float64(3.971062574237294e-06), 'behavior_loss': np.float64(0.3714362919330597)}

Episode step 33920, time diff 4.344333648681641, total time dif 8029.614030122757)
step: 33920 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(1.3627456), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05945712011307478), 'actor_loss': np.float64(-0.9733954012393952), 'hyper_actor_loss': np.float64(4.027397471872973e-06), 'behavior_loss': np.float64(0.37959634363651273)}

Episode step 33930, time diff 4.352910041809082, total time dif 8033.958363771439)
step: 33930 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(2.036474), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07327169924974442), 'actor_loss': np.float64(-0.9704520463943481), 'hyper_actor_loss': np.float64(4.406843481774559e-06), 'behavior_loss': np.float64(0.3840195000171661)}

Episode step 33940, time diff 4.423723220825195, total time dif 8038.311273813248)
step: 33940 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.4830738), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06477907150983811), 'actor_loss': np.float64(-0.9817259311676025), 'hyper_actor_loss': np.float64(4.0067857980830015e-06), 'behavior_loss': np.float64(0.3818244606256485)}

Episode step 33950, time diff 4.352925062179565, total time dif 8042.734997034073)
step: 33950 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(2.0905495), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06716155335307121), 'actor_loss': np.float64(-0.971591317653656), 'hyper_actor_loss': np.float64(3.37475444212032e-06), 'behavior_loss': np.float64(0.37957956492900846)}

Episode step 33960, time diff 4.49541163444519, total time dif 8047.087922096252)
step: 33960 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(7.20924), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06829416267573833), 'actor_loss': np.float64(-0.9771038353443146), 'hyper_actor_loss': np.float64(3.5522890584616107e-06), 'behavior_loss': np.float64(0.37855343520641327)}

Episode step 33970, time diff 4.395339012145996, total time dif 8051.583333730698)
step: 33970 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(3.5040317), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07554136104881763), 'actor_loss': np.float64(-0.9907413244247436), 'hyper_actor_loss': np.float64(3.4518935080996014e-06), 'behavior_loss': np.float64(0.3790866047143936)}

Episode step 33980, time diff 4.337649583816528, total time dif 8055.978672742844)
step: 33980 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(1.3837894), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07468316853046417), 'actor_loss': np.float64(-0.9954997360706329), 'hyper_actor_loss': np.float64(1.7273853927690652e-05), 'behavior_loss': np.float64(0.38245369493961334)}

Episode step 33990, time diff 4.370153427124023, total time dif 8060.31632232666)
step: 33990 @ episode report: {'average_total_reward': np.float32(10.310001), 'reward_variance': np.float32(6.991914), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0519270533695817), 'actor_loss': np.float64(-0.9717093944549561), 'hyper_actor_loss': np.float64(9.964278342522448e-06), 'behavior_loss': np.float64(0.3796910375356674)}

Episode step 34000, time diff 4.3186352252960205, total time dif 8064.686475753784)
step: 34000 @ episode report: {'average_total_reward': np.float32(9.090001), 'reward_variance': np.float32(3.7128015), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06918379105627537), 'actor_loss': np.float64(-0.951891964673996), 'hyper_actor_loss': np.float64(4.737536473840009e-05), 'behavior_loss': np.float64(0.38103507161140443)}

Episode step 34010, time diff 4.3200907707214355, total time dif 8069.00511097908)
step: 34010 @ episode report: {'average_total_reward': np.float32(8.887779), 'reward_variance': np.float32(2.6378145), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062336407601833344), 'actor_loss': np.float64(-0.9826875030994415), 'hyper_actor_loss': np.float64(2.3787497275407078e-05), 'behavior_loss': np.float64(0.38416907787322996)}

Episode step 34020, time diff 4.34987998008728, total time dif 8073.325201749802)
step: 34020 @ episode report: {'average_total_reward': np.float32(8.626666), 'reward_variance': np.float32(1.0484989), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06104593425989151), 'actor_loss': np.float64(-0.9764993607997894), 'hyper_actor_loss': np.float64(4.982340669812402e-06), 'behavior_loss': np.float64(0.38642965257167816)}

Episode step 34030, time diff 4.278622150421143, total time dif 8077.675081729889)
step: 34030 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(2.0736115), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06426396928727626), 'actor_loss': np.float64(-0.9675002038478852), 'hyper_actor_loss': np.float64(4.908592541141843e-06), 'behavior_loss': np.float64(0.3824372857809067)}

Episode step 34040, time diff 4.3084211349487305, total time dif 8081.95370388031)
step: 34040 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(1.6001284), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05359810702502728), 'actor_loss': np.float64(-0.9727498352527618), 'hyper_actor_loss': np.float64(6.746031749571557e-06), 'behavior_loss': np.float64(0.3848671823740005)}

Episode step 34050, time diff 4.3247246742248535, total time dif 8086.262125015259)
step: 34050 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(2.8711123), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07002606084570288), 'actor_loss': np.float64(-0.9693674743175507), 'hyper_actor_loss': np.float64(5.6332566146011235e-06), 'behavior_loss': np.float64(0.3906241118907928)}

Episode step 34060, time diff 4.330780744552612, total time dif 8090.586849689484)
step: 34060 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(2.163734), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07188283614814281), 'actor_loss': np.float64(-0.9793685078620911), 'hyper_actor_loss': np.float64(5.8262585298507474e-06), 'behavior_loss': np.float64(0.38627817332744596)}

Episode step 34070, time diff 4.331710577011108, total time dif 8094.917630434036)
step: 34070 @ episode report: {'average_total_reward': np.float32(9.214445), 'reward_variance': np.float32(1.4133348), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07349472790956497), 'actor_loss': np.float64(-0.9944565534591675), 'hyper_actor_loss': np.float64(4.990862998965895e-06), 'behavior_loss': np.float64(0.3952567487955093)}

Episode step 34080, time diff 4.3318469524383545, total time dif 8099.249341011047)
step: 34080 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(1.1194679), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07454906292259693), 'actor_loss': np.float64(-0.9889796555042267), 'hyper_actor_loss': np.float64(5.422656477094278e-06), 'behavior_loss': np.float64(0.39203673005104067)}

Episode step 34090, time diff 4.345218896865845, total time dif 8103.581187963486)
step: 34090 @ episode report: {'average_total_reward': np.float32(9.424446), 'reward_variance': np.float32(2.7955995), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0819041296839714), 'actor_loss': np.float64(-0.9863985121250153), 'hyper_actor_loss': np.float64(8.056697242864175e-06), 'behavior_loss': np.float64(0.39207499027252196)}

Episode step 34100, time diff 4.319409370422363, total time dif 8107.926406860352)
step: 34100 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(3.2384942), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061711175739765166), 'actor_loss': np.float64(-0.9892463326454163), 'hyper_actor_loss': np.float64(8.938879363995512e-06), 'behavior_loss': np.float64(0.3908604383468628)}

Episode step 34110, time diff 4.293784141540527, total time dif 8112.245816230774)
step: 34110 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(1.5448164), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06698239706456662), 'actor_loss': np.float64(-0.970939302444458), 'hyper_actor_loss': np.float64(9.73639344010735e-06), 'behavior_loss': np.float64(0.39077249765396116)}

Episode step 34120, time diff 4.331994533538818, total time dif 8116.539600372314)
step: 34120 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(2.723987), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06950858607888222), 'actor_loss': np.float64(-0.9789431393146515), 'hyper_actor_loss': np.float64(1.2219064410601277e-05), 'behavior_loss': np.float64(0.3891159534454346)}

Episode step 34130, time diff 4.485560655593872, total time dif 8120.871594905853)
step: 34130 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(1.4853395), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07587982714176178), 'actor_loss': np.float64(-0.9921197414398193), 'hyper_actor_loss': np.float64(9.420069045518175e-06), 'behavior_loss': np.float64(0.3951777547597885)}

Episode step 34140, time diff 4.288979530334473, total time dif 8125.357155561447)
step: 34140 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(3.5453038), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06062013655900955), 'actor_loss': np.float64(-0.9672719061374664), 'hyper_actor_loss': np.float64(8.934211382438662e-06), 'behavior_loss': np.float64(0.39412851333618165)}

Episode step 34150, time diff 4.285539150238037, total time dif 8129.646135091782)
step: 34150 @ episode report: {'average_total_reward': np.float32(8.651113), 'reward_variance': np.float32(2.1256347), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0723039299249649), 'actor_loss': np.float64(-0.9594022989273071), 'hyper_actor_loss': np.float64(1.2816130401915871e-05), 'behavior_loss': np.float64(0.3899521678686142)}

Episode step 34160, time diff 4.3378071784973145, total time dif 8133.93167424202)
step: 34160 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(3.2630086), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06645332090556622), 'actor_loss': np.float64(-0.9958984196186066), 'hyper_actor_loss': np.float64(1.2149041685916017e-05), 'behavior_loss': np.float64(0.39681021571159364)}

Episode step 34170, time diff 4.326806545257568, total time dif 8138.269481420517)
step: 34170 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(1.2985051), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07006674893200397), 'actor_loss': np.float64(-0.989014595746994), 'hyper_actor_loss': np.float64(6.606910392292775e-06), 'behavior_loss': np.float64(0.38952513337135314)}

Episode step 34180, time diff 4.32353663444519, total time dif 8142.5962879657745)
step: 34180 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(1.0534817), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06837981753051281), 'actor_loss': np.float64(-0.9770009756088257), 'hyper_actor_loss': np.float64(5.685171799996169e-06), 'behavior_loss': np.float64(0.40035463869571686)}

Episode step 34190, time diff 4.353197336196899, total time dif 8146.91982460022)
step: 34190 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(2.6883512), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06787225268781186), 'actor_loss': np.float64(-0.9717676103115082), 'hyper_actor_loss': np.float64(7.239344586196239e-06), 'behavior_loss': np.float64(0.40027444064617157)}

Episode step 34200, time diff 4.346883773803711, total time dif 8151.273021936417)
step: 34200 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.3283262), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07216772213578224), 'actor_loss': np.float64(-0.9744778156280518), 'hyper_actor_loss': np.float64(9.059557851287536e-06), 'behavior_loss': np.float64(0.399655881524086)}

Episode step 34210, time diff 4.365025043487549, total time dif 8155.61990571022)
step: 34210 @ episode report: {'average_total_reward': np.float32(10.024446), 'reward_variance': np.float32(2.6975265), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0706706129014492), 'actor_loss': np.float64(-0.973997312784195), 'hyper_actor_loss': np.float64(9.434465937374625e-06), 'behavior_loss': np.float64(0.39035028517246245)}

Episode step 34220, time diff 4.375555038452148, total time dif 8159.984930753708)
step: 34220 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(3.148291), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06618545092642307), 'actor_loss': np.float64(-0.986154705286026), 'hyper_actor_loss': np.float64(7.927992510303738e-06), 'behavior_loss': np.float64(0.39614203870296477)}

Episode step 34230, time diff 4.393012046813965, total time dif 8164.36048579216)
step: 34230 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(2.5608754), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05322787500917912), 'actor_loss': np.float64(-0.9663995265960693), 'hyper_actor_loss': np.float64(7.290441953955451e-06), 'behavior_loss': np.float64(0.3977192103862762)}

Episode step 34240, time diff 4.43068528175354, total time dif 8168.753497838974)
step: 34240 @ episode report: {'average_total_reward': np.float32(8.414446), 'reward_variance': np.float32(2.8600519), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06997874602675438), 'actor_loss': np.float64(-0.9599160969257354), 'hyper_actor_loss': np.float64(9.216917896992527e-06), 'behavior_loss': np.float64(0.3902192622423172)}

Episode step 34250, time diff 4.392666816711426, total time dif 8173.1841831207275)
step: 34250 @ episode report: {'average_total_reward': np.float32(8.975556), 'reward_variance': np.float32(2.3641682), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0795846488326788), 'actor_loss': np.float64(-0.9986237525939942), 'hyper_actor_loss': np.float64(8.71453812578693e-06), 'behavior_loss': np.float64(0.40219702422618864)}

Episode step 34260, time diff 4.419957399368286, total time dif 8177.576849937439)
step: 34260 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(4.4918966), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07176472526043653), 'actor_loss': np.float64(-0.9970271229743958), 'hyper_actor_loss': np.float64(6.510536559289904e-06), 'behavior_loss': np.float64(0.397137850522995)}

Episode step 34270, time diff 4.456058740615845, total time dif 8181.996807336807)
step: 34270 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(2.0165238), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0700381699949503), 'actor_loss': np.float64(-0.9822465002536773), 'hyper_actor_loss': np.float64(8.26691489237419e-06), 'behavior_loss': np.float64(0.39751299023628234)}

Episode step 34280, time diff 4.460025787353516, total time dif 8186.452866077423)
step: 34280 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.7189643), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062257895991206166), 'actor_loss': np.float64(-0.9792104244232178), 'hyper_actor_loss': np.float64(1.2062797122780466e-05), 'behavior_loss': np.float64(0.4029912203550339)}

Episode step 34290, time diff 4.625119686126709, total time dif 8190.912891864777)
step: 34290 @ episode report: {'average_total_reward': np.float32(8.975556), 'reward_variance': np.float32(2.3093038), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06875610481947661), 'actor_loss': np.float64(-0.9697455167770386), 'hyper_actor_loss': np.float64(1.935941163537791e-05), 'behavior_loss': np.float64(0.4039105087518692)}

Episode step 34300, time diff 4.5013508796691895, total time dif 8195.538011550903)
step: 34300 @ episode report: {'average_total_reward': np.float32(8.502222), 'reward_variance': np.float32(1.883452), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06169714964926243), 'actor_loss': np.float64(-0.9656719505786896), 'hyper_actor_loss': np.float64(2.981181241921149e-05), 'behavior_loss': np.float64(0.40528793036937716)}

Episode step 34310, time diff 4.566970348358154, total time dif 8200.039362430573)
step: 34310 @ episode report: {'average_total_reward': np.float32(8.314445), 'reward_variance': np.float32(1.0454332), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06843835934996605), 'actor_loss': np.float64(-0.9710480868816376), 'hyper_actor_loss': np.float64(4.366803404991515e-05), 'behavior_loss': np.float64(0.4079760670661926)}

Episode step 34320, time diff 4.592235565185547, total time dif 8204.60633277893)
step: 34320 @ episode report: {'average_total_reward': np.float32(8.228889), 'reward_variance': np.float32(2.9234626), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06726978085935116), 'actor_loss': np.float64(-0.9796302378177643), 'hyper_actor_loss': np.float64(2.4003462203836535e-05), 'behavior_loss': np.float64(0.4071365773677826)}

Episode step 34330, time diff 4.643735885620117, total time dif 8209.198568344116)
step: 34330 @ episode report: {'average_total_reward': np.float32(9.35111), 'reward_variance': np.float32(4.4102774), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06543397698551416), 'actor_loss': np.float64(-0.9782083570957184), 'hyper_actor_loss': np.float64(1.1573546089493902e-05), 'behavior_loss': np.float64(0.40413062274456024)}

Episode step 34340, time diff 4.595452547073364, total time dif 8213.842304229736)
step: 34340 @ episode report: {'average_total_reward': np.float32(8.490001), 'reward_variance': np.float32(3.2471728), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07963303476572037), 'actor_loss': np.float64(-0.99105024933815), 'hyper_actor_loss': np.float64(7.855711464799242e-06), 'behavior_loss': np.float64(0.41397108137607574)}

Episode step 34350, time diff 4.596500873565674, total time dif 8218.43775677681)
step: 34350 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(4.792973), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07960804142057895), 'actor_loss': np.float64(-0.994733315706253), 'hyper_actor_loss': np.float64(7.131664415283012e-06), 'behavior_loss': np.float64(0.41799152493476865)}

Episode step 34360, time diff 4.600141763687134, total time dif 8223.034257650375)
step: 34360 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(1.4325488), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0789209883660078), 'actor_loss': np.float64(-0.9964233994483948), 'hyper_actor_loss': np.float64(6.922263628439395e-06), 'behavior_loss': np.float64(0.4175181001424789)}

Episode step 34370, time diff 4.642093896865845, total time dif 8227.634399414062)
step: 34370 @ episode report: {'average_total_reward': np.float32(9.663333), 'reward_variance': np.float32(2.418347), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06673156209290028), 'actor_loss': np.float64(-0.9773369252681732), 'hyper_actor_loss': np.float64(7.758399260637815e-06), 'behavior_loss': np.float64(0.4172185927629471)}

Episode step 34380, time diff 4.6707763671875, total time dif 8232.276493310928)
step: 34380 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(3.2423654), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06940689757466316), 'actor_loss': np.float64(-0.9716929197311401), 'hyper_actor_loss': np.float64(8.0539379723632e-06), 'behavior_loss': np.float64(0.4193721622228622)}

Episode step 34390, time diff 4.6872453689575195, total time dif 8236.947269678116)
step: 34390 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.5957648), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05770054142922163), 'actor_loss': np.float64(-0.981643807888031), 'hyper_actor_loss': np.float64(7.826829505575007e-06), 'behavior_loss': np.float64(0.4103392601013184)}

Episode step 34400, time diff 4.68239951133728, total time dif 8241.634515047073)
step: 34400 @ episode report: {'average_total_reward': np.float32(9.663333), 'reward_variance': np.float32(2.1968906), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06903420388698578), 'actor_loss': np.float64(-0.9780397117137909), 'hyper_actor_loss': np.float64(7.514444678236032e-06), 'behavior_loss': np.float64(0.4193277031183243)}

Episode step 34410, time diff 4.695667266845703, total time dif 8246.31691455841)
step: 34410 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(1.1985481), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0888740949332714), 'actor_loss': np.float64(-0.9863829672336578), 'hyper_actor_loss': np.float64(7.244639709824696e-06), 'behavior_loss': np.float64(0.4166289210319519)}

Episode step 34420, time diff 4.716697454452515, total time dif 8251.012581825256)
step: 34420 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(5.2126884), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07352501340210438), 'actor_loss': np.float64(-1.0117250204086303), 'hyper_actor_loss': np.float64(7.465562839570339e-06), 'behavior_loss': np.float64(0.41797753274440763)}

Episode step 34430, time diff 4.731081008911133, total time dif 8255.729279279709)
step: 34430 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(3.3409505), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0685091845691204), 'actor_loss': np.float64(-0.9830355405807495), 'hyper_actor_loss': np.float64(1.0251843150399509e-05), 'behavior_loss': np.float64(0.4136211425065994)}

Episode step 34440, time diff 4.735314130783081, total time dif 8260.46036028862)
step: 34440 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(3.736342), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07476731315255165), 'actor_loss': np.float64(-0.975507664680481), 'hyper_actor_loss': np.float64(3.112262147624279e-05), 'behavior_loss': np.float64(0.4251595586538315)}

Episode step 34450, time diff 4.807167291641235, total time dif 8265.195674419403)
step: 34450 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.1109643), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07316715270280838), 'actor_loss': np.float64(-0.9919580817222595), 'hyper_actor_loss': np.float64(7.091439292707946e-05), 'behavior_loss': np.float64(0.43141188621521)}

Episode step 34460, time diff 4.9947404861450195, total time dif 8270.002841711044)
step: 34460 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.5003709), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.066811284981668), 'actor_loss': np.float64(-0.9826088011264801), 'hyper_actor_loss': np.float64(8.966464883997105e-05), 'behavior_loss': np.float64(0.43737448155879977)}

Episode step 34470, time diff 4.83264946937561, total time dif 8274.99758219719)
step: 34470 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(3.9887357), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07189653106033803), 'actor_loss': np.float64(-0.97121861577034), 'hyper_actor_loss': np.float64(0.00011227844297536648), 'behavior_loss': np.float64(0.44438415169715884)}

Episode step 34480, time diff 4.869976282119751, total time dif 8279.830231666565)
step: 34480 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.2531378), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07179677002131939), 'actor_loss': np.float64(-0.9811336994171143), 'hyper_actor_loss': np.float64(0.00025507309474051), 'behavior_loss': np.float64(0.4678084820508957)}

Episode step 34490, time diff 5.122456312179565, total time dif 8284.700207948685)
step: 34490 @ episode report: {'average_total_reward': np.float32(8.502222), 'reward_variance': np.float32(3.3976989), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06335294172167778), 'actor_loss': np.float64(-1.017686778306961), 'hyper_actor_loss': np.float64(0.0005030265194363893), 'behavior_loss': np.float64(0.49705871045589445)}

Episode step 34500, time diff 4.967792272567749, total time dif 8289.822664260864)
step: 34500 @ episode report: {'average_total_reward': np.float32(4.1133337), 'reward_variance': np.float32(5.320884), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(1.8000002), 'average_n_step': np.float32(5.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0680818386375904), 'actor_loss': np.float64(-1.0797937512397766), 'hyper_actor_loss': np.float64(0.0006988249951973557), 'behavior_loss': np.float64(0.5834576308727264)}

Episode step 34510, time diff 5.176215171813965, total time dif 8294.790456533432)
step: 34510 @ episode report: {'average_total_reward': np.float32(0.43888894), 'reward_variance': np.float32(0.49015445), 'max_total_reward': np.float32(1.8000001), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.7), 'max_n_step': np.float32(4.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06654353812336922), 'actor_loss': np.float64(-1.0315111458301545), 'hyper_actor_loss': np.float64(0.0022989559802226722), 'behavior_loss': np.float64(0.8496450126171112)}

Episode step 34520, time diff 5.102107763290405, total time dif 8299.966671705246)
step: 34520 @ episode report: {'average_total_reward': np.float32(0.0144444285), 'reward_variance': np.float32(0.026816046), 'max_total_reward': np.float32(0.3111111), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.3), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07182139232754707), 'actor_loss': np.float64(-0.9585208535194397), 'hyper_actor_loss': np.float64(0.0036433581728488205), 'behavior_loss': np.float64(0.9667539298534393)}

Episode step 34530, time diff 4.7268006801605225, total time dif 8305.068779468536)
step: 34530 @ episode report: {'average_total_reward': np.float32(0.07555554), 'reward_variance': np.float32(0.05078519), 'max_total_reward': np.float32(0.5555556), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.3), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06845441162586212), 'actor_loss': np.float64(-0.9692752540111542), 'hyper_actor_loss': np.float64(0.0029299610061571), 'behavior_loss': np.float64(0.929615604877472)}

Episode step 34540, time diff 4.641085147857666, total time dif 8309.795580148697)
step: 34540 @ episode report: {'average_total_reward': np.float32(0.114444435), 'reward_variance': np.float32(0.049482714), 'max_total_reward': np.float32(0.5555555), 'min_total_reward': np.float32(-0.07777779), 'average_n_step': np.float32(2.4), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0794505350291729), 'actor_loss': np.float64(-0.9913282275199891), 'hyper_actor_loss': np.float64(0.0023064267821609973), 'behavior_loss': np.float64(0.8795300483703613)}

Episode step 34550, time diff 4.644183397293091, total time dif 8314.436665296555)
step: 34550 @ episode report: {'average_total_reward': np.float32(0.022222215), 'reward_variance': np.float32(0.025086422), 'max_total_reward': np.float32(0.43333337), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.1), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07697404213249684), 'actor_loss': np.float64(-0.9902135491371155), 'hyper_actor_loss': np.float64(0.0018437993247061967), 'behavior_loss': np.float64(0.8485618889331817)}

Episode step 34560, time diff 4.538080930709839, total time dif 8319.080848693848)
step: 34560 @ episode report: {'average_total_reward': np.float32(0.0122222), 'reward_variance': np.float32(0.013591357), 'max_total_reward': np.float32(0.31111106), 'min_total_reward': np.float32(-0.077777795), 'average_n_step': np.float32(2.2), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07328884042799473), 'actor_loss': np.float64(-0.9534734964370728), 'hyper_actor_loss': np.float64(0.0015006682951934636), 'behavior_loss': np.float64(0.8234860479831696)}

Episode step 34570, time diff 4.470493793487549, total time dif 8323.618929624557)
step: 34570 @ episode report: {'average_total_reward': np.float32(0.03888888), 'reward_variance': np.float32(0.02524074), 'max_total_reward': np.float32(0.3111111), 'min_total_reward': np.float32(-0.077777795), 'average_n_step': np.float32(2.3), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0787371501326561), 'actor_loss': np.float64(-0.959339702129364), 'hyper_actor_loss': np.float64(0.0012371210032142699), 'behavior_loss': np.float64(0.8003830254077912)}

Episode step 34580, time diff 4.443018436431885, total time dif 8328.089423418045)
step: 34580 @ episode report: {'average_total_reward': np.float32(0.07777776), 'reward_variance': np.float32(0.044716053), 'max_total_reward': np.float32(0.5555555), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.4), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07818526439368725), 'actor_loss': np.float64(-0.978899997472763), 'hyper_actor_loss': np.float64(0.000995943759335205), 'behavior_loss': np.float64(0.7821643888950348)}

Episode step 34590, time diff 4.384693145751953, total time dif 8332.532441854477)
step: 34590 @ episode report: {'average_total_reward': np.float32(-0.03666668), 'reward_variance': np.float32(0.013482717), 'max_total_reward': np.float32(0.18888888), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.2), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08382199741899968), 'actor_loss': np.float64(-0.9607893168926239), 'hyper_actor_loss': np.float64(0.0008125414664391428), 'behavior_loss': np.float64(0.7666127324104309)}

Episode step 34600, time diff 4.4055821895599365, total time dif 8336.917135000229)
step: 34600 @ episode report: {'average_total_reward': np.float32(-0.07333336), 'reward_variance': np.float32(0.01311605), 'max_total_reward': np.float32(0.06666665), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.2), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07424567323178052), 'actor_loss': np.float64(-0.9558489561080933), 'hyper_actor_loss': np.float64(0.0006586949923075736), 'behavior_loss': np.float64(0.7532870352268219)}

Episode step 34610, time diff 4.410679817199707, total time dif 8341.322717189789)
step: 34610 @ episode report: {'average_total_reward': np.float32(-0.051111124), 'reward_variance': np.float32(0.021881482), 'max_total_reward': np.float32(0.3111111), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.1), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08104374706745147), 'actor_loss': np.float64(-0.9417764782905579), 'hyper_actor_loss': np.float64(0.0005614804395008832), 'behavior_loss': np.float64(0.744346535205841)}

Episode step 34620, time diff 4.566977262496948, total time dif 8345.733397006989)
step: 34620 @ episode report: {'average_total_reward': np.float32(-0.09000002), 'reward_variance': np.float32(0.01329506), 'max_total_reward': np.float32(0.044444434), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.0), 'max_n_step': np.float32(2.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08827655091881752), 'actor_loss': np.float64(-0.9437296688556671), 'hyper_actor_loss': np.float64(0.0004588844807585701), 'behavior_loss': np.float64(0.7314031004905701)}

Episode step 34630, time diff 4.362037658691406, total time dif 8350.300374269485)
step: 34630 @ episode report: {'average_total_reward': np.float32(-0.06555557), 'reward_variance': np.float32(0.010307407), 'max_total_reward': np.float32(0.04444443), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.0), 'max_n_step': np.float32(2.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0800595000386238), 'actor_loss': np.float64(-0.9616109549999237), 'hyper_actor_loss': np.float64(0.00037868161452934144), 'behavior_loss': np.float64(0.7234109580516815)}

Episode step 34640, time diff 4.400416135787964, total time dif 8354.662411928177)
step: 34640 @ episode report: {'average_total_reward': np.float32(-0.11444446), 'reward_variance': np.float32(0.0121), 'max_total_reward': np.float32(0.044444434), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.0), 'max_n_step': np.float32(2.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08089952506124973), 'actor_loss': np.float64(-0.9192580878734589), 'hyper_actor_loss': np.float64(0.0003188343223882839), 'behavior_loss': np.float64(0.7123131155967712)}

Episode step 34650, time diff 4.361222267150879, total time dif 8359.062828063965)
step: 34650 @ episode report: {'average_total_reward': np.float32(-0.11444446), 'reward_variance': np.float32(0.006124691), 'max_total_reward': np.float32(0.044444427), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.0), 'max_n_step': np.float32(2.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09085583761334419), 'actor_loss': np.float64(-0.9253986895084381), 'hyper_actor_loss': np.float64(0.00027423867722973225), 'behavior_loss': np.float64(0.705197012424469)}

Episode step 34660, time diff 4.451154470443726, total time dif 8363.424050331116)
step: 34660 @ episode report: {'average_total_reward': np.float32(-0.08777779), 'reward_variance': np.float32(0.017467903), 'max_total_reward': np.float32(0.18888889), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.1), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08650888688862324), 'actor_loss': np.float64(-0.9416794717311859), 'hyper_actor_loss': np.float64(0.00023017896455712616), 'behavior_loss': np.float64(0.6919046819210053)}

Episode step 34670, time diff 4.356196641921997, total time dif 8367.87520480156)
step: 34670 @ episode report: {'average_total_reward': np.float32(-0.07555558), 'reward_variance': np.float32(0.0065629617), 'max_total_reward': np.float32(0.06666662), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.1), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08624611832201481), 'actor_loss': np.float64(-0.939240324497223), 'hyper_actor_loss': np.float64(0.00019401038007345052), 'behavior_loss': np.float64(0.6815737664699555)}

Episode step 34680, time diff 4.279557466506958, total time dif 8372.231401443481)
step: 34680 @ episode report: {'average_total_reward': np.float32(-0.03888891), 'reward_variance': np.float32(0.009537037), 'max_total_reward': np.float32(0.06666662), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.1), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08701133169233799), 'actor_loss': np.float64(-0.9016485989093781), 'hyper_actor_loss': np.float64(0.00016848713858053088), 'behavior_loss': np.float64(0.6746911406517029)}

Episode step 34690, time diff 4.423303127288818, total time dif 8376.510958909988)
step: 34690 @ episode report: {'average_total_reward': np.float32(0.01444442), 'reward_variance': np.float32(0.04229753), 'max_total_reward': np.float32(0.43333334), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.3), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07633383311331272), 'actor_loss': np.float64(-0.9085423946380615), 'hyper_actor_loss': np.float64(0.00016230268374783918), 'behavior_loss': np.float64(0.6725898385047913)}

Episode step 34700, time diff 4.38007664680481, total time dif 8380.934262037277)
step: 34700 @ episode report: {'average_total_reward': np.float32(0.08999999), 'reward_variance': np.float32(0.03329507), 'max_total_reward': np.float32(0.43333334), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.4), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.10241627022624016), 'actor_loss': np.float64(-0.8807359933853149), 'hyper_actor_loss': np.float64(0.0001438521168893203), 'behavior_loss': np.float64(0.6687806725502015)}

Episode step 34710, time diff 4.443400859832764, total time dif 8385.314338684082)
step: 34710 @ episode report: {'average_total_reward': np.float32(0.16777776), 'reward_variance': np.float32(0.052850615), 'max_total_reward': np.float32(0.67777777), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.6), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09707702845335006), 'actor_loss': np.float64(-0.8871270954608917), 'hyper_actor_loss': np.float64(0.00016564233519602566), 'behavior_loss': np.float64(0.6726587116718292)}

Episode step 34720, time diff 4.402699708938599, total time dif 8389.757739543915)
step: 34720 @ episode report: {'average_total_reward': np.float32(0.02666665), 'reward_variance': np.float32(0.032424692), 'max_total_reward': np.float32(0.3111111), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.3), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08814561851322651), 'actor_loss': np.float64(-0.8636894345283508), 'hyper_actor_loss': np.float64(0.0001587264850968495), 'behavior_loss': np.float64(0.6779218196868897)}

Episode step 34730, time diff 4.430528163909912, total time dif 8394.160439252853)
step: 34730 @ episode report: {'average_total_reward': np.float32(0.026666647), 'reward_variance': np.float32(0.015856791), 'max_total_reward': np.float32(0.3111111), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.3), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07569652646780015), 'actor_loss': np.float64(-0.8063388586044311), 'hyper_actor_loss': np.float64(0.00016060386406024918), 'behavior_loss': np.float64(0.6740028142929078)}

Episode step 34740, time diff 4.54891300201416, total time dif 8398.590967416763)
step: 34740 @ episode report: {'average_total_reward': np.float32(0.0922222), 'reward_variance': np.float32(0.04264321), 'max_total_reward': np.float32(0.4333333), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.5), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07838841564953328), 'actor_loss': np.float64(-0.6535688519477845), 'hyper_actor_loss': np.float64(0.00016464863147120924), 'behavior_loss': np.float64(0.6845732569694519)}

Episode step 34750, time diff 4.6163671016693115, total time dif 8403.139880418777)
step: 34750 @ episode report: {'average_total_reward': np.float32(0.112222195), 'reward_variance': np.float32(0.110579014), 'max_total_reward': np.float32(0.9222222), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.3), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06702089570462703), 'actor_loss': np.float64(-0.4303807556629181), 'hyper_actor_loss': np.float64(0.0001406258437782526), 'behavior_loss': np.float64(0.6634412288665772)}

Episode step 34760, time diff 4.572520971298218, total time dif 8407.756247520447)
step: 34760 @ episode report: {'average_total_reward': np.float32(0.50222224), 'reward_variance': np.float32(0.1550568), 'max_total_reward': np.float32(1.0444446), 'min_total_reward': np.float32(-0.20000002), 'average_n_step': np.float32(2.8), 'max_n_step': np.float32(3.0), 'min_n_step': np.float32(2.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07336346805095673), 'actor_loss': np.float64(-0.36203862726688385), 'hyper_actor_loss': np.float64(0.0001406037583365105), 'behavior_loss': np.float64(0.6379571735858918)}

Episode step 34770, time diff 4.754631519317627, total time dif 8412.328768491745)
step: 34770 @ episode report: {'average_total_reward': np.float32(2.1177778), 'reward_variance': np.float32(1.4359803), 'max_total_reward': np.float32(4.5333333), 'min_total_reward': np.float32(0.8), 'average_n_step': np.float32(4.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06256376020610332), 'actor_loss': np.float64(-0.6594569206237793), 'hyper_actor_loss': np.float64(0.0001704873880953528), 'behavior_loss': np.float64(0.5645484685897827)}

Episode step 34780, time diff 4.514935255050659, total time dif 8417.083400011063)
step: 34780 @ episode report: {'average_total_reward': np.float32(6.6922226), 'reward_variance': np.float32(4.2137055), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06407675705850124), 'actor_loss': np.float64(-0.9259144604206085), 'hyper_actor_loss': np.float64(0.00028344812890281903), 'behavior_loss': np.float64(0.523963326215744)}

Episode step 34790, time diff 4.499046802520752, total time dif 8421.598335266113)
step: 34790 @ episode report: {'average_total_reward': np.float32(9.475557), 'reward_variance': np.float32(0.9570077), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05555468425154686), 'actor_loss': np.float64(-0.9211841464042664), 'hyper_actor_loss': np.float64(0.00017520257679279894), 'behavior_loss': np.float64(0.5001177817583085)}

Episode step 34800, time diff 4.477009057998657, total time dif 8426.097382068634)
step: 34800 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.7613845), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06317505314946174), 'actor_loss': np.float64(-0.9608703315258026), 'hyper_actor_loss': np.float64(0.00012608422621269711), 'behavior_loss': np.float64(0.4913674592971802)}

Episode step 34810, time diff 4.415374279022217, total time dif 8430.574391126633)
step: 34810 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(2.184718), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055866924673318864), 'actor_loss': np.float64(-0.9600315749645233), 'hyper_actor_loss': np.float64(0.0001121888701163698), 'behavior_loss': np.float64(0.48535542488098143)}

Episode step 34820, time diff 4.419625997543335, total time dif 8434.989765405655)
step: 34820 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(1.7245533), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.050784799829125406), 'actor_loss': np.float64(-0.9494403064250946), 'hyper_actor_loss': np.float64(0.00010628371164784766), 'behavior_loss': np.float64(0.48327712118625643)}

Episode step 34830, time diff 4.393639087677002, total time dif 8439.409391403198)
step: 34830 @ episode report: {'average_total_reward': np.float32(8.202223), 'reward_variance': np.float32(5.7691064), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05210014209151268), 'actor_loss': np.float64(-0.9383719444274903), 'hyper_actor_loss': np.float64(9.981372058973648e-05), 'behavior_loss': np.float64(0.48488602936267855)}

Episode step 34840, time diff 4.3913893699646, total time dif 8443.803030490875)
step: 34840 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(1.2017393), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06094401329755783), 'actor_loss': np.float64(-0.9494659900665283), 'hyper_actor_loss': np.float64(9.482116438448429e-05), 'behavior_loss': np.float64(0.4872997522354126)}

Episode step 34850, time diff 4.357937812805176, total time dif 8448.19441986084)
step: 34850 @ episode report: {'average_total_reward': np.float32(8.763334), 'reward_variance': np.float32(2.2429643), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0608521144837141), 'actor_loss': np.float64(-0.9566855669021607), 'hyper_actor_loss': np.float64(9.269932197639719e-05), 'behavior_loss': np.float64(0.47811802923679353)}

Episode step 34860, time diff 4.362470626831055, total time dif 8452.552357673645)
step: 34860 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(2.1603522), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08565470315515995), 'actor_loss': np.float64(-0.9987103164196014), 'hyper_actor_loss': np.float64(8.402243111049756e-05), 'behavior_loss': np.float64(0.48385396003723147)}

Episode step 34870, time diff 4.3705198764801025, total time dif 8456.914828300476)
step: 34870 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.8395364), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08080963045358658), 'actor_loss': np.float64(-0.9964408814907074), 'hyper_actor_loss': np.float64(7.956841218401678e-05), 'behavior_loss': np.float64(0.48438785374164584)}

Episode step 34880, time diff 4.308083534240723, total time dif 8461.285348176956)
step: 34880 @ episode report: {'average_total_reward': np.float32(8.651113), 'reward_variance': np.float32(4.1162043), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08216918222606182), 'actor_loss': np.float64(-0.9692455708980561), 'hyper_actor_loss': np.float64(7.613568886881694e-05), 'behavior_loss': np.float64(0.48699096143245696)}

Episode step 34890, time diff 4.348322153091431, total time dif 8465.593431711197)
step: 34890 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.8042492), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07189627103507519), 'actor_loss': np.float64(-0.9710086584091187), 'hyper_actor_loss': np.float64(7.206107220554258e-05), 'behavior_loss': np.float64(0.4850578993558884)}

Episode step 34900, time diff 4.3496315479278564, total time dif 8469.941753864288)
step: 34900 @ episode report: {'average_total_reward': np.float32(9.075557), 'reward_variance': np.float32(1.1796743), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07735715769231319), 'actor_loss': np.float64(-0.9677117347717286), 'hyper_actor_loss': np.float64(6.957992627576459e-05), 'behavior_loss': np.float64(0.4868136137723923)}

Episode step 34910, time diff 4.322100877761841, total time dif 8474.291385412216)
step: 34910 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(2.521378), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07682083062827587), 'actor_loss': np.float64(-0.9660164713859558), 'hyper_actor_loss': np.float64(6.229931896086782e-05), 'behavior_loss': np.float64(0.4849562138319016)}

Episode step 34920, time diff 4.370695114135742, total time dif 8478.613486289978)
step: 34920 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(4.6242585), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07765405252575874), 'actor_loss': np.float64(-0.9889520585536957), 'hyper_actor_loss': np.float64(6.49228437396232e-05), 'behavior_loss': np.float64(0.4836679071187973)}

Episode step 34930, time diff 4.3816704750061035, total time dif 8482.984181404114)
step: 34930 @ episode report: {'average_total_reward': np.float32(9.9), 'reward_variance': np.float32(2.664889), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0709216471761465), 'actor_loss': np.float64(-0.9502549111843109), 'hyper_actor_loss': np.float64(6.19004167674575e-05), 'behavior_loss': np.float64(0.49036561548709867)}

Episode step 34940, time diff 4.558084964752197, total time dif 8487.36585187912)
step: 34940 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(2.2983327), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07005183249711991), 'actor_loss': np.float64(-0.9326167106628418), 'hyper_actor_loss': np.float64(6.222061165317427e-05), 'behavior_loss': np.float64(0.48861830234527587)}

Episode step 34950, time diff 4.3726301193237305, total time dif 8491.923936843872)
step: 34950 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.5986731), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0778368752449751), 'actor_loss': np.float64(-0.9690514385700226), 'hyper_actor_loss': np.float64(6.148823231342249e-05), 'behavior_loss': np.float64(0.48609638810157774)}

Episode step 34960, time diff 4.479670286178589, total time dif 8496.296566963196)
step: 34960 @ episode report: {'average_total_reward': np.float32(9.224445), 'reward_variance': np.float32(2.0097487), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07838921137154102), 'actor_loss': np.float64(-0.9713666081428528), 'hyper_actor_loss': np.float64(5.785481662314851e-05), 'behavior_loss': np.float64(0.48134366869926454)}

Episode step 34970, time diff 4.377297639846802, total time dif 8500.776237249374)
step: 34970 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(3.081192), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08040355667471885), 'actor_loss': np.float64(-0.9913155436515808), 'hyper_actor_loss': np.float64(6.169291918922681e-05), 'behavior_loss': np.float64(0.4810649693012238)}

Episode step 34980, time diff 4.324478626251221, total time dif 8505.153534889221)
step: 34980 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.4885545), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08784220069646835), 'actor_loss': np.float64(-0.9867122113704682), 'hyper_actor_loss': np.float64(6.463660101871938e-05), 'behavior_loss': np.float64(0.48837628960609436)}

Episode step 34990, time diff 4.3906049728393555, total time dif 8509.478013515472)
step: 34990 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(2.3154836), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07955958023667335), 'actor_loss': np.float64(-0.9848787307739257), 'hyper_actor_loss': np.float64(6.174181326059624e-05), 'behavior_loss': np.float64(0.47933052480220795)}

Episode step 35000, time diff 4.328098297119141, total time dif 8513.868618488312)
step: 35000 @ episode report: {'average_total_reward': np.float32(8.975556), 'reward_variance': np.float32(1.3840942), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08708522170782089), 'actor_loss': np.float64(-0.9990588903427124), 'hyper_actor_loss': np.float64(6.148735956230666e-05), 'behavior_loss': np.float64(0.48757271766662597)}

Episode step 35010, time diff 4.321233510971069, total time dif 8518.19671678543)
step: 35010 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(0.5378712), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08100298047065735), 'actor_loss': np.float64(-0.9569016873836518), 'hyper_actor_loss': np.float64(5.6432542987749915e-05), 'behavior_loss': np.float64(0.4875727206468582)}

Episode step 35020, time diff 4.345860481262207, total time dif 8522.517950296402)
step: 35020 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.0286925), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07756991013884544), 'actor_loss': np.float64(-0.9706497371196747), 'hyper_actor_loss': np.float64(5.7520894551998934e-05), 'behavior_loss': np.float64(0.47973012924194336)}

Episode step 35030, time diff 4.332019090652466, total time dif 8526.863810777664)
step: 35030 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(3.025388), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07899895422160626), 'actor_loss': np.float64(-0.9881324827671051), 'hyper_actor_loss': np.float64(5.705119510821532e-05), 'behavior_loss': np.float64(0.4762758404016495)}

Episode step 35040, time diff 4.351059436798096, total time dif 8531.195829868317)
step: 35040 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.8162484), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09236193597316741), 'actor_loss': np.float64(-0.9766372084617615), 'hyper_actor_loss': np.float64(5.7047392328968274e-05), 'behavior_loss': np.float64(0.4807194948196411)}

Episode step 35050, time diff 4.3963096141815186, total time dif 8535.546889305115)
step: 35050 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(1.8963941), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0801062449812889), 'actor_loss': np.float64(-0.9805867195129394), 'hyper_actor_loss': np.float64(5.312647081154864e-05), 'behavior_loss': np.float64(0.4940280973911285)}

Episode step 35060, time diff 4.370048999786377, total time dif 8539.943198919296)
step: 35060 @ episode report: {'average_total_reward': np.float32(9.536668), 'reward_variance': np.float32(2.117903), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07466709800064564), 'actor_loss': np.float64(-0.9515009999275208), 'hyper_actor_loss': np.float64(5.3733317326987165e-05), 'behavior_loss': np.float64(0.483765634894371)}

Episode step 35070, time diff 4.3440515995025635, total time dif 8544.313247919083)
step: 35070 @ episode report: {'average_total_reward': np.float32(9.126668), 'reward_variance': np.float32(0.74585676), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0886980153620243), 'actor_loss': np.float64(-0.975167316198349), 'hyper_actor_loss': np.float64(5.4730746342102066e-05), 'behavior_loss': np.float64(0.4831530570983887)}

Episode step 35080, time diff 4.373305559158325, total time dif 8548.657299518585)
step: 35080 @ episode report: {'average_total_reward': np.float32(7.965556), 'reward_variance': np.float32(3.2710488), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07495155930519104), 'actor_loss': np.float64(-0.9949093163013458), 'hyper_actor_loss': np.float64(5.581650621024892e-05), 'behavior_loss': np.float64(0.4796450585126877)}

Episode step 35090, time diff 4.315486907958984, total time dif 8553.030605077744)
step: 35090 @ episode report: {'average_total_reward': np.float32(9.002223), 'reward_variance': np.float32(1.3032542), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0767416801303625), 'actor_loss': np.float64(-0.9664485692977905), 'hyper_actor_loss': np.float64(5.872146866749972e-05), 'behavior_loss': np.float64(0.4759593039751053)}

Episode step 35100, time diff 4.4800026416778564, total time dif 8557.346091985703)
step: 35100 @ episode report: {'average_total_reward': np.float32(9.200002), 'reward_variance': np.float32(2.4250126), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08402671702206135), 'actor_loss': np.float64(-0.9857014358043671), 'hyper_actor_loss': np.float64(5.817938144900836e-05), 'behavior_loss': np.float64(0.4735158681869507)}

Episode step 35110, time diff 4.3404271602630615, total time dif 8561.82609462738)
step: 35110 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(3.2770076), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08284845389425755), 'actor_loss': np.float64(-0.9936411440372467), 'hyper_actor_loss': np.float64(5.6810064415913074e-05), 'behavior_loss': np.float64(0.4780654549598694)}

Episode step 35120, time diff 4.34319806098938, total time dif 8566.166521787643)
step: 35120 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.1467965), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07383869700133801), 'actor_loss': np.float64(-0.9847621858119965), 'hyper_actor_loss': np.float64(5.7450819076620975e-05), 'behavior_loss': np.float64(0.4814850717782974)}

Episode step 35130, time diff 4.327344655990601, total time dif 8570.509719848633)
step: 35130 @ episode report: {'average_total_reward': np.float32(8.638889), 'reward_variance': np.float32(2.430624), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08728279285132885), 'actor_loss': np.float64(-0.9714547574520112), 'hyper_actor_loss': np.float64(5.406720629252959e-05), 'behavior_loss': np.float64(0.4788369208574295)}

Episode step 35140, time diff 4.340941429138184, total time dif 8574.837064504623)
step: 35140 @ episode report: {'average_total_reward': np.float32(9.748891), 'reward_variance': np.float32(1.6150916), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0741352841258049), 'actor_loss': np.float64(-0.9946715652942657), 'hyper_actor_loss': np.float64(5.270144210953731e-05), 'behavior_loss': np.float64(0.4752340465784073)}

Episode step 35150, time diff 4.334758281707764, total time dif 8579.178005933762)
step: 35150 @ episode report: {'average_total_reward': np.float32(9.9), 'reward_variance': np.float32(1.8084942), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0807549949735403), 'actor_loss': np.float64(-0.982223391532898), 'hyper_actor_loss': np.float64(5.481897678691894e-05), 'behavior_loss': np.float64(0.47941609621047976)}

Episode step 35160, time diff 4.298406600952148, total time dif 8583.51276421547)
step: 35160 @ episode report: {'average_total_reward': np.float32(8.377779), 'reward_variance': np.float32(3.0794578), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06832604669034481), 'actor_loss': np.float64(-0.9600844621658325), 'hyper_actor_loss': np.float64(5.440047498268541e-05), 'behavior_loss': np.float64(0.47916861772537234)}

Episode step 35170, time diff 4.28783106803894, total time dif 8587.811170816422)
step: 35170 @ episode report: {'average_total_reward': np.float32(9.002223), 'reward_variance': np.float32(2.5890825), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08223899602890014), 'actor_loss': np.float64(-0.9713382184505462), 'hyper_actor_loss': np.float64(5.4157165868673475e-05), 'behavior_loss': np.float64(0.4696920096874237)}

Episode step 35180, time diff 4.3277997970581055, total time dif 8592.09900188446)
step: 35180 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(1.6998024), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08174170330166816), 'actor_loss': np.float64(-0.9934586703777313), 'hyper_actor_loss': np.float64(5.40392935363343e-05), 'behavior_loss': np.float64(0.47570779025554655)}

Episode step 35190, time diff 4.363307237625122, total time dif 8596.426801681519)
step: 35190 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(2.1746192), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07882100492715835), 'actor_loss': np.float64(-0.9667616724967957), 'hyper_actor_loss': np.float64(5.0559710507513957e-05), 'behavior_loss': np.float64(0.47309841215610504)}

Episode step 35200, time diff 4.300698757171631, total time dif 8600.790108919144)
step: 35200 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(3.6432223), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08348762765526771), 'actor_loss': np.float64(-0.9764984488487244), 'hyper_actor_loss': np.float64(4.8630190576659516e-05), 'behavior_loss': np.float64(0.4752226918935776)}

Episode step 35210, time diff 4.315310001373291, total time dif 8605.090807676315)
step: 35210 @ episode report: {'average_total_reward': np.float32(9.700002), 'reward_variance': np.float32(1.8986914), 'max_total_reward': np.float32(12.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07121081538498401), 'actor_loss': np.float64(-0.9920818626880645), 'hyper_actor_loss': np.float64(5.021887700422667e-05), 'behavior_loss': np.float64(0.47800578773021696)}

Episode step 35220, time diff 4.3351216316223145, total time dif 8609.406117677689)
step: 35220 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.6066122), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07781748399138451), 'actor_loss': np.float64(-0.9544241845607757), 'hyper_actor_loss': np.float64(5.0920321154990236e-05), 'behavior_loss': np.float64(0.4782221645116806)}

Episode step 35230, time diff 4.3217761516571045, total time dif 8613.741239309311)
step: 35230 @ episode report: {'average_total_reward': np.float32(9.475557), 'reward_variance': np.float32(1.2029089), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08013473711907863), 'actor_loss': np.float64(-0.9729130089282989), 'hyper_actor_loss': np.float64(4.881540735368617e-05), 'behavior_loss': np.float64(0.4749438941478729)}

Episode step 35240, time diff 4.302200794219971, total time dif 8618.063015460968)
step: 35240 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(1.9017975), 'max_total_reward': np.float32(13.022222), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07728345692157745), 'actor_loss': np.float64(-0.9945554554462432), 'hyper_actor_loss': np.float64(4.685627172875684e-05), 'behavior_loss': np.float64(0.469573438167572)}

Episode step 35250, time diff 4.301959276199341, total time dif 8622.365216255188)
step: 35250 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(2.515483), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0911524921655655), 'actor_loss': np.float64(-0.9971634745597839), 'hyper_actor_loss': np.float64(4.615890975401271e-05), 'behavior_loss': np.float64(0.4711994379758835)}

Episode step 35260, time diff 4.336343050003052, total time dif 8626.667175531387)
step: 35260 @ episode report: {'average_total_reward': np.float32(9.43889), 'reward_variance': np.float32(1.2751422), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09136349186301232), 'actor_loss': np.float64(-0.9975351512432098), 'hyper_actor_loss': np.float64(4.8475791845703495e-05), 'behavior_loss': np.float64(0.4733492583036423)}

Episode step 35270, time diff 4.484284162521362, total time dif 8631.00351858139)
step: 35270 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.5438089), 'max_total_reward': np.float32(10.900002), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0822403434664011), 'actor_loss': np.float64(-0.9898161232471466), 'hyper_actor_loss': np.float64(4.8557177069596944e-05), 'behavior_loss': np.float64(0.477531298995018)}

Episode step 35280, time diff 4.295922517776489, total time dif 8635.487802743912)
step: 35280 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(3.216617), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0908911269158125), 'actor_loss': np.float64(-0.9895290970802307), 'hyper_actor_loss': np.float64(4.923215892631561e-05), 'behavior_loss': np.float64(0.47812330424785615)}

Episode step 35290, time diff 4.26661491394043, total time dif 8639.783725261688)
step: 35290 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(1.6254867), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09065580144524574), 'actor_loss': np.float64(-0.9852205991744996), 'hyper_actor_loss': np.float64(4.732399211206939e-05), 'behavior_loss': np.float64(0.48305493891239165)}

Episode step 35300, time diff 4.296576499938965, total time dif 8644.050340175629)
step: 35300 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(2.0012603), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06847031228244305), 'actor_loss': np.float64(-0.9705303072929382), 'hyper_actor_loss': np.float64(5.020104581490159e-05), 'behavior_loss': np.float64(0.47199541628360747)}

Episode step 35310, time diff 4.343458652496338, total time dif 8648.346916675568)
step: 35310 @ episode report: {'average_total_reward': np.float32(9.002223), 'reward_variance': np.float32(3.6654027), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08425192162394524), 'actor_loss': np.float64(-0.9671403706073761), 'hyper_actor_loss': np.float64(5.0523892423370856e-05), 'behavior_loss': np.float64(0.4882683098316193)}

Episode step 35320, time diff 4.291228294372559, total time dif 8652.690375328064)
step: 35320 @ episode report: {'average_total_reward': np.float32(10.024445), 'reward_variance': np.float32(1.5558468), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08194161653518676), 'actor_loss': np.float64(-0.9739992141723632), 'hyper_actor_loss': np.float64(4.895313068118412e-05), 'behavior_loss': np.float64(0.484533479809761)}

Episode step 35330, time diff 4.268222332000732, total time dif 8656.981603622437)
step: 35330 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(1.8840606), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07973941452801228), 'actor_loss': np.float64(-0.9782215476036071), 'hyper_actor_loss': np.float64(4.7755035120644605e-05), 'behavior_loss': np.float64(0.47849767804145815)}

Episode step 35340, time diff 4.265591621398926, total time dif 8661.249825954437)
step: 35340 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(2.6251159), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07117166705429553), 'actor_loss': np.float64(-0.9698883295059204), 'hyper_actor_loss': np.float64(4.6946639486122876e-05), 'behavior_loss': np.float64(0.4790484458208084)}

Episode step 35350, time diff 4.302307844161987, total time dif 8665.515417575836)
step: 35350 @ episode report: {'average_total_reward': np.float32(8.863335), 'reward_variance': np.float32(2.8640509), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07871491350233555), 'actor_loss': np.float64(-0.9552656173706054), 'hyper_actor_loss': np.float64(4.74359421787085e-05), 'behavior_loss': np.float64(0.4761210411787033)}

Episode step 35360, time diff 4.501673698425293, total time dif 8669.817725419998)
step: 35360 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(4.5275073), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07805080637335778), 'actor_loss': np.float64(-0.9799430012702942), 'hyper_actor_loss': np.float64(4.7458966946578585e-05), 'behavior_loss': np.float64(0.47583172023296355)}

Episode step 35370, time diff 4.380470275878906, total time dif 8674.319399118423)
step: 35370 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(1.3062422), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06636118125170469), 'actor_loss': np.float64(-0.9690288722515106), 'hyper_actor_loss': np.float64(4.9792457502917387e-05), 'behavior_loss': np.float64(0.46865649819374083)}

Episode step 35380, time diff 4.450868368148804, total time dif 8678.699869394302)
step: 35380 @ episode report: {'average_total_reward': np.float32(8.626667), 'reward_variance': np.float32(2.3737087), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0828975223004818), 'actor_loss': np.float64(-0.9676360130310059), 'hyper_actor_loss': np.float64(4.839777866436634e-05), 'behavior_loss': np.float64(0.47110685110092165)}

Episode step 35390, time diff 4.420077323913574, total time dif 8683.150737762451)
step: 35390 @ episode report: {'average_total_reward': np.float32(9.326668), 'reward_variance': np.float32(1.5129678), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08615205027163028), 'actor_loss': np.float64(-1.011355209350586), 'hyper_actor_loss': np.float64(4.960144942742772e-05), 'behavior_loss': np.float64(0.48331031799316404)}

Episode step 35400, time diff 4.277876615524292, total time dif 8687.570815086365)
step: 35400 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(1.8890638), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08694013655185699), 'actor_loss': np.float64(-0.9972070693969727), 'hyper_actor_loss': np.float64(5.095101259939838e-05), 'behavior_loss': np.float64(0.48035457730293274)}

Episode step 35410, time diff 4.362519025802612, total time dif 8691.848691701889)
step: 35410 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(1.888909), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07550997585058213), 'actor_loss': np.float64(-0.9754703581333161), 'hyper_actor_loss': np.float64(4.95048429002054e-05), 'behavior_loss': np.float64(0.4837734252214432)}

Episode step 35420, time diff 4.382224082946777, total time dif 8696.211210727692)
step: 35420 @ episode report: {'average_total_reward': np.float32(8.253334), 'reward_variance': np.float32(2.940735), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09017806332558394), 'actor_loss': np.float64(-0.979697299003601), 'hyper_actor_loss': np.float64(5.291412635415327e-05), 'behavior_loss': np.float64(0.48064483404159547)}

Episode step 35430, time diff 4.4647486209869385, total time dif 8700.593434810638)
step: 35430 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(3.5643077), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.080106470733881), 'actor_loss': np.float64(-0.9805115759372711), 'hyper_actor_loss': np.float64(4.9110727195511573e-05), 'behavior_loss': np.float64(0.47737427055835724)}

Episode step 35440, time diff 4.247227191925049, total time dif 8705.058183431625)
step: 35440 @ episode report: {'average_total_reward': np.float32(8.851112), 'reward_variance': np.float32(2.325338), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08925077393651008), 'actor_loss': np.float64(-0.982142573595047), 'hyper_actor_loss': np.float64(5.0065833056578414e-05), 'behavior_loss': np.float64(0.48073925971984866)}

Episode step 35450, time diff 4.315852165222168, total time dif 8709.30541062355)
step: 35450 @ episode report: {'average_total_reward': np.float32(8.302223), 'reward_variance': np.float32(2.461428), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0795327540487051), 'actor_loss': np.float64(-0.9989462196826935), 'hyper_actor_loss': np.float64(4.967899112671148e-05), 'behavior_loss': np.float64(0.47726174890995027)}

Episode step 35460, time diff 4.328871250152588, total time dif 8713.621262788773)
step: 35460 @ episode report: {'average_total_reward': np.float32(8.428889), 'reward_variance': np.float32(0.86156094), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07532409876585007), 'actor_loss': np.float64(-0.9727148234844207), 'hyper_actor_loss': np.float64(4.873181060247589e-05), 'behavior_loss': np.float64(0.4694005072116852)}

Episode step 35470, time diff 4.9754958152771, total time dif 8717.950134038925)
step: 35470 @ episode report: {'average_total_reward': np.float32(9.575557), 'reward_variance': np.float32(1.0660689), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07692701406776906), 'actor_loss': np.float64(-0.9656894147396088), 'hyper_actor_loss': np.float64(5.130389181431383e-05), 'behavior_loss': np.float64(0.4803014576435089)}

Episode step 35480, time diff 4.6619713306427, total time dif 8722.925629854202)
step: 35480 @ episode report: {'average_total_reward': np.float32(8.43889), 'reward_variance': np.float32(2.2831912), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07916686944663524), 'actor_loss': np.float64(-0.9719741940498352), 'hyper_actor_loss': np.float64(5.047918784839567e-05), 'behavior_loss': np.float64(0.4869946211576462)}

Episode step 35490, time diff 4.7195470333099365, total time dif 8727.587601184845)
step: 35490 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(3.7673843), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08514780849218369), 'actor_loss': np.float64(-0.9786803960800171), 'hyper_actor_loss': np.float64(5.093686195323244e-05), 'behavior_loss': np.float64(0.4759844899177551)}

Episode step 35500, time diff 4.598421573638916, total time dif 8732.307148218155)
step: 35500 @ episode report: {'average_total_reward': np.float32(8.875557), 'reward_variance': np.float32(4.9636016), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08235075399279594), 'actor_loss': np.float64(-0.9817332565784455), 'hyper_actor_loss': np.float64(4.7975406050682065e-05), 'behavior_loss': np.float64(0.4776454985141754)}

Episode step 35510, time diff 4.710772275924683, total time dif 8736.905569791794)
step: 35510 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(4.779685), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07276759296655655), 'actor_loss': np.float64(-0.9807730555534363), 'hyper_actor_loss': np.float64(5.000313431082759e-05), 'behavior_loss': np.float64(0.4726355105638504)}

Episode step 35520, time diff 4.516354322433472, total time dif 8741.616342067719)
step: 35520 @ episode report: {'average_total_reward': np.float32(8.577779), 'reward_variance': np.float32(1.5430622), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06743543222546577), 'actor_loss': np.float64(-0.9661659240722656), 'hyper_actor_loss': np.float64(4.9819311243481934e-05), 'behavior_loss': np.float64(0.4747521489858627)}

Episode step 35530, time diff 4.429448127746582, total time dif 8746.132696390152)
step: 35530 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(5.025655), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08720383979380131), 'actor_loss': np.float64(-0.9748480498790741), 'hyper_actor_loss': np.float64(4.775750421686098e-05), 'behavior_loss': np.float64(0.4814945787191391)}

Episode step 35540, time diff 4.367319822311401, total time dif 8750.562144517899)
step: 35540 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(1.2607605), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07268517278134823), 'actor_loss': np.float64(-0.9768498301506042), 'hyper_actor_loss': np.float64(4.9832951845019124e-05), 'behavior_loss': np.float64(0.47743867337703705)}

Episode step 35550, time diff 4.348802328109741, total time dif 8754.92946434021)
step: 35550 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.4657826), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09781062081456185), 'actor_loss': np.float64(-0.9807865977287292), 'hyper_actor_loss': np.float64(4.74100645078579e-05), 'behavior_loss': np.float64(0.4799380421638489)}

Episode step 35560, time diff 4.425220727920532, total time dif 8759.27826666832)
step: 35560 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(1.9761232), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08389162719249725), 'actor_loss': np.float64(-0.9900371432304382), 'hyper_actor_loss': np.float64(4.8081357817864046e-05), 'behavior_loss': np.float64(0.4783776462078094)}

Episode step 35570, time diff 4.3621296882629395, total time dif 8763.70348739624)
step: 35570 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.713304), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08470752090215683), 'actor_loss': np.float64(-0.9739164054393769), 'hyper_actor_loss': np.float64(4.653325268009212e-05), 'behavior_loss': np.float64(0.4796935677528381)}

Episode step 35580, time diff 4.648916244506836, total time dif 8768.065617084503)
step: 35580 @ episode report: {'average_total_reward': np.float32(8.889999), 'reward_variance': np.float32(1.35127), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07750327363610268), 'actor_loss': np.float64(-0.9942644119262696), 'hyper_actor_loss': np.float64(4.667787507059984e-05), 'behavior_loss': np.float64(0.4723014771938324)}

Episode step 35590, time diff 4.7497429847717285, total time dif 8772.71453332901)
step: 35590 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(3.570174), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09575619027018548), 'actor_loss': np.float64(-0.9853383123874664), 'hyper_actor_loss': np.float64(4.570927085296716e-05), 'behavior_loss': np.float64(0.48922672867774963)}

Episode step 35600, time diff 5.285142660140991, total time dif 8777.464276313782)
step: 35600 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(5.0674524), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07874312326312065), 'actor_loss': np.float64(-0.9766553640365601), 'hyper_actor_loss': np.float64(4.60394388937857e-05), 'behavior_loss': np.float64(0.4795846790075302)}

Episode step 35610, time diff 4.796392202377319, total time dif 8782.749418973923)
step: 35610 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.3080013), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08216766826808453), 'actor_loss': np.float64(-0.9688621759414673), 'hyper_actor_loss': np.float64(4.804682394023985e-05), 'behavior_loss': np.float64(0.46868679821491244)}

Episode step 35620, time diff 4.521048545837402, total time dif 8787.5458111763)
step: 35620 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(4.082396), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07940807342529296), 'actor_loss': np.float64(-0.9986634433269501), 'hyper_actor_loss': np.float64(4.80515696835937e-05), 'behavior_loss': np.float64(0.4793799787759781)}

Episode step 35630, time diff 4.611840724945068, total time dif 8792.066859722137)
step: 35630 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(2.8441746), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07692053727805614), 'actor_loss': np.float64(-0.9815014421939849), 'hyper_actor_loss': np.float64(4.435829287103843e-05), 'behavior_loss': np.float64(0.476293671131134)}

Episode step 35640, time diff 4.408398151397705, total time dif 8796.678700447083)
step: 35640 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(1.5183811), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06566852405667305), 'actor_loss': np.float64(-0.9641373634338379), 'hyper_actor_loss': np.float64(4.6864207251928744e-05), 'behavior_loss': np.float64(0.4743414044380188)}

Episode step 35650, time diff 4.405273199081421, total time dif 8801.08709859848)
step: 35650 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(2.387311), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0821726679801941), 'actor_loss': np.float64(-0.9797085106372834), 'hyper_actor_loss': np.float64(4.581093890010379e-05), 'behavior_loss': np.float64(0.47912766337394713)}

Episode step 35660, time diff 4.415830612182617, total time dif 8805.492371797562)
step: 35660 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(2.676889), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08907449841499329), 'actor_loss': np.float64(-0.9824248373508453), 'hyper_actor_loss': np.float64(4.686156389652751e-05), 'behavior_loss': np.float64(0.47353461384773254)}

Episode step 35670, time diff 4.360337734222412, total time dif 8809.908202409744)
step: 35670 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(2.9656553), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08776698969304561), 'actor_loss': np.float64(-1.0149996399879455), 'hyper_actor_loss': np.float64(4.3722157352021894e-05), 'behavior_loss': np.float64(0.4717836558818817)}

Episode step 35680, time diff 4.35701322555542, total time dif 8814.268540143967)
step: 35680 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(1.3332102), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07576447501778602), 'actor_loss': np.float64(-0.9857222974300385), 'hyper_actor_loss': np.float64(4.573390360746998e-05), 'behavior_loss': np.float64(0.4741418868303299)}

Episode step 35690, time diff 4.386707544326782, total time dif 8818.625553369522)
step: 35690 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(3.2167478), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08113711141049862), 'actor_loss': np.float64(-0.958721923828125), 'hyper_actor_loss': np.float64(4.7217142855515705e-05), 'behavior_loss': np.float64(0.4735124856233597)}

Episode step 35700, time diff 4.391857147216797, total time dif 8823.012260913849)
step: 35700 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(4.154432), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06952265948057175), 'actor_loss': np.float64(-0.9766118466854096), 'hyper_actor_loss': np.float64(4.5070783016853966e-05), 'behavior_loss': np.float64(0.47030138671398164)}

Episode step 35710, time diff 4.387299060821533, total time dif 8827.404118061066)
step: 35710 @ episode report: {'average_total_reward': np.float32(9.114445), 'reward_variance': np.float32(7.2192373), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08468187339603901), 'actor_loss': np.float64(-0.9761761069297791), 'hyper_actor_loss': np.float64(4.3386558536440135e-05), 'behavior_loss': np.float64(0.4754402548074722)}

Episode step 35720, time diff 4.3389856815338135, total time dif 8831.791417121887)
step: 35720 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(2.8988447), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08214728087186814), 'actor_loss': np.float64(-0.9870758950710297), 'hyper_actor_loss': np.float64(4.431838096934371e-05), 'behavior_loss': np.float64(0.48164488971233366)}

Episode step 35730, time diff 4.407629013061523, total time dif 8836.130402803421)
step: 35730 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(1.8948847), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08391097858548165), 'actor_loss': np.float64(-0.9748747050762177), 'hyper_actor_loss': np.float64(4.4481527220341376e-05), 'behavior_loss': np.float64(0.4852091908454895)}

Episode step 35740, time diff 4.383404493331909, total time dif 8840.538031816483)
step: 35740 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(1.9911658), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0708786839619279), 'actor_loss': np.float64(-0.9517986655235291), 'hyper_actor_loss': np.float64(4.576721876219381e-05), 'behavior_loss': np.float64(0.4748424172401428)}

Episode step 35750, time diff 4.335524559020996, total time dif 8844.921436309814)
step: 35750 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.272464), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08167238608002662), 'actor_loss': np.float64(-0.9594480812549591), 'hyper_actor_loss': np.float64(4.2865035720751624e-05), 'behavior_loss': np.float64(0.4717540264129639)}

Episode step 35760, time diff 4.5428078174591064, total time dif 8849.256960868835)
step: 35760 @ episode report: {'average_total_reward': np.float32(9.012222), 'reward_variance': np.float32(4.937394), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08131727129220963), 'actor_loss': np.float64(-1.0073727309703826), 'hyper_actor_loss': np.float64(4.215715016471222e-05), 'behavior_loss': np.float64(0.47438118755817416)}

Episode step 35770, time diff 4.423861980438232, total time dif 8853.799768686295)
step: 35770 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(4.733389), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07463728114962578), 'actor_loss': np.float64(-0.9837169647216797), 'hyper_actor_loss': np.float64(4.259790694050025e-05), 'behavior_loss': np.float64(0.47141177356243136)}

Episode step 35780, time diff 4.3670814037323, total time dif 8858.223630666733)
step: 35780 @ episode report: {'average_total_reward': np.float32(9.9), 'reward_variance': np.float32(4.9232836), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07554048597812653), 'actor_loss': np.float64(-0.9633470892906189), 'hyper_actor_loss': np.float64(4.235254345985595e-05), 'behavior_loss': np.float64(0.4725992202758789)}

Episode step 35790, time diff 4.3561553955078125, total time dif 8862.590712070465)
step: 35790 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(4.1988215), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07584381587803364), 'actor_loss': np.float64(-0.9778109192848206), 'hyper_actor_loss': np.float64(4.1132073965854944e-05), 'behavior_loss': np.float64(0.47310799062252046)}

Episode step 35800, time diff 4.336498975753784, total time dif 8866.946867465973)
step: 35800 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(1.664464), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0880692683160305), 'actor_loss': np.float64(-0.9837786495685578), 'hyper_actor_loss': np.float64(4.322227978263982e-05), 'behavior_loss': np.float64(0.4741644024848938)}

Episode step 35810, time diff 4.389515161514282, total time dif 8871.283366441727)
step: 35810 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(4.640094), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09311013296246529), 'actor_loss': np.float64(-0.9954755008220673), 'hyper_actor_loss': np.float64(4.405909603519831e-05), 'behavior_loss': np.float64(0.4771305531263351)}

Episode step 35820, time diff 4.360744953155518, total time dif 8875.672881603241)
step: 35820 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(2.1813102), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09164547100663185), 'actor_loss': np.float64(-1.0088057160377502), 'hyper_actor_loss': np.float64(4.215551962261088e-05), 'behavior_loss': np.float64(0.47844484746456145)}

Episode step 35830, time diff 4.352557897567749, total time dif 8880.033626556396)
step: 35830 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(4.7413945), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07615116909146309), 'actor_loss': np.float64(-0.9841412603855133), 'hyper_actor_loss': np.float64(4.42863478383515e-05), 'behavior_loss': np.float64(0.4684131950139999)}

Episode step 35840, time diff 4.331099510192871, total time dif 8884.386184453964)
step: 35840 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.532248), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07728752717375756), 'actor_loss': np.float64(-0.9917377710342408), 'hyper_actor_loss': np.float64(4.528290228336118e-05), 'behavior_loss': np.float64(0.47296114563941954)}

Episode step 35850, time diff 4.339024782180786, total time dif 8888.717283964157)
step: 35850 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.3526866), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0865145243704319), 'actor_loss': np.float64(-0.9835316598415375), 'hyper_actor_loss': np.float64(4.0871665260056036e-05), 'behavior_loss': np.float64(0.47910878956317904)}

Episode step 35860, time diff 4.396010637283325, total time dif 8893.056308746338)
step: 35860 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(3.7135177), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0840518020093441), 'actor_loss': np.float64(-0.970185399055481), 'hyper_actor_loss': np.float64(4.08115181926405e-05), 'behavior_loss': np.float64(0.47680815756320954)}

Episode step 35870, time diff 4.415248870849609, total time dif 8897.452319383621)
step: 35870 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.9020257), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07359283715486527), 'actor_loss': np.float64(-0.9776690840721131), 'hyper_actor_loss': np.float64(4.267923650331795e-05), 'behavior_loss': np.float64(0.4691583693027496)}

Episode step 35880, time diff 4.500710487365723, total time dif 8901.86756825447)
step: 35880 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(1.3762335), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0717784445732832), 'actor_loss': np.float64(-0.9796190023422241), 'hyper_actor_loss': np.float64(4.091041409992613e-05), 'behavior_loss': np.float64(0.47469022274017336)}

Episode step 35890, time diff 4.423909902572632, total time dif 8906.368278741837)
step: 35890 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(2.4215171), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08880984038114548), 'actor_loss': np.float64(-0.9705370306968689), 'hyper_actor_loss': np.float64(4.160613680141978e-05), 'behavior_loss': np.float64(0.47512598633766173)}

Episode step 35900, time diff 4.371086359024048, total time dif 8910.79218864441)
step: 35900 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.9467957), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06884138863533736), 'actor_loss': np.float64(-0.9646868765354156), 'hyper_actor_loss': np.float64(4.032358483527787e-05), 'behavior_loss': np.float64(0.46769734025001525)}

Episode step 35910, time diff 4.38668155670166, total time dif 8915.163275003433)
step: 35910 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(1.9513111), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07623308189213276), 'actor_loss': np.float64(-0.9683691620826721), 'hyper_actor_loss': np.float64(3.937849978683516e-05), 'behavior_loss': np.float64(0.4726182699203491)}

Episode step 35920, time diff 4.509256601333618, total time dif 8919.549956560135)
step: 35920 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(1.521246), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09140127710998058), 'actor_loss': np.float64(-0.9855915188789368), 'hyper_actor_loss': np.float64(3.8956836942816156e-05), 'behavior_loss': np.float64(0.4756691366434097)}

Episode step 35930, time diff 4.643377780914307, total time dif 8924.059213161469)
step: 35930 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(1.056001), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07523038052022457), 'actor_loss': np.float64(-0.9827329576015472), 'hyper_actor_loss': np.float64(4.018852487206459e-05), 'behavior_loss': np.float64(0.4696629077196121)}

Episode step 35940, time diff 4.49038028717041, total time dif 8928.702590942383)
step: 35940 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(1.5400016), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08025062866508961), 'actor_loss': np.float64(-0.9656445443630218), 'hyper_actor_loss': np.float64(3.920080271200277e-05), 'behavior_loss': np.float64(0.4685471445322037)}

Episode step 35950, time diff 4.440819978713989, total time dif 8933.192971229553)
step: 35950 @ episode report: {'average_total_reward': np.float32(9.8977785), 'reward_variance': np.float32(1.6232055), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08283965513110161), 'actor_loss': np.float64(-0.9910272359848022), 'hyper_actor_loss': np.float64(3.72657068510307e-05), 'behavior_loss': np.float64(0.46660600900650023)}

Episode step 35960, time diff 4.527631044387817, total time dif 8937.633791208267)
step: 35960 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(3.0697987), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07982646971940995), 'actor_loss': np.float64(-0.9854318678379059), 'hyper_actor_loss': np.float64(3.841409634333104e-05), 'behavior_loss': np.float64(0.4697455555200577)}

Episode step 35970, time diff 4.499342203140259, total time dif 8942.161422252655)
step: 35970 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(0.8281592), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07499200142920018), 'actor_loss': np.float64(-0.9761561095714569), 'hyper_actor_loss': np.float64(3.9605506754014644e-05), 'behavior_loss': np.float64(0.47236433923244475)}

Episode step 35980, time diff 4.399033784866333, total time dif 8946.660764455795)
step: 35980 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(1.6506538), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08456303179264069), 'actor_loss': np.float64(-0.9627317249774933), 'hyper_actor_loss': np.float64(3.9896051384857854e-05), 'behavior_loss': np.float64(0.47227378487586974)}

Episode step 35990, time diff 4.3817853927612305, total time dif 8951.059798240662)
step: 35990 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.249952), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0765667088329792), 'actor_loss': np.float64(-0.984984177350998), 'hyper_actor_loss': np.float64(4.025247872050386e-05), 'behavior_loss': np.float64(0.4667680740356445)}

Episode step 36000, time diff 4.39261531829834, total time dif 8955.441583633423)
step: 36000 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(0.8784149), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08155789263546467), 'actor_loss': np.float64(-0.9882285296916962), 'hyper_actor_loss': np.float64(4.014179357909597e-05), 'behavior_loss': np.float64(0.4698210805654526)}

Episode step 36010, time diff 4.47028660774231, total time dif 8959.834198951721)
step: 36010 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(1.2569093), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07189957648515702), 'actor_loss': np.float64(-0.9653312921524048), 'hyper_actor_loss': np.float64(4.114816219953354e-05), 'behavior_loss': np.float64(0.4711132287979126)}

Episode step 36020, time diff 4.757135391235352, total time dif 8964.304485559464)
step: 36020 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(2.620909), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08372893705964088), 'actor_loss': np.float64(-0.9695155560970307), 'hyper_actor_loss': np.float64(4.235664709995035e-05), 'behavior_loss': np.float64(0.4670180916786194)}

Episode step 36030, time diff 4.6970484256744385, total time dif 8969.061620950699)
step: 36030 @ episode report: {'average_total_reward': np.float32(11.595556), 'reward_variance': np.float32(1.1523502), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.655556), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06956455279141664), 'actor_loss': np.float64(-0.972079473733902), 'hyper_actor_loss': np.float64(4.370516035123728e-05), 'behavior_loss': np.float64(0.46528683602809906)}

Episode step 36040, time diff 4.595259428024292, total time dif 8973.758669376373)
step: 36040 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(4.326422), 'max_total_reward': np.float32(14.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06566046308726073), 'actor_loss': np.float64(-0.9651761054992676), 'hyper_actor_loss': np.float64(4.6921286775614134e-05), 'behavior_loss': np.float64(0.457187220454216)}

Episode step 36050, time diff 4.3513171672821045, total time dif 8978.353928804398)
step: 36050 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(1.7243166), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07797523476183414), 'actor_loss': np.float64(-0.9630687177181244), 'hyper_actor_loss': np.float64(4.8108766350196674e-05), 'behavior_loss': np.float64(0.47973716259002686)}

Episode step 36060, time diff 4.467479228973389, total time dif 8982.70524597168)
step: 36060 @ episode report: {'average_total_reward': np.float32(10.683334), 'reward_variance': np.float32(1.7490187), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0849093295633793), 'actor_loss': np.float64(-0.9694135844707489), 'hyper_actor_loss': np.float64(4.915945937682409e-05), 'behavior_loss': np.float64(0.46948507726192473)}

Episode step 36070, time diff 4.530027151107788, total time dif 8987.172725200653)
step: 36070 @ episode report: {'average_total_reward': np.float32(11.532224), 'reward_variance': np.float32(0.5334187), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(10.9), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0770306047052145), 'actor_loss': np.float64(-0.9948693335056304), 'hyper_actor_loss': np.float64(5.007659856346436e-05), 'behavior_loss': np.float64(0.45839773416519164)}

Episode step 36080, time diff 4.49303412437439, total time dif 8991.70275235176)
step: 36080 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(1.1319124), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08534122593700885), 'actor_loss': np.float64(-0.9852989792823792), 'hyper_actor_loss': np.float64(5.0169851601822304e-05), 'behavior_loss': np.float64(0.46435060501098635)}

Episode step 36090, time diff 4.444295883178711, total time dif 8996.195786476135)
step: 36090 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.1334336), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07874711751937866), 'actor_loss': np.float64(-0.9809157907962799), 'hyper_actor_loss': np.float64(5.1329196503502315e-05), 'behavior_loss': np.float64(0.4603100001811981)}

Episode step 36100, time diff 4.525783061981201, total time dif 9000.640082359314)
step: 36100 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(1.3053589), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07401675246655941), 'actor_loss': np.float64(-0.9771236121654511), 'hyper_actor_loss': np.float64(5.194366713112686e-05), 'behavior_loss': np.float64(0.46242772936820986)}

Episode step 36110, time diff 4.366010665893555, total time dif 9005.165865421295)
step: 36110 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(3.3752334), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08285847827792167), 'actor_loss': np.float64(-0.979009598493576), 'hyper_actor_loss': np.float64(5.454059355542995e-05), 'behavior_loss': np.float64(0.4664777249097824)}

Episode step 36120, time diff 4.336628198623657, total time dif 9009.531876087189)
step: 36120 @ episode report: {'average_total_reward': np.float32(11.620001), 'reward_variance': np.float32(1.8185126), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08306380771100522), 'actor_loss': np.float64(-0.9893939435482025), 'hyper_actor_loss': np.float64(5.6146722272387706e-05), 'behavior_loss': np.float64(0.46336140036582946)}

Episode step 36130, time diff 4.377178192138672, total time dif 9013.868504285812)
step: 36130 @ episode report: {'average_total_reward': np.float32(12.317779), 'reward_variance': np.float32(1.6694868), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(11.0222225), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07985718324780464), 'actor_loss': np.float64(-0.9796053349971772), 'hyper_actor_loss': np.float64(5.843191720487084e-05), 'behavior_loss': np.float64(0.4613566011190414)}

Episode step 36140, time diff 4.3699116706848145, total time dif 9018.245682477951)
step: 36140 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(2.370347), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07311746180057525), 'actor_loss': np.float64(-0.9735481679439545), 'hyper_actor_loss': np.float64(6.083441730879713e-05), 'behavior_loss': np.float64(0.46490115821361544)}

Episode step 36150, time diff 4.381515264511108, total time dif 9022.615594148636)
step: 36150 @ episode report: {'average_total_reward': np.float32(11.320001), 'reward_variance': np.float32(2.6664882), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07782684862613679), 'actor_loss': np.float64(-0.9740433394908905), 'hyper_actor_loss': np.float64(6.428922315535602e-05), 'behavior_loss': np.float64(0.46304520070552824)}

Episode step 36160, time diff 4.516204357147217, total time dif 9026.997109413147)
step: 36160 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(3.3153884), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0879271648824215), 'actor_loss': np.float64(-0.9796177446842194), 'hyper_actor_loss': np.float64(6.55355179333128e-05), 'behavior_loss': np.float64(0.4625012457370758)}

Episode step 36170, time diff 4.463502407073975, total time dif 9031.513313770294)
step: 36170 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.2374585), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08982664309442043), 'actor_loss': np.float64(-1.0058012664318086), 'hyper_actor_loss': np.float64(6.439065909944475e-05), 'behavior_loss': np.float64(0.46661790907382966)}

Episode step 36180, time diff 4.430539846420288, total time dif 9035.976816177368)
step: 36180 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(7.5503283), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08103595413267613), 'actor_loss': np.float64(-0.9727578282356262), 'hyper_actor_loss': np.float64(6.561153968505095e-05), 'behavior_loss': np.float64(0.45801042914390566)}

Episode step 36190, time diff 4.5202720165252686, total time dif 9040.407356023788)
step: 36190 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.2176251), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07392702661454678), 'actor_loss': np.float64(-0.9639357507228852), 'hyper_actor_loss': np.float64(6.1021209694445135e-05), 'behavior_loss': np.float64(0.45797693729400635)}

Episode step 36200, time diff 4.447309494018555, total time dif 9044.927628040314)
step: 36200 @ episode report: {'average_total_reward': np.float32(11.556667), 'reward_variance': np.float32(4.4311724), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07788756638765335), 'actor_loss': np.float64(-0.9898523926734925), 'hyper_actor_loss': np.float64(6.304282978817355e-05), 'behavior_loss': np.float64(0.4590201139450073)}

Episode step 36210, time diff 4.40961217880249, total time dif 9049.374937534332)
step: 36210 @ episode report: {'average_total_reward': np.float32(10.846666), 'reward_variance': np.float32(5.1832542), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09519822970032692), 'actor_loss': np.float64(-0.9958210468292237), 'hyper_actor_loss': np.float64(5.960068847343791e-05), 'behavior_loss': np.float64(0.46647204756736754)}

Episode step 36220, time diff 4.460181713104248, total time dif 9053.784549713135)
step: 36220 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(3.5124002), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07300308346748352), 'actor_loss': np.float64(-0.9840292036533356), 'hyper_actor_loss': np.float64(5.689171739504672e-05), 'behavior_loss': np.float64(0.45121189653873445)}

Episode step 36230, time diff 4.419197082519531, total time dif 9058.244731426239)
step: 36230 @ episode report: {'average_total_reward': np.float32(10.871111), 'reward_variance': np.float32(2.5323257), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08648527264595032), 'actor_loss': np.float64(-0.9679914057254791), 'hyper_actor_loss': np.float64(5.838039178343024e-05), 'behavior_loss': np.float64(0.4606997460126877)}

Episode step 36240, time diff 4.522450923919678, total time dif 9062.663928508759)
step: 36240 @ episode report: {'average_total_reward': np.float32(11.444446), 'reward_variance': np.float32(1.693828), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07764554917812347), 'actor_loss': np.float64(-0.9834051370620728), 'hyper_actor_loss': np.float64(5.880915377929341e-05), 'behavior_loss': np.float64(0.45505093932151797)}

Episode step 36250, time diff 4.4731292724609375, total time dif 9067.186379432678)
step: 36250 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(1.7143967), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08021921142935753), 'actor_loss': np.float64(-0.9817389249801636), 'hyper_actor_loss': np.float64(5.988411394355353e-05), 'behavior_loss': np.float64(0.45910861194133756)}

Episode step 36260, time diff 4.593781232833862, total time dif 9071.65950870514)
step: 36260 @ episode report: {'average_total_reward': np.float32(11.183333), 'reward_variance': np.float32(4.58287), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07533106878399849), 'actor_loss': np.float64(-0.9839873731136322), 'hyper_actor_loss': np.float64(6.21742794464808e-05), 'behavior_loss': np.float64(0.4528979778289795)}

Episode step 36270, time diff 4.625837802886963, total time dif 9076.253289937973)
step: 36270 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(1.0972893), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0705886997282505), 'actor_loss': np.float64(-0.9769699573516846), 'hyper_actor_loss': np.float64(7.360062882071361e-05), 'behavior_loss': np.float64(0.45393050014972686)}

Episode step 36280, time diff 4.392313241958618, total time dif 9080.87912774086)
step: 36280 @ episode report: {'average_total_reward': np.float32(11.2322235), 'reward_variance': np.float32(3.1437407), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08386551626026631), 'actor_loss': np.float64(-0.9713550984859467), 'hyper_actor_loss': np.float64(7.653452412341722e-05), 'behavior_loss': np.float64(0.45629875361919403)}

Episode step 36290, time diff 4.514204263687134, total time dif 9085.271440982819)
step: 36290 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(3.7383733), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07959285154938697), 'actor_loss': np.float64(-0.9872223913669587), 'hyper_actor_loss': np.float64(8.884134658728726e-05), 'behavior_loss': np.float64(0.4598040610551834)}

Episode step 36300, time diff 4.752880811691284, total time dif 9089.785645246506)
step: 36300 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(1.4176098), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06147449649870396), 'actor_loss': np.float64(-0.972221851348877), 'hyper_actor_loss': np.float64(0.00010905916933552362), 'behavior_loss': np.float64(0.45106695890426635)}

Episode step 36310, time diff 4.41918683052063, total time dif 9094.538526058197)
step: 36310 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(3.836514), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07784660309553146), 'actor_loss': np.float64(-0.9646072506904602), 'hyper_actor_loss': np.float64(0.00012838259353884496), 'behavior_loss': np.float64(0.4598450005054474)}

Episode step 36320, time diff 4.366166591644287, total time dif 9098.957712888718)
step: 36320 @ episode report: {'average_total_reward': np.float32(10.697779), 'reward_variance': np.float32(7.168958), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07702491506934166), 'actor_loss': np.float64(-0.9882787942886353), 'hyper_actor_loss': np.float64(0.00016187442088266835), 'behavior_loss': np.float64(0.4633555680513382)}

Episode step 36330, time diff 4.328657627105713, total time dif 9103.323879480362)
step: 36330 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(1.3578781), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08018135279417038), 'actor_loss': np.float64(-0.9836678087711335), 'hyper_actor_loss': np.float64(0.00021308904979377985), 'behavior_loss': np.float64(0.46474029719829557)}

Episode step 36340, time diff 4.260600328445435, total time dif 9107.652537107468)
step: 36340 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(1.0191967), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08015135899186135), 'actor_loss': np.float64(-0.9916928052902222), 'hyper_actor_loss': np.float64(0.0003184940083883703), 'behavior_loss': np.float64(0.47157482206821444)}

Episode step 36350, time diff 4.309500455856323, total time dif 9111.913137435913)
step: 36350 @ episode report: {'average_total_reward': np.float32(11.383334), 'reward_variance': np.float32(0.56311756), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08087475560605525), 'actor_loss': np.float64(-0.9858311533927917), 'hyper_actor_loss': np.float64(0.0005132232879986986), 'behavior_loss': np.float64(0.4747136801481247)}

Episode step 36360, time diff 4.419086694717407, total time dif 9116.22263789177)
step: 36360 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(4.94248), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08335445635020733), 'actor_loss': np.float64(-0.9747398674488068), 'hyper_actor_loss': np.float64(0.0006242800387553871), 'behavior_loss': np.float64(0.4826229989528656)}

Episode step 36370, time diff 4.479494571685791, total time dif 9120.641724586487)
step: 36370 @ episode report: {'average_total_reward': np.float32(11.681112), 'reward_variance': np.float32(3.2848907), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07657575383782386), 'actor_loss': np.float64(-1.0010220408439636), 'hyper_actor_loss': np.float64(0.0006392275332473219), 'behavior_loss': np.float64(0.476791125535965)}

Episode step 36380, time diff 4.408049821853638, total time dif 9125.121219158173)
step: 36380 @ episode report: {'average_total_reward': np.float32(11.444445), 'reward_variance': np.float32(2.5896056), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07989432625472545), 'actor_loss': np.float64(-0.993179851770401), 'hyper_actor_loss': np.float64(0.0005132524092914537), 'behavior_loss': np.float64(0.4757328301668167)}

Episode step 36390, time diff 4.439868688583374, total time dif 9129.529268980026)
step: 36390 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(1.8723705), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07711713351309299), 'actor_loss': np.float64(-0.9597536087036133), 'hyper_actor_loss': np.float64(0.0003756834514206275), 'behavior_loss': np.float64(0.47521257400512695)}

Episode step 36400, time diff 4.438840627670288, total time dif 9133.96913766861)
step: 36400 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(2.5111809), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08015613816678524), 'actor_loss': np.float64(-0.9757684767246246), 'hyper_actor_loss': np.float64(0.00023540105030406266), 'behavior_loss': np.float64(0.45592209994792937)}

Episode step 36410, time diff 4.400611877441406, total time dif 9138.40797829628)
step: 36410 @ episode report: {'average_total_reward': np.float32(11.756668), 'reward_variance': np.float32(1.486061), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08183700069785119), 'actor_loss': np.float64(-1.0016678750514985), 'hyper_actor_loss': np.float64(0.00015027647459646686), 'behavior_loss': np.float64(0.46514259576797484)}

Episode step 36420, time diff 4.379330396652222, total time dif 9142.808590173721)
step: 36420 @ episode report: {'average_total_reward': np.float32(10.036669), 'reward_variance': np.float32(2.7417305), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07485532015562057), 'actor_loss': np.float64(-0.9670848846435547), 'hyper_actor_loss': np.float64(0.00012625756426132284), 'behavior_loss': np.float64(0.45918814837932587)}

Episode step 36430, time diff 4.357395648956299, total time dif 9147.187920570374)
step: 36430 @ episode report: {'average_total_reward': np.float32(11.456667), 'reward_variance': np.float32(2.6080613), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07263111267238856), 'actor_loss': np.float64(-0.9609714269638061), 'hyper_actor_loss': np.float64(0.00011814355020760559), 'behavior_loss': np.float64(0.4542302280664444)}

Episode step 36440, time diff 4.604863166809082, total time dif 9151.54531621933)
step: 36440 @ episode report: {'average_total_reward': np.float32(11.358889), 'reward_variance': np.float32(3.583236), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08481642901897431), 'actor_loss': np.float64(-0.9915950179100037), 'hyper_actor_loss': np.float64(0.00011987439793301747), 'behavior_loss': np.float64(0.45265149474143984)}

Episode step 36450, time diff 4.39925217628479, total time dif 9156.150179386139)
step: 36450 @ episode report: {'average_total_reward': np.float32(11.781113), 'reward_variance': np.float32(2.854076), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08117686212062836), 'actor_loss': np.float64(-0.9945077002048492), 'hyper_actor_loss': np.float64(0.00011502490087877959), 'behavior_loss': np.float64(0.4548203319311142)}

Episode step 36460, time diff 4.394002676010132, total time dif 9160.549431562424)
step: 36460 @ episode report: {'average_total_reward': np.float32(11.020001), 'reward_variance': np.float32(7.928093), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07700086496770382), 'actor_loss': np.float64(-0.9706081986427307), 'hyper_actor_loss': np.float64(0.00010651473421603441), 'behavior_loss': np.float64(0.45726566314697265)}

Episode step 36470, time diff 4.348041772842407, total time dif 9164.943434238434)
step: 36470 @ episode report: {'average_total_reward': np.float32(12.342222), 'reward_variance': np.float32(5.2525144), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08185029774904251), 'actor_loss': np.float64(-0.9711807668209076), 'hyper_actor_loss': np.float64(0.00010512116205063649), 'behavior_loss': np.float64(0.45580879747867586)}

Episode step 36480, time diff 4.38477087020874, total time dif 9169.291476011276)
step: 36480 @ episode report: {'average_total_reward': np.float32(11.568891), 'reward_variance': np.float32(2.3141675), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07441716529428959), 'actor_loss': np.float64(-0.9829633355140686), 'hyper_actor_loss': np.float64(9.851453505689278e-05), 'behavior_loss': np.float64(0.45879330337047575)}

Episode step 36490, time diff 4.372419834136963, total time dif 9173.676246881485)
step: 36490 @ episode report: {'average_total_reward': np.float32(11.46889), 'reward_variance': np.float32(2.9623902), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07358611300587654), 'actor_loss': np.float64(-0.9772194206714631), 'hyper_actor_loss': np.float64(9.935452180798166e-05), 'behavior_loss': np.float64(0.4560712605714798)}

Episode step 36500, time diff 4.342068433761597, total time dif 9178.048666715622)
step: 36500 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(1.6986921), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07327081821858883), 'actor_loss': np.float64(-0.9712316632270813), 'hyper_actor_loss': np.float64(0.00010260308554279617), 'behavior_loss': np.float64(0.45434264838695526)}

Episode step 36510, time diff 4.432262897491455, total time dif 9182.390735149384)
step: 36510 @ episode report: {'average_total_reward': np.float32(11.132223), 'reward_variance': np.float32(1.5126042), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07680404670536518), 'actor_loss': np.float64(-0.9645200729370117), 'hyper_actor_loss': np.float64(9.709931546240114e-05), 'behavior_loss': np.float64(0.4485007733106613)}

Episode step 36520, time diff 4.39073371887207, total time dif 9186.822998046875)
step: 36520 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.9013333), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09246355220675469), 'actor_loss': np.float64(-0.9963879466056824), 'hyper_actor_loss': np.float64(9.877983102342114e-05), 'behavior_loss': np.float64(0.4544577419757843)}

Episode step 36530, time diff 4.430877685546875, total time dif 9191.213731765747)
step: 36530 @ episode report: {'average_total_reward': np.float32(11.097777), 'reward_variance': np.float32(4.552737), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0807630654424429), 'actor_loss': np.float64(-1.0083499908447267), 'hyper_actor_loss': np.float64(8.852267419570125e-05), 'behavior_loss': np.float64(0.44577219486236574)}

Episode step 36540, time diff 4.449014663696289, total time dif 9195.644609451294)
step: 36540 @ episode report: {'average_total_reward': np.float32(11.1466675), 'reward_variance': np.float32(2.079057), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07445297129452229), 'actor_loss': np.float64(-0.9749756693840027), 'hyper_actor_loss': np.float64(9.433097948203794e-05), 'behavior_loss': np.float64(0.44719848930835726)}

Episode step 36550, time diff 4.4523138999938965, total time dif 9200.09362411499)
step: 36550 @ episode report: {'average_total_reward': np.float32(10.673334), 'reward_variance': np.float32(1.5154378), 'max_total_reward': np.float32(14.022223), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0906037762761116), 'actor_loss': np.float64(-0.989593905210495), 'hyper_actor_loss': np.float64(0.00010488592670299113), 'behavior_loss': np.float64(0.4533373087644577)}

Episode step 36560, time diff 4.4786834716796875, total time dif 9204.545938014984)
step: 36560 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(3.632155), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08203774504363537), 'actor_loss': np.float64(-1.0174460411071777), 'hyper_actor_loss': np.float64(0.00010781199780467432), 'behavior_loss': np.float64(0.45029066503047943)}

Episode step 36570, time diff 4.524770021438599, total time dif 9209.024621486664)
step: 36570 @ episode report: {'average_total_reward': np.float32(10.646667), 'reward_variance': np.float32(1.721328), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07483116425573826), 'actor_loss': np.float64(-0.9749605536460877), 'hyper_actor_loss': np.float64(0.00012034081591991708), 'behavior_loss': np.float64(0.44876875579357145)}

Episode step 36580, time diff 4.7064125537872314, total time dif 9213.549391508102)
step: 36580 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(2.6196108), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08633032217621803), 'actor_loss': np.float64(-0.9640345096588134), 'hyper_actor_loss': np.float64(0.0001233485876582563), 'behavior_loss': np.float64(0.4587282776832581)}

Episode step 36590, time diff 4.4754638671875, total time dif 9218.25580406189)
step: 36590 @ episode report: {'average_total_reward': np.float32(11.683334), 'reward_variance': np.float32(3.2936854), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08676792457699775), 'actor_loss': np.float64(-0.989399653673172), 'hyper_actor_loss': np.float64(0.0001275877279113047), 'behavior_loss': np.float64(0.45406795144081114)}

Episode step 36600, time diff 4.4795708656311035, total time dif 9222.731267929077)
step: 36600 @ episode report: {'average_total_reward': np.float32(12.268889), 'reward_variance': np.float32(5.9629583), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(13.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0742531593888998), 'actor_loss': np.float64(-0.9875775396823883), 'hyper_actor_loss': np.float64(0.00012745201747748068), 'behavior_loss': np.float64(0.44267858266830445)}

Episode step 36610, time diff 4.598205089569092, total time dif 9227.210838794708)
step: 36610 @ episode report: {'average_total_reward': np.float32(10.895556), 'reward_variance': np.float32(1.9834869), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07248396463692189), 'actor_loss': np.float64(-0.9690927922725677), 'hyper_actor_loss': np.float64(0.00012909581419080497), 'behavior_loss': np.float64(0.45043555796146395)}

Episode step 36620, time diff 4.374454736709595, total time dif 9231.809043884277)
step: 36620 @ episode report: {'average_total_reward': np.float32(11.244446), 'reward_variance': np.float32(1.8805679), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08656918220221996), 'actor_loss': np.float64(-0.974241304397583), 'hyper_actor_loss': np.float64(0.00012187475731479935), 'behavior_loss': np.float64(0.45333726406097413)}

Episode step 36630, time diff 4.426429510116577, total time dif 9236.183498620987)
step: 36630 @ episode report: {'average_total_reward': np.float32(11.2322235), 'reward_variance': np.float32(1.9666526), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0727828823029995), 'actor_loss': np.float64(-0.9938335061073303), 'hyper_actor_loss': np.float64(0.0001133692596340552), 'behavior_loss': np.float64(0.4484908223152161)}

Episode step 36640, time diff 4.455062627792358, total time dif 9240.609928131104)
step: 36640 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(0.96533984), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0747552290558815), 'actor_loss': np.float64(-0.9663337230682373), 'hyper_actor_loss': np.float64(0.0001092683829483576), 'behavior_loss': np.float64(0.4452017188072205)}

Episode step 36650, time diff 4.467885971069336, total time dif 9245.064990758896)
step: 36650 @ episode report: {'average_total_reward': np.float32(9.8977785), 'reward_variance': np.float32(5.1220455), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0705843672156334), 'actor_loss': np.float64(-0.9585371136665344), 'hyper_actor_loss': np.float64(9.306310894316994e-05), 'behavior_loss': np.float64(0.4456706941127777)}

Episode step 36660, time diff 4.412684679031372, total time dif 9249.532876729965)
step: 36660 @ episode report: {'average_total_reward': np.float32(12.144445), 'reward_variance': np.float32(2.7855809), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0661103766411543), 'actor_loss': np.float64(-0.9688952744007111), 'hyper_actor_loss': np.float64(9.683515309006907e-05), 'behavior_loss': np.float64(0.4446118175983429)}

Episode step 36670, time diff 4.499963998794556, total time dif 9253.945561408997)
step: 36670 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(3.8588364), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0770913828164339), 'actor_loss': np.float64(-0.9788605511188507), 'hyper_actor_loss': np.float64(8.867229262250475e-05), 'behavior_loss': np.float64(0.44971018731594087)}

Episode step 36680, time diff 4.534741640090942, total time dif 9258.445525407791)
step: 36680 @ episode report: {'average_total_reward': np.float32(11.395556), 'reward_variance': np.float32(1.3357084), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08309692554175854), 'actor_loss': np.float64(-0.9759484469890595), 'hyper_actor_loss': np.float64(8.271648657682817e-05), 'behavior_loss': np.float64(0.4536375761032104)}

Episode step 36690, time diff 4.464261531829834, total time dif 9262.980267047882)
step: 36690 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.4300058), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08138278014957905), 'actor_loss': np.float64(-0.9827678501605988), 'hyper_actor_loss': np.float64(7.769011644995772e-05), 'behavior_loss': np.float64(0.4437118023633957)}

Episode step 36700, time diff 4.591564893722534, total time dif 9267.444528579712)
step: 36700 @ episode report: {'average_total_reward': np.float32(11.781113), 'reward_variance': np.float32(4.951383), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07763840183615685), 'actor_loss': np.float64(-0.9698648929595948), 'hyper_actor_loss': np.float64(0.00011047790467273444), 'behavior_loss': np.float64(0.4510195255279541)}

Episode step 36710, time diff 4.565288782119751, total time dif 9272.036093473434)
step: 36710 @ episode report: {'average_total_reward': np.float32(12.093335), 'reward_variance': np.float32(3.5747693), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09099700525403023), 'actor_loss': np.float64(-0.9941879451274872), 'hyper_actor_loss': np.float64(0.00013720554052270016), 'behavior_loss': np.float64(0.44976273477077483)}

Episode step 36720, time diff 4.524653673171997, total time dif 9276.601382255554)
step: 36720 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(1.5147603), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08045963421463967), 'actor_loss': np.float64(-1.0020043075084686), 'hyper_actor_loss': np.float64(0.0001357255081529729), 'behavior_loss': np.float64(0.44061338901519775)}

Episode step 36730, time diff 4.497268199920654, total time dif 9281.126035928726)
step: 36730 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(1.7382915), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07688669078052043), 'actor_loss': np.float64(-0.9686529099941253), 'hyper_actor_loss': np.float64(0.00010267241377732716), 'behavior_loss': np.float64(0.43859497308731077)}

Episode step 36740, time diff 4.498276472091675, total time dif 9285.623304128647)
step: 36740 @ episode report: {'average_total_reward': np.float32(11.0222225), 'reward_variance': np.float32(1.4164686), 'max_total_reward': np.float32(13.1444435), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07727065756917), 'actor_loss': np.float64(-0.9838194847106934), 'hyper_actor_loss': np.float64(9.322740588686429e-05), 'behavior_loss': np.float64(0.43796230256557467)}

Episode step 36750, time diff 4.5200183391571045, total time dif 9290.121580600739)
step: 36750 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(3.930406), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07846744582057), 'actor_loss': np.float64(-0.9905946671962738), 'hyper_actor_loss': np.float64(7.419381036015693e-05), 'behavior_loss': np.float64(0.44228486716747284)}

Episode step 36760, time diff 4.569284439086914, total time dif 9294.641598939896)
step: 36760 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.866106), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06654998362064361), 'actor_loss': np.float64(-0.9772026002407074), 'hyper_actor_loss': np.float64(7.561693419120275e-05), 'behavior_loss': np.float64(0.44594783782958985)}

Episode step 36770, time diff 4.465422630310059, total time dif 9299.210883378983)
step: 36770 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(3.2954624), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07828689627349376), 'actor_loss': np.float64(-0.9601424932479858), 'hyper_actor_loss': np.float64(7.899523370724637e-05), 'behavior_loss': np.float64(0.4458938628435135)}

Episode step 36780, time diff 4.6903088092803955, total time dif 9303.676306009293)
step: 36780 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(3.4390666), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07465054206550122), 'actor_loss': np.float64(-0.984079784154892), 'hyper_actor_loss': np.float64(8.334704834851436e-05), 'behavior_loss': np.float64(0.4419890850782394)}

Episode step 36790, time diff 4.487150430679321, total time dif 9308.366614818573)
step: 36790 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(4.3393784), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07875397838652134), 'actor_loss': np.float64(-0.9965850532054901), 'hyper_actor_loss': np.float64(9.130116523010656e-05), 'behavior_loss': np.float64(0.44937840700149534)}

Episode step 36800, time diff 4.543872117996216, total time dif 9312.853765249252)
step: 36800 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(1.3023707), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07444238848984241), 'actor_loss': np.float64(-0.9795784592628479), 'hyper_actor_loss': np.float64(0.00010447951281093993), 'behavior_loss': np.float64(0.4394877851009369)}

Episode step 36810, time diff 4.5574517250061035, total time dif 9317.397637367249)
step: 36810 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(3.7723813), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0790349118411541), 'actor_loss': np.float64(-0.990255331993103), 'hyper_actor_loss': np.float64(0.00013436275548883714), 'behavior_loss': np.float64(0.44189250767230986)}

Episode step 36820, time diff 4.497125625610352, total time dif 9321.955089092255)
step: 36820 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(5.7465186), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0760661069303751), 'actor_loss': np.float64(-0.9981819033622742), 'hyper_actor_loss': np.float64(0.00017637393757468088), 'behavior_loss': np.float64(0.4515614748001099)}

Episode step 36830, time diff 4.545411825180054, total time dif 9326.452214717865)
step: 36830 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(1.8549932), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07162057720124722), 'actor_loss': np.float64(-0.9750515341758728), 'hyper_actor_loss': np.float64(0.00024589800596004353), 'behavior_loss': np.float64(0.44930221140384674)}

Episode step 36840, time diff 4.611997604370117, total time dif 9330.997626543045)
step: 36840 @ episode report: {'average_total_reward': np.float32(10.758889), 'reward_variance': np.float32(2.5843215), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06757326163351536), 'actor_loss': np.float64(-0.9669158637523652), 'hyper_actor_loss': np.float64(0.0003745355614228174), 'behavior_loss': np.float64(0.4561563581228256)}

Episode step 36850, time diff 4.869879484176636, total time dif 9335.609624147415)
step: 36850 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.6627955), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06942077092826367), 'actor_loss': np.float64(-0.9813694655895233), 'hyper_actor_loss': np.float64(0.0006426310836104676), 'behavior_loss': np.float64(0.46896158158779144)}

Episode step 36860, time diff 4.724978446960449, total time dif 9340.479503631592)
step: 36860 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(1.7233593), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08021415397524834), 'actor_loss': np.float64(-0.9887594819068909), 'hyper_actor_loss': np.float64(0.0007455638435203582), 'behavior_loss': np.float64(0.46490134596824645)}

Episode step 36870, time diff 4.76992654800415, total time dif 9345.204482078552)
step: 36870 @ episode report: {'average_total_reward': np.float32(10.658891), 'reward_variance': np.float32(4.2181997), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07954142428934574), 'actor_loss': np.float64(-1.0038054287433624), 'hyper_actor_loss': np.float64(0.0008723056118469686), 'behavior_loss': np.float64(0.46956290006637574)}

Episode step 36880, time diff 4.740346431732178, total time dif 9349.974408626556)
step: 36880 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(3.620259), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07642953209578991), 'actor_loss': np.float64(-1.0021578311920165), 'hyper_actor_loss': np.float64(0.0007978647714480758), 'behavior_loss': np.float64(0.4645880818367004)}

Episode step 36890, time diff 4.758012533187866, total time dif 9354.714755058289)
step: 36890 @ episode report: {'average_total_reward': np.float32(11.507779), 'reward_variance': np.float32(3.3802738), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07777492962777614), 'actor_loss': np.float64(-0.9767776429653168), 'hyper_actor_loss': np.float64(0.0008940400672145188), 'behavior_loss': np.float64(0.4653631687164307)}

Episode step 36900, time diff 4.838214635848999, total time dif 9359.472767591476)
step: 36900 @ episode report: {'average_total_reward': np.float32(10.783335), 'reward_variance': np.float32(3.7492409), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0767250869423151), 'actor_loss': np.float64(-0.9854365408420562), 'hyper_actor_loss': np.float64(0.0009669333696365357), 'behavior_loss': np.float64(0.45949954390525816)}

Episode step 36910, time diff 4.749260902404785, total time dif 9364.310982227325)
step: 36910 @ episode report: {'average_total_reward': np.float32(10.197778), 'reward_variance': np.float32(3.0006614), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07407292537391186), 'actor_loss': np.float64(-1.0027263283729553), 'hyper_actor_loss': np.float64(0.0009233820950612426), 'behavior_loss': np.float64(0.45438631176948546)}

Episode step 36920, time diff 4.77893590927124, total time dif 9369.06024312973)
step: 36920 @ episode report: {'average_total_reward': np.float32(9.861113), 'reward_variance': np.float32(1.4231915), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07130181342363358), 'actor_loss': np.float64(-1.0104302763938904), 'hyper_actor_loss': np.float64(0.000977587344823405), 'behavior_loss': np.float64(0.459351921081543)}

Episode step 36930, time diff 4.7406415939331055, total time dif 9373.839179039001)
step: 36930 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(1.42013), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06820011362433434), 'actor_loss': np.float64(-0.9779455125331878), 'hyper_actor_loss': np.float64(0.000765870901523158), 'behavior_loss': np.float64(0.44669542610645296)}

Episode step 36940, time diff 4.73190975189209, total time dif 9378.579820632935)
step: 36940 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(4.1268687), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08123290240764618), 'actor_loss': np.float64(-0.9684444606304169), 'hyper_actor_loss': np.float64(0.0007783165958244354), 'behavior_loss': np.float64(0.4612894803285599)}

Episode step 36950, time diff 4.882723569869995, total time dif 9383.311730384827)
step: 36950 @ episode report: {'average_total_reward': np.float32(11.86889), 'reward_variance': np.float32(1.1886624), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0748612131923437), 'actor_loss': np.float64(-0.9804262042045593), 'hyper_actor_loss': np.float64(0.0007767486618831754), 'behavior_loss': np.float64(0.4488241016864777)}

Episode step 36960, time diff 4.7691874504089355, total time dif 9388.194453954697)
step: 36960 @ episode report: {'average_total_reward': np.float32(10.634444), 'reward_variance': np.float32(1.9653689), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0763239074498415), 'actor_loss': np.float64(-1.002439296245575), 'hyper_actor_loss': np.float64(0.0007652031199540943), 'behavior_loss': np.float64(0.4538422852754593)}

Episode step 36970, time diff 4.771006345748901, total time dif 9392.963641405106)
step: 36970 @ episode report: {'average_total_reward': np.float32(11.383334), 'reward_variance': np.float32(2.5781302), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07462462112307548), 'actor_loss': np.float64(-0.9970844447612762), 'hyper_actor_loss': np.float64(0.000705594796454534), 'behavior_loss': np.float64(0.44439318776130676)}

Episode step 36980, time diff 4.759547472000122, total time dif 9397.734647750854)
step: 36980 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(3.2131665), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0813119113445282), 'actor_loss': np.float64(-1.003052258491516), 'hyper_actor_loss': np.float64(0.0006598502106498927), 'behavior_loss': np.float64(0.44304179549217226)}

Episode step 36990, time diff 4.689241409301758, total time dif 9402.494195222855)
step: 36990 @ episode report: {'average_total_reward': np.float32(11.993334), 'reward_variance': np.float32(2.559758), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0822648424655199), 'actor_loss': np.float64(-1.008094274997711), 'hyper_actor_loss': np.float64(0.0006380705104675144), 'behavior_loss': np.float64(0.4459062606096268)}

Episode step 37000, time diff 4.751253843307495, total time dif 9407.183436632156)
step: 37000 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(4.7117243), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08252394050359727), 'actor_loss': np.float64(-1.0073180794715881), 'hyper_actor_loss': np.float64(0.000646987312939018), 'behavior_loss': np.float64(0.4315116167068481)}

Episode step 37010, time diff 4.786413908004761, total time dif 9411.934690475464)
step: 37010 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(3.5670567), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07785627096891404), 'actor_loss': np.float64(-1.0139404296875), 'hyper_actor_loss': np.float64(0.0007064134057145566), 'behavior_loss': np.float64(0.4320574939250946)}

Episode step 37020, time diff 4.810793876647949, total time dif 9416.721104383469)
step: 37020 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.331822), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08491552211344242), 'actor_loss': np.float64(-1.0025353968143462), 'hyper_actor_loss': np.float64(0.0007411620288621634), 'behavior_loss': np.float64(0.44493854939937594)}

Episode step 37030, time diff 4.808996200561523, total time dif 9421.531898260117)
step: 37030 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(1.60405), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08120599128305912), 'actor_loss': np.float64(-1.0057505249977112), 'hyper_actor_loss': np.float64(0.0006950112991034984), 'behavior_loss': np.float64(0.44233318567276003)}

Episode step 37040, time diff 4.742578744888306, total time dif 9426.340894460678)
step: 37040 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(2.4005635), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08610091432929039), 'actor_loss': np.float64(-1.0075168013572693), 'hyper_actor_loss': np.float64(0.00058620385825634), 'behavior_loss': np.float64(0.42779935598373414)}

Episode step 37050, time diff 4.72800612449646, total time dif 9431.083473205566)
step: 37050 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(2.6350675), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0738236278295517), 'actor_loss': np.float64(-1.0157818794250488), 'hyper_actor_loss': np.float64(0.0005426893825642764), 'behavior_loss': np.float64(0.42322365641593934)}

Episode step 37060, time diff 4.750285387039185, total time dif 9435.811479330063)
step: 37060 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(3.4362087), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08507759496569633), 'actor_loss': np.float64(-1.0099001049995422), 'hyper_actor_loss': np.float64(0.000494634301867336), 'behavior_loss': np.float64(0.42101752758026123)}

Episode step 37070, time diff 4.749032497406006, total time dif 9440.561764717102)
step: 37070 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(3.6355076), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07819942049682141), 'actor_loss': np.float64(-1.0098109364509582), 'hyper_actor_loss': np.float64(0.0004720957047538832), 'behavior_loss': np.float64(0.42038273215293886)}

Episode step 37080, time diff 4.699016571044922, total time dif 9445.310797214508)
step: 37080 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(3.3671665), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07811389863491058), 'actor_loss': np.float64(-1.0093025147914887), 'hyper_actor_loss': np.float64(0.00042718162294477227), 'behavior_loss': np.float64(0.4270678162574768)}

Episode step 37090, time diff 4.6948113441467285, total time dif 9450.009813785553)
step: 37090 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(1.7152708), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07487356513738633), 'actor_loss': np.float64(-0.9806770861148835), 'hyper_actor_loss': np.float64(0.000371741980779916), 'behavior_loss': np.float64(0.4159756422042847)}

Episode step 37100, time diff 4.826766014099121, total time dif 9454.7046251297)
step: 37100 @ episode report: {'average_total_reward': np.float32(12.2300005), 'reward_variance': np.float32(2.8286438), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06954633705317974), 'actor_loss': np.float64(-0.9865306437015533), 'hyper_actor_loss': np.float64(0.0003639214352006093), 'behavior_loss': np.float64(0.4190667808055878)}

Episode step 37110, time diff 5.032902479171753, total time dif 9459.531391143799)
step: 37110 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(3.4465632), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08181765004992485), 'actor_loss': np.float64(-0.9929839193820953), 'hyper_actor_loss': np.float64(0.0004026675829663873), 'behavior_loss': np.float64(0.4177899152040482)}

Episode step 37120, time diff 4.764081001281738, total time dif 9464.56429362297)
step: 37120 @ episode report: {'average_total_reward': np.float32(11.795557), 'reward_variance': np.float32(1.7319062), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(9.655556), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06463173776865005), 'actor_loss': np.float64(-0.9877356469631196), 'hyper_actor_loss': np.float64(0.00037611016014125196), 'behavior_loss': np.float64(0.4096035838127136)}

Episode step 37130, time diff 4.733959674835205, total time dif 9469.328374624252)
step: 37130 @ episode report: {'average_total_reward': np.float32(11.058889), 'reward_variance': np.float32(0.9921735), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08443584777414799), 'actor_loss': np.float64(-0.9934784650802613), 'hyper_actor_loss': np.float64(0.00039680829795543107), 'behavior_loss': np.float64(0.41155418157577517)}

Episode step 37140, time diff 4.72400975227356, total time dif 9474.062334299088)
step: 37140 @ episode report: {'average_total_reward': np.float32(11.807778), 'reward_variance': np.float32(4.50047), 'max_total_reward': np.float32(15.388888), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08799651861190796), 'actor_loss': np.float64(-1.0141495943069458), 'hyper_actor_loss': np.float64(0.0004704615770606324), 'behavior_loss': np.float64(0.4191968709230423)}

Episode step 37150, time diff 4.771775960922241, total time dif 9478.786344051361)
step: 37150 @ episode report: {'average_total_reward': np.float32(10.783335), 'reward_variance': np.float32(3.3826492), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08792101293802261), 'actor_loss': np.float64(-1.0262623190879823), 'hyper_actor_loss': np.float64(0.00042672191630117596), 'behavior_loss': np.float64(0.42519888281822205)}

Episode step 37160, time diff 4.696117877960205, total time dif 9483.558120012283)
step: 37160 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(2.5982819), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07911715805530548), 'actor_loss': np.float64(-1.0119294822216034), 'hyper_actor_loss': np.float64(0.0003995609993580729), 'behavior_loss': np.float64(0.41329960227012635)}

Episode step 37170, time diff 4.724651336669922, total time dif 9488.254237890244)
step: 37170 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(3.6235504), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08615634888410569), 'actor_loss': np.float64(-0.9923452377319336), 'hyper_actor_loss': np.float64(0.0003598845360102132), 'behavior_loss': np.float64(0.4196931540966034)}

Episode step 37180, time diff 4.713388442993164, total time dif 9492.978889226913)
step: 37180 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(2.6878576), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08692890070378781), 'actor_loss': np.float64(-1.0133587419986725), 'hyper_actor_loss': np.float64(0.00036629221285693346), 'behavior_loss': np.float64(0.4140578627586365)}

Episode step 37190, time diff 4.734837532043457, total time dif 9497.692277669907)
step: 37190 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(2.8910635), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08367097862064839), 'actor_loss': np.float64(-1.0136723637580871), 'hyper_actor_loss': np.float64(0.0003645339369541034), 'behavior_loss': np.float64(0.4007702887058258)}

Episode step 37200, time diff 4.690361261367798, total time dif 9502.42711520195)
step: 37200 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(0.8594618), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06532445549964905), 'actor_loss': np.float64(-1.0013402819633483), 'hyper_actor_loss': np.float64(0.00032695130503270777), 'behavior_loss': np.float64(0.39022003412246703)}

Episode step 37210, time diff 4.717468738555908, total time dif 9507.117476463318)
step: 37210 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(2.9485881), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07067451849579812), 'actor_loss': np.float64(-0.9635806083679199), 'hyper_actor_loss': np.float64(0.0003448989620665088), 'behavior_loss': np.float64(0.3951973497867584)}

Episode step 37220, time diff 4.731031894683838, total time dif 9511.834945201874)
step: 37220 @ episode report: {'average_total_reward': np.float32(10.373334), 'reward_variance': np.float32(4.5541034), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0820236936211586), 'actor_loss': np.float64(-0.9880992233753204), 'hyper_actor_loss': np.float64(0.00032718285510782155), 'behavior_loss': np.float64(0.3918920546770096)}

Episode step 37230, time diff 4.721103668212891, total time dif 9516.565977096558)
step: 37230 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.0524316), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08002417162060738), 'actor_loss': np.float64(-1.0161148190498352), 'hyper_actor_loss': np.float64(0.0003680640074890107), 'behavior_loss': np.float64(0.39925791323184967)}

Episode step 37240, time diff 4.752455949783325, total time dif 9521.28708076477)
step: 37240 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.2616163), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07199542075395585), 'actor_loss': np.float64(-0.9828267693519592), 'hyper_actor_loss': np.float64(0.0003917994938092306), 'behavior_loss': np.float64(0.4099919945001602)}

Episode step 37250, time diff 4.680009126663208, total time dif 9526.039536714554)
step: 37250 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(3.304217), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0847306739538908), 'actor_loss': np.float64(-0.9776825666427612), 'hyper_actor_loss': np.float64(0.0003704503120388836), 'behavior_loss': np.float64(0.39441769421100614)}

Episode step 37260, time diff 4.7371666431427, total time dif 9530.719545841217)
step: 37260 @ episode report: {'average_total_reward': np.float32(12.017778), 'reward_variance': np.float32(8.455957), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08214218206703663), 'actor_loss': np.float64(-1.001012921333313), 'hyper_actor_loss': np.float64(0.00037510762049350885), 'behavior_loss': np.float64(0.4117849826812744)}

Episode step 37270, time diff 4.83237099647522, total time dif 9535.45671248436)
step: 37270 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(2.904031), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06899833492934704), 'actor_loss': np.float64(-0.9884899914264679), 'hyper_actor_loss': np.float64(0.0003124290757114068), 'behavior_loss': np.float64(0.3947173863649368)}

Episode step 37280, time diff 4.596435785293579, total time dif 9540.289083480835)
step: 37280 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.9457302), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0848932147026062), 'actor_loss': np.float64(-0.9957943856716156), 'hyper_actor_loss': np.float64(0.00025747418840182945), 'behavior_loss': np.float64(0.38672715723514556)}

Episode step 37290, time diff 4.610116481781006, total time dif 9544.885519266129)
step: 37290 @ episode report: {'average_total_reward': np.float32(11.956667), 'reward_variance': np.float32(5.2715673), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(8.411111), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07482609022408723), 'actor_loss': np.float64(-1.0026578664779664), 'hyper_actor_loss': np.float64(0.00023890813463367522), 'behavior_loss': np.float64(0.4002726346254349)}

Episode step 37300, time diff 4.614910840988159, total time dif 9549.49563574791)
step: 37300 @ episode report: {'average_total_reward': np.float32(10.895556), 'reward_variance': np.float32(3.4124494), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08187025114893913), 'actor_loss': np.float64(-0.9971234202384949), 'hyper_actor_loss': np.float64(0.0002137616029358469), 'behavior_loss': np.float64(0.38571240901947024)}

Episode step 37310, time diff 4.544320106506348, total time dif 9554.110546588898)
step: 37310 @ episode report: {'average_total_reward': np.float32(10.910001), 'reward_variance': np.float32(2.3677402), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07797866277396678), 'actor_loss': np.float64(-0.9874329268932343), 'hyper_actor_loss': np.float64(0.00020192608353681863), 'behavior_loss': np.float64(0.39090984761714936)}

Episode step 37320, time diff 4.5561363697052, total time dif 9558.654866695404)
step: 37320 @ episode report: {'average_total_reward': np.float32(11.283334), 'reward_variance': np.float32(6.6910667), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08224365077912807), 'actor_loss': np.float64(-0.9817982256412506), 'hyper_actor_loss': np.float64(0.000209064016235061), 'behavior_loss': np.float64(0.40725380182266235)}

Episode step 37330, time diff 4.535052537918091, total time dif 9563.21100306511)
step: 37330 @ episode report: {'average_total_reward': np.float32(11.844446), 'reward_variance': np.float32(2.679876), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08207575865089893), 'actor_loss': np.float64(-1.0030427515506743), 'hyper_actor_loss': np.float64(0.00020143490546615795), 'behavior_loss': np.float64(0.38946153819561)}

Episode step 37340, time diff 4.56022572517395, total time dif 9567.746055603027)
step: 37340 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(5.2412653), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08770536743104458), 'actor_loss': np.float64(-1.002182799577713), 'hyper_actor_loss': np.float64(0.0002008739480515942), 'behavior_loss': np.float64(0.4030883997678757)}

Episode step 37350, time diff 4.606569290161133, total time dif 9572.306281328201)
step: 37350 @ episode report: {'average_total_reward': np.float32(11.8566675), 'reward_variance': np.float32(2.3189747), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07436171248555183), 'actor_loss': np.float64(-0.9760565638542176), 'hyper_actor_loss': np.float64(0.0002056114753941074), 'behavior_loss': np.float64(0.3988564729690552)}

Episode step 37360, time diff 4.755917310714722, total time dif 9576.912850618362)
step: 37360 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(1.1826617), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08885503560304642), 'actor_loss': np.float64(-0.9901842772960663), 'hyper_actor_loss': np.float64(0.00020338501926744356), 'behavior_loss': np.float64(0.4045741379261017)}

Episode step 37370, time diff 4.775305986404419, total time dif 9581.668767929077)
step: 37370 @ episode report: {'average_total_reward': np.float32(10.783335), 'reward_variance': np.float32(4.141267), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07829378582537175), 'actor_loss': np.float64(-1.0005706071853637), 'hyper_actor_loss': np.float64(0.00019812871469184756), 'behavior_loss': np.float64(0.41181257665157317)}

Episode step 37380, time diff 4.571244955062866, total time dif 9586.444073915482)
step: 37380 @ episode report: {'average_total_reward': np.float32(11.295555), 'reward_variance': np.float32(2.6255853), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08492208309471608), 'actor_loss': np.float64(-0.977878475189209), 'hyper_actor_loss': np.float64(0.00019379882578505204), 'behavior_loss': np.float64(0.3955551117658615)}

Episode step 37390, time diff 4.632718563079834, total time dif 9591.015318870544)
step: 37390 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(5.986697), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09200321435928345), 'actor_loss': np.float64(-1.0093628644943238), 'hyper_actor_loss': np.float64(0.00020538914250209928), 'behavior_loss': np.float64(0.38867242336273194)}

Episode step 37400, time diff 4.622321128845215, total time dif 9595.648037433624)
step: 37400 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(2.2692795), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0813113048672676), 'actor_loss': np.float64(-1.0144315838813782), 'hyper_actor_loss': np.float64(0.00020294528658268974), 'behavior_loss': np.float64(0.385500505566597)}

Episode step 37410, time diff 4.614498615264893, total time dif 9600.27035856247)
step: 37410 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(4.309854), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07451940812170506), 'actor_loss': np.float64(-0.996210366487503), 'hyper_actor_loss': np.float64(0.00022075758606661112), 'behavior_loss': np.float64(0.38628020882606506)}

Episode step 37420, time diff 4.534700632095337, total time dif 9604.884857177734)
step: 37420 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(2.3293288), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08805937245488167), 'actor_loss': np.float64(-0.9894840300083161), 'hyper_actor_loss': np.float64(0.00023970035836100577), 'behavior_loss': np.float64(0.39754705131053925)}

Episode step 37430, time diff 4.512285470962524, total time dif 9609.41955780983)
step: 37430 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(2.0539374), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08352674320340156), 'actor_loss': np.float64(-0.9986685752868653), 'hyper_actor_loss': np.float64(0.00023752729030093178), 'behavior_loss': np.float64(0.40446583926677704)}

Episode step 37440, time diff 4.669805288314819, total time dif 9613.931843280792)
step: 37440 @ episode report: {'average_total_reward': np.float32(10.034445), 'reward_variance': np.float32(5.4329004), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08235705569386483), 'actor_loss': np.float64(-0.9850120782852173), 'hyper_actor_loss': np.float64(0.00023839076311560348), 'behavior_loss': np.float64(0.4127480715513229)}

Episode step 37450, time diff 4.569839239120483, total time dif 9618.601648569107)
step: 37450 @ episode report: {'average_total_reward': np.float32(10.21), 'reward_variance': np.float32(4.814728), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08197009488940239), 'actor_loss': np.float64(-0.986862176656723), 'hyper_actor_loss': np.float64(0.00023338529135799035), 'behavior_loss': np.float64(0.40460373759269713)}

Episode step 37460, time diff 4.498912811279297, total time dif 9623.171487808228)
step: 37460 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(3.4327183), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07552413158118725), 'actor_loss': np.float64(-0.9751305341720581), 'hyper_actor_loss': np.float64(0.00022913663851795719), 'behavior_loss': np.float64(0.4090767800807953)}

Episode step 37470, time diff 4.565019369125366, total time dif 9627.670400619507)
step: 37470 @ episode report: {'average_total_reward': np.float32(12.081112), 'reward_variance': np.float32(4.4137797), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0737386304885149), 'actor_loss': np.float64(-0.9772412419319153), 'hyper_actor_loss': np.float64(0.0002287996932864189), 'behavior_loss': np.float64(0.3999790459871292)}

Episode step 37480, time diff 4.586256742477417, total time dif 9632.235419988632)
step: 37480 @ episode report: {'average_total_reward': np.float32(10.7711115), 'reward_variance': np.float32(3.0684257), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07952421642839909), 'actor_loss': np.float64(-0.9985545575618744), 'hyper_actor_loss': np.float64(0.00022696078958688303), 'behavior_loss': np.float64(0.3973303705453873)}

Episode step 37490, time diff 4.563013553619385, total time dif 9636.82167673111)
step: 37490 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(3.615142), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08607014119625092), 'actor_loss': np.float64(-1.0080107569694519), 'hyper_actor_loss': np.float64(0.00022245010768529028), 'behavior_loss': np.float64(0.39092342257499696)}

Episode step 37500, time diff 4.468171119689941, total time dif 9641.384690284729)
step: 37500 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(4.24202), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07723026499152183), 'actor_loss': np.float64(-1.0063252687454223), 'hyper_actor_loss': np.float64(0.0002102364451275207), 'behavior_loss': np.float64(0.3904265224933624)}

Episode step 37510, time diff 4.516810178756714, total time dif 9645.852861404419)
step: 37510 @ episode report: {'average_total_reward': np.float32(10.81), 'reward_variance': np.float32(2.5470245), 'max_total_reward': np.float32(13.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07202942669391632), 'actor_loss': np.float64(-0.983985835313797), 'hyper_actor_loss': np.float64(0.00019992756861029193), 'behavior_loss': np.float64(0.39799667298793795)}

Episode step 37520, time diff 4.545380353927612, total time dif 9650.369671583176)
step: 37520 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(4.514712), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07333972379565239), 'actor_loss': np.float64(-0.9836668372154236), 'hyper_actor_loss': np.float64(0.00018187921086791902), 'behavior_loss': np.float64(0.39978393018245695)}

Episode step 37530, time diff 4.5815980434417725, total time dif 9654.915051937103)
step: 37530 @ episode report: {'average_total_reward': np.float32(11.195556), 'reward_variance': np.float32(2.5946977), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08482571318745613), 'actor_loss': np.float64(-1.0014961242675782), 'hyper_actor_loss': np.float64(0.00018939710862468927), 'behavior_loss': np.float64(0.38609673380851744)}

Episode step 37540, time diff 4.59916615486145, total time dif 9659.496649980545)
step: 37540 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(1.9982736), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08544809296727181), 'actor_loss': np.float64(-1.0162277579307557), 'hyper_actor_loss': np.float64(0.00017359924095217139), 'behavior_loss': np.float64(0.3918279469013214)}

Episode step 37550, time diff 4.606223106384277, total time dif 9664.095816135406)
step: 37550 @ episode report: {'average_total_reward': np.float32(11.207778), 'reward_variance': np.float32(3.7090392), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07667580917477608), 'actor_loss': np.float64(-1.014762783050537), 'hyper_actor_loss': np.float64(0.00018209568370366468), 'behavior_loss': np.float64(0.37921157479286194)}

Episode step 37560, time diff 4.596858263015747, total time dif 9668.70203924179)
step: 37560 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(3.292994), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08919935077428817), 'actor_loss': np.float64(-0.9986125111579895), 'hyper_actor_loss': np.float64(0.00018118223670171574), 'behavior_loss': np.float64(0.39745971858501433)}

Episode step 37570, time diff 4.532667636871338, total time dif 9673.298897504807)
step: 37570 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(3.6676662), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08496313020586968), 'actor_loss': np.float64(-0.9994139909744263), 'hyper_actor_loss': np.float64(0.00018396511295577512), 'behavior_loss': np.float64(0.4055735766887665)}

Episode step 37580, time diff 4.5873706340789795, total time dif 9677.831565141678)
step: 37580 @ episode report: {'average_total_reward': np.float32(11.032224), 'reward_variance': np.float32(2.6321094), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07129869759082794), 'actor_loss': np.float64(-1.0003712356090546), 'hyper_actor_loss': np.float64(0.00019508416298776864), 'behavior_loss': np.float64(0.39164191484451294)}

Episode step 37590, time diff 4.545402526855469, total time dif 9682.418935775757)
step: 37590 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.0097395), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08997778333723545), 'actor_loss': np.float64(-0.9720068991184234), 'hyper_actor_loss': np.float64(0.0001886084326542914), 'behavior_loss': np.float64(0.39598060250282285)}

Episode step 37600, time diff 4.581254243850708, total time dif 9686.964338302612)
step: 37600 @ episode report: {'average_total_reward': np.float32(10.920001), 'reward_variance': np.float32(1.209007), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07795315273106099), 'actor_loss': np.float64(-0.9858957469463349), 'hyper_actor_loss': np.float64(0.0001816836986108683), 'behavior_loss': np.float64(0.38875285983085633)}

Episode step 37610, time diff 4.744176387786865, total time dif 9691.545592546463)
step: 37610 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(2.2967107), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07519502714276313), 'actor_loss': np.float64(-0.985340029001236), 'hyper_actor_loss': np.float64(0.00017808347329264506), 'behavior_loss': np.float64(0.39614874720573423)}

Episode step 37620, time diff 4.664571523666382, total time dif 9696.28976893425)
step: 37620 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(4.230259), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07803574800491334), 'actor_loss': np.float64(-1.006835651397705), 'hyper_actor_loss': np.float64(0.00017168269259855152), 'behavior_loss': np.float64(0.38236669898033143)}

Episode step 37630, time diff 4.894551992416382, total time dif 9700.954340457916)
step: 37630 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(5.265167), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08727268502116203), 'actor_loss': np.float64(-1.0031635165214539), 'hyper_actor_loss': np.float64(0.00017598170525161548), 'behavior_loss': np.float64(0.401174858212471)}

Episode step 37640, time diff 4.64194393157959, total time dif 9705.848892450333)
step: 37640 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(1.3584213), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07767941541969776), 'actor_loss': np.float64(-0.9798104763031006), 'hyper_actor_loss': np.float64(0.0001809472742024809), 'behavior_loss': np.float64(0.38399273753166197)}

Episode step 37650, time diff 4.5991716384887695, total time dif 9710.490836381912)
step: 37650 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(3.5448155), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08120529241859913), 'actor_loss': np.float64(-1.0071802735328674), 'hyper_actor_loss': np.float64(0.0001824878389015794), 'behavior_loss': np.float64(0.3880531758069992)}

Episode step 37660, time diff 4.662538051605225, total time dif 9715.090008020401)
step: 37660 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(4.5608506), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07436315082013607), 'actor_loss': np.float64(-1.0096091270446776), 'hyper_actor_loss': np.float64(0.00019770085345953702), 'behavior_loss': np.float64(0.3944019913673401)}

Episode step 37670, time diff 4.637237071990967, total time dif 9719.752546072006)
step: 37670 @ episode report: {'average_total_reward': np.float32(9.873335), 'reward_variance': np.float32(3.1033385), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08279489427804947), 'actor_loss': np.float64(-0.9917823553085328), 'hyper_actor_loss': np.float64(0.00018868856423068792), 'behavior_loss': np.float64(0.38007608354091643)}

Episode step 37680, time diff 4.597870588302612, total time dif 9724.389783143997)
step: 37680 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(2.4006712), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07927217930555344), 'actor_loss': np.float64(-1.0165539622306823), 'hyper_actor_loss': np.float64(0.00019586280541261658), 'behavior_loss': np.float64(0.38257559239864347)}

Episode step 37690, time diff 4.596309423446655, total time dif 9728.9876537323)
step: 37690 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(1.5503815), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07700813151896), 'actor_loss': np.float64(-1.0011166095733643), 'hyper_actor_loss': np.float64(0.00019204751442885025), 'behavior_loss': np.float64(0.37809655368328093)}

Episode step 37700, time diff 4.584695100784302, total time dif 9733.583963155746)
step: 37700 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.3913147), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06805809661746025), 'actor_loss': np.float64(-0.981257164478302), 'hyper_actor_loss': np.float64(0.00020641146547859535), 'behavior_loss': np.float64(0.3814865559339523)}

Episode step 37710, time diff 4.624560356140137, total time dif 9738.16865825653)
step: 37710 @ episode report: {'average_total_reward': np.float32(11.271112), 'reward_variance': np.float32(3.5555866), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06946900300681591), 'actor_loss': np.float64(-0.9732278347015381), 'hyper_actor_loss': np.float64(0.00019550093275029213), 'behavior_loss': np.float64(0.37741022408008573)}

Episode step 37720, time diff 4.598349571228027, total time dif 9742.79321861267)
step: 37720 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(1.9748265), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07502598315477371), 'actor_loss': np.float64(-0.9866805076599121), 'hyper_actor_loss': np.float64(0.00018462362786522135), 'behavior_loss': np.float64(0.381204754114151)}

Episode step 37730, time diff 4.602935791015625, total time dif 9747.391568183899)
step: 37730 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(4.2751226), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06834665276110172), 'actor_loss': np.float64(-0.9879622519016266), 'hyper_actor_loss': np.float64(0.000178549975680653), 'behavior_loss': np.float64(0.37595103681087494)}

Episode step 37740, time diff 4.598726511001587, total time dif 9751.994503974915)
step: 37740 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.3777568), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0744295846670866), 'actor_loss': np.float64(-0.9821781694889069), 'hyper_actor_loss': np.float64(0.00019153379107592628), 'behavior_loss': np.float64(0.3648416519165039)}

Episode step 37750, time diff 4.683062553405762, total time dif 9756.593230485916)
step: 37750 @ episode report: {'average_total_reward': np.float32(11.344445), 'reward_variance': np.float32(5.9237294), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0689497534185648), 'actor_loss': np.float64(-1.0030514538288116), 'hyper_actor_loss': np.float64(0.00019123086385661737), 'behavior_loss': np.float64(0.36528458297252653)}

Episode step 37760, time diff 4.620973110198975, total time dif 9761.276293039322)
step: 37760 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(8.625166), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07463007867336273), 'actor_loss': np.float64(-1.011390006542206), 'hyper_actor_loss': np.float64(0.00019817700958810747), 'behavior_loss': np.float64(0.35326510965824126)}

Episode step 37770, time diff 4.8020408153533936, total time dif 9765.89726614952)
step: 37770 @ episode report: {'average_total_reward': np.float32(11.720001), 'reward_variance': np.float32(4.423427), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06761288531124592), 'actor_loss': np.float64(-1.0065353631973266), 'hyper_actor_loss': np.float64(0.00022493763244710863), 'behavior_loss': np.float64(0.34827521741390227)}

Episode step 37780, time diff 4.6595587730407715, total time dif 9770.699306964874)
step: 37780 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(3.8984802), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06484481617808342), 'actor_loss': np.float64(-0.9959004521369934), 'hyper_actor_loss': np.float64(0.00022813747636973858), 'behavior_loss': np.float64(0.3618475556373596)}

Episode step 37790, time diff 4.617528676986694, total time dif 9775.358865737915)
step: 37790 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(2.1294377), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07246776409447193), 'actor_loss': np.float64(-0.9814731895923614), 'hyper_actor_loss': np.float64(0.00021826417359989138), 'behavior_loss': np.float64(0.3584304749965668)}

Episode step 37800, time diff 4.667063236236572, total time dif 9779.976394414902)
step: 37800 @ episode report: {'average_total_reward': np.float32(11.644445), 'reward_variance': np.float32(5.3423977), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777777), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07077495977282525), 'actor_loss': np.float64(-0.9969475150108338), 'hyper_actor_loss': np.float64(0.0002257369371363893), 'behavior_loss': np.float64(0.341939839720726)}

Episode step 37810, time diff 4.673882484436035, total time dif 9784.643457651138)
step: 37810 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(6.0917993), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07627832181751729), 'actor_loss': np.float64(-1.0242704510688783), 'hyper_actor_loss': np.float64(0.00021660580387106165), 'behavior_loss': np.float64(0.3382065325975418)}

Episode step 37820, time diff 4.6443445682525635, total time dif 9789.317340135574)
step: 37820 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(1.6117299), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07239467799663543), 'actor_loss': np.float64(-1.0094377100467682), 'hyper_actor_loss': np.float64(0.00021106458152644336), 'behavior_loss': np.float64(0.3433386474847794)}

Episode step 37830, time diff 4.615124702453613, total time dif 9793.961684703827)
step: 37830 @ episode report: {'average_total_reward': np.float32(10.934445), 'reward_variance': np.float32(2.5392206), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05824015326797962), 'actor_loss': np.float64(-0.9859016537666321), 'hyper_actor_loss': np.float64(0.0002030804942478426), 'behavior_loss': np.float64(0.3318290799856186)}

Episode step 37840, time diff 4.694594144821167, total time dif 9798.57680940628)
step: 37840 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(7.974475), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07055842094123363), 'actor_loss': np.float64(-0.9911524295806885), 'hyper_actor_loss': np.float64(0.00018720974621828647), 'behavior_loss': np.float64(0.3378379732370377)}

Episode step 37850, time diff 4.648540258407593, total time dif 9803.271403551102)
step: 37850 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(1.4147171), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0676464568823576), 'actor_loss': np.float64(-1.0047919273376464), 'hyper_actor_loss': np.float64(0.00020325339719420298), 'behavior_loss': np.float64(0.33020882308483124)}

Episode step 37860, time diff 4.668072700500488, total time dif 9807.91994380951)
step: 37860 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(5.8660693), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05846880227327347), 'actor_loss': np.float64(-1.0020367562770844), 'hyper_actor_loss': np.float64(0.0001815516923670657), 'behavior_loss': np.float64(0.3215386807918549)}

Episode step 37870, time diff 4.670559406280518, total time dif 9812.58801651001)
step: 37870 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.5138566), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07368759401142597), 'actor_loss': np.float64(-0.9981221795082093), 'hyper_actor_loss': np.float64(0.00019546747498679906), 'behavior_loss': np.float64(0.3317675679922104)}

Episode step 37880, time diff 4.7368621826171875, total time dif 9817.25857591629)
step: 37880 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(4.6311717), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07022937759757042), 'actor_loss': np.float64(-1.010942780971527), 'hyper_actor_loss': np.float64(0.0001910205653985031), 'behavior_loss': np.float64(0.3220650374889374)}

Episode step 37890, time diff 4.754940509796143, total time dif 9821.995438098907)
step: 37890 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(2.7544208), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0711330559104681), 'actor_loss': np.float64(-1.000902247428894), 'hyper_actor_loss': np.float64(0.00018755000055534766), 'behavior_loss': np.float64(0.3268352568149567)}

Episode step 37900, time diff 4.732849836349487, total time dif 9826.750378608704)
step: 37900 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(3.371369), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06414118334650994), 'actor_loss': np.float64(-0.9794370710849762), 'hyper_actor_loss': np.float64(0.00019901348277926446), 'behavior_loss': np.float64(0.3291129291057587)}

Episode step 37910, time diff 4.610060453414917, total time dif 9831.483228445053)
step: 37910 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(1.7383163), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07228133268654346), 'actor_loss': np.float64(-0.9904853343963623), 'hyper_actor_loss': np.float64(0.00019804047769866884), 'behavior_loss': np.float64(0.3148064613342285)}

Episode step 37920, time diff 4.631997346878052, total time dif 9836.093288898468)
step: 37920 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(1.4175014), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06839544279500842), 'actor_loss': np.float64(-1.0158113360404968), 'hyper_actor_loss': np.float64(0.00017981670825975015), 'behavior_loss': np.float64(0.3172157764434814)}

Episode step 37930, time diff 4.792953014373779, total time dif 9840.725286245346)
step: 37930 @ episode report: {'average_total_reward': np.float32(10.934444), 'reward_variance': np.float32(2.4793694), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0697631012648344), 'actor_loss': np.float64(-0.9957683861255646), 'hyper_actor_loss': np.float64(0.0001817795535316691), 'behavior_loss': np.float64(0.3107563018798828)}

Episode step 37940, time diff 4.657506704330444, total time dif 9845.51823925972)
step: 37940 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(0.90634096), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06486962623894214), 'actor_loss': np.float64(-0.995702451467514), 'hyper_actor_loss': np.float64(0.00019514460727805273), 'behavior_loss': np.float64(0.3245025485754013)}

Episode step 37950, time diff 4.618781566619873, total time dif 9850.17574596405)
step: 37950 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(2.7124305), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06387045420706272), 'actor_loss': np.float64(-1.004981482028961), 'hyper_actor_loss': np.float64(0.00016903670330066235), 'behavior_loss': np.float64(0.3071191877126694)}

Episode step 37960, time diff 4.597005367279053, total time dif 9854.79452753067)
step: 37960 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(3.700721), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06429123245179653), 'actor_loss': np.float64(-1.0099200129508972), 'hyper_actor_loss': np.float64(0.00016460565238958226), 'behavior_loss': np.float64(0.31944086849689485)}

Episode step 37970, time diff 4.635692596435547, total time dif 9859.39153289795)
step: 37970 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(4.7391562), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07048396747559309), 'actor_loss': np.float64(-1.0090737879276275), 'hyper_actor_loss': np.float64(0.00017557930405018852), 'behavior_loss': np.float64(0.3049500614404678)}

Episode step 37980, time diff 4.657954216003418, total time dif 9864.027225494385)
step: 37980 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.009112), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06921423561871051), 'actor_loss': np.float64(-1.0181919097900392), 'hyper_actor_loss': np.float64(0.0001680158660747111), 'behavior_loss': np.float64(0.3080170601606369)}

Episode step 37990, time diff 4.671714782714844, total time dif 9868.685179710388)
step: 37990 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(4.278919), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.079123130813241), 'actor_loss': np.float64(-1.0285862565040589), 'hyper_actor_loss': np.float64(0.0001563349098432809), 'behavior_loss': np.float64(0.31189176738262175)}

Episode step 38000, time diff 4.653920650482178, total time dif 9873.356894493103)
step: 38000 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(2.9032097), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06430683061480522), 'actor_loss': np.float64(-1.0127676367759704), 'hyper_actor_loss': np.float64(0.00016641971451463177), 'behavior_loss': np.float64(0.30894385278224945)}

Episode step 38010, time diff 4.606913805007935, total time dif 9878.010815143585)
step: 38010 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(4.1634135), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06996988765895366), 'actor_loss': np.float64(-0.98729088306427), 'hyper_actor_loss': np.float64(0.00016754757525632157), 'behavior_loss': np.float64(0.31279272139072417)}

Episode step 38020, time diff 4.657456874847412, total time dif 9882.617728948593)
step: 38020 @ episode report: {'average_total_reward': np.float32(11.756668), 'reward_variance': np.float32(2.79732), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07182440422475338), 'actor_loss': np.float64(-1.0057605266571046), 'hyper_actor_loss': np.float64(0.00016051489728852175), 'behavior_loss': np.float64(0.3075448304414749)}

Episode step 38030, time diff 4.601717472076416, total time dif 9887.27518582344)
step: 38030 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.485063), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05629916451871395), 'actor_loss': np.float64(-0.9877207398414611), 'hyper_actor_loss': np.float64(0.0001894200497190468), 'behavior_loss': np.float64(0.31044159531593324)}

Episode step 38040, time diff 4.682100057601929, total time dif 9891.876903295517)
step: 38040 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(1.4699557), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07083847001194954), 'actor_loss': np.float64(-0.9857038617134094), 'hyper_actor_loss': np.float64(0.00030160984315443786), 'behavior_loss': np.float64(0.31988289952278137)}

Episode step 38050, time diff 4.6634016036987305, total time dif 9896.559003353119)
step: 38050 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(2.0174024), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0648578904569149), 'actor_loss': np.float64(-1.006972962617874), 'hyper_actor_loss': np.float64(0.0003523439692799002), 'behavior_loss': np.float64(0.313273549079895)}

Episode step 38060, time diff 4.6608216762542725, total time dif 9901.222404956818)
step: 38060 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(2.7730076), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06304460428655148), 'actor_loss': np.float64(-0.9846444368362427), 'hyper_actor_loss': np.float64(0.0003228515852242708), 'behavior_loss': np.float64(0.3199730783700943)}

Episode step 38070, time diff 4.654884099960327, total time dif 9905.883226633072)
step: 38070 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.026617), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06940410882234574), 'actor_loss': np.float64(-0.9800538182258606), 'hyper_actor_loss': np.float64(0.0002947417087852955), 'behavior_loss': np.float64(0.3197619765996933)}

Episode step 38080, time diff 4.680640459060669, total time dif 9910.538110733032)
step: 38080 @ episode report: {'average_total_reward': np.float32(11.544445), 'reward_variance': np.float32(3.0744698), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06424458622932434), 'actor_loss': np.float64(-0.9983498573303222), 'hyper_actor_loss': np.float64(0.00029966257570777086), 'behavior_loss': np.float64(0.31860553920269014)}

Episode step 38090, time diff 4.6268393993377686, total time dif 9915.218751192093)
step: 38090 @ episode report: {'average_total_reward': np.float32(11.744445), 'reward_variance': np.float32(2.9537783), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06406884361058474), 'actor_loss': np.float64(-0.9868729650974274), 'hyper_actor_loss': np.float64(0.00028165102266939355), 'behavior_loss': np.float64(0.3193831771612167)}

Episode step 38100, time diff 4.821611166000366, total time dif 9919.84559059143)
step: 38100 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(2.8741002), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06275510601699352), 'actor_loss': np.float64(-0.989619517326355), 'hyper_actor_loss': np.float64(0.00025983232480939476), 'behavior_loss': np.float64(0.3112711727619171)}

Episode step 38110, time diff 4.702104330062866, total time dif 9924.667201757431)
step: 38110 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(3.5972219), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06576762571930886), 'actor_loss': np.float64(-0.996115905046463), 'hyper_actor_loss': np.float64(0.0002576576705905609), 'behavior_loss': np.float64(0.32160081565380094)}

Episode step 38120, time diff 4.653337240219116, total time dif 9929.369306087494)
step: 38120 @ episode report: {'average_total_reward': np.float32(11.595556), 'reward_variance': np.float32(2.8850675), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07541816607117653), 'actor_loss': np.float64(-1.0041476249694825), 'hyper_actor_loss': np.float64(0.0002551274810684845), 'behavior_loss': np.float64(0.3238562673330307)}

Episode step 38130, time diff 4.659203052520752, total time dif 9934.022643327713)
step: 38130 @ episode report: {'average_total_reward': np.float32(12.2300005), 'reward_variance': np.float32(3.277533), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06608742773532868), 'actor_loss': np.float64(-1.0050382912158966), 'hyper_actor_loss': np.float64(0.00021870836208108811), 'behavior_loss': np.float64(0.3223093956708908)}

Episode step 38140, time diff 4.669746160507202, total time dif 9938.681846380234)
step: 38140 @ episode report: {'average_total_reward': np.float32(11.9811125), 'reward_variance': np.float32(3.4003232), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05624820478260517), 'actor_loss': np.float64(-0.9770034611225128), 'hyper_actor_loss': np.float64(0.00023043792170938105), 'behavior_loss': np.float64(0.32165070474147794)}

Episode step 38150, time diff 4.708208799362183, total time dif 9943.351592540741)
step: 38150 @ episode report: {'average_total_reward': np.float32(11.332223), 'reward_variance': np.float32(1.2286036), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05465967860072851), 'actor_loss': np.float64(-0.9726522862911224), 'hyper_actor_loss': np.float64(0.00022240613325266167), 'behavior_loss': np.float64(0.3233258754014969)}

Episode step 38160, time diff 4.789228200912476, total time dif 9948.059801340103)
step: 38160 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(5.7529755), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05463595874607563), 'actor_loss': np.float64(-0.9761343419551849), 'hyper_actor_loss': np.float64(0.00021920139843132347), 'behavior_loss': np.float64(0.3164704918861389)}

Episode step 38170, time diff 4.677111625671387, total time dif 9952.849029541016)
step: 38170 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(1.4254868), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05720244850963354), 'actor_loss': np.float64(-0.9887880742549896), 'hyper_actor_loss': np.float64(0.00023352098505711183), 'behavior_loss': np.float64(0.3187292069196701)}

Episode step 38180, time diff 4.67808723449707, total time dif 9957.526141166687)
step: 38180 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(4.681502), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07110520806163549), 'actor_loss': np.float64(-0.9908057808876037), 'hyper_actor_loss': np.float64(0.00021301386586856098), 'behavior_loss': np.float64(0.3186873972415924)}

Episode step 38190, time diff 4.67992091178894, total time dif 9962.204228401184)
step: 38190 @ episode report: {'average_total_reward': np.float32(10.461111), 'reward_variance': np.float32(1.7811911), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07832664139568805), 'actor_loss': np.float64(-1.0213454842567444), 'hyper_actor_loss': np.float64(0.00020947012671967968), 'behavior_loss': np.float64(0.32073734402656556)}

Episode step 38200, time diff 4.6050496101379395, total time dif 9966.884149312973)
step: 38200 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(1.7761103), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06724905483424663), 'actor_loss': np.float64(-1.014844250679016), 'hyper_actor_loss': np.float64(0.00018774834170471878), 'behavior_loss': np.float64(0.3216497987508774)}

Episode step 38210, time diff 4.612172842025757, total time dif 9971.489198923111)
step: 38210 @ episode report: {'average_total_reward': np.float32(11.917779), 'reward_variance': np.float32(4.484623), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06477275509387255), 'actor_loss': np.float64(-0.980798190832138), 'hyper_actor_loss': np.float64(0.00018135337450075895), 'behavior_loss': np.float64(0.32011643052101135)}

Episode step 38220, time diff 4.597322463989258, total time dif 9976.101371765137)
step: 38220 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.0636065), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07164240665733815), 'actor_loss': np.float64(-0.9893123388290406), 'hyper_actor_loss': np.float64(0.00017091317422455177), 'behavior_loss': np.float64(0.3217669576406479)}

Episode step 38230, time diff 4.566399097442627, total time dif 9980.698694229126)
step: 38230 @ episode report: {'average_total_reward': np.float32(11.607779), 'reward_variance': np.float32(0.9701004), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06472620517015457), 'actor_loss': np.float64(-1.0036506354808807), 'hyper_actor_loss': np.float64(0.00017510952311567963), 'behavior_loss': np.float64(0.31967245042324066)}

Episode step 38240, time diff 4.598716497421265, total time dif 9985.265093326569)
step: 38240 @ episode report: {'average_total_reward': np.float32(10.883334), 'reward_variance': np.float32(1.1682535), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07330494225025178), 'actor_loss': np.float64(-1.0015054583549499), 'hyper_actor_loss': np.float64(0.00017919525998877361), 'behavior_loss': np.float64(0.3230676412582397)}

Episode step 38250, time diff 4.615769147872925, total time dif 9989.86380982399)
step: 38250 @ episode report: {'average_total_reward': np.float32(10.697779), 'reward_variance': np.float32(2.8566387), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06660451106727124), 'actor_loss': np.float64(-0.9929794669151306), 'hyper_actor_loss': np.float64(0.00019732156215468422), 'behavior_loss': np.float64(0.32302066683769226)}

Episode step 38260, time diff 4.598928451538086, total time dif 9994.479578971863)
step: 38260 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(1.8809652), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05723727289587259), 'actor_loss': np.float64(-0.9961168587207794), 'hyper_actor_loss': np.float64(0.00018611613631946966), 'behavior_loss': np.float64(0.3215657830238342)}

Episode step 38270, time diff 4.794872999191284, total time dif 9999.0785074234)
step: 38270 @ episode report: {'average_total_reward': np.float32(10.434444), 'reward_variance': np.float32(7.1469507), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07259682044386864), 'actor_loss': np.float64(-0.996125453710556), 'hyper_actor_loss': np.float64(0.0001734036937705241), 'behavior_loss': np.float64(0.3213728040456772)}

Episode step 38280, time diff 4.646549940109253, total time dif 10003.873380422592)
step: 38280 @ episode report: {'average_total_reward': np.float32(11.432222), 'reward_variance': np.float32(4.555444), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06797308046370745), 'actor_loss': np.float64(-1.0062386870384217), 'hyper_actor_loss': np.float64(0.00016580414376221597), 'behavior_loss': np.float64(0.32229095697402954)}

Episode step 38290, time diff 4.641164302825928, total time dif 10008.519930362701)
step: 38290 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(5.8946433), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06546836979687214), 'actor_loss': np.float64(-0.9883753180503845), 'hyper_actor_loss': np.float64(0.00016376741841668264), 'behavior_loss': np.float64(0.32094027400016784)}

Episode step 38300, time diff 4.5842156410217285, total time dif 10013.161094665527)
step: 38300 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(6.3663263), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06947211921215057), 'actor_loss': np.float64(-0.9893299221992493), 'hyper_actor_loss': np.float64(0.00014746109009138307), 'behavior_loss': np.float64(0.31898301243782046)}

Episode step 38310, time diff 4.629441976547241, total time dif 10017.745310306549)
step: 38310 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(2.9777648), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07737579382956028), 'actor_loss': np.float64(-1.0168538808822631), 'hyper_actor_loss': np.float64(0.0001674094171903562), 'behavior_loss': np.float64(0.3172524482011795)}

Episode step 38320, time diff 4.5433502197265625, total time dif 10022.374752283096)
step: 38320 @ episode report: {'average_total_reward': np.float32(11.544445), 'reward_variance': np.float32(2.7313335), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05522811822593212), 'actor_loss': np.float64(-1.00177361369133), 'hyper_actor_loss': np.float64(0.00015738227084511892), 'behavior_loss': np.float64(0.31313257813453677)}

Episode step 38330, time diff 4.568100452423096, total time dif 10026.918102502823)
step: 38330 @ episode report: {'average_total_reward': np.float32(11.781112), 'reward_variance': np.float32(3.437137), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06713600680232049), 'actor_loss': np.float64(-0.9674481809139251), 'hyper_actor_loss': np.float64(0.00014326459713629447), 'behavior_loss': np.float64(0.32445252537727354)}

Episode step 38340, time diff 4.537831783294678, total time dif 10031.486202955246)
step: 38340 @ episode report: {'average_total_reward': np.float32(10.24889), 'reward_variance': np.float32(2.0601034), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06573095489293337), 'actor_loss': np.float64(-1.000416523218155), 'hyper_actor_loss': np.float64(0.0001587152379215695), 'behavior_loss': np.float64(0.3229669272899628)}

Episode step 38350, time diff 4.547528982162476, total time dif 10036.02403473854)
step: 38350 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(2.0011404), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06254706084728241), 'actor_loss': np.float64(-1.0025052547454834), 'hyper_actor_loss': np.float64(0.00015075509727466852), 'behavior_loss': np.float64(0.3177866876125336)}

Episode step 38360, time diff 4.53548264503479, total time dif 10040.571563720703)
step: 38360 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.4167767), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07458587139844894), 'actor_loss': np.float64(-0.9897668898105622), 'hyper_actor_loss': np.float64(0.00015087144129211083), 'behavior_loss': np.float64(0.32426602244377134)}

Episode step 38370, time diff 4.55704665184021, total time dif 10045.107046365738)
step: 38370 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(2.4704738), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0694416344165802), 'actor_loss': np.float64(-0.9967031598091125), 'hyper_actor_loss': np.float64(0.00014067690863157624), 'behavior_loss': np.float64(0.32804752290248873)}

Episode step 38380, time diff 4.569094896316528, total time dif 10049.664093017578)
step: 38380 @ episode report: {'average_total_reward': np.float32(10.858889), 'reward_variance': np.float32(5.396867), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07289822362363338), 'actor_loss': np.float64(-1.003407007455826), 'hyper_actor_loss': np.float64(0.0001573017187183723), 'behavior_loss': np.float64(0.31690033674240115)}

Episode step 38390, time diff 4.609953165054321, total time dif 10054.233187913895)
step: 38390 @ episode report: {'average_total_reward': np.float32(11.220001), 'reward_variance': np.float32(3.733279), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06776986215263606), 'actor_loss': np.float64(-1.005530297756195), 'hyper_actor_loss': np.float64(0.00014593921005143785), 'behavior_loss': np.float64(0.31030750572681426)}

Episode step 38400, time diff 4.5760087966918945, total time dif 10058.843141078949)
step: 38400 @ episode report: {'average_total_reward': np.float32(11.020001), 'reward_variance': np.float32(1.8750814), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06857505328953266), 'actor_loss': np.float64(-1.0063138484954834), 'hyper_actor_loss': np.float64(0.0001437499864550773), 'behavior_loss': np.float64(0.31786690652370453)}

Episode step 38410, time diff 4.685099840164185, total time dif 10063.41914987564)
step: 38410 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(1.9959457), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055289564095437525), 'actor_loss': np.float64(-0.9960756480693818), 'hyper_actor_loss': np.float64(0.00015290803276002407), 'behavior_loss': np.float64(0.32532246708869933)}

Episode step 38420, time diff 4.62853479385376, total time dif 10068.104249715805)
step: 38420 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.7532802), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07961051315069198), 'actor_loss': np.float64(-0.9809620261192322), 'hyper_actor_loss': np.float64(0.0001465921501221601), 'behavior_loss': np.float64(0.32501606941223143)}

Episode step 38430, time diff 4.55459189414978, total time dif 10072.732784509659)
step: 38430 @ episode report: {'average_total_reward': np.float32(10.8977785), 'reward_variance': np.float32(2.61523), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08184824958443641), 'actor_loss': np.float64(-1.016383135318756), 'hyper_actor_loss': np.float64(0.0001420871471054852), 'behavior_loss': np.float64(0.32618233263492585)}

Episode step 38440, time diff 4.741086959838867, total time dif 10077.287376403809)
step: 38440 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(5.2686973), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06309739612042904), 'actor_loss': np.float64(-1.0233458995819091), 'hyper_actor_loss': np.float64(0.00014440333397942596), 'behavior_loss': np.float64(0.31393882632255554)}

Episode step 38450, time diff 4.54666543006897, total time dif 10082.028463363647)
step: 38450 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(3.0774567), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07295453064143657), 'actor_loss': np.float64(-0.9870925605297088), 'hyper_actor_loss': np.float64(0.00015170018814387732), 'behavior_loss': np.float64(0.323655241727829)}

Episode step 38460, time diff 4.525533199310303, total time dif 10086.575128793716)
step: 38460 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(1.9483821), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05321964481845498), 'actor_loss': np.float64(-0.9860410213470459), 'hyper_actor_loss': np.float64(0.0001346246324828826), 'behavior_loss': np.float64(0.32047416269779205)}

Episode step 38470, time diff 4.5662689208984375, total time dif 10091.100661993027)
step: 38470 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.9847972), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07393527440726758), 'actor_loss': np.float64(-0.9911762297153472), 'hyper_actor_loss': np.float64(0.00013213036436354741), 'behavior_loss': np.float64(0.32164350152015686)}

Episode step 38480, time diff 4.477924108505249, total time dif 10095.666930913925)
step: 38480 @ episode report: {'average_total_reward': np.float32(10.983334), 'reward_variance': np.float32(3.5205002), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06751403883099556), 'actor_loss': np.float64(-1.0123255491256713), 'hyper_actor_loss': np.float64(0.00013378437943174505), 'behavior_loss': np.float64(0.31990899741649625)}

Episode step 38490, time diff 4.542782783508301, total time dif 10100.14485502243)
step: 38490 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.8690186), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07737269811332226), 'actor_loss': np.float64(-1.00430788397789), 'hyper_actor_loss': np.float64(0.0001469365968659986), 'behavior_loss': np.float64(0.3226394295692444)}

Episode step 38500, time diff 4.55394721031189, total time dif 10104.687637805939)
step: 38500 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.7189643), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06785956844687462), 'actor_loss': np.float64(-1.0166032195091248), 'hyper_actor_loss': np.float64(0.0001367768127238378), 'behavior_loss': np.float64(0.3232484102249146)}

Episode step 38510, time diff 4.533538341522217, total time dif 10109.24158501625)
step: 38510 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(5.606218), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07424927465617656), 'actor_loss': np.float64(-0.9982982575893402), 'hyper_actor_loss': np.float64(0.00012928781579830685), 'behavior_loss': np.float64(0.334039106965065)}

Episode step 38520, time diff 4.554843187332153, total time dif 10113.775123357773)
step: 38520 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(0.43800473), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056459158286452295), 'actor_loss': np.float64(-0.9862542867660522), 'hyper_actor_loss': np.float64(0.00014251048196456395), 'behavior_loss': np.float64(0.3162066787481308)}

Episode step 38530, time diff 4.527933359146118, total time dif 10118.329966545105)
step: 38530 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(1.0577052), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06406376212835312), 'actor_loss': np.float64(-0.9847133040428162), 'hyper_actor_loss': np.float64(0.000151114947948372), 'behavior_loss': np.float64(0.32127278447151186)}

Episode step 38540, time diff 4.528585195541382, total time dif 10122.857899904251)
step: 38540 @ episode report: {'average_total_reward': np.float32(10.4366665), 'reward_variance': np.float32(2.2314825), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06153249144554138), 'actor_loss': np.float64(-0.9941614985466003), 'hyper_actor_loss': np.float64(0.00014770777270314283), 'behavior_loss': np.float64(0.322196164727211)}

Episode step 38550, time diff 4.503310918807983, total time dif 10127.386485099792)
step: 38550 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.4774618), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07002614326775074), 'actor_loss': np.float64(-1.0020579636096953), 'hyper_actor_loss': np.float64(0.00014198646458680743), 'behavior_loss': np.float64(0.31572359800338745)}

Episode step 38560, time diff 4.584152460098267, total time dif 10131.8897960186)
step: 38560 @ episode report: {'average_total_reward': np.float32(9.985556), 'reward_variance': np.float32(5.5005193), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0689954213798046), 'actor_loss': np.float64(-1.011829948425293), 'hyper_actor_loss': np.float64(0.00014321835624286905), 'behavior_loss': np.float64(0.31651822924613954)}

Episode step 38570, time diff 4.584298372268677, total time dif 10136.473948478699)
step: 38570 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(4.5917788), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07109344899654388), 'actor_loss': np.float64(-1.000781911611557), 'hyper_actor_loss': np.float64(0.0001495796605013311), 'behavior_loss': np.float64(0.3171281486749649)}

Episode step 38580, time diff 4.537163019180298, total time dif 10141.058246850967)
step: 38580 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(2.87876), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06081681288778782), 'actor_loss': np.float64(-0.9955963969230652), 'hyper_actor_loss': np.float64(0.0001396397754433565), 'behavior_loss': np.float64(0.32138100266456604)}

Episode step 38590, time diff 4.548664331436157, total time dif 10145.595409870148)
step: 38590 @ episode report: {'average_total_reward': np.float32(9.6877775), 'reward_variance': np.float32(1.0123069), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058887270465493205), 'actor_loss': np.float64(-0.9887268483638764), 'hyper_actor_loss': np.float64(0.00012942645626026205), 'behavior_loss': np.float64(0.3157981842756271)}

Episode step 38600, time diff 4.550677537918091, total time dif 10150.144074201584)
step: 38600 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(5.4754624), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0531347531825304), 'actor_loss': np.float64(-0.9764767050743103), 'hyper_actor_loss': np.float64(0.00013500284549081699), 'behavior_loss': np.float64(0.31089660823345183)}

Episode step 38610, time diff 4.74284553527832, total time dif 10154.694751739502)
step: 38610 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.3807461), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06408703736960888), 'actor_loss': np.float64(-0.9913007140159606), 'hyper_actor_loss': np.float64(0.00016504686354892328), 'behavior_loss': np.float64(0.31523350477218626)}

Episode step 38620, time diff 4.599255323410034, total time dif 10159.43759727478)
step: 38620 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(3.202771), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07318478226661682), 'actor_loss': np.float64(-0.9963576078414917), 'hyper_actor_loss': np.float64(0.00015123381454031914), 'behavior_loss': np.float64(0.32327663600444795)}

Episode step 38630, time diff 4.638310194015503, total time dif 10164.03685259819)
step: 38630 @ episode report: {'average_total_reward': np.float32(10.461111), 'reward_variance': np.float32(2.1143765), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06673829182982445), 'actor_loss': np.float64(-1.0027437686920166), 'hyper_actor_loss': np.float64(0.0001634006461245008), 'behavior_loss': np.float64(0.3083627074956894)}

Episode step 38640, time diff 4.627227783203125, total time dif 10168.675162792206)
step: 38640 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.9118574), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07056635841727257), 'actor_loss': np.float64(-1.0081135153770446), 'hyper_actor_loss': np.float64(0.00017625747132115065), 'behavior_loss': np.float64(0.30800879299640654)}

Episode step 38650, time diff 4.678303241729736, total time dif 10173.302390575409)
step: 38650 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(3.0555522), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07109883427619934), 'actor_loss': np.float64(-0.9961028277873993), 'hyper_actor_loss': np.float64(0.00019255107908975334), 'behavior_loss': np.float64(0.31271040737628936)}

Episode step 38660, time diff 4.698871612548828, total time dif 10177.980693817139)
step: 38660 @ episode report: {'average_total_reward': np.float32(10.073333), 'reward_variance': np.float32(5.899512), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06633720584213734), 'actor_loss': np.float64(-0.994996702671051), 'hyper_actor_loss': np.float64(0.00018491939408704638), 'behavior_loss': np.float64(0.30804134607315065)}

Episode step 38670, time diff 4.718452215194702, total time dif 10182.679565429688)
step: 38670 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(2.2524393), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06392760388553143), 'actor_loss': np.float64(-1.002616810798645), 'hyper_actor_loss': np.float64(0.0001770999384461902), 'behavior_loss': np.float64(0.312556067109108)}

Episode step 38680, time diff 4.752249717712402, total time dif 10187.398017644882)
step: 38680 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(1.4775562), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06874584536999465), 'actor_loss': np.float64(-0.9887847423553466), 'hyper_actor_loss': np.float64(0.00019150680309394376), 'behavior_loss': np.float64(0.30876792371273043)}

Episode step 38690, time diff 4.782449007034302, total time dif 10192.150267362595)
step: 38690 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.3653393), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07280617244541646), 'actor_loss': np.float64(-1.0169467687606812), 'hyper_actor_loss': np.float64(0.00018701397348195315), 'behavior_loss': np.float64(0.31501293182373047)}

Episode step 38700, time diff 4.667253732681274, total time dif 10196.932716369629)
step: 38700 @ episode report: {'average_total_reward': np.float32(10.34889), 'reward_variance': np.float32(3.2190175), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0726870309561491), 'actor_loss': np.float64(-1.007021391391754), 'hyper_actor_loss': np.float64(0.00018270066211698577), 'behavior_loss': np.float64(0.3161831021308899)}

Episode step 38710, time diff 4.673063278198242, total time dif 10201.59997010231)
step: 38710 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(3.9395874), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0566479342058301), 'actor_loss': np.float64(-0.9909753322601318), 'hyper_actor_loss': np.float64(0.00018424098379909992), 'behavior_loss': np.float64(0.3000412732362747)}

Episode step 38720, time diff 4.6584625244140625, total time dif 10206.273033380508)
step: 38720 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(1.6295907), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06830619536340236), 'actor_loss': np.float64(-0.9815835237503052), 'hyper_actor_loss': np.float64(0.0001745588073390536), 'behavior_loss': np.float64(0.30336000621318815)}

Episode step 38730, time diff 4.619428396224976, total time dif 10210.931495904922)
step: 38730 @ episode report: {'average_total_reward': np.float32(9.84889), 'reward_variance': np.float32(1.5901775), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06848639585077762), 'actor_loss': np.float64(-0.9968017339706421), 'hyper_actor_loss': np.float64(0.00017564265290275215), 'behavior_loss': np.float64(0.3115990340709686)}

Episode step 38740, time diff 4.591696500778198, total time dif 10215.550924301147)
step: 38740 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(3.6077092), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07318319194018841), 'actor_loss': np.float64(-1.0092094182968139), 'hyper_actor_loss': np.float64(0.00015986859434633516), 'behavior_loss': np.float64(0.3092534333467484)}

Episode step 38750, time diff 4.5236992835998535, total time dif 10220.142620801926)
step: 38750 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.2695012), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.051356295309960845), 'actor_loss': np.float64(-0.9878168702125549), 'hyper_actor_loss': np.float64(0.000145080842776224), 'behavior_loss': np.float64(0.29297299683094025)}

Episode step 38760, time diff 4.5341410636901855, total time dif 10224.666320085526)
step: 38760 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(1.2536165), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06509072966873646), 'actor_loss': np.float64(-0.9722734153270721), 'hyper_actor_loss': np.float64(0.0001412387893651612), 'behavior_loss': np.float64(0.30462604463100434)}

Episode step 38770, time diff 4.586119174957275, total time dif 10229.200461149216)
step: 38770 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(0.7168099), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07191939763724804), 'actor_loss': np.float64(-1.005573534965515), 'hyper_actor_loss': np.float64(0.00014831923181191087), 'behavior_loss': np.float64(0.30734359920024873)}

Episode step 38780, time diff 4.851831674575806, total time dif 10233.786580324173)
step: 38780 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.0747216), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07889412194490433), 'actor_loss': np.float64(-1.0339340448379517), 'hyper_actor_loss': np.float64(0.0001559079515573103), 'behavior_loss': np.float64(0.3009390771389008)}

Episode step 38790, time diff 4.6898417472839355, total time dif 10238.638411998749)
step: 38790 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.9352362), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06243031285703182), 'actor_loss': np.float64(-1.003487080335617), 'hyper_actor_loss': np.float64(0.00015173936990322545), 'behavior_loss': np.float64(0.3043502151966095)}

Episode step 38800, time diff 4.63679838180542, total time dif 10243.328253746033)
step: 38800 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(2.959649), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06023454405367375), 'actor_loss': np.float64(-0.9642825484275818), 'hyper_actor_loss': np.float64(0.00015168552490649745), 'behavior_loss': np.float64(0.3044199973344803)}

Episode step 38810, time diff 4.681893348693848, total time dif 10247.965052127838)
step: 38810 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(4.1607165), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06795158311724663), 'actor_loss': np.float64(-0.9854634463787079), 'hyper_actor_loss': np.float64(0.00015665604296373204), 'behavior_loss': np.float64(0.2991901457309723)}

Episode step 38820, time diff 4.719054937362671, total time dif 10252.646945476532)
step: 38820 @ episode report: {'average_total_reward': np.float32(9.126667), 'reward_variance': np.float32(2.2535853), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0661547001451254), 'actor_loss': np.float64(-1.0055269956588746), 'hyper_actor_loss': np.float64(0.00015883324085734784), 'behavior_loss': np.float64(0.3115559846162796)}

Episode step 38830, time diff 4.692477226257324, total time dif 10257.366000413895)
step: 38830 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(3.7227955), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07203704118728638), 'actor_loss': np.float64(-0.9935327410697937), 'hyper_actor_loss': np.float64(0.0001694712816970423), 'behavior_loss': np.float64(0.29694590270519255)}

Episode step 38840, time diff 4.656409740447998, total time dif 10262.058477640152)
step: 38840 @ episode report: {'average_total_reward': np.float32(9.190001), 'reward_variance': np.float32(2.0360856), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06871729753911496), 'actor_loss': np.float64(-1.0026501774787904), 'hyper_actor_loss': np.float64(0.00016727261972846464), 'behavior_loss': np.float64(0.2998728036880493)}

Episode step 38850, time diff 4.707275390625, total time dif 10266.7148873806)
step: 38850 @ episode report: {'average_total_reward': np.float32(9.312223), 'reward_variance': np.float32(4.7743077), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07002656981348991), 'actor_loss': np.float64(-0.9973012626171112), 'hyper_actor_loss': np.float64(0.00016748009074945002), 'behavior_loss': np.float64(0.2944538861513138)}

Episode step 38860, time diff 4.632680416107178, total time dif 10271.422162771225)
step: 38860 @ episode report: {'average_total_reward': np.float32(8.663335), 'reward_variance': np.float32(2.1056314), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059357345849275586), 'actor_loss': np.float64(-0.9972261488437653), 'hyper_actor_loss': np.float64(0.00016010541003197432), 'behavior_loss': np.float64(0.3008605748414993)}

Episode step 38870, time diff 4.669868469238281, total time dif 10276.054843187332)
step: 38870 @ episode report: {'average_total_reward': np.float32(9.824444), 'reward_variance': np.float32(4.743773), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06681608818471432), 'actor_loss': np.float64(-0.9917155027389526), 'hyper_actor_loss': np.float64(0.00015992239132174292), 'behavior_loss': np.float64(0.2984857767820358)}

Episode step 38880, time diff 4.628518581390381, total time dif 10280.72471165657)
step: 38880 @ episode report: {'average_total_reward': np.float32(8.514444), 'reward_variance': np.float32(6.7624693), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06306651476770639), 'actor_loss': np.float64(-0.9868575096130371), 'hyper_actor_loss': np.float64(0.00014536849557771346), 'behavior_loss': np.float64(0.2935555398464203)}

Episode step 38890, time diff 4.547924995422363, total time dif 10285.35323023796)
step: 38890 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(1.5551618), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.048753576911985876), 'actor_loss': np.float64(-0.9758724570274353), 'hyper_actor_loss': np.float64(0.0001482003426644951), 'behavior_loss': np.float64(0.292217481136322)}

Episode step 38900, time diff 4.5807945728302, total time dif 10289.901155233383)
step: 38900 @ episode report: {'average_total_reward': np.float32(8.302222), 'reward_variance': np.float32(2.9681685), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07281415499746799), 'actor_loss': np.float64(-0.9897019386291503), 'hyper_actor_loss': np.float64(0.000149416542990366), 'behavior_loss': np.float64(0.30248402059078217)}

Episode step 38910, time diff 4.644332408905029, total time dif 10294.481949806213)
step: 38910 @ episode report: {'average_total_reward': np.float32(8.98778), 'reward_variance': np.float32(2.0813692), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06329981572926044), 'actor_loss': np.float64(-1.0084218859672547), 'hyper_actor_loss': np.float64(0.0001637499954085797), 'behavior_loss': np.float64(0.29364771246910093)}

Episode step 38920, time diff 4.5513083934783936, total time dif 10299.126282215118)
step: 38920 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(2.1459024), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.052019865810871126), 'actor_loss': np.float64(-0.9880462348461151), 'hyper_actor_loss': np.float64(0.00014816831753705628), 'behavior_loss': np.float64(0.2908256024122238)}

Episode step 38930, time diff 4.555372714996338, total time dif 10303.677590608597)
step: 38930 @ episode report: {'average_total_reward': np.float32(8.765556), 'reward_variance': np.float32(1.951987), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06917582601308822), 'actor_loss': np.float64(-0.9802950859069824), 'hyper_actor_loss': np.float64(0.00016046027740230785), 'behavior_loss': np.float64(0.2989832520484924)}

Episode step 38940, time diff 4.733864784240723, total time dif 10308.232963323593)
step: 38940 @ episode report: {'average_total_reward': np.float32(8.877778), 'reward_variance': np.float32(3.1813326), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0673988837748766), 'actor_loss': np.float64(-0.9956086099147796), 'hyper_actor_loss': np.float64(0.00016229907050728797), 'behavior_loss': np.float64(0.29770750999450685)}

Episode step 38950, time diff 4.625203847885132, total time dif 10312.966828107834)
step: 38950 @ episode report: {'average_total_reward': np.float32(9.414445), 'reward_variance': np.float32(1.1587421), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05984498932957649), 'actor_loss': np.float64(-0.9992785453796387), 'hyper_actor_loss': np.float64(0.00014884532065480017), 'behavior_loss': np.float64(0.2829246312379837)}

Episode step 38960, time diff 4.578522205352783, total time dif 10317.592031955719)
step: 38960 @ episode report: {'average_total_reward': np.float32(8.551111), 'reward_variance': np.float32(5.7195606), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06056881323456764), 'actor_loss': np.float64(-0.9849353730678558), 'hyper_actor_loss': np.float64(0.00016623122282908299), 'behavior_loss': np.float64(0.28931053578853605)}

Episode step 38970, time diff 4.626619815826416, total time dif 10322.170554161072)
step: 38970 @ episode report: {'average_total_reward': np.float32(8.090001), 'reward_variance': np.float32(2.467912), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07414021566510201), 'actor_loss': np.float64(-1.0028144657611846), 'hyper_actor_loss': np.float64(0.00016028546742745674), 'behavior_loss': np.float64(0.28588915467262266)}

Episode step 38980, time diff 4.657712697982788, total time dif 10326.797173976898)
step: 38980 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(1.3902735), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07001056410372257), 'actor_loss': np.float64(-1.0282576084136963), 'hyper_actor_loss': np.float64(0.0001790528607671149), 'behavior_loss': np.float64(0.2859396129846573)}

Episode step 38990, time diff 4.636965274810791, total time dif 10331.454886674881)
step: 38990 @ episode report: {'average_total_reward': np.float32(8.265556), 'reward_variance': np.float32(3.2860863), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(4.411111), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06314291059970856), 'actor_loss': np.float64(-0.9897916078567505), 'hyper_actor_loss': np.float64(0.00016954742750385776), 'behavior_loss': np.float64(0.3022279620170593)}

Episode step 39000, time diff 4.620754241943359, total time dif 10336.091851949692)
step: 39000 @ episode report: {'average_total_reward': np.float32(8.402224), 'reward_variance': np.float32(2.16523), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07681946307420731), 'actor_loss': np.float64(-0.9758602499961853), 'hyper_actor_loss': np.float64(0.0001573394700244535), 'behavior_loss': np.float64(0.2907075107097626)}

Episode step 39010, time diff 4.508261203765869, total time dif 10340.712606191635)
step: 39010 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(6.0884447), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06372893005609512), 'actor_loss': np.float64(-1.0085444152355194), 'hyper_actor_loss': np.float64(0.00013895273732487113), 'behavior_loss': np.float64(0.2956532835960388)}

Episode step 39020, time diff 4.533987998962402, total time dif 10345.220867395401)
step: 39020 @ episode report: {'average_total_reward': np.float32(9.338888), 'reward_variance': np.float32(0.7512405), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06594361402094365), 'actor_loss': np.float64(-0.9881688177585601), 'hyper_actor_loss': np.float64(0.0001332583837211132), 'behavior_loss': np.float64(0.2965396225452423)}

Episode step 39030, time diff 4.526087999343872, total time dif 10349.754855394363)
step: 39030 @ episode report: {'average_total_reward': np.float32(8.214445), 'reward_variance': np.float32(2.8472857), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07083643153309822), 'actor_loss': np.float64(-0.9916163682937622), 'hyper_actor_loss': np.float64(0.0001316233530815225), 'behavior_loss': np.float64(0.2914833456277847)}

Episode step 39040, time diff 4.568882942199707, total time dif 10354.280943393707)
step: 39040 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(0.6928686), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06691098362207412), 'actor_loss': np.float64(-1.0062674164772034), 'hyper_actor_loss': np.float64(0.00012728276851703413), 'behavior_loss': np.float64(0.28073922395706175)}

Episode step 39050, time diff 4.624641418457031, total time dif 10358.849826335907)
step: 39050 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(3.3458226), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07257385402917862), 'actor_loss': np.float64(-1.0063917636871338), 'hyper_actor_loss': np.float64(0.00014552547509083524), 'behavior_loss': np.float64(0.2871734082698822)}

Episode step 39060, time diff 4.613653182983398, total time dif 10363.474467754364)
step: 39060 @ episode report: {'average_total_reward': np.float32(8.004445), 'reward_variance': np.float32(1.7229431), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07164719961583614), 'actor_loss': np.float64(-0.9949390888214111), 'hyper_actor_loss': np.float64(0.00014365744864335283), 'behavior_loss': np.float64(0.29885385036468504)}

Episode step 39070, time diff 4.576572418212891, total time dif 10368.088120937347)
step: 39070 @ episode report: {'average_total_reward': np.float32(8.502222), 'reward_variance': np.float32(3.1428351), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06494696289300919), 'actor_loss': np.float64(-0.9901640117168427), 'hyper_actor_loss': np.float64(0.00014025127020431683), 'behavior_loss': np.float64(0.29762332439422606)}

Episode step 39080, time diff 4.505961179733276, total time dif 10372.66469335556)
step: 39080 @ episode report: {'average_total_reward': np.float32(8.614446), 'reward_variance': np.float32(3.00331), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06886334922164679), 'actor_loss': np.float64(-0.9988943696022033), 'hyper_actor_loss': np.float64(0.00012532295222626998), 'behavior_loss': np.float64(0.28124418556690217)}

Episode step 39090, time diff 4.714812755584717, total time dif 10377.170654535294)
step: 39090 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(3.054425), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06256939806044101), 'actor_loss': np.float64(-1.0016198635101319), 'hyper_actor_loss': np.float64(0.00013180455935071222), 'behavior_loss': np.float64(0.2880107581615448)}

Episode step 39100, time diff 4.546191215515137, total time dif 10381.885467290878)
step: 39100 @ episode report: {'average_total_reward': np.float32(8.365557), 'reward_variance': np.float32(3.3443565), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06314377300441265), 'actor_loss': np.float64(-0.9946955502033233), 'hyper_actor_loss': np.float64(0.0001232377231644932), 'behavior_loss': np.float64(0.2762432560324669)}

Episode step 39110, time diff 4.480464458465576, total time dif 10386.431658506393)
step: 39110 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(1.8510323), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07177910394966602), 'actor_loss': np.float64(-0.9968915998935699), 'hyper_actor_loss': np.float64(0.0001288516228669323), 'behavior_loss': np.float64(0.2832126885652542)}

Episode step 39120, time diff 4.421828746795654, total time dif 10390.912122964859)
step: 39120 @ episode report: {'average_total_reward': np.float32(7.8144455), 'reward_variance': np.float32(4.7419033), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06940563283860683), 'actor_loss': np.float64(-0.9961154222488403), 'hyper_actor_loss': np.float64(0.00010972902964567766), 'behavior_loss': np.float64(0.2977194130420685)}

Episode step 39130, time diff 4.479804515838623, total time dif 10395.333951711655)
step: 39130 @ episode report: {'average_total_reward': np.float32(9.387779), 'reward_variance': np.float32(4.3653936), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07149963025003672), 'actor_loss': np.float64(-1.011843466758728), 'hyper_actor_loss': np.float64(0.00010704055821406656), 'behavior_loss': np.float64(0.2854646548628807)}

Episode step 39140, time diff 4.510605335235596, total time dif 10399.813756227493)
step: 39140 @ episode report: {'average_total_reward': np.float32(7.816667), 'reward_variance': np.float32(2.792772), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06400017365813256), 'actor_loss': np.float64(-0.9924048840999603), 'hyper_actor_loss': np.float64(0.00011298764366074465), 'behavior_loss': np.float64(0.29256882071495055)}

Episode step 39150, time diff 4.4603564739227295, total time dif 10404.324361562729)
step: 39150 @ episode report: {'average_total_reward': np.float32(8.004446), 'reward_variance': np.float32(2.823709), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06083190198987722), 'actor_loss': np.float64(-0.9784340798854828), 'hyper_actor_loss': np.float64(0.00012050510122207925), 'behavior_loss': np.float64(0.2898848816752434)}

Episode step 39160, time diff 4.427747488021851, total time dif 10408.784718036652)
step: 39160 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.722531), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06744337026029826), 'actor_loss': np.float64(-0.9956052720546722), 'hyper_actor_loss': np.float64(0.00012539845411083662), 'behavior_loss': np.float64(0.2794483006000519)}

Episode step 39170, time diff 4.383463144302368, total time dif 10413.212465524673)
step: 39170 @ episode report: {'average_total_reward': np.float32(9.114446), 'reward_variance': np.float32(2.926371), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05977984499186277), 'actor_loss': np.float64(-1.0000776529312134), 'hyper_actor_loss': np.float64(0.0001194646320072934), 'behavior_loss': np.float64(0.2942685231566429)}

Episode step 39180, time diff 4.385800123214722, total time dif 10417.595928668976)
step: 39180 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.9050236), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07351426109671592), 'actor_loss': np.float64(-0.9921648144721985), 'hyper_actor_loss': np.float64(0.00011709394530043938), 'behavior_loss': np.float64(0.2914846628904343)}

Episode step 39190, time diff 4.335448980331421, total time dif 10421.98172879219)
step: 39190 @ episode report: {'average_total_reward': np.float32(8.165556), 'reward_variance': np.float32(2.8357158), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07248295433819293), 'actor_loss': np.float64(-1.0030170559883118), 'hyper_actor_loss': np.float64(0.00011699778915499336), 'behavior_loss': np.float64(0.2890323638916016)}

Episode step 39200, time diff 4.407677173614502, total time dif 10426.317177772522)
step: 39200 @ episode report: {'average_total_reward': np.float32(8.241112), 'reward_variance': np.float32(3.23899), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06465076953172684), 'actor_loss': np.float64(-0.992979621887207), 'hyper_actor_loss': np.float64(0.00012048539647366852), 'behavior_loss': np.float64(0.29496284425258634)}

Episode step 39210, time diff 4.417569160461426, total time dif 10430.724854946136)
step: 39210 @ episode report: {'average_total_reward': np.float32(8.426667), 'reward_variance': np.float32(3.5680547), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0726014468818903), 'actor_loss': np.float64(-0.9896898448467255), 'hyper_actor_loss': np.float64(0.00010956959376926534), 'behavior_loss': np.float64(0.28493650555610656)}

Episode step 39220, time diff 4.422871112823486, total time dif 10435.142424106598)
step: 39220 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(2.4534116), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06688433140516281), 'actor_loss': np.float64(-1.0068161487579346), 'hyper_actor_loss': np.float64(0.0001142405359132681), 'behavior_loss': np.float64(0.28764388859272005)}

Episode step 39230, time diff 4.467095851898193, total time dif 10439.565295219421)
step: 39230 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(1.9834188), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07919437363743782), 'actor_loss': np.float64(-0.9984824001789093), 'hyper_actor_loss': np.float64(0.0001223368824867066), 'behavior_loss': np.float64(0.30186328291893005)}

Episode step 39240, time diff 4.536916017532349, total time dif 10444.03239107132)
step: 39240 @ episode report: {'average_total_reward': np.float32(7.8288894), 'reward_variance': np.float32(3.361288), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07230608016252518), 'actor_loss': np.float64(-0.9982472896575928), 'hyper_actor_loss': np.float64(0.00011704239368555136), 'behavior_loss': np.float64(0.29911095201969146)}

Episode step 39250, time diff 4.507545709609985, total time dif 10448.569307088852)
step: 39250 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(3.5113893), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07005928382277489), 'actor_loss': np.float64(-0.982081937789917), 'hyper_actor_loss': np.float64(0.0001311117004661355), 'behavior_loss': np.float64(0.2985293954610825)}

Episode step 39260, time diff 4.625202894210815, total time dif 10453.076852798462)
step: 39260 @ episode report: {'average_total_reward': np.float32(8.651111), 'reward_variance': np.float32(3.8673139), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06597814653068781), 'actor_loss': np.float64(-0.9927356123924256), 'hyper_actor_loss': np.float64(0.00013740193753619678), 'behavior_loss': np.float64(0.30433018803596495)}

Episode step 39270, time diff 4.576495409011841, total time dif 10457.702055692673)
step: 39270 @ episode report: {'average_total_reward': np.float32(8.277779), 'reward_variance': np.float32(2.1857045), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05465603824704886), 'actor_loss': np.float64(-0.9848886549472808), 'hyper_actor_loss': np.float64(0.00014920985340722838), 'behavior_loss': np.float64(0.28836485743522644)}

Episode step 39280, time diff 4.529953718185425, total time dif 10462.278551101685)
step: 39280 @ episode report: {'average_total_reward': np.float32(8.341112), 'reward_variance': np.float32(3.2607675), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07091788575053215), 'actor_loss': np.float64(-0.9811216175556183), 'hyper_actor_loss': np.float64(0.00015259055217029527), 'behavior_loss': np.float64(0.2897693932056427)}

Episode step 39290, time diff 4.574379205703735, total time dif 10466.80850481987)
step: 39290 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(1.9818087), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07214304096996785), 'actor_loss': np.float64(-1.0087639212608337), 'hyper_actor_loss': np.float64(0.000151956282206811), 'behavior_loss': np.float64(0.2873771548271179)}

Episode step 39300, time diff 4.563677549362183, total time dif 10471.382884025574)
step: 39300 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(3.383501), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.1666675), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05508923418819904), 'actor_loss': np.float64(-1.0006254196166993), 'hyper_actor_loss': np.float64(0.0001797912365873344), 'behavior_loss': np.float64(0.2857573926448822)}

Episode step 39310, time diff 4.5822930335998535, total time dif 10475.946561574936)
step: 39310 @ episode report: {'average_total_reward': np.float32(10.597779), 'reward_variance': np.float32(1.8956985), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06460744589567184), 'actor_loss': np.float64(-0.9769620180130005), 'hyper_actor_loss': np.float64(0.00018793987546814606), 'behavior_loss': np.float64(0.29431177377700807)}

Episode step 39320, time diff 4.626943111419678, total time dif 10480.528854608536)
step: 39320 @ episode report: {'average_total_reward': np.float32(9.861113), 'reward_variance': np.float32(4.5329943), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06856884509325027), 'actor_loss': np.float64(-0.9934074878692627), 'hyper_actor_loss': np.float64(0.00017561197455506772), 'behavior_loss': np.float64(0.29304391145706177)}

Episode step 39330, time diff 4.58328652381897, total time dif 10485.155797719955)
step: 39330 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.0584884), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05977840479463339), 'actor_loss': np.float64(-0.9966880023479462), 'hyper_actor_loss': np.float64(0.00017769831174518913), 'behavior_loss': np.float64(0.30121291875839235)}

Episode step 39340, time diff 4.6184000968933105, total time dif 10489.739084243774)
step: 39340 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(4.453607), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05499153919517994), 'actor_loss': np.float64(-0.9700303137302398), 'hyper_actor_loss': np.float64(0.00017793599981814624), 'behavior_loss': np.float64(0.3093849152326584)}

Episode step 39350, time diff 4.631820917129517, total time dif 10494.357484340668)
step: 39350 @ episode report: {'average_total_reward': np.float32(10.04889), 'reward_variance': np.float32(2.944721), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05621894598007202), 'actor_loss': np.float64(-0.9589342713356018), 'hyper_actor_loss': np.float64(0.00016884358919924125), 'behavior_loss': np.float64(0.2891915261745453)}

Episode step 39360, time diff 4.6262500286102295, total time dif 10498.989305257797)
step: 39360 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(2.2267027), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06133527923375368), 'actor_loss': np.float64(-0.9869341969490051), 'hyper_actor_loss': np.float64(0.00016945348324952646), 'behavior_loss': np.float64(0.2970772355794907)}

Episode step 39370, time diff 4.645642280578613, total time dif 10503.615555286407)
step: 39370 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(3.058539), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06425961386412382), 'actor_loss': np.float64(-0.9934410572052002), 'hyper_actor_loss': np.float64(0.00017795048624975608), 'behavior_loss': np.float64(0.2968941807746887)}

Episode step 39380, time diff 4.66691255569458, total time dif 10508.261197566986)
step: 39380 @ episode report: {'average_total_reward': np.float32(11.244446), 'reward_variance': np.float32(2.8606424), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07086000349372626), 'actor_loss': np.float64(-0.9884887874126435), 'hyper_actor_loss': np.float64(0.00019828717049676924), 'behavior_loss': np.float64(0.29470897018909453)}

Episode step 39390, time diff 4.642733812332153, total time dif 10512.92811012268)
step: 39390 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(3.0230327), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06870912499725819), 'actor_loss': np.float64(-0.9939499139785767), 'hyper_actor_loss': np.float64(0.0002696270414162427), 'behavior_loss': np.float64(0.30163473784923556)}

Episode step 39400, time diff 4.616201400756836, total time dif 10517.570843935013)
step: 39400 @ episode report: {'average_total_reward': np.float32(10.073335), 'reward_variance': np.float32(2.9652648), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05448110345751047), 'actor_loss': np.float64(-0.9853835642337799), 'hyper_actor_loss': np.float64(0.00032702187600079923), 'behavior_loss': np.float64(0.30933322906494143)}

Episode step 39410, time diff 4.651214599609375, total time dif 10522.18704533577)
step: 39410 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(3.0768838), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07317519895732402), 'actor_loss': np.float64(-0.9778461217880249), 'hyper_actor_loss': np.float64(0.00037813984672538934), 'behavior_loss': np.float64(0.2994582176208496)}

Episode step 39420, time diff 4.805964946746826, total time dif 10526.838259935379)
step: 39420 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(5.266475), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056463449448347094), 'actor_loss': np.float64(-1.0018622994422912), 'hyper_actor_loss': np.float64(0.0003953975741751492), 'behavior_loss': np.float64(0.31384955942630766)}

Episode step 39430, time diff 4.600160121917725, total time dif 10531.644224882126)
step: 39430 @ episode report: {'average_total_reward': np.float32(10.758889), 'reward_variance': np.float32(1.0486182), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06816430184990167), 'actor_loss': np.float64(-0.9953727185726166), 'hyper_actor_loss': np.float64(0.00039702234498690815), 'behavior_loss': np.float64(0.3085064232349396)}

Episode step 39440, time diff 4.661752700805664, total time dif 10536.244385004044)
step: 39440 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(1.353369), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05769269410520792), 'actor_loss': np.float64(-0.9849721789360046), 'hyper_actor_loss': np.float64(0.00040804985328577457), 'behavior_loss': np.float64(0.3123577356338501)}

Episode step 39450, time diff 4.636032342910767, total time dif 10540.90613770485)
step: 39450 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(1.4968841), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06771319657564163), 'actor_loss': np.float64(-0.9910089433193207), 'hyper_actor_loss': np.float64(0.0004092775809112936), 'behavior_loss': np.float64(0.32371685802936556)}

Episode step 39460, time diff 4.624241590499878, total time dif 10545.54217004776)
step: 39460 @ episode report: {'average_total_reward': np.float32(10.573335), 'reward_variance': np.float32(3.1426966), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06091489344835281), 'actor_loss': np.float64(-0.9889614462852478), 'hyper_actor_loss': np.float64(0.0003929796104785055), 'behavior_loss': np.float64(0.319284051656723)}

Episode step 39470, time diff 4.594613552093506, total time dif 10550.16641163826)
step: 39470 @ episode report: {'average_total_reward': np.float32(10.248888), 'reward_variance': np.float32(1.8356594), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06916380450129508), 'actor_loss': np.float64(-0.980825912952423), 'hyper_actor_loss': np.float64(0.0003513055096846074), 'behavior_loss': np.float64(0.3236387848854065)}

Episode step 39480, time diff 4.633387088775635, total time dif 10554.761025190353)
step: 39480 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(3.098919), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06835235469043255), 'actor_loss': np.float64(-0.9868951976299286), 'hyper_actor_loss': np.float64(0.00033743460080586373), 'behavior_loss': np.float64(0.3243803560733795)}

Episode step 39490, time diff 4.582854270935059, total time dif 10559.394412279129)
step: 39490 @ episode report: {'average_total_reward': np.float32(11.195556), 'reward_variance': np.float32(3.3288689), 'max_total_reward': np.float32(14.388888), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060466003604233265), 'actor_loss': np.float64(-0.998876690864563), 'hyper_actor_loss': np.float64(0.0003257526026573032), 'behavior_loss': np.float64(0.3150318145751953)}

Episode step 39500, time diff 4.566071033477783, total time dif 10563.977266550064)
step: 39500 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(2.8716066), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07013996839523315), 'actor_loss': np.float64(-1.0010413825511932), 'hyper_actor_loss': np.float64(0.0002889622395741753), 'behavior_loss': np.float64(0.32104827761650084)}

Episode step 39510, time diff 4.614516496658325, total time dif 10568.543337583542)
step: 39510 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(3.55928), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0665944691747427), 'actor_loss': np.float64(-0.9898382902145386), 'hyper_actor_loss': np.float64(0.00026337861054344104), 'behavior_loss': np.float64(0.3210599958896637)}

Episode step 39520, time diff 4.592799663543701, total time dif 10573.1578540802)
step: 39520 @ episode report: {'average_total_reward': np.float32(11.520001), 'reward_variance': np.float32(2.256489), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06770162209868431), 'actor_loss': np.float64(-0.9999414265155793), 'hyper_actor_loss': np.float64(0.00024217002501245589), 'behavior_loss': np.float64(0.3263569861650467)}

Episode step 39530, time diff 4.578960180282593, total time dif 10577.750653743744)
step: 39530 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(2.842693), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06450532972812653), 'actor_loss': np.float64(-1.0016321897506715), 'hyper_actor_loss': np.float64(0.0002313552628038451), 'behavior_loss': np.float64(0.32636128067970277)}

Episode step 39540, time diff 4.576725244522095, total time dif 10582.329613924026)
step: 39540 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.2580004), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06779036521911622), 'actor_loss': np.float64(-0.974092161655426), 'hyper_actor_loss': np.float64(0.00021864425216335803), 'behavior_loss': np.float64(0.32953945100307463)}

Episode step 39550, time diff 4.558149099349976, total time dif 10586.906339168549)
step: 39550 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(1.8614769), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05661890916526317), 'actor_loss': np.float64(-0.9791596055030822), 'hyper_actor_loss': np.float64(0.0002121490295394324), 'behavior_loss': np.float64(0.32490564584732057)}

Episode step 39560, time diff 4.593840837478638, total time dif 10591.464488267899)
step: 39560 @ episode report: {'average_total_reward': np.float32(11.868889), 'reward_variance': np.float32(1.4619949), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06168813407421112), 'actor_loss': np.float64(-0.9862615048885346), 'hyper_actor_loss': np.float64(0.00022287300380412488), 'behavior_loss': np.float64(0.32128030359745025)}

Episode step 39570, time diff 4.570137023925781, total time dif 10596.058329105377)
step: 39570 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(2.1587453), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05817460063844919), 'actor_loss': np.float64(-0.9954620599746704), 'hyper_actor_loss': np.float64(0.00021223104995442553), 'behavior_loss': np.float64(0.33568153381347654)}

Episode step 39580, time diff 4.561397314071655, total time dif 10600.628466129303)
step: 39580 @ episode report: {'average_total_reward': np.float32(11.781112), 'reward_variance': np.float32(3.938891), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05989543367177248), 'actor_loss': np.float64(-0.9892661392688751), 'hyper_actor_loss': np.float64(0.00019161072559654714), 'behavior_loss': np.float64(0.3253728836774826)}

Episode step 39590, time diff 4.763768911361694, total time dif 10605.189863443375)
step: 39590 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(2.3324463), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053146278113126756), 'actor_loss': np.float64(-0.9782879650592804), 'hyper_actor_loss': np.float64(0.00019811915117315949), 'behavior_loss': np.float64(0.3260753214359283)}

Episode step 39600, time diff 4.612525463104248, total time dif 10609.953632354736)
step: 39600 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(1.8733835), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057556443847715855), 'actor_loss': np.float64(-0.9787946164608001), 'hyper_actor_loss': np.float64(0.0001983091191505082), 'behavior_loss': np.float64(0.33077714443206785)}

Episode step 39610, time diff 4.6120524406433105, total time dif 10614.56615781784)
step: 39610 @ episode report: {'average_total_reward': np.float32(10.871112), 'reward_variance': np.float32(4.1448207), 'max_total_reward': np.float32(15.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07342835813760758), 'actor_loss': np.float64(-0.9963690876960755), 'hyper_actor_loss': np.float64(0.0001818726639612578), 'behavior_loss': np.float64(0.33101609647274016)}

Episode step 39620, time diff 4.590223073959351, total time dif 10619.178210258484)
step: 39620 @ episode report: {'average_total_reward': np.float32(10.54889), 'reward_variance': np.float32(5.064821), 'max_total_reward': np.float32(15.266667), 'min_total_reward': np.float32(7.2888894), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056825007498264316), 'actor_loss': np.float64(-1.0004551470279694), 'hyper_actor_loss': np.float64(0.00018220786150777713), 'behavior_loss': np.float64(0.33948133885860443)}

Episode step 39630, time diff 4.541383504867554, total time dif 10623.768433332443)
step: 39630 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(1.4874123), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06684575229883194), 'actor_loss': np.float64(-0.9744136214256287), 'hyper_actor_loss': np.float64(0.00017483740084571763), 'behavior_loss': np.float64(0.33971490859985354)}

Episode step 39640, time diff 4.531292915344238, total time dif 10628.30981683731)
step: 39640 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(1.7890927), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07101466991007328), 'actor_loss': np.float64(-0.9862078905105591), 'hyper_actor_loss': np.float64(0.0001623608739464544), 'behavior_loss': np.float64(0.34248978197574614)}

Episode step 39650, time diff 4.506248474121094, total time dif 10632.841109752655)
step: 39650 @ episode report: {'average_total_reward': np.float32(10.934445), 'reward_variance': np.float32(1.5042827), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06460955291986466), 'actor_loss': np.float64(-0.9969942033290863), 'hyper_actor_loss': np.float64(0.00015639291377738117), 'behavior_loss': np.float64(0.33400696218013765)}

Episode step 39660, time diff 4.509546279907227, total time dif 10637.347358226776)
step: 39660 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(1.6968019), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06323204338550567), 'actor_loss': np.float64(-0.9965043663978577), 'hyper_actor_loss': np.float64(0.00013471195125021042), 'behavior_loss': np.float64(0.34259977340698244)}

Episode step 39670, time diff 4.563004732131958, total time dif 10641.856904506683)
step: 39670 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(2.936345), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07194209136068822), 'actor_loss': np.float64(-0.984358012676239), 'hyper_actor_loss': np.float64(0.00014535171721945516), 'behavior_loss': np.float64(0.33100059628486633)}

Episode step 39680, time diff 4.525993824005127, total time dif 10646.419909238815)
step: 39680 @ episode report: {'average_total_reward': np.float32(12.017778), 'reward_variance': np.float32(4.02593), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06017615031450987), 'actor_loss': np.float64(-0.991831374168396), 'hyper_actor_loss': np.float64(0.00012169926339993254), 'behavior_loss': np.float64(0.34317573010921476)}

Episode step 39690, time diff 4.539795875549316, total time dif 10650.94590306282)
step: 39690 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(3.884105), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058304260671138766), 'actor_loss': np.float64(-0.9750582814216614), 'hyper_actor_loss': np.float64(0.00012116672514821403), 'behavior_loss': np.float64(0.3415302842855453)}

Episode step 39700, time diff 4.520955324172974, total time dif 10655.48569893837)
step: 39700 @ episode report: {'average_total_reward': np.float32(11.283335), 'reward_variance': np.float32(2.412154), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07564756162464618), 'actor_loss': np.float64(-0.9857917368412018), 'hyper_actor_loss': np.float64(9.972655971068889e-05), 'behavior_loss': np.float64(0.3390908479690552)}

Episode step 39710, time diff 4.563508987426758, total time dif 10660.006654262543)
step: 39710 @ episode report: {'average_total_reward': np.float32(10.297777), 'reward_variance': np.float32(4.484514), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0586816169321537), 'actor_loss': np.float64(-0.997953063249588), 'hyper_actor_loss': np.float64(0.00011012357426807285), 'behavior_loss': np.float64(0.3469065070152283)}

Episode step 39720, time diff 4.53928279876709, total time dif 10664.57016324997)
step: 39720 @ episode report: {'average_total_reward': np.float32(10.834445), 'reward_variance': np.float32(5.3713465), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06482479721307755), 'actor_loss': np.float64(-0.9705824732780457), 'hyper_actor_loss': np.float64(9.977776498999446e-05), 'behavior_loss': np.float64(0.34301199615001676)}

Episode step 39730, time diff 4.53107476234436, total time dif 10669.109446048737)
step: 39730 @ episode report: {'average_total_reward': np.float32(10.685556), 'reward_variance': np.float32(3.1538043), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06534832417964935), 'actor_loss': np.float64(-0.9818984389305114), 'hyper_actor_loss': np.float64(0.00010018240500357934), 'behavior_loss': np.float64(0.3327401399612427)}

Episode step 39740, time diff 4.591170787811279, total time dif 10673.640520811081)
step: 39740 @ episode report: {'average_total_reward': np.float32(10.434445), 'reward_variance': np.float32(2.8570743), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06190521791577339), 'actor_loss': np.float64(-1.0020256400108338), 'hyper_actor_loss': np.float64(0.00011280893450020813), 'behavior_loss': np.float64(0.33639320731163025)}

Episode step 39750, time diff 4.658345937728882, total time dif 10678.231691598892)
step: 39750 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.5314643), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057507631927728654), 'actor_loss': np.float64(-0.9778818249702453), 'hyper_actor_loss': np.float64(9.953345579560846e-05), 'behavior_loss': np.float64(0.34925098717212677)}

Episode step 39760, time diff 4.5221006870269775, total time dif 10682.890037536621)
step: 39760 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.9973197), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06629242524504661), 'actor_loss': np.float64(-0.964131212234497), 'hyper_actor_loss': np.float64(0.00011139709458802826), 'behavior_loss': np.float64(0.3443881839513779)}

Episode step 39770, time diff 4.507543087005615, total time dif 10687.412138223648)
step: 39770 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(2.4411922), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655555), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06618768870830535), 'actor_loss': np.float64(-1.0063466548919677), 'hyper_actor_loss': np.float64(9.828527399804443e-05), 'behavior_loss': np.float64(0.3388436079025269)}

Episode step 39780, time diff 4.559334993362427, total time dif 10691.919681310654)
step: 39780 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(2.6664891), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06572391726076603), 'actor_loss': np.float64(-1.0105559408664704), 'hyper_actor_loss': np.float64(9.628339976188726e-05), 'behavior_loss': np.float64(0.3444053530693054)}

Episode step 39790, time diff 4.521586656570435, total time dif 10696.479016304016)
step: 39790 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(2.642044), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053373702242970465), 'actor_loss': np.float64(-0.9733522593975067), 'hyper_actor_loss': np.float64(9.133129424299113e-05), 'behavior_loss': np.float64(0.3406921297311783)}

Episode step 39800, time diff 4.579783916473389, total time dif 10701.000602960587)
step: 39800 @ episode report: {'average_total_reward': np.float32(11.993335), 'reward_variance': np.float32(1.7307955), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(11.022222), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06356900036334992), 'actor_loss': np.float64(-0.9711784720420837), 'hyper_actor_loss': np.float64(9.81531680736225e-05), 'behavior_loss': np.float64(0.3453690767288208)}

Episode step 39810, time diff 4.596450567245483, total time dif 10705.58038687706)
step: 39810 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.148691), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06409209556877613), 'actor_loss': np.float64(-0.9892018914222718), 'hyper_actor_loss': np.float64(0.00010110207367688417), 'behavior_loss': np.float64(0.3546499103307724)}

Episode step 39820, time diff 4.604585409164429, total time dif 10710.176837444305)
step: 39820 @ episode report: {'average_total_reward': np.float32(11.071112), 'reward_variance': np.float32(1.2933879), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06515228524804115), 'actor_loss': np.float64(-0.9958795249462128), 'hyper_actor_loss': np.float64(0.00010374329067417421), 'behavior_loss': np.float64(0.3410752058029175)}

Episode step 39830, time diff 4.566230773925781, total time dif 10714.78142285347)
step: 39830 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(1.2710731), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05834347791969776), 'actor_loss': np.float64(-0.9952024221420288), 'hyper_actor_loss': np.float64(9.997313318308443e-05), 'behavior_loss': np.float64(0.3508876174688339)}

Episode step 39840, time diff 4.5686728954315186, total time dif 10719.347653627396)
step: 39840 @ episode report: {'average_total_reward': np.float32(9.948891), 'reward_variance': np.float32(2.8889432), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0555355142802), 'actor_loss': np.float64(-0.9685241520404816), 'hyper_actor_loss': np.float64(0.00011171933001605794), 'behavior_loss': np.float64(0.3588707894086838)}

Episode step 39850, time diff 4.537348031997681, total time dif 10723.916326522827)
step: 39850 @ episode report: {'average_total_reward': np.float32(9.936668), 'reward_variance': np.float32(3.2864957), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07743994519114494), 'actor_loss': np.float64(-0.9857595324516296), 'hyper_actor_loss': np.float64(0.00011440776434028521), 'behavior_loss': np.float64(0.35327447950839996)}

Episode step 39860, time diff 4.556612730026245, total time dif 10728.453674554825)
step: 39860 @ episode report: {'average_total_reward': np.float32(11.320002), 'reward_variance': np.float32(1.4315516), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07279914747923613), 'actor_loss': np.float64(-1.0248035073280335), 'hyper_actor_loss': np.float64(0.00011206876006326638), 'behavior_loss': np.float64(0.3580582857131958)}

Episode step 39870, time diff 4.59269905090332, total time dif 10733.010287284851)
step: 39870 @ episode report: {'average_total_reward': np.float32(11.456668), 'reward_variance': np.float32(1.7397153), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06631390154361724), 'actor_loss': np.float64(-1.005798226594925), 'hyper_actor_loss': np.float64(0.00011274226126261056), 'behavior_loss': np.float64(0.35439459383487704)}

Episode step 39880, time diff 4.555743455886841, total time dif 10737.602986335754)
step: 39880 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(1.8294928), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06341857947409153), 'actor_loss': np.float64(-0.9882321000099182), 'hyper_actor_loss': np.float64(0.00010802243996295146), 'behavior_loss': np.float64(0.3634972184896469)}

Episode step 39890, time diff 4.615097761154175, total time dif 10742.158729791641)
step: 39890 @ episode report: {'average_total_reward': np.float32(11.632223), 'reward_variance': np.float32(5.249221), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07348606809973716), 'actor_loss': np.float64(-0.9782753407955169), 'hyper_actor_loss': np.float64(0.00010636820006766356), 'behavior_loss': np.float64(0.3588723182678223)}

Episode step 39900, time diff 4.550064563751221, total time dif 10746.773827552795)
step: 39900 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(0.8675615), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0629322849214077), 'actor_loss': np.float64(-1.0006492376327514), 'hyper_actor_loss': np.float64(0.00010842043266166002), 'behavior_loss': np.float64(0.35846937298774717)}

Episode step 39910, time diff 4.502737760543823, total time dif 10751.323892116547)
step: 39910 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(3.7669148), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06387954540550708), 'actor_loss': np.float64(-1.0001931488513947), 'hyper_actor_loss': np.float64(0.00011076833616243675), 'behavior_loss': np.float64(0.3633997321128845)}

Episode step 39920, time diff 4.539034366607666, total time dif 10755.82662987709)
step: 39920 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(2.3788264), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08078236393630504), 'actor_loss': np.float64(-0.9860620975494385), 'hyper_actor_loss': np.float64(0.00010747636915766634), 'behavior_loss': np.float64(0.3645453631877899)}

Episode step 39930, time diff 4.6610634326934814, total time dif 10760.365664243698)
step: 39930 @ episode report: {'average_total_reward': np.float32(11.768889), 'reward_variance': np.float32(1.2145882), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0719541035592556), 'actor_loss': np.float64(-1.0178962707519532), 'hyper_actor_loss': np.float64(0.00010989317524945363), 'behavior_loss': np.float64(0.36119373738765714)}

Episode step 39940, time diff 4.4591381549835205, total time dif 10765.026727676392)
step: 39940 @ episode report: {'average_total_reward': np.float32(11.8566675), 'reward_variance': np.float32(4.216283), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0657075148075819), 'actor_loss': np.float64(-1.004874575138092), 'hyper_actor_loss': np.float64(0.00011761245332309045), 'behavior_loss': np.float64(0.36966132521629336)}

Episode step 39950, time diff 4.508406162261963, total time dif 10769.485865831375)
step: 39950 @ episode report: {'average_total_reward': np.float32(12.205557), 'reward_variance': np.float32(1.9913146), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(13.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0609450738877058), 'actor_loss': np.float64(-0.9758900940418244), 'hyper_actor_loss': np.float64(0.00012098474107915535), 'behavior_loss': np.float64(0.37594920098781587)}

Episode step 39960, time diff 4.480675935745239, total time dif 10773.994271993637)
step: 39960 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(1.5168597), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06942973658442497), 'actor_loss': np.float64(-0.9889984250068664), 'hyper_actor_loss': np.float64(0.00010897510219365358), 'behavior_loss': np.float64(0.3703631043434143)}

Episode step 39970, time diff 4.461174964904785, total time dif 10778.474947929382)
step: 39970 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(2.135304), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05991896726191044), 'actor_loss': np.float64(-0.9973515629768371), 'hyper_actor_loss': np.float64(0.0001122804569604341), 'behavior_loss': np.float64(0.3789826244115829)}

Episode step 39980, time diff 4.461236953735352, total time dif 10782.936122894287)
step: 39980 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(1.9269447), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0679216643795371), 'actor_loss': np.float64(-0.9810888051986695), 'hyper_actor_loss': np.float64(0.00010356964412494563), 'behavior_loss': np.float64(0.37647510766983033)}

Episode step 39990, time diff 4.465650320053101, total time dif 10787.397359848022)
step: 39990 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(3.5447457), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06696488335728645), 'actor_loss': np.float64(-0.9905892312526703), 'hyper_actor_loss': np.float64(9.778531675692647e-05), 'behavior_loss': np.float64(0.37762047350406647)}

Episode step 40000, time diff 4.484525203704834, total time dif 10791.863010168076)
step: 40000 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.8445392), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07756034433841705), 'actor_loss': np.float64(-1.00669783949852), 'hyper_actor_loss': np.float64(9.869259883998893e-05), 'behavior_loss': np.float64(0.3817939877510071)}

Episode step 40010, time diff 4.397645473480225, total time dif 10796.34753537178)
step: 40010 @ episode report: {'average_total_reward': np.float32(11.395556), 'reward_variance': np.float32(0.52521515), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06112239379435778), 'actor_loss': np.float64(-1.0000957429409028), 'hyper_actor_loss': np.float64(8.82052096130792e-05), 'behavior_loss': np.float64(0.37803708016872406)}

Episode step 40020, time diff 4.4411396980285645, total time dif 10800.74518084526)
step: 40020 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(4.901956), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05374868959188461), 'actor_loss': np.float64(-0.9751113712787628), 'hyper_actor_loss': np.float64(9.285890409955755e-05), 'behavior_loss': np.float64(0.38226023614406585)}

Episode step 40030, time diff 4.457591772079468, total time dif 10805.18632054329)
step: 40030 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(3.8435752), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0665595442056656), 'actor_loss': np.float64(-0.9709788084030151), 'hyper_actor_loss': np.float64(8.41463370306883e-05), 'behavior_loss': np.float64(0.3825626164674759)}

Episode step 40040, time diff 4.452171325683594, total time dif 10809.643912315369)
step: 40040 @ episode report: {'average_total_reward': np.float32(11.881112), 'reward_variance': np.float32(6.3350377), 'max_total_reward': np.float32(15.633333), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05940109035000205), 'actor_loss': np.float64(-0.993792712688446), 'hyper_actor_loss': np.float64(8.281784685095772e-05), 'behavior_loss': np.float64(0.3787947952747345)}

Episode step 40050, time diff 4.442725419998169, total time dif 10814.096083641052)
step: 40050 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(4.284027), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06651782728731633), 'actor_loss': np.float64(-0.9863892674446106), 'hyper_actor_loss': np.float64(8.487915401929058e-05), 'behavior_loss': np.float64(0.39228661060333253)}

Episode step 40060, time diff 4.424858570098877, total time dif 10818.53880906105)
step: 40060 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(5.4247174), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06260027382522822), 'actor_loss': np.float64(-0.9869808733463288), 'hyper_actor_loss': np.float64(8.350069911102764e-05), 'behavior_loss': np.float64(0.3806288719177246)}

Episode step 40070, time diff 4.432652711868286, total time dif 10822.96366763115)
step: 40070 @ episode report: {'average_total_reward': np.float32(10.734445), 'reward_variance': np.float32(3.972382), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05285744983702898), 'actor_loss': np.float64(-0.9857403635978699), 'hyper_actor_loss': np.float64(8.353024459211156e-05), 'behavior_loss': np.float64(0.38477985858917235)}

Episode step 40080, time diff 4.41743803024292, total time dif 10827.396320343018)
step: 40080 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(3.606094), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07551990635693073), 'actor_loss': np.float64(-0.9881159782409668), 'hyper_actor_loss': np.float64(8.100105915218592e-05), 'behavior_loss': np.float64(0.3854539841413498)}

Episode step 40090, time diff 4.549424171447754, total time dif 10831.81375837326)
step: 40090 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.8989139), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0632328126579523), 'actor_loss': np.float64(-0.9995804190635681), 'hyper_actor_loss': np.float64(7.698506960878149e-05), 'behavior_loss': np.float64(0.38621812164783476)}

Episode step 40100, time diff 4.470864772796631, total time dif 10836.363182544708)
step: 40100 @ episode report: {'average_total_reward': np.float32(10.697779), 'reward_variance': np.float32(2.6705875), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06981141828000545), 'actor_loss': np.float64(-0.9930568516254425), 'hyper_actor_loss': np.float64(7.216269550553988e-05), 'behavior_loss': np.float64(0.38948001265525817)}

Episode step 40110, time diff 4.456534147262573, total time dif 10840.834047317505)
step: 40110 @ episode report: {'average_total_reward': np.float32(10.397778), 'reward_variance': np.float32(4.76928), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06971263289451599), 'actor_loss': np.float64(-1.0035761415958404), 'hyper_actor_loss': np.float64(7.264186860993505e-05), 'behavior_loss': np.float64(0.38420013189315794)}

Episode step 40120, time diff 4.39133095741272, total time dif 10845.290581464767)
step: 40120 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(4.7983522), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06499960198998452), 'actor_loss': np.float64(-1.00028555393219), 'hyper_actor_loss': np.float64(7.312741508940235e-05), 'behavior_loss': np.float64(0.38905530571937563)}

Episode step 40130, time diff 4.338803768157959, total time dif 10849.68191242218)
step: 40130 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.4291967), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07126635685563087), 'actor_loss': np.float64(-0.9893382847309112), 'hyper_actor_loss': np.float64(7.084696117090061e-05), 'behavior_loss': np.float64(0.3917827934026718)}

Episode step 40140, time diff 4.375204563140869, total time dif 10854.020716190338)
step: 40140 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.8694084), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07367518115788699), 'actor_loss': np.float64(-0.9888625323772431), 'hyper_actor_loss': np.float64(7.716411710134707e-05), 'behavior_loss': np.float64(0.38885889053344724)}

Episode step 40150, time diff 4.39167857170105, total time dif 10858.395920753479)
step: 40150 @ episode report: {'average_total_reward': np.float32(11.171111), 'reward_variance': np.float32(6.619117), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06233038976788521), 'actor_loss': np.float64(-0.995423036813736), 'hyper_actor_loss': np.float64(7.34395238396246e-05), 'behavior_loss': np.float64(0.387666729092598)}

Episode step 40160, time diff 4.426220417022705, total time dif 10862.78759932518)
step: 40160 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.1128647), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06662507839500904), 'actor_loss': np.float64(-0.9902524650096893), 'hyper_actor_loss': np.float64(7.083322270773351e-05), 'behavior_loss': np.float64(0.39498150646686553)}

Episode step 40170, time diff 4.438296556472778, total time dif 10867.213819742203)
step: 40170 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.578821), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06775450333952904), 'actor_loss': np.float64(-0.979798001050949), 'hyper_actor_loss': np.float64(7.360740273725242e-05), 'behavior_loss': np.float64(0.39616128504276277)}

Episode step 40180, time diff 4.427393198013306, total time dif 10871.652116298676)
step: 40180 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(7.659921), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0681473758071661), 'actor_loss': np.float64(-0.9919925153255462), 'hyper_actor_loss': np.float64(6.963400664972141e-05), 'behavior_loss': np.float64(0.3872168004512787)}

Episode step 40190, time diff 4.461524724960327, total time dif 10876.079509496689)
step: 40190 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(5.847162), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054182297363877294), 'actor_loss': np.float64(-0.9948786079883576), 'hyper_actor_loss': np.float64(7.583564729429781e-05), 'behavior_loss': np.float64(0.38996462523937225)}

Episode step 40200, time diff 4.383760452270508, total time dif 10880.54103422165)
step: 40200 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.9732294), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0699331158772111), 'actor_loss': np.float64(-0.9746445655822754), 'hyper_actor_loss': np.float64(7.34801225917181e-05), 'behavior_loss': np.float64(0.38865731060504916)}

Episode step 40210, time diff 4.3857057094573975, total time dif 10884.92479467392)
step: 40210 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(2.4241254), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05267754700034857), 'actor_loss': np.float64(-0.9883512675762176), 'hyper_actor_loss': np.float64(7.513744931202382e-05), 'behavior_loss': np.float64(0.3861768454313278)}

Episode step 40220, time diff 4.400766611099243, total time dif 10889.310500383377)
step: 40220 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.5597584), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06516058370471), 'actor_loss': np.float64(-0.9911689519882202), 'hyper_actor_loss': np.float64(7.391548024315853e-05), 'behavior_loss': np.float64(0.39651545584201814)}

Episode step 40230, time diff 4.357468366622925, total time dif 10893.711266994476)
step: 40230 @ episode report: {'average_total_reward': np.float32(11.656667), 'reward_variance': np.float32(2.8920605), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06790894903242588), 'actor_loss': np.float64(-0.9887268960475921), 'hyper_actor_loss': np.float64(6.994809446041473e-05), 'behavior_loss': np.float64(0.395397225022316)}

Episode step 40240, time diff 4.411287069320679, total time dif 10898.0687353611)
step: 40240 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.391295), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0731925442814827), 'actor_loss': np.float64(-1.008776742219925), 'hyper_actor_loss': np.float64(7.240878621814773e-05), 'behavior_loss': np.float64(0.3902559757232666)}

Episode step 40250, time diff 4.373013019561768, total time dif 10902.48002243042)
step: 40250 @ episode report: {'average_total_reward': np.float32(10.922223), 'reward_variance': np.float32(4.1614084), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06338805258274079), 'actor_loss': np.float64(-1.000925999879837), 'hyper_actor_loss': np.float64(7.01128774380777e-05), 'behavior_loss': np.float64(0.3884435534477234)}

Episode step 40260, time diff 4.58636736869812, total time dif 10906.853035449982)
step: 40260 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(2.0978281), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059840960800647734), 'actor_loss': np.float64(-0.9808893442153931), 'hyper_actor_loss': np.float64(6.743057056155522e-05), 'behavior_loss': np.float64(0.3950521647930145)}

Episode step 40270, time diff 4.368295907974243, total time dif 10911.43940281868)
step: 40270 @ episode report: {'average_total_reward': np.float32(10.485557), 'reward_variance': np.float32(2.6723974), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0652325578033924), 'actor_loss': np.float64(-0.9750608146190644), 'hyper_actor_loss': np.float64(6.798270915169269e-05), 'behavior_loss': np.float64(0.39917024970054626)}

Episode step 40280, time diff 4.37346625328064, total time dif 10915.807698726654)
step: 40280 @ episode report: {'average_total_reward': np.float32(11.656668), 'reward_variance': np.float32(4.663172), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05901615619659424), 'actor_loss': np.float64(-0.987189769744873), 'hyper_actor_loss': np.float64(6.492659813375212e-05), 'behavior_loss': np.float64(0.38934420645236967)}

Episode step 40290, time diff 4.401419162750244, total time dif 10920.181164979935)
step: 40290 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.1622424), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05966370329260826), 'actor_loss': np.float64(-0.9966251134872437), 'hyper_actor_loss': np.float64(6.855053215986118e-05), 'behavior_loss': np.float64(0.3931335836648941)}

Episode step 40300, time diff 4.376706600189209, total time dif 10924.582584142685)
step: 40300 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(3.5118275), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06894528847187757), 'actor_loss': np.float64(-0.9970825433731079), 'hyper_actor_loss': np.float64(6.54870342259528e-05), 'behavior_loss': np.float64(0.3985051304101944)}

Episode step 40310, time diff 4.3213372230529785, total time dif 10928.959290742874)
step: 40310 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.6616304), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05511770397424698), 'actor_loss': np.float64(-0.9774068117141723), 'hyper_actor_loss': np.float64(6.562214512086939e-05), 'behavior_loss': np.float64(0.3959705650806427)}

Episode step 40320, time diff 4.375858783721924, total time dif 10933.280627965927)
step: 40320 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.593561), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06387024708092212), 'actor_loss': np.float64(-0.9784313559532165), 'hyper_actor_loss': np.float64(6.772903907403816e-05), 'behavior_loss': np.float64(0.3907101482152939)}

Episode step 40330, time diff 4.400153398513794, total time dif 10937.656486749649)
step: 40330 @ episode report: {'average_total_reward': np.float32(10.385556), 'reward_variance': np.float32(2.2091618), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0698987290263176), 'actor_loss': np.float64(-1.0082131564617156), 'hyper_actor_loss': np.float64(6.934722478035837e-05), 'behavior_loss': np.float64(0.38697672486305235)}

Episode step 40340, time diff 4.412919044494629, total time dif 10942.056640148163)
step: 40340 @ episode report: {'average_total_reward': np.float32(11.046667), 'reward_variance': np.float32(3.9012294), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06125315986573696), 'actor_loss': np.float64(-1.0096600115299226), 'hyper_actor_loss': np.float64(6.935629826330115e-05), 'behavior_loss': np.float64(0.3899152994155884)}

Episode step 40350, time diff 4.379789352416992, total time dif 10946.469559192657)
step: 40350 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.701043), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06369580440223217), 'actor_loss': np.float64(-0.9831914663314819), 'hyper_actor_loss': np.float64(7.486784888897092e-05), 'behavior_loss': np.float64(0.39689638316631315)}

Episode step 40360, time diff 4.445146560668945, total time dif 10950.849348545074)
step: 40360 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(3.9847214), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07121359370648861), 'actor_loss': np.float64(-0.9756916224956512), 'hyper_actor_loss': np.float64(7.075354369590059e-05), 'behavior_loss': np.float64(0.4002882272005081)}

Episode step 40370, time diff 4.430882453918457, total time dif 10955.294495105743)
step: 40370 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(2.3225746), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07889779284596443), 'actor_loss': np.float64(-1.003320002555847), 'hyper_actor_loss': np.float64(7.579043667647057e-05), 'behavior_loss': np.float64(0.3974421411752701)}

Episode step 40380, time diff 4.4735002517700195, total time dif 10959.725377559662)
step: 40380 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(1.6429384), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07193073369562626), 'actor_loss': np.float64(-1.0158885955810546), 'hyper_actor_loss': np.float64(7.955605033203029e-05), 'behavior_loss': np.float64(0.39878401160240173)}

Episode step 40390, time diff 4.481393337249756, total time dif 10964.198877811432)
step: 40390 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(2.013527), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.056602165848016736), 'actor_loss': np.float64(-0.9789498448371887), 'hyper_actor_loss': np.float64(9.17875237064436e-05), 'behavior_loss': np.float64(0.39738489985466)}

Episode step 40400, time diff 4.478755712509155, total time dif 10968.680271148682)
step: 40400 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(3.6515567), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06481739692389965), 'actor_loss': np.float64(-0.9685859084129333), 'hyper_actor_loss': np.float64(9.884700193651952e-05), 'behavior_loss': np.float64(0.39490302801132204)}

Episode step 40410, time diff 4.411975860595703, total time dif 10973.15902686119)
step: 40410 @ episode report: {'average_total_reward': np.float32(8.951111), 'reward_variance': np.float32(2.1183758), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06847005374729634), 'actor_loss': np.float64(-1.0066660284996032), 'hyper_actor_loss': np.float64(0.00010136795317521319), 'behavior_loss': np.float64(0.39157318472862246)}

Episode step 40420, time diff 4.583164215087891, total time dif 10977.571002721786)
step: 40420 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.5822177), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05812384132295847), 'actor_loss': np.float64(-0.9973121285438538), 'hyper_actor_loss': np.float64(9.674387547420338e-05), 'behavior_loss': np.float64(0.39086456000804903)}

Episode step 40430, time diff 4.423647165298462, total time dif 10982.154166936874)
step: 40430 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(2.9502168), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0775467611849308), 'actor_loss': np.float64(-0.9762226998806), 'hyper_actor_loss': np.float64(9.991558908950537e-05), 'behavior_loss': np.float64(0.3992127537727356)}

Episode step 40440, time diff 4.432288885116577, total time dif 10986.577814102173)
step: 40440 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(2.3943026), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06995780915021896), 'actor_loss': np.float64(-1.0098535656929015), 'hyper_actor_loss': np.float64(9.821661515161396e-05), 'behavior_loss': np.float64(0.39353591203689575)}

Episode step 40450, time diff 4.452912330627441, total time dif 10991.01010298729)
step: 40450 @ episode report: {'average_total_reward': np.float32(8.638889), 'reward_variance': np.float32(1.1163765), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07060092054307461), 'actor_loss': np.float64(-1.0082531332969666), 'hyper_actor_loss': np.float64(9.398686597705819e-05), 'behavior_loss': np.float64(0.3956010639667511)}

Episode step 40460, time diff 4.427846193313599, total time dif 10995.463015317917)
step: 40460 @ episode report: {'average_total_reward': np.float32(9.973334), 'reward_variance': np.float32(1.0110912), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06413675583899021), 'actor_loss': np.float64(-0.9861879765987396), 'hyper_actor_loss': np.float64(8.995378666440957e-05), 'behavior_loss': np.float64(0.39211927354335785)}

Episode step 40470, time diff 4.419870853424072, total time dif 10999.89086151123)
step: 40470 @ episode report: {'average_total_reward': np.float32(9.763334), 'reward_variance': np.float32(4.1796556), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06696091126650572), 'actor_loss': np.float64(-0.988269430398941), 'hyper_actor_loss': np.float64(8.804542885627597e-05), 'behavior_loss': np.float64(0.387840136885643)}

Episode step 40480, time diff 4.467224597930908, total time dif 11004.310732364655)
step: 40480 @ episode report: {'average_total_reward': np.float32(10.885556), 'reward_variance': np.float32(6.3642488), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07236860059201718), 'actor_loss': np.float64(-0.9982467114925384), 'hyper_actor_loss': np.float64(9.234702374669724e-05), 'behavior_loss': np.float64(0.39606360495090487)}

Episode step 40490, time diff 4.478770971298218, total time dif 11008.777956962585)
step: 40490 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(4.2415853), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053934603184461596), 'actor_loss': np.float64(-0.9839529633522034), 'hyper_actor_loss': np.float64(9.06354071048554e-05), 'behavior_loss': np.float64(0.39216170012950896)}

Episode step 40500, time diff 4.450477123260498, total time dif 11013.256727933884)
step: 40500 @ episode report: {'average_total_reward': np.float32(9.551111), 'reward_variance': np.float32(4.6476846), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06715052165091037), 'actor_loss': np.float64(-0.9807411134243011), 'hyper_actor_loss': np.float64(8.770683125476353e-05), 'behavior_loss': np.float64(0.3958924859762192)}

Episode step 40510, time diff 4.369130611419678, total time dif 11017.707205057144)
step: 40510 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(2.0165257), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07294880002737045), 'actor_loss': np.float64(-0.9873283863067627), 'hyper_actor_loss': np.float64(9.172056379611604e-05), 'behavior_loss': np.float64(0.3975550174713135)}

Episode step 40520, time diff 4.489827871322632, total time dif 11022.076335668564)
step: 40520 @ episode report: {'average_total_reward': np.float32(9.775557), 'reward_variance': np.float32(1.5194511), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06561573296785354), 'actor_loss': np.float64(-1.0058919548988343), 'hyper_actor_loss': np.float64(8.830268561723642e-05), 'behavior_loss': np.float64(0.39024807810783385)}

Episode step 40530, time diff 4.4122092723846436, total time dif 11026.566163539886)
step: 40530 @ episode report: {'average_total_reward': np.float32(10.285557), 'reward_variance': np.float32(3.4526691), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05939887836575508), 'actor_loss': np.float64(-0.9893646538257599), 'hyper_actor_loss': np.float64(8.792349472059868e-05), 'behavior_loss': np.float64(0.39544826447963716)}

Episode step 40540, time diff 4.460880279541016, total time dif 11030.978372812271)
step: 40540 @ episode report: {'average_total_reward': np.float32(8.526667), 'reward_variance': np.float32(2.454943), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06086745895445347), 'actor_loss': np.float64(-0.9765212833881378), 'hyper_actor_loss': np.float64(8.633185716462321e-05), 'behavior_loss': np.float64(0.39389958679676057)}

Episode step 40550, time diff 4.420681476593018, total time dif 11035.439253091812)
step: 40550 @ episode report: {'average_total_reward': np.float32(9.700002), 'reward_variance': np.float32(1.4527906), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07209891341626644), 'actor_loss': np.float64(-0.9898374855518342), 'hyper_actor_loss': np.float64(8.405130793107674e-05), 'behavior_loss': np.float64(0.3922170877456665)}

Episode step 40560, time diff 4.470934629440308, total time dif 11039.859934568405)
step: 40560 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(2.1646442), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07094042673707009), 'actor_loss': np.float64(-1.0028984308242799), 'hyper_actor_loss': np.float64(8.477901210426352e-05), 'behavior_loss': np.float64(0.3909844011068344)}

Episode step 40570, time diff 4.412032604217529, total time dif 11044.330869197845)
step: 40570 @ episode report: {'average_total_reward': np.float32(8.214445), 'reward_variance': np.float32(2.50215), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07332975752651691), 'actor_loss': np.float64(-1.0033284068107604), 'hyper_actor_loss': np.float64(8.602974266977981e-05), 'behavior_loss': np.float64(0.3972413927316666)}

Episode step 40580, time diff 4.470679521560669, total time dif 11048.742901802063)
step: 40580 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(1.5825942), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0631385862827301), 'actor_loss': np.float64(-0.9873874187469482), 'hyper_actor_loss': np.float64(8.566704636905342e-05), 'behavior_loss': np.float64(0.39380590319633485)}

Episode step 40590, time diff 4.675700902938843, total time dif 11053.213581323624)
step: 40590 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(5.3390145), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05741037018597126), 'actor_loss': np.float64(-0.9757280230522156), 'hyper_actor_loss': np.float64(8.074194702203385e-05), 'behavior_loss': np.float64(0.39174750447273254)}

Episode step 40600, time diff 4.475258827209473, total time dif 11057.889282226562)
step: 40600 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.69205), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06929172426462174), 'actor_loss': np.float64(-0.9898515582084656), 'hyper_actor_loss': np.float64(8.069840696407482e-05), 'behavior_loss': np.float64(0.39545934796333315)}

Episode step 40610, time diff 4.475246429443359, total time dif 11062.364541053772)
step: 40610 @ episode report: {'average_total_reward': np.float32(9.800001), 'reward_variance': np.float32(2.6177285), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06338763795793056), 'actor_loss': np.float64(-0.9993911981582642), 'hyper_actor_loss': np.float64(8.432274917140603e-05), 'behavior_loss': np.float64(0.3909677118062973)}

Episode step 40620, time diff 4.484322786331177, total time dif 11066.839787483215)
step: 40620 @ episode report: {'average_total_reward': np.float32(9.163333), 'reward_variance': np.float32(1.5798037), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06509103104472161), 'actor_loss': np.float64(-0.9891607940196991), 'hyper_actor_loss': np.float64(8.754521259106696e-05), 'behavior_loss': np.float64(0.39763656854629514)}

Episode step 40630, time diff 4.467493772506714, total time dif 11071.324110269547)
step: 40630 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(0.70339364), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05618298016488552), 'actor_loss': np.float64(-0.9839813470840454), 'hyper_actor_loss': np.float64(8.556704488000832e-05), 'behavior_loss': np.float64(0.39708961844444274)}

Episode step 40640, time diff 4.471193313598633, total time dif 11075.791604042053)
step: 40640 @ episode report: {'average_total_reward': np.float32(9.412224), 'reward_variance': np.float32(3.1165059), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07882949933409691), 'actor_loss': np.float64(-0.9785946607589722), 'hyper_actor_loss': np.float64(8.49763280712068e-05), 'behavior_loss': np.float64(0.3981413781642914)}

Episode step 40650, time diff 4.489983797073364, total time dif 11080.262797355652)
step: 40650 @ episode report: {'average_total_reward': np.float32(9.936667), 'reward_variance': np.float32(1.9478042), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0647546548396349), 'actor_loss': np.float64(-0.9892723560333252), 'hyper_actor_loss': np.float64(8.447326908935792e-05), 'behavior_loss': np.float64(0.39484905898571016)}

Episode step 40660, time diff 4.520730495452881, total time dif 11084.752781152725)
step: 40660 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(0.8096306), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06817811615765094), 'actor_loss': np.float64(-0.9920133590698242), 'hyper_actor_loss': np.float64(8.853556937538087e-05), 'behavior_loss': np.float64(0.38874254524707796)}

Episode step 40670, time diff 4.490410089492798, total time dif 11089.273511648178)
step: 40670 @ episode report: {'average_total_reward': np.float32(10.161111), 'reward_variance': np.float32(3.0196607), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07517992593348026), 'actor_loss': np.float64(-0.9997335910797119), 'hyper_actor_loss': np.float64(8.979447375168093e-05), 'behavior_loss': np.float64(0.39305928647518157)}

Episode step 40680, time diff 4.447744131088257, total time dif 11093.76392173767)
step: 40680 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(1.3906467), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06344678066670895), 'actor_loss': np.float64(-1.000181394815445), 'hyper_actor_loss': np.float64(8.792776861810125e-05), 'behavior_loss': np.float64(0.39194459915161134)}

Episode step 40690, time diff 4.561609506607056, total time dif 11098.21166586876)
step: 40690 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.3787477), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06655654236674309), 'actor_loss': np.float64(-0.9783410549163818), 'hyper_actor_loss': np.float64(8.883733680704608e-05), 'behavior_loss': np.float64(0.392008376121521)}

Episode step 40700, time diff 4.551069021224976, total time dif 11102.773275375366)
step: 40700 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.4501498), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055973952263593675), 'actor_loss': np.float64(-0.9799830377101898), 'hyper_actor_loss': np.float64(9.326719300588593e-05), 'behavior_loss': np.float64(0.3891730546951294)}

Episode step 40710, time diff 4.535522222518921, total time dif 11107.324344396591)
step: 40710 @ episode report: {'average_total_reward': np.float32(9.126667), 'reward_variance': np.float32(1.5044739), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06969793029129505), 'actor_loss': np.float64(-0.985901141166687), 'hyper_actor_loss': np.float64(8.856133863446302e-05), 'behavior_loss': np.float64(0.393869948387146)}

Episode step 40720, time diff 4.548879623413086, total time dif 11111.85986661911)
step: 40720 @ episode report: {'average_total_reward': np.float32(9.463335), 'reward_variance': np.float32(0.85173017), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07105274163186551), 'actor_loss': np.float64(-1.0030910789966583), 'hyper_actor_loss': np.float64(8.632212848169729e-05), 'behavior_loss': np.float64(0.3891889572143555)}

Episode step 40730, time diff 4.493086099624634, total time dif 11116.408746242523)
step: 40730 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(2.1868262), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06647569984197617), 'actor_loss': np.float64(-0.9936369180679321), 'hyper_actor_loss': np.float64(8.7078103388194e-05), 'behavior_loss': np.float64(0.3908846527338028)}

Episode step 40740, time diff 4.524973154067993, total time dif 11120.901832342148)
step: 40740 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(1.5687357), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07104770131409169), 'actor_loss': np.float64(-0.9891581833362579), 'hyper_actor_loss': np.float64(8.408041394432075e-05), 'behavior_loss': np.float64(0.38671697676181793)}

Episode step 40750, time diff 4.638807535171509, total time dif 11125.426805496216)
step: 40750 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(2.3107905), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06712188497185707), 'actor_loss': np.float64(-1.009155511856079), 'hyper_actor_loss': np.float64(9.237613994628191e-05), 'behavior_loss': np.float64(0.3882633924484253)}

Episode step 40760, time diff 4.533462762832642, total time dif 11130.065613031387)
step: 40760 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(1.1562278), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07252983376383781), 'actor_loss': np.float64(-0.9943638563156127), 'hyper_actor_loss': np.float64(9.184603986795991e-05), 'behavior_loss': np.float64(0.3897882401943207)}

Episode step 40770, time diff 4.476969003677368, total time dif 11134.59907579422)
step: 40770 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(3.2440553), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06104026697576046), 'actor_loss': np.float64(-0.9760436236858367), 'hyper_actor_loss': np.float64(9.788907846086659e-05), 'behavior_loss': np.float64(0.38319948613643645)}

Episode step 40780, time diff 4.501279354095459, total time dif 11139.076044797897)
step: 40780 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(5.3024645), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07615348547697068), 'actor_loss': np.float64(-0.9917055368423462), 'hyper_actor_loss': np.float64(9.307584405178204e-05), 'behavior_loss': np.float64(0.39356106221675874)}

Episode step 40790, time diff 4.478619575500488, total time dif 11143.577324151993)
step: 40790 @ episode report: {'average_total_reward': np.float32(8.802222), 'reward_variance': np.float32(4.0139456), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06080684997141361), 'actor_loss': np.float64(-1.0000543594360352), 'hyper_actor_loss': np.float64(9.495301856077276e-05), 'behavior_loss': np.float64(0.3894391030073166)}

Episode step 40800, time diff 4.5083699226379395, total time dif 11148.055943727493)
step: 40800 @ episode report: {'average_total_reward': np.float32(8.041112), 'reward_variance': np.float32(1.418198), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06396635994315147), 'actor_loss': np.float64(-0.9874250829219818), 'hyper_actor_loss': np.float64(0.0001015808476950042), 'behavior_loss': np.float64(0.3922640115022659)}

Episode step 40810, time diff 4.473092317581177, total time dif 11152.564313650131)
step: 40810 @ episode report: {'average_total_reward': np.float32(9.224444), 'reward_variance': np.float32(2.489057), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05648275781422853), 'actor_loss': np.float64(-0.9634185075759888), 'hyper_actor_loss': np.float64(9.806824164115824e-05), 'behavior_loss': np.float64(0.3854806572198868)}

Episode step 40820, time diff 4.4882612228393555, total time dif 11157.037405967712)
step: 40820 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(2.221388), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07061556428670883), 'actor_loss': np.float64(-0.9767928004264832), 'hyper_actor_loss': np.float64(0.00010702316794777288), 'behavior_loss': np.float64(0.3912594199180603)}

Episode step 40830, time diff 4.496048212051392, total time dif 11161.525667190552)
step: 40830 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(2.5362172), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05709223430603742), 'actor_loss': np.float64(-0.9870636880397796), 'hyper_actor_loss': np.float64(0.00010105174296768383), 'behavior_loss': np.float64(0.3887070655822754)}

Episode step 40840, time diff 4.534937620162964, total time dif 11166.021715402603)
step: 40840 @ episode report: {'average_total_reward': np.float32(9.200001), 'reward_variance': np.float32(1.2449381), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07063412293791771), 'actor_loss': np.float64(-0.9779086291790009), 'hyper_actor_loss': np.float64(9.840268321568147e-05), 'behavior_loss': np.float64(0.38643641769886017)}

Episode step 40850, time diff 4.472831964492798, total time dif 11170.556653022766)
step: 40850 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(4.7300367), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054672416672110556), 'actor_loss': np.float64(-0.984841525554657), 'hyper_actor_loss': np.float64(9.367787206429056e-05), 'behavior_loss': np.float64(0.3865413963794708)}

Episode step 40860, time diff 4.470652341842651, total time dif 11175.029484987259)
step: 40860 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(2.0086474), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0788411594927311), 'actor_loss': np.float64(-0.9802497804164887), 'hyper_actor_loss': np.float64(9.375333320349455e-05), 'behavior_loss': np.float64(0.3869711011648178)}

Episode step 40870, time diff 4.545550346374512, total time dif 11179.500137329102)
step: 40870 @ episode report: {'average_total_reward': np.float32(7.6922226), 'reward_variance': np.float32(3.3493342), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08067062199115753), 'actor_loss': np.float64(-1.0109529674053193), 'hyper_actor_loss': np.float64(9.206137183355167e-05), 'behavior_loss': np.float64(0.38406671285629274)}

Episode step 40880, time diff 4.5047266483306885, total time dif 11184.045687675476)
step: 40880 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(2.9697795), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05886169411242008), 'actor_loss': np.float64(-1.0133219003677367), 'hyper_actor_loss': np.float64(9.483025642111897e-05), 'behavior_loss': np.float64(0.3798477858304977)}

Episode step 40890, time diff 4.469957113265991, total time dif 11188.550414323807)
step: 40890 @ episode report: {'average_total_reward': np.float32(8.390001), 'reward_variance': np.float32(1.2341839), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06305571421980857), 'actor_loss': np.float64(-0.9669732213020324), 'hyper_actor_loss': np.float64(8.974644078989513e-05), 'behavior_loss': np.float64(0.38604859709739686)}

Episode step 40900, time diff 4.48526668548584, total time dif 11193.020371437073)
step: 40900 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(3.2827606), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0640945689752698), 'actor_loss': np.float64(-0.9794991075992584), 'hyper_actor_loss': np.float64(8.902653862605803e-05), 'behavior_loss': np.float64(0.3872911512851715)}

Episode step 40910, time diff 4.449784517288208, total time dif 11197.505638122559)
step: 40910 @ episode report: {'average_total_reward': np.float32(9.277777), 'reward_variance': np.float32(3.0944197), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.28889), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05796478465199471), 'actor_loss': np.float64(-0.9813612163066864), 'hyper_actor_loss': np.float64(8.667393631185405e-05), 'behavior_loss': np.float64(0.3841373473405838)}

Episode step 40920, time diff 4.6300201416015625, total time dif 11201.955422639847)
step: 40920 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(0.51645076), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0688006479293108), 'actor_loss': np.float64(-0.9763085663318634), 'hyper_actor_loss': np.float64(8.745454106247052e-05), 'behavior_loss': np.float64(0.38660743534564973)}

Episode step 40930, time diff 4.466930627822876, total time dif 11206.585442781448)
step: 40930 @ episode report: {'average_total_reward': np.float32(8.551111), 'reward_variance': np.float32(2.894055), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06749642603099346), 'actor_loss': np.float64(-0.9957624435424804), 'hyper_actor_loss': np.float64(8.63525303429924e-05), 'behavior_loss': np.float64(0.38774110078811647)}

Episode step 40940, time diff 4.487734794616699, total time dif 11211.052373409271)
step: 40940 @ episode report: {'average_total_reward': np.float32(9.424445), 'reward_variance': np.float32(1.8429587), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05654508359730244), 'actor_loss': np.float64(-0.9841550290584564), 'hyper_actor_loss': np.float64(8.595653853262774e-05), 'behavior_loss': np.float64(0.3861364841461182)}

Episode step 40950, time diff 4.466857194900513, total time dif 11215.540108203888)
step: 40950 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(0.8136307), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060125408321619035), 'actor_loss': np.float64(-0.9620432794094086), 'hyper_actor_loss': np.float64(8.21690475277137e-05), 'behavior_loss': np.float64(0.3916003078222275)}

Episode step 40960, time diff 4.456971645355225, total time dif 11220.006965398788)
step: 40960 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(1.9726193), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06642109639942646), 'actor_loss': np.float64(-0.9748161911964417), 'hyper_actor_loss': np.float64(8.3348793850746e-05), 'behavior_loss': np.float64(0.383009535074234)}

Episode step 40970, time diff 4.491528034210205, total time dif 11224.463937044144)
step: 40970 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(1.7048938), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05972959622740746), 'actor_loss': np.float64(-0.9873752415180206), 'hyper_actor_loss': np.float64(8.259867609012872e-05), 'behavior_loss': np.float64(0.3895455151796341)}

Episode step 40980, time diff 4.454354524612427, total time dif 11228.955465078354)
step: 40980 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(1.611091), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055083504691720006), 'actor_loss': np.float64(-0.9688940048217773), 'hyper_actor_loss': np.float64(8.431964160990902e-05), 'behavior_loss': np.float64(0.3854308664798737)}

Episode step 40990, time diff 4.466808319091797, total time dif 11233.409819602966)
step: 40990 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(3.0016608), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0702844636514783), 'actor_loss': np.float64(-0.9616317749023438), 'hyper_actor_loss': np.float64(7.920925054349937e-05), 'behavior_loss': np.float64(0.3896032124757767)}

Episode step 41000, time diff 4.478655099868774, total time dif 11237.876627922058)
step: 41000 @ episode report: {'average_total_reward': np.float32(8.8144455), 'reward_variance': np.float32(3.5553098), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0553665641695261), 'actor_loss': np.float64(-0.9925991475582123), 'hyper_actor_loss': np.float64(7.816153738531284e-05), 'behavior_loss': np.float64(0.3777797698974609)}

Episode step 41010, time diff 4.465290784835815, total time dif 11242.355283021927)
step: 41010 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.7187458), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06503729969263077), 'actor_loss': np.float64(-0.9926459968090058), 'hyper_actor_loss': np.float64(7.442473724950105e-05), 'behavior_loss': np.float64(0.38403200507164004)}

Episode step 41020, time diff 4.487778902053833, total time dif 11246.820573806763)
step: 41020 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(2.423236), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06465510614216327), 'actor_loss': np.float64(-0.9762472033500671), 'hyper_actor_loss': np.float64(7.809093876858242e-05), 'behavior_loss': np.float64(0.3876516193151474)}

Episode step 41030, time diff 4.5154430866241455, total time dif 11251.308352708817)
step: 41030 @ episode report: {'average_total_reward': np.float32(8.802222), 'reward_variance': np.float32(3.7590823), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06552816368639469), 'actor_loss': np.float64(-0.9933698356151581), 'hyper_actor_loss': np.float64(7.30491603462724e-05), 'behavior_loss': np.float64(0.3870286077260971)}

Episode step 41040, time diff 4.471930503845215, total time dif 11255.82379579544)
step: 41040 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(5.1469646), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06263613291084766), 'actor_loss': np.float64(-0.9833269596099854), 'hyper_actor_loss': np.float64(7.248315778269898e-05), 'behavior_loss': np.float64(0.38636102676391604)}

Episode step 41050, time diff 4.537407875061035, total time dif 11260.295726299286)
step: 41050 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(2.3492634), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06354644671082496), 'actor_loss': np.float64(-0.978604131937027), 'hyper_actor_loss': np.float64(6.974237512622495e-05), 'behavior_loss': np.float64(0.3827789783477783)}

Episode step 41060, time diff 4.494014263153076, total time dif 11264.833134174347)
step: 41060 @ episode report: {'average_total_reward': np.float32(9.187779), 'reward_variance': np.float32(1.0291717), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06342252129688859), 'actor_loss': np.float64(-0.9856920003890991), 'hyper_actor_loss': np.float64(7.047159633657429e-05), 'behavior_loss': np.float64(0.385982358455658)}

Episode step 41070, time diff 4.501647472381592, total time dif 11269.3271484375)
step: 41070 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(2.4408445), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07267110534012318), 'actor_loss': np.float64(-0.996341323852539), 'hyper_actor_loss': np.float64(6.615820493607316e-05), 'behavior_loss': np.float64(0.3873074948787689)}

Episode step 41080, time diff 4.591602325439453, total time dif 11273.828795909882)
step: 41080 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(2.3773286), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0626603677868843), 'actor_loss': np.float64(-0.9854872047901153), 'hyper_actor_loss': np.float64(6.829648409620859e-05), 'behavior_loss': np.float64(0.393527153134346)}

Episode step 41090, time diff 4.5206544399261475, total time dif 11278.420398235321)
step: 41090 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.3872056), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06546435821801425), 'actor_loss': np.float64(-0.9776722133159638), 'hyper_actor_loss': np.float64(6.380773411365225e-05), 'behavior_loss': np.float64(0.389301061630249)}

Episode step 41100, time diff 4.523057222366333, total time dif 11282.941052675247)
step: 41100 @ episode report: {'average_total_reward': np.float32(9.075557), 'reward_variance': np.float32(2.2450328), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07100475542247295), 'actor_loss': np.float64(-0.9801640629768371), 'hyper_actor_loss': np.float64(6.264354669838212e-05), 'behavior_loss': np.float64(0.3884059935808182)}

Episode step 41110, time diff 4.503815412521362, total time dif 11287.464109897614)
step: 41110 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(1.2385235), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05551087334752083), 'actor_loss': np.float64(-0.979893958568573), 'hyper_actor_loss': np.float64(6.085173117753584e-05), 'behavior_loss': np.float64(0.38870830833911896)}

Episode step 41120, time diff 4.501004934310913, total time dif 11291.967925310135)
step: 41120 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(3.8130696), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.533333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06453751921653747), 'actor_loss': np.float64(-0.960141658782959), 'hyper_actor_loss': np.float64(6.171526038087905e-05), 'behavior_loss': np.float64(0.3894850015640259)}

Episode step 41130, time diff 4.459700345993042, total time dif 11296.468930244446)
step: 41130 @ episode report: {'average_total_reward': np.float32(9.673334), 'reward_variance': np.float32(2.7489188), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06349350102245807), 'actor_loss': np.float64(-0.9883413732051849), 'hyper_actor_loss': np.float64(5.6848185340641064e-05), 'behavior_loss': np.float64(0.38486050963401797)}

Episode step 41140, time diff 4.5527215003967285, total time dif 11300.928630590439)
step: 41140 @ episode report: {'average_total_reward': np.float32(9.712222), 'reward_variance': np.float32(2.3812454), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07580804452300072), 'actor_loss': np.float64(-0.9924712479114532), 'hyper_actor_loss': np.float64(5.739626321883406e-05), 'behavior_loss': np.float64(0.3902470976114273)}

Episode step 41150, time diff 4.438435792922974, total time dif 11305.481352090836)
step: 41150 @ episode report: {'average_total_reward': np.float32(9.538889), 'reward_variance': np.float32(3.8699315), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06437694784253836), 'actor_loss': np.float64(-0.9886764824390412), 'hyper_actor_loss': np.float64(5.5376144155161454e-05), 'behavior_loss': np.float64(0.38590517938137053)}

Episode step 41160, time diff 4.490389108657837, total time dif 11309.919787883759)
step: 41160 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(1.3357782), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0634525515139103), 'actor_loss': np.float64(-0.9763964295387269), 'hyper_actor_loss': np.float64(5.6418141684844156e-05), 'behavior_loss': np.float64(0.38798078894615173)}

Episode step 41170, time diff 4.474168062210083, total time dif 11314.410176992416)
step: 41170 @ episode report: {'average_total_reward': np.float32(9.351112), 'reward_variance': np.float32(1.6825478), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08250542692840099), 'actor_loss': np.float64(-0.999059545993805), 'hyper_actor_loss': np.float64(5.202128922974225e-05), 'behavior_loss': np.float64(0.3934119284152985)}

Episode step 41180, time diff 4.484239339828491, total time dif 11318.884345054626)
step: 41180 @ episode report: {'average_total_reward': np.float32(9.214445), 'reward_variance': np.float32(1.2457545), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05470634214580059), 'actor_loss': np.float64(-0.9949801743030549), 'hyper_actor_loss': np.float64(4.693031842180062e-05), 'behavior_loss': np.float64(0.3912927478551865)}

Episode step 41190, time diff 4.4657580852508545, total time dif 11323.368584394455)
step: 41190 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.8884203), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07816817462444306), 'actor_loss': np.float64(-0.9821471154689789), 'hyper_actor_loss': np.float64(4.9387260514777156e-05), 'behavior_loss': np.float64(0.3887373358011246)}

Episode step 41200, time diff 4.478982448577881, total time dif 11327.834342479706)
step: 41200 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.0025975), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07238943204283714), 'actor_loss': np.float64(-0.9935256719589234), 'hyper_actor_loss': np.float64(4.5071363638271576e-05), 'behavior_loss': np.float64(0.39058879017829895)}

Episode step 41210, time diff 4.503526210784912, total time dif 11332.313324928284)
step: 41210 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(1.5054133), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06650714725255966), 'actor_loss': np.float64(-0.9881016254425049), 'hyper_actor_loss': np.float64(4.252449725754559e-05), 'behavior_loss': np.float64(0.3913825660943985)}

Episode step 41220, time diff 4.441069841384888, total time dif 11336.816851139069)
step: 41220 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(1.7768161), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06548870243132114), 'actor_loss': np.float64(-0.9836580455303192), 'hyper_actor_loss': np.float64(4.2485322046559305e-05), 'behavior_loss': np.float64(0.39134857058525085)}

Episode step 41230, time diff 4.520042657852173, total time dif 11341.257920980453)
step: 41230 @ episode report: {'average_total_reward': np.float32(8.441112), 'reward_variance': np.float32(3.089434), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06969350278377533), 'actor_loss': np.float64(-0.9836083173751831), 'hyper_actor_loss': np.float64(4.0465161146130416e-05), 'behavior_loss': np.float64(0.3935294717550278)}

Episode step 41240, time diff 4.508841276168823, total time dif 11345.777963638306)
step: 41240 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.4052114), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06670242920517921), 'actor_loss': np.float64(-0.9840673685073853), 'hyper_actor_loss': np.float64(3.9782295425538904e-05), 'behavior_loss': np.float64(0.38651992976665495)}

Episode step 41250, time diff 4.665833473205566, total time dif 11350.286804914474)
step: 41250 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(4.5326724), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058410928770899775), 'actor_loss': np.float64(-0.9825100004673004), 'hyper_actor_loss': np.float64(3.81514657419757e-05), 'behavior_loss': np.float64(0.3929345041513443)}

Episode step 41260, time diff 4.4974095821380615, total time dif 11354.95263838768)
step: 41260 @ episode report: {'average_total_reward': np.float32(10.261112), 'reward_variance': np.float32(4.6814137), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07648322731256485), 'actor_loss': np.float64(-0.9823620736598968), 'hyper_actor_loss': np.float64(3.715309921972221e-05), 'behavior_loss': np.float64(0.39434130787849425)}

Episode step 41270, time diff 4.539688587188721, total time dif 11359.450047969818)
step: 41270 @ episode report: {'average_total_reward': np.float32(9.163335), 'reward_variance': np.float32(2.376816), 'max_total_reward': np.float32(11.9), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06353220790624618), 'actor_loss': np.float64(-0.9814566969871521), 'hyper_actor_loss': np.float64(3.5521917197911534e-05), 'behavior_loss': np.float64(0.38759405016899107)}

Episode step 41280, time diff 4.458742380142212, total time dif 11363.989736557007)
step: 41280 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(2.8635254), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06730140261352062), 'actor_loss': np.float64(-0.9839404165744782), 'hyper_actor_loss': np.float64(3.476485253486317e-05), 'behavior_loss': np.float64(0.3884509474039078)}

Episode step 41290, time diff 4.485251426696777, total time dif 11368.448478937149)
step: 41290 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(0.80452347), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06023906357586384), 'actor_loss': np.float64(-0.9932155132293701), 'hyper_actor_loss': np.float64(3.290964396001073e-05), 'behavior_loss': np.float64(0.38175772726535795)}

Episode step 41300, time diff 4.491867303848267, total time dif 11372.933730363846)
step: 41300 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.8390574), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07581932954490185), 'actor_loss': np.float64(-0.9952307522296906), 'hyper_actor_loss': np.float64(3.329193496028893e-05), 'behavior_loss': np.float64(0.387199929356575)}

Episode step 41310, time diff 4.509328842163086, total time dif 11377.425597667694)
step: 41310 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(2.1513095), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0688111636787653), 'actor_loss': np.float64(-1.010616958141327), 'hyper_actor_loss': np.float64(3.2340172765543684e-05), 'behavior_loss': np.float64(0.3862453788518906)}

Episode step 41320, time diff 4.497725248336792, total time dif 11381.934926509857)
step: 41320 @ episode report: {'average_total_reward': np.float32(8.102223), 'reward_variance': np.float32(1.3992544), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07295442931354046), 'actor_loss': np.float64(-0.9880515575408936), 'hyper_actor_loss': np.float64(3.240111054765293e-05), 'behavior_loss': np.float64(0.3889788329601288)}

Episode step 41330, time diff 4.538801670074463, total time dif 11386.432651758194)
step: 41330 @ episode report: {'average_total_reward': np.float32(9.126668), 'reward_variance': np.float32(3.4556603), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0640057347714901), 'actor_loss': np.float64(-0.9921955943107605), 'hyper_actor_loss': np.float64(3.0824704299448055e-05), 'behavior_loss': np.float64(0.3840287744998932)}

Episode step 41340, time diff 4.49550724029541, total time dif 11390.971453428268)
step: 41340 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.4699558), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06423821859061718), 'actor_loss': np.float64(-0.9816141963005066), 'hyper_actor_loss': np.float64(3.0060065000725444e-05), 'behavior_loss': np.float64(0.3879952132701874)}

Episode step 41350, time diff 4.531353235244751, total time dif 11395.466960668564)
step: 41350 @ episode report: {'average_total_reward': np.float32(10.061112), 'reward_variance': np.float32(4.3359838), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06164130941033363), 'actor_loss': np.float64(-0.9611405074596405), 'hyper_actor_loss': np.float64(3.153032266709488e-05), 'behavior_loss': np.float64(0.3890355736017227)}

Episode step 41360, time diff 4.516038179397583, total time dif 11399.998313903809)
step: 41360 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(0.5227164), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0653270747512579), 'actor_loss': np.float64(-0.9762808918952942), 'hyper_actor_loss': np.float64(2.7387457885197362e-05), 'behavior_loss': np.float64(0.38479415476322176)}

Episode step 41370, time diff 4.56228232383728, total time dif 11404.514352083206)
step: 41370 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(1.8552898), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07192492969334126), 'actor_loss': np.float64(-1.007134199142456), 'hyper_actor_loss': np.float64(2.9153476862120443e-05), 'behavior_loss': np.float64(0.38906386196613313)}

Episode step 41380, time diff 4.556199312210083, total time dif 11409.076634407043)
step: 41380 @ episode report: {'average_total_reward': np.float32(8.763334), 'reward_variance': np.float32(2.6290145), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07492427863180637), 'actor_loss': np.float64(-0.9943301498889923), 'hyper_actor_loss': np.float64(2.683764905668795e-05), 'behavior_loss': np.float64(0.3813532620668411)}

Episode step 41390, time diff 4.476083993911743, total time dif 11413.632833719254)
step: 41390 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(0.87592125), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07100571021437645), 'actor_loss': np.float64(-0.9909510970115661), 'hyper_actor_loss': np.float64(2.8372924498398787e-05), 'behavior_loss': np.float64(0.39777171015739443)}

Episode step 41400, time diff 4.469869375228882, total time dif 11418.108917713165)
step: 41400 @ episode report: {'average_total_reward': np.float32(8.963333), 'reward_variance': np.float32(2.488051), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07052791714668274), 'actor_loss': np.float64(-0.9780599117279053), 'hyper_actor_loss': np.float64(2.6985735894413664e-05), 'behavior_loss': np.float64(0.3795853018760681)}

Episode step 41410, time diff 4.722518444061279, total time dif 11422.578787088394)
step: 41410 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.1936343), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07985707893967628), 'actor_loss': np.float64(-1.0087350010871887), 'hyper_actor_loss': np.float64(2.8339656819298397e-05), 'behavior_loss': np.float64(0.382264244556427)}

Episode step 41420, time diff 4.55106258392334, total time dif 11427.301305532455)
step: 41420 @ episode report: {'average_total_reward': np.float32(8.63889), 'reward_variance': np.float32(0.94380903), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06587354838848114), 'actor_loss': np.float64(-1.0092860758304596), 'hyper_actor_loss': np.float64(3.030010611837497e-05), 'behavior_loss': np.float64(0.3766702592372894)}

Episode step 41430, time diff 4.510485887527466, total time dif 11431.852368116379)
step: 41430 @ episode report: {'average_total_reward': np.float32(9.4388895), 'reward_variance': np.float32(2.2372906), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06461949720978737), 'actor_loss': np.float64(-0.9806711614131928), 'hyper_actor_loss': np.float64(3.0142292598611675e-05), 'behavior_loss': np.float64(0.3848966032266617)}

Episode step 41440, time diff 4.5287253856658936, total time dif 11436.362854003906)
step: 41440 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(1.1559764), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06961526647210121), 'actor_loss': np.float64(-0.9716695010662079), 'hyper_actor_loss': np.float64(3.053787513636052e-05), 'behavior_loss': np.float64(0.38104931712150575)}

Episode step 41450, time diff 4.490330696105957, total time dif 11440.891579389572)
step: 41450 @ episode report: {'average_total_reward': np.float32(8.565557), 'reward_variance': np.float32(2.3500109), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05767102539539337), 'actor_loss': np.float64(-0.9939581871032714), 'hyper_actor_loss': np.float64(2.9170451671234333e-05), 'behavior_loss': np.float64(0.38309083580970765)}

Episode step 41460, time diff 4.515641212463379, total time dif 11445.381910085678)
step: 41460 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(3.3751626), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07470311522483826), 'actor_loss': np.float64(-0.9849536776542663), 'hyper_actor_loss': np.float64(2.8441775248211344e-05), 'behavior_loss': np.float64(0.37980854213237764)}

Episode step 41470, time diff 4.528113126754761, total time dif 11449.897551298141)
step: 41470 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(4.0484986), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06043773200362921), 'actor_loss': np.float64(-0.9834955394268036), 'hyper_actor_loss': np.float64(2.978693537443178e-05), 'behavior_loss': np.float64(0.3804595172405243)}

Episode step 41480, time diff 4.507276296615601, total time dif 11454.425664424896)
step: 41480 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(1.9338272), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0626998346298933), 'actor_loss': np.float64(-0.9767548203468323), 'hyper_actor_loss': np.float64(2.9835966051905417e-05), 'behavior_loss': np.float64(0.3768154472112656)}

Episode step 41490, time diff 4.492525815963745, total time dif 11458.932940721512)
step: 41490 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(1.8945196), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05940026789903641), 'actor_loss': np.float64(-0.9811484456062317), 'hyper_actor_loss': np.float64(2.8923885292897466e-05), 'behavior_loss': np.float64(0.38021464049816134)}

Episode step 41500, time diff 4.5328757762908936, total time dif 11463.425466537476)
step: 41500 @ episode report: {'average_total_reward': np.float32(8.675556), 'reward_variance': np.float32(1.8853283), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0664208073168993), 'actor_loss': np.float64(-0.9673976182937623), 'hyper_actor_loss': np.float64(2.7845544900628737e-05), 'behavior_loss': np.float64(0.38678706884384156)}

Episode step 41510, time diff 4.532510042190552, total time dif 11467.958342313766)
step: 41510 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(1.7248199), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06930058971047401), 'actor_loss': np.float64(-0.9819406986236572), 'hyper_actor_loss': np.float64(2.7564933770918287e-05), 'behavior_loss': np.float64(0.37462934851646423)}

Episode step 41520, time diff 4.495909929275513, total time dif 11472.490852355957)
step: 41520 @ episode report: {'average_total_reward': np.float32(9.375557), 'reward_variance': np.float32(2.4948347), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060988932475447656), 'actor_loss': np.float64(-0.9945707380771637), 'hyper_actor_loss': np.float64(2.935809698101366e-05), 'behavior_loss': np.float64(0.3710947781801224)}

Episode step 41530, time diff 4.545102119445801, total time dif 11476.986762285233)
step: 41530 @ episode report: {'average_total_reward': np.float32(9.961112), 'reward_variance': np.float32(1.1808212), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07451248504221439), 'actor_loss': np.float64(-0.9832076251506805), 'hyper_actor_loss': np.float64(3.2411375468655026e-05), 'behavior_loss': np.float64(0.3705423802137375)}

Episode step 41540, time diff 4.534872055053711, total time dif 11481.531864404678)
step: 41540 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.0466291), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07181071862578392), 'actor_loss': np.float64(-1.0004469811916352), 'hyper_actor_loss': np.float64(3.252935493947007e-05), 'behavior_loss': np.float64(0.36994668245315554)}

Episode step 41550, time diff 4.560852766036987, total time dif 11486.066736459732)
step: 41550 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(3.0031562), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07404591366648675), 'actor_loss': np.float64(-1.0022187888622285), 'hyper_actor_loss': np.float64(3.3147615249617955e-05), 'behavior_loss': np.float64(0.3699535220861435)}

Episode step 41560, time diff 4.491700649261475, total time dif 11490.627589225769)
step: 41560 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(2.1013196), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06016551833599806), 'actor_loss': np.float64(-0.9762708425521851), 'hyper_actor_loss': np.float64(3.337212092446862e-05), 'behavior_loss': np.float64(0.37367597222328186)}

Episode step 41570, time diff 4.464885473251343, total time dif 11495.11928987503)
step: 41570 @ episode report: {'average_total_reward': np.float32(8.914446), 'reward_variance': np.float32(1.9067913), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06877987049520015), 'actor_loss': np.float64(-0.9675386726856232), 'hyper_actor_loss': np.float64(3.160496326017892e-05), 'behavior_loss': np.float64(0.3662552237510681)}

Episode step 41580, time diff 4.703466176986694, total time dif 11499.584175348282)
step: 41580 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(1.1895664), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07255537621676922), 'actor_loss': np.float64(-0.987449187040329), 'hyper_actor_loss': np.float64(3.336465470056282e-05), 'behavior_loss': np.float64(0.3742019206285477)}

Episode step 41590, time diff 4.531992197036743, total time dif 11504.287641525269)
step: 41590 @ episode report: {'average_total_reward': np.float32(8.814445), 'reward_variance': np.float32(1.2136312), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07817828431725501), 'actor_loss': np.float64(-1.0024214684963226), 'hyper_actor_loss': np.float64(3.1358654996438415e-05), 'behavior_loss': np.float64(0.37291002869606016)}

Episode step 41600, time diff 4.518560886383057, total time dif 11508.819633722305)
step: 41600 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(2.391315), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06579467244446277), 'actor_loss': np.float64(-1.0024548172950745), 'hyper_actor_loss': np.float64(2.9730657661275473e-05), 'behavior_loss': np.float64(0.37110822200775145)}

Episode step 41610, time diff 4.498655557632446, total time dif 11513.338194608688)
step: 41610 @ episode report: {'average_total_reward': np.float32(9.251111), 'reward_variance': np.float32(1.7176342), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06494781486690045), 'actor_loss': np.float64(-0.9768783807754516), 'hyper_actor_loss': np.float64(2.8192910940560977e-05), 'behavior_loss': np.float64(0.37533560395240784)}

Episode step 41620, time diff 4.5421388149261475, total time dif 11517.83685016632)
step: 41620 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(1.907419), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07585803791880608), 'actor_loss': np.float64(-0.9771924138069152), 'hyper_actor_loss': np.float64(2.7739524193748365e-05), 'behavior_loss': np.float64(0.37169085443019867)}

Episode step 41630, time diff 4.517052412033081, total time dif 11522.378988981247)
step: 41630 @ episode report: {'average_total_reward': np.float32(8.641111), 'reward_variance': np.float32(1.7541494), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07121509872376919), 'actor_loss': np.float64(-1.0080065846443176), 'hyper_actor_loss': np.float64(2.8757330073858612e-05), 'behavior_loss': np.float64(0.3686177462339401)}

Episode step 41640, time diff 4.538431167602539, total time dif 11526.89604139328)
step: 41640 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(2.5578876), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06027725599706173), 'actor_loss': np.float64(-0.9817464590072632), 'hyper_actor_loss': np.float64(2.5979284873756115e-05), 'behavior_loss': np.float64(0.37037463784217833)}

Episode step 41650, time diff 4.492513418197632, total time dif 11531.434472560883)
step: 41650 @ episode report: {'average_total_reward': np.float32(8.902223), 'reward_variance': np.float32(2.1603649), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07003399655222893), 'actor_loss': np.float64(-0.965260922908783), 'hyper_actor_loss': np.float64(2.484513297531521e-05), 'behavior_loss': np.float64(0.3692736029624939)}

Episode step 41660, time diff 4.47752046585083, total time dif 11535.92698597908)
step: 41660 @ episode report: {'average_total_reward': np.float32(8.390001), 'reward_variance': np.float32(0.53043056), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06955686509609223), 'actor_loss': np.float64(-0.987915301322937), 'hyper_actor_loss': np.float64(2.3379810045298655e-05), 'behavior_loss': np.float64(0.38069237768650055)}

Episode step 41670, time diff 4.720199108123779, total time dif 11540.404506444931)
step: 41670 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.3028846), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06399926543235779), 'actor_loss': np.float64(-0.9901338160037995), 'hyper_actor_loss': np.float64(2.435087280900916e-05), 'behavior_loss': np.float64(0.371022492647171)}

Episode step 41680, time diff 4.460296869277954, total time dif 11545.124705553055)
step: 41680 @ episode report: {'average_total_reward': np.float32(9.663334), 'reward_variance': np.float32(1.2991124), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06934934742748737), 'actor_loss': np.float64(-0.9890634834766387), 'hyper_actor_loss': np.float64(2.198042238887865e-05), 'behavior_loss': np.float64(0.37123064398765565)}

Episode step 41690, time diff 4.628188610076904, total time dif 11549.585002422333)
step: 41690 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(3.4191308), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055788700096309186), 'actor_loss': np.float64(-0.9658432126045227), 'hyper_actor_loss': np.float64(2.1057381854916456e-05), 'behavior_loss': np.float64(0.3711356997489929)}

Episode step 41700, time diff 4.739239931106567, total time dif 11554.21319103241)
step: 41700 @ episode report: {'average_total_reward': np.float32(9.663335), 'reward_variance': np.float32(2.106619), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07405248433351516), 'actor_loss': np.float64(-0.9669837296009064), 'hyper_actor_loss': np.float64(2.048193728114711e-05), 'behavior_loss': np.float64(0.36997780203819275)}

Episode step 41710, time diff 4.744388580322266, total time dif 11558.952430963516)
step: 41710 @ episode report: {'average_total_reward': np.float32(8.987778), 'reward_variance': np.float32(2.029493), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061046187207102774), 'actor_loss': np.float64(-1.0015178084373475), 'hyper_actor_loss': np.float64(2.0380335263325834e-05), 'behavior_loss': np.float64(0.3688214927911758)}

Episode step 41720, time diff 4.707553863525391, total time dif 11563.696819543839)
step: 41720 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(1.0890483), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06844888180494309), 'actor_loss': np.float64(-0.9831534385681152), 'hyper_actor_loss': np.float64(1.9666444677568508e-05), 'behavior_loss': np.float64(0.36949690878391267)}

Episode step 41730, time diff 4.670891046524048, total time dif 11568.404373407364)
step: 41730 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(4.014144), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07167303189635277), 'actor_loss': np.float64(-0.9794827818870544), 'hyper_actor_loss': np.float64(1.9167571917932946e-05), 'behavior_loss': np.float64(0.37129026055336)}

Episode step 41740, time diff 4.9176881313323975, total time dif 11573.075264453888)
step: 41740 @ episode report: {'average_total_reward': np.float32(9.663335), 'reward_variance': np.float32(2.4213357), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07353154234588147), 'actor_loss': np.float64(-0.9943644225597381), 'hyper_actor_loss': np.float64(1.8573604029370473e-05), 'behavior_loss': np.float64(0.37346862256526947)}

Episode step 41750, time diff 4.72314190864563, total time dif 11577.99295258522)
step: 41750 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(0.5845728), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07284840457141399), 'actor_loss': np.float64(-0.9998003363609314), 'hyper_actor_loss': np.float64(1.5957177038217196e-05), 'behavior_loss': np.float64(0.36751386523246765)}

Episode step 41760, time diff 4.732887506484985, total time dif 11582.716094493866)
step: 41760 @ episode report: {'average_total_reward': np.float32(8.190001), 'reward_variance': np.float32(3.3717887), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06807400919497013), 'actor_loss': np.float64(-0.990973311662674), 'hyper_actor_loss': np.float64(1.7010119336191565e-05), 'behavior_loss': np.float64(0.37764136493206024)}

Episode step 41770, time diff 4.693907022476196, total time dif 11587.448982000351)
step: 41770 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(2.345902), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057220907881855965), 'actor_loss': np.float64(-0.9664059460163117), 'hyper_actor_loss': np.float64(1.592765111126937e-05), 'behavior_loss': np.float64(0.3682736992835999)}

Episode step 41780, time diff 4.790690183639526, total time dif 11592.142889022827)
step: 41780 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(3.1150236), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07369777858257294), 'actor_loss': np.float64(-0.9768135011196136), 'hyper_actor_loss': np.float64(1.5922963575576433e-05), 'behavior_loss': np.float64(0.3709546238183975)}

Episode step 41790, time diff 4.7253382205963135, total time dif 11596.933579206467)
step: 41790 @ episode report: {'average_total_reward': np.float32(8.777778), 'reward_variance': np.float32(2.1691108), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05757510382682085), 'actor_loss': np.float64(-0.9944631159305573), 'hyper_actor_loss': np.float64(1.70717881701421e-05), 'behavior_loss': np.float64(0.3710699021816254)}

Episode step 41800, time diff 4.638577461242676, total time dif 11601.658917427063)
step: 41800 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(3.0599372), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058373641408979896), 'actor_loss': np.float64(-0.9659031689167022), 'hyper_actor_loss': np.float64(1.7852929522632623e-05), 'behavior_loss': np.float64(0.36003096103668214)}

Episode step 41810, time diff 4.605717420578003, total time dif 11606.297494888306)
step: 41810 @ episode report: {'average_total_reward': np.float32(9.187779), 'reward_variance': np.float32(1.9573692), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05416901409626007), 'actor_loss': np.float64(-0.9682859420776367), 'hyper_actor_loss': np.float64(1.6252867681032512e-05), 'behavior_loss': np.float64(0.37096541225910185)}

Episode step 41820, time diff 4.836129665374756, total time dif 11610.903212308884)
step: 41820 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(1.3247426), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06952124908566475), 'actor_loss': np.float64(-0.9732929348945618), 'hyper_actor_loss': np.float64(1.713397214189172e-05), 'behavior_loss': np.float64(0.37102816700935365)}

Episode step 41830, time diff 4.8876330852508545, total time dif 11615.739341974258)
step: 41830 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(2.2743762), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.063687165081501), 'actor_loss': np.float64(-0.9902348935604095), 'hyper_actor_loss': np.float64(1.7553739417053295e-05), 'behavior_loss': np.float64(0.37041195631027224)}

Episode step 41840, time diff 4.931570053100586, total time dif 11620.62697505951)
step: 41840 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(3.7111802), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06211485154926777), 'actor_loss': np.float64(-0.9729489505290985), 'hyper_actor_loss': np.float64(1.6736919496906923e-05), 'behavior_loss': np.float64(0.3728867769241333)}

Episode step 41850, time diff 4.904438018798828, total time dif 11625.55854511261)
step: 41850 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(1.3082768), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07553678005933762), 'actor_loss': np.float64(-0.9716216146945953), 'hyper_actor_loss': np.float64(1.6368783508369234e-05), 'behavior_loss': np.float64(0.3710226833820343)}

Episode step 41860, time diff 4.79568886756897, total time dif 11630.462983131409)
step: 41860 @ episode report: {'average_total_reward': np.float32(9.026667), 'reward_variance': np.float32(0.933067), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054097529873251914), 'actor_loss': np.float64(-0.9920639753341675), 'hyper_actor_loss': np.float64(1.742338063195348e-05), 'behavior_loss': np.float64(0.3681335061788559)}

Episode step 41870, time diff 4.78921365737915, total time dif 11635.258671998978)
step: 41870 @ episode report: {'average_total_reward': np.float32(8.975556), 'reward_variance': np.float32(2.9502175), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06912514455616474), 'actor_loss': np.float64(-0.9783154904842377), 'hyper_actor_loss': np.float64(1.5877410714892903e-05), 'behavior_loss': np.float64(0.36441779136657715)}

Episode step 41880, time diff 4.571725845336914, total time dif 11640.047885656357)
step: 41880 @ episode report: {'average_total_reward': np.float32(9.7733345), 'reward_variance': np.float32(3.3850186), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0690532922744751), 'actor_loss': np.float64(-0.9775780975818634), 'hyper_actor_loss': np.float64(1.5844752942939523e-05), 'behavior_loss': np.float64(0.37059723436832426)}

Episode step 41890, time diff 4.6234517097473145, total time dif 11644.619611501694)
step: 41890 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(2.1153936), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06071653366088867), 'actor_loss': np.float64(-0.9783002674579621), 'hyper_actor_loss': np.float64(1.586177786521148e-05), 'behavior_loss': np.float64(0.36829374432563783)}

Episode step 41900, time diff 4.741316080093384, total time dif 11649.243063211441)
step: 41900 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(4.0430818), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07046661078929901), 'actor_loss': np.float64(-0.9852909624576569), 'hyper_actor_loss': np.float64(1.6515601419087034e-05), 'behavior_loss': np.float64(0.3666536122560501)}

Episode step 41910, time diff 4.586467027664185, total time dif 11653.984379291534)
step: 41910 @ episode report: {'average_total_reward': np.float32(9.375557), 'reward_variance': np.float32(2.9771314), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08140438981354237), 'actor_loss': np.float64(-1.0073505878448485), 'hyper_actor_loss': np.float64(1.499945765317534e-05), 'behavior_loss': np.float64(0.36800777316093447)}

Episode step 41920, time diff 4.60477352142334, total time dif 11658.570846319199)
step: 41920 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(4.663581), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060141980089247225), 'actor_loss': np.float64(-0.9915738821029663), 'hyper_actor_loss': np.float64(1.5316913868446135e-05), 'behavior_loss': np.float64(0.36197160482406615)}

Episode step 41930, time diff 4.64608097076416, total time dif 11663.175619840622)
step: 41930 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(0.31123954), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06289732027798892), 'actor_loss': np.float64(-0.9675008893013001), 'hyper_actor_loss': np.float64(1.4739317975909216e-05), 'behavior_loss': np.float64(0.37097392678260804)}

Episode step 41940, time diff 4.632340908050537, total time dif 11667.821700811386)
step: 41940 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(1.861903), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06756406258791685), 'actor_loss': np.float64(-0.976292073726654), 'hyper_actor_loss': np.float64(1.3210375618655234e-05), 'behavior_loss': np.float64(0.3727679461240768)}

Episode step 41950, time diff 4.694505929946899, total time dif 11672.454041719437)
step: 41950 @ episode report: {'average_total_reward': np.float32(9.5633335), 'reward_variance': np.float32(3.20415), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0743737805634737), 'actor_loss': np.float64(-0.9794279515743256), 'hyper_actor_loss': np.float64(1.3837597816745983e-05), 'behavior_loss': np.float64(0.3712132006883621)}

Episode step 41960, time diff 4.560394287109375, total time dif 11677.148547649384)
step: 41960 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(3.0644991), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06047223322093487), 'actor_loss': np.float64(-0.9800003945827485), 'hyper_actor_loss': np.float64(1.2834183144150301e-05), 'behavior_loss': np.float64(0.36375702917575836)}

Episode step 41970, time diff 4.630990982055664, total time dif 11681.708941936493)
step: 41970 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(0.8624446), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06294077932834626), 'actor_loss': np.float64(-0.9716456949710846), 'hyper_actor_loss': np.float64(1.2763054746756097e-05), 'behavior_loss': np.float64(0.36719556152820587)}

Episode step 41980, time diff 4.671179533004761, total time dif 11686.339932918549)
step: 41980 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(3.1234832), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07279800586402416), 'actor_loss': np.float64(-0.9840505838394165), 'hyper_actor_loss': np.float64(1.2529403284133877e-05), 'behavior_loss': np.float64(0.3714851140975952)}

Episode step 41990, time diff 4.614028453826904, total time dif 11691.011112451553)
step: 41990 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(1.5175812), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06312707476317883), 'actor_loss': np.float64(-0.9890878438949585), 'hyper_actor_loss': np.float64(1.2255384626769228e-05), 'behavior_loss': np.float64(0.36674738526344297)}

Episode step 42000, time diff 4.7336225509643555, total time dif 11695.62514090538)
step: 42000 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(4.755952), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06986459996551275), 'actor_loss': np.float64(-0.9704687416553497), 'hyper_actor_loss': np.float64(1.2161273843958043e-05), 'behavior_loss': np.float64(0.35585724115371703)}

Episode step 42010, time diff 4.665660381317139, total time dif 11700.358763456345)
step: 42010 @ episode report: {'average_total_reward': np.float32(9.548891), 'reward_variance': np.float32(1.8213876), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06638380363583565), 'actor_loss': np.float64(-1.000771051645279), 'hyper_actor_loss': np.float64(1.0641645803843858e-05), 'behavior_loss': np.float64(0.3686175405979156)}

Episode step 42020, time diff 4.629439115524292, total time dif 11705.024423837662)
step: 42020 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(1.7928009), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07147457152605056), 'actor_loss': np.float64(-0.9912535846233368), 'hyper_actor_loss': np.float64(1.0704273609007942e-05), 'behavior_loss': np.float64(0.37224926352500914)}

Episode step 42030, time diff 4.585512161254883, total time dif 11709.653862953186)
step: 42030 @ episode report: {'average_total_reward': np.float32(10.822223), 'reward_variance': np.float32(1.0009136), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07817660495638848), 'actor_loss': np.float64(-0.9863322079181671), 'hyper_actor_loss': np.float64(1.0689070404623634e-05), 'behavior_loss': np.float64(0.36717313826084136)}

Episode step 42040, time diff 4.703588962554932, total time dif 11714.239375114441)
step: 42040 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(1.351042), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07358708120882511), 'actor_loss': np.float64(-0.9954902708530426), 'hyper_actor_loss': np.float64(1.0205847502220422e-05), 'behavior_loss': np.float64(0.3691834002733231)}

Episode step 42050, time diff 4.7468883991241455, total time dif 11718.942964076996)
step: 42050 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(2.493946), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06960179097950459), 'actor_loss': np.float64(-0.9795448899269104), 'hyper_actor_loss': np.float64(9.693516176412231e-06), 'behavior_loss': np.float64(0.37542362213134767)}

Episode step 42060, time diff 4.7515575885772705, total time dif 11723.68985247612)
step: 42060 @ episode report: {'average_total_reward': np.float32(9.487778), 'reward_variance': np.float32(2.1837401), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055291719175875186), 'actor_loss': np.float64(-0.9689618647098541), 'hyper_actor_loss': np.float64(9.380462688568515e-06), 'behavior_loss': np.float64(0.36257190704345704)}

Episode step 42070, time diff 4.689246654510498, total time dif 11728.441410064697)
step: 42070 @ episode report: {'average_total_reward': np.float32(9.536667), 'reward_variance': np.float32(1.7288656), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06015156470239162), 'actor_loss': np.float64(-0.9758358001708984), 'hyper_actor_loss': np.float64(9.187462183035677e-06), 'behavior_loss': np.float64(0.36666391491889955)}

Episode step 42080, time diff 4.694629430770874, total time dif 11733.130656719208)
step: 42080 @ episode report: {'average_total_reward': np.float32(9.351112), 'reward_variance': np.float32(1.7004738), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05859067402780056), 'actor_loss': np.float64(-0.9740100800991058), 'hyper_actor_loss': np.float64(9.105706794798607e-06), 'behavior_loss': np.float64(0.3653027415275574)}

Episode step 42090, time diff 4.690274000167847, total time dif 11737.825286149979)
step: 42090 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(4.6174574), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07309092842042446), 'actor_loss': np.float64(-0.9772460281848907), 'hyper_actor_loss': np.float64(8.243995307566365e-06), 'behavior_loss': np.float64(0.3800898104906082)}

Episode step 42100, time diff 4.716813802719116, total time dif 11742.515560150146)
step: 42100 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(1.1408949), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.052104602381587026), 'actor_loss': np.float64(-0.9724562346935273), 'hyper_actor_loss': np.float64(8.266204440587899e-06), 'behavior_loss': np.float64(0.37411861419677733)}

Episode step 42110, time diff 4.728966236114502, total time dif 11747.232373952866)
step: 42110 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.6944001), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06060670502483845), 'actor_loss': np.float64(-0.9538615703582763), 'hyper_actor_loss': np.float64(8.535106053386698e-06), 'behavior_loss': np.float64(0.3785793274641037)}

Episode step 42120, time diff 4.733725070953369, total time dif 11751.96134018898)
step: 42120 @ episode report: {'average_total_reward': np.float32(9.363333), 'reward_variance': np.float32(2.1382234), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06643064338713885), 'actor_loss': np.float64(-0.969708377122879), 'hyper_actor_loss': np.float64(8.002384720384726e-06), 'behavior_loss': np.float64(0.36799634993076324)}

Episode step 42130, time diff 4.6128089427948, total time dif 11756.695065259933)
step: 42130 @ episode report: {'average_total_reward': np.float32(9.561112), 'reward_variance': np.float32(4.7470937), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07705953568220139), 'actor_loss': np.float64(-1.0016461670398713), 'hyper_actor_loss': np.float64(8.051052782320767e-06), 'behavior_loss': np.float64(0.37473633885383606)}

Episode step 42140, time diff 4.656827688217163, total time dif 11761.307874202728)
step: 42140 @ episode report: {'average_total_reward': np.float32(9.736668), 'reward_variance': np.float32(1.9164951), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07328473739326), 'actor_loss': np.float64(-0.99830482006073), 'hyper_actor_loss': np.float64(7.92813443695195e-06), 'behavior_loss': np.float64(0.36551755368709565)}

Episode step 42150, time diff 4.670287370681763, total time dif 11765.964701890945)
step: 42150 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(2.296909), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07331887111067772), 'actor_loss': np.float64(-0.9875138461589813), 'hyper_actor_loss': np.float64(7.1174045388033845e-06), 'behavior_loss': np.float64(0.37414657771587373)}

Episode step 42160, time diff 4.7331109046936035, total time dif 11770.634989261627)
step: 42160 @ episode report: {'average_total_reward': np.float32(8.665556), 'reward_variance': np.float32(1.8729492), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06850681956857443), 'actor_loss': np.float64(-0.9909988522529602), 'hyper_actor_loss': np.float64(7.0462824623973574e-06), 'behavior_loss': np.float64(0.3700032502412796)}

Episode step 42170, time diff 4.810222387313843, total time dif 11775.36810016632)
step: 42170 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(2.031057), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06658079028129578), 'actor_loss': np.float64(-0.9976074457168579), 'hyper_actor_loss': np.float64(6.725667844875716e-06), 'behavior_loss': np.float64(0.3766837537288666)}

Episode step 42180, time diff 4.8515849113464355, total time dif 11780.178322553635)
step: 42180 @ episode report: {'average_total_reward': np.float32(8.951111), 'reward_variance': np.float32(2.3243508), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06793889999389649), 'actor_loss': np.float64(-0.9725914418697357), 'hyper_actor_loss': np.float64(6.421002126444364e-06), 'behavior_loss': np.float64(0.38293305933475497)}

Episode step 42190, time diff 4.714127779006958, total time dif 11785.029907464981)
step: 42190 @ episode report: {'average_total_reward': np.float32(8.738889), 'reward_variance': np.float32(1.1134627), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06158852241933346), 'actor_loss': np.float64(-0.962356299161911), 'hyper_actor_loss': np.float64(6.020217006152961e-06), 'behavior_loss': np.float64(0.37549750208854676)}

Episode step 42200, time diff 4.850143909454346, total time dif 11789.744035243988)
step: 42200 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(2.378851), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06492715291678905), 'actor_loss': np.float64(-0.9760047197341919), 'hyper_actor_loss': np.float64(6.459784162871074e-06), 'behavior_loss': np.float64(0.3800478518009186)}

Episode step 42210, time diff 4.806820869445801, total time dif 11794.594179153442)
step: 42210 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(1.9731319), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06456332057714462), 'actor_loss': np.float64(-0.9889341473579407), 'hyper_actor_loss': np.float64(6.045413283572998e-06), 'behavior_loss': np.float64(0.37217732667922976)}

Episode step 42220, time diff 4.643923997879028, total time dif 11799.401000022888)
step: 42220 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(1.7191166), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0659013569355011), 'actor_loss': np.float64(-0.9873673141002655), 'hyper_actor_loss': np.float64(6.715111339872237e-06), 'behavior_loss': np.float64(0.3668550103902817)}

Episode step 42230, time diff 4.756365537643433, total time dif 11804.044924020767)
step: 42230 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(1.5208509), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060230851918458936), 'actor_loss': np.float64(-0.9894334733486175), 'hyper_actor_loss': np.float64(6.818383508289117e-06), 'behavior_loss': np.float64(0.3750074714422226)}

Episode step 42240, time diff 4.602556228637695, total time dif 11808.80128955841)
step: 42240 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(4.9494686), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06895269490778447), 'actor_loss': np.float64(-0.9781474351882935), 'hyper_actor_loss': np.float64(7.457243282260606e-06), 'behavior_loss': np.float64(0.38405373096466067)}

Episode step 42250, time diff 4.609841823577881, total time dif 11813.403845787048)
step: 42250 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(2.896711), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07286986298859119), 'actor_loss': np.float64(-0.9809160709381104), 'hyper_actor_loss': np.float64(7.524349575760425e-06), 'behavior_loss': np.float64(0.37768363058567045)}

Episode step 42260, time diff 4.602275133132935, total time dif 11818.013687610626)
step: 42260 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.063881), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06452432908117771), 'actor_loss': np.float64(-0.9883234798908234), 'hyper_actor_loss': np.float64(7.70813358030864e-06), 'behavior_loss': np.float64(0.38586257100105287)}

Episode step 42270, time diff 4.662998199462891, total time dif 11822.61596274376)
step: 42270 @ episode report: {'average_total_reward': np.float32(9.026668), 'reward_variance': np.float32(2.854819), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054581510834395885), 'actor_loss': np.float64(-0.9663767099380494), 'hyper_actor_loss': np.float64(7.191079203039408e-06), 'behavior_loss': np.float64(0.3841659426689148)}

Episode step 42280, time diff 4.623151540756226, total time dif 11827.278960943222)
step: 42280 @ episode report: {'average_total_reward': np.float32(8.851111), 'reward_variance': np.float32(2.6430426), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05892472993582487), 'actor_loss': np.float64(-0.9564329445362091), 'hyper_actor_loss': np.float64(7.026866569503909e-06), 'behavior_loss': np.float64(0.382440984249115)}

Episode step 42290, time diff 4.5980143547058105, total time dif 11831.902112483978)
step: 42290 @ episode report: {'average_total_reward': np.float32(10.297777), 'reward_variance': np.float32(3.025131), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06220025084912777), 'actor_loss': np.float64(-0.9729448556900024), 'hyper_actor_loss': np.float64(6.674909900539205e-06), 'behavior_loss': np.float64(0.37966338992118837)}

Episode step 42300, time diff 4.639057636260986, total time dif 11836.500126838684)
step: 42300 @ episode report: {'average_total_reward': np.float32(9.412224), 'reward_variance': np.float32(0.34885073), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0751607920974493), 'actor_loss': np.float64(-0.9918350756168366), 'hyper_actor_loss': np.float64(6.011545247019967e-06), 'behavior_loss': np.float64(0.37767176032066346)}

Episode step 42310, time diff 4.648659944534302, total time dif 11841.139184474945)
step: 42310 @ episode report: {'average_total_reward': np.float32(8.302223), 'reward_variance': np.float32(4.28839), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06517741158604622), 'actor_loss': np.float64(-1.0039700448513031), 'hyper_actor_loss': np.float64(6.806233704992337e-06), 'behavior_loss': np.float64(0.37943094670772554)}

Episode step 42320, time diff 4.6389007568359375, total time dif 11845.78784441948)
step: 42320 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(5.450099), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05875384248793125), 'actor_loss': np.float64(-0.9679289877414703), 'hyper_actor_loss': np.float64(7.086375580911408e-06), 'behavior_loss': np.float64(0.3796531230211258)}

Episode step 42330, time diff 4.683151721954346, total time dif 11850.426745176315)
step: 42330 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(1.7199619), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06206312272697687), 'actor_loss': np.float64(-0.964782452583313), 'hyper_actor_loss': np.float64(6.5710044964362165e-06), 'behavior_loss': np.float64(0.38360423743724825)}

Episode step 42340, time diff 4.596471309661865, total time dif 11855.10989689827)
step: 42340 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(3.4265385), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07592957504093648), 'actor_loss': np.float64(-0.9879234433174133), 'hyper_actor_loss': np.float64(6.125467643869343e-06), 'behavior_loss': np.float64(0.37765983641147616)}

Episode step 42350, time diff 4.902329921722412, total time dif 11859.706368207932)
step: 42350 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.4102335), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081169784069061), 'actor_loss': np.float64(-0.9952490448951721), 'hyper_actor_loss': np.float64(5.9371132465457775e-06), 'behavior_loss': np.float64(0.38070119321346285)}

Episode step 42360, time diff 4.9321699142456055, total time dif 11864.608698129654)
step: 42360 @ episode report: {'average_total_reward': np.float32(8.875556), 'reward_variance': np.float32(3.3396251), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06864090897142887), 'actor_loss': np.float64(-0.9937354564666748), 'hyper_actor_loss': np.float64(5.96187373957946e-06), 'behavior_loss': np.float64(0.3811532348394394)}

Episode step 42370, time diff 4.7190101146698, total time dif 11869.5408680439)
step: 42370 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(2.3951602), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06983125284314155), 'actor_loss': np.float64(-0.984891140460968), 'hyper_actor_loss': np.float64(5.755052052336396e-06), 'behavior_loss': np.float64(0.37401887476444245)}

Episode step 42380, time diff 4.784587144851685, total time dif 11874.25987815857)
step: 42380 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(1.7164955), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06305909268558026), 'actor_loss': np.float64(-0.9944583177566528), 'hyper_actor_loss': np.float64(5.366418690755381e-06), 'behavior_loss': np.float64(0.37767243683338164)}

Episode step 42390, time diff 4.756098508834839, total time dif 11879.044465303421)
step: 42390 @ episode report: {'average_total_reward': np.float32(9.526667), 'reward_variance': np.float32(2.2120297), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06950273513793945), 'actor_loss': np.float64(-0.989498895406723), 'hyper_actor_loss': np.float64(5.395948937803041e-06), 'behavior_loss': np.float64(0.37922758758068087)}

Episode step 42400, time diff 4.944054365158081, total time dif 11883.800563812256)
step: 42400 @ episode report: {'average_total_reward': np.float32(8.577779), 'reward_variance': np.float32(1.6039011), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06540462914854288), 'actor_loss': np.float64(-0.9823556184768677), 'hyper_actor_loss': np.float64(4.89781423311797e-06), 'behavior_loss': np.float64(0.383248370885849)}

Episode step 42410, time diff 4.848457098007202, total time dif 11888.744618177414)
step: 42410 @ episode report: {'average_total_reward': np.float32(8.53889), 'reward_variance': np.float32(1.7342288), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07163135558366776), 'actor_loss': np.float64(-0.9886888444423676), 'hyper_actor_loss': np.float64(5.049393394074286e-06), 'behavior_loss': np.float64(0.38342011868953707)}

Episode step 42420, time diff 4.850083589553833, total time dif 11893.593075275421)
step: 42420 @ episode report: {'average_total_reward': np.float32(9.751112), 'reward_variance': np.float32(3.0544496), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08650606498122215), 'actor_loss': np.float64(-1.0044167757034301), 'hyper_actor_loss': np.float64(4.597312590703951e-06), 'behavior_loss': np.float64(0.3854097932577133)}

Episode step 42430, time diff 4.900875806808472, total time dif 11898.443158864975)
step: 42430 @ episode report: {'average_total_reward': np.float32(9.200001), 'reward_variance': np.float32(1.4693825), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0814314067363739), 'actor_loss': np.float64(-1.0082544207572937), 'hyper_actor_loss': np.float64(4.695100051321787e-06), 'behavior_loss': np.float64(0.3900498032569885)}

Episode step 42440, time diff 4.820009469985962, total time dif 11903.344034671783)
step: 42440 @ episode report: {'average_total_reward': np.float32(9.773334), 'reward_variance': np.float32(2.0383508), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07016383334994317), 'actor_loss': np.float64(-0.9996794044971467), 'hyper_actor_loss': np.float64(5.209850814935635e-06), 'behavior_loss': np.float64(0.38213286399841306)}

Episode step 42450, time diff 4.845593452453613, total time dif 11908.16404414177)
step: 42450 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(2.0185976), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07229949571192265), 'actor_loss': np.float64(-0.9798481523990631), 'hyper_actor_loss': np.float64(5.1361200348765125e-06), 'behavior_loss': np.float64(0.3909516245126724)}

Episode step 42460, time diff 4.858070373535156, total time dif 11913.009637594223)
step: 42460 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(0.8496159), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06909469366073609), 'actor_loss': np.float64(-0.9761052846908569), 'hyper_actor_loss': np.float64(5.376738135964842e-06), 'behavior_loss': np.float64(0.38743957579135896)}

Episode step 42470, time diff 4.772674322128296, total time dif 11917.867707967758)
step: 42470 @ episode report: {'average_total_reward': np.float32(8.390001), 'reward_variance': np.float32(3.1893442), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06855031363666057), 'actor_loss': np.float64(-0.9854174017906189), 'hyper_actor_loss': np.float64(5.720759463656577e-06), 'behavior_loss': np.float64(0.3888484239578247)}

Episode step 42480, time diff 4.855790615081787, total time dif 11922.640382289886)
step: 42480 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(0.9931313), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06264453958719969), 'actor_loss': np.float64(-0.9818674564361572), 'hyper_actor_loss': np.float64(5.582358562605805e-06), 'behavior_loss': np.float64(0.38906507194042206)}

Episode step 42490, time diff 4.828843832015991, total time dif 11927.496172904968)
step: 42490 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(0.71444577), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06310031078755855), 'actor_loss': np.float64(-0.9692735314369202), 'hyper_actor_loss': np.float64(5.425983863460715e-06), 'behavior_loss': np.float64(0.38754112720489503)}

Episode step 42500, time diff 4.74749493598938, total time dif 11932.325016736984)
step: 42500 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(2.4969335), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06805858574807644), 'actor_loss': np.float64(-0.9723755240440368), 'hyper_actor_loss': np.float64(5.3012686748843406e-06), 'behavior_loss': np.float64(0.3966882735490799)}

Episode step 42510, time diff 4.856709241867065, total time dif 11937.072511672974)
step: 42510 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(0.77709174), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062885233014822), 'actor_loss': np.float64(-0.9799979865550995), 'hyper_actor_loss': np.float64(5.2885663990309695e-06), 'behavior_loss': np.float64(0.3887301743030548)}

Episode step 42520, time diff 4.883608341217041, total time dif 11941.92922091484)
step: 42520 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(3.5941925), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05872267298400402), 'actor_loss': np.float64(-0.9785979390144348), 'hyper_actor_loss': np.float64(5.118322678754339e-06), 'behavior_loss': np.float64(0.38631689846515654)}

Episode step 42530, time diff 4.869654655456543, total time dif 11946.812829256058)
step: 42530 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(2.7467163), 'max_total_reward': np.float32(13.1444435), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08315561190247536), 'actor_loss': np.float64(-0.9832299411296844), 'hyper_actor_loss': np.float64(4.9675716354613545e-06), 'behavior_loss': np.float64(0.39167867600917816)}

Episode step 42540, time diff 4.876417636871338, total time dif 11951.682483911514)
step: 42540 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(2.7039015), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06246653664857149), 'actor_loss': np.float64(-0.9943348944187165), 'hyper_actor_loss': np.float64(4.654825352190528e-06), 'behavior_loss': np.float64(0.39260877668857574)}

Episode step 42550, time diff 4.752752304077148, total time dif 11956.558901548386)
step: 42550 @ episode report: {'average_total_reward': np.float32(8.914445), 'reward_variance': np.float32(3.1906178), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(6.411112), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07307302504777909), 'actor_loss': np.float64(-0.9763616979122162), 'hyper_actor_loss': np.float64(4.547084654404898e-06), 'behavior_loss': np.float64(0.3866114795207977)}

Episode step 42560, time diff 4.892032623291016, total time dif 11961.311653852463)
step: 42560 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(2.1872058), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07229675501585006), 'actor_loss': np.float64(-0.9863495886325836), 'hyper_actor_loss': np.float64(4.502073534240481e-06), 'behavior_loss': np.float64(0.3931295216083527)}

Episode step 42570, time diff 4.739869594573975, total time dif 11966.203686475754)
step: 42570 @ episode report: {'average_total_reward': np.float32(9.987779), 'reward_variance': np.float32(3.2349992), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05767418928444386), 'actor_loss': np.float64(-0.9894782662391662), 'hyper_actor_loss': np.float64(4.3343068682588635e-06), 'behavior_loss': np.float64(0.3855830758810043)}

Episode step 42580, time diff 4.641631364822388, total time dif 11970.943556070328)
step: 42580 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(2.0090423), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06480466891080142), 'actor_loss': np.float64(-0.9728440582752228), 'hyper_actor_loss': np.float64(4.700032786786324e-06), 'behavior_loss': np.float64(0.38940801918506623)}

Episode step 42590, time diff 4.639060974121094, total time dif 11975.58518743515)
step: 42590 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.6999061), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060457246378064156), 'actor_loss': np.float64(-0.9712594628334046), 'hyper_actor_loss': np.float64(4.425384577189106e-06), 'behavior_loss': np.float64(0.3893346577882767)}

Episode step 42600, time diff 4.669701814651489, total time dif 11980.224248409271)
step: 42600 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(0.5760852), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07467397227883339), 'actor_loss': np.float64(-0.9742215216159821), 'hyper_actor_loss': np.float64(4.779986920766533e-06), 'behavior_loss': np.float64(0.39238781332969663)}

Episode step 42610, time diff 4.591974258422852, total time dif 11984.893950223923)
step: 42610 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(1.9488903), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06584261320531368), 'actor_loss': np.float64(-0.9943569421768188), 'hyper_actor_loss': np.float64(4.466211112230667e-06), 'behavior_loss': np.float64(0.38816908895969393)}

Episode step 42620, time diff 4.70906925201416, total time dif 11989.485924482346)
step: 42620 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.135364), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06798878200352192), 'actor_loss': np.float64(-0.989119154214859), 'hyper_actor_loss': np.float64(4.099589318684593e-06), 'behavior_loss': np.float64(0.38351731598377226)}

Episode step 42630, time diff 4.828237056732178, total time dif 11994.19499373436)
step: 42630 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(0.89527893), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0723636008799076), 'actor_loss': np.float64(-0.9860231935977936), 'hyper_actor_loss': np.float64(4.012382396467728e-06), 'behavior_loss': np.float64(0.39951370656490326)}

Episode step 42640, time diff 4.853257179260254, total time dif 11999.023230791092)
step: 42640 @ episode report: {'average_total_reward': np.float32(9.002223), 'reward_variance': np.float32(3.550686), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07554837092757224), 'actor_loss': np.float64(-0.9876559913158417), 'hyper_actor_loss': np.float64(3.7028943097539014e-06), 'behavior_loss': np.float64(0.38825746178627013)}

Episode step 42650, time diff 4.7579665184021, total time dif 12003.876487970352)
step: 42650 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(1.8036655), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07142156045883893), 'actor_loss': np.float64(-0.9877722024917602), 'hyper_actor_loss': np.float64(3.827104160336603e-06), 'behavior_loss': np.float64(0.39314043819904326)}

Episode step 42660, time diff 4.652379274368286, total time dif 12008.634454488754)
step: 42660 @ episode report: {'average_total_reward': np.float32(8.863335), 'reward_variance': np.float32(1.4076556), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06545500233769416), 'actor_loss': np.float64(-0.9894125640392304), 'hyper_actor_loss': np.float64(3.652785244412371e-06), 'behavior_loss': np.float64(0.3922919750213623)}

Episode step 42670, time diff 4.797658443450928, total time dif 12013.286833763123)
step: 42670 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.600279), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07141982577741146), 'actor_loss': np.float64(-0.9871037721633911), 'hyper_actor_loss': np.float64(3.4511448802732046e-06), 'behavior_loss': np.float64(0.3882861614227295)}

Episode step 42680, time diff 4.662433862686157, total time dif 12018.084492206573)
step: 42680 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(3.3328452), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057520841807127), 'actor_loss': np.float64(-0.9740760862827301), 'hyper_actor_loss': np.float64(3.009713600476971e-06), 'behavior_loss': np.float64(0.39298364520072937)}

Episode step 42690, time diff 4.644877672195435, total time dif 12022.74692606926)
step: 42690 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(4.101428), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05253078229725361), 'actor_loss': np.float64(-0.9595840334892273), 'hyper_actor_loss': np.float64(2.8548734007927123e-06), 'behavior_loss': np.float64(0.38568336367607114)}

Episode step 42700, time diff 4.568398714065552, total time dif 12027.391803741455)
step: 42700 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(2.0622919), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05928830504417419), 'actor_loss': np.float64(-0.9658070981502533), 'hyper_actor_loss': np.float64(2.6114350021089193e-06), 'behavior_loss': np.float64(0.392030131816864)}

Episode step 42710, time diff 4.7594451904296875, total time dif 12031.96020245552)
step: 42710 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.5434194), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07203061319887638), 'actor_loss': np.float64(-0.9790639996528625), 'hyper_actor_loss': np.float64(2.3555543293696246e-06), 'behavior_loss': np.float64(0.39543519020080564)}

Episode step 42720, time diff 4.770935773849487, total time dif 12036.71964764595)
step: 42720 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(1.0333283), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07331718988716603), 'actor_loss': np.float64(-0.9861663699150085), 'hyper_actor_loss': np.float64(2.3040101268634315e-06), 'behavior_loss': np.float64(0.3901748597621918)}

Episode step 42730, time diff 5.020816802978516, total time dif 12041.4905834198)
step: 42730 @ episode report: {'average_total_reward': np.float32(9.761111), 'reward_variance': np.float32(1.223117), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06848104223608971), 'actor_loss': np.float64(-0.9886462807655334), 'hyper_actor_loss': np.float64(2.295908802807389e-06), 'behavior_loss': np.float64(0.39294286370277404)}

Episode step 42740, time diff 4.828446388244629, total time dif 12046.511400222778)
step: 42740 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(0.9467403), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05916208513081074), 'actor_loss': np.float64(-0.9742622613906861), 'hyper_actor_loss': np.float64(2.0213517473166576e-06), 'behavior_loss': np.float64(0.3960937589406967)}

Episode step 42750, time diff 4.789617300033569, total time dif 12051.339846611023)
step: 42750 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(2.0223317), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0774949312210083), 'actor_loss': np.float64(-0.9737888395786285), 'hyper_actor_loss': np.float64(1.9448382204245716e-06), 'behavior_loss': np.float64(0.39636307060718534)}

Episode step 42760, time diff 4.826454401016235, total time dif 12056.129463911057)
step: 42760 @ episode report: {'average_total_reward': np.float32(9.700002), 'reward_variance': np.float32(1.2068894), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07397025302052498), 'actor_loss': np.float64(-0.994755494594574), 'hyper_actor_loss': np.float64(1.8835448599929806e-06), 'behavior_loss': np.float64(0.3913819670677185)}

Episode step 42770, time diff 4.717201471328735, total time dif 12060.955918312073)
step: 42770 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(1.2834371), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06538243591785431), 'actor_loss': np.float64(-0.9867076873779297), 'hyper_actor_loss': np.float64(1.749593116073811e-06), 'behavior_loss': np.float64(0.38942205905914307)}

Episode step 42780, time diff 4.641881465911865, total time dif 12065.673119783401)
step: 42780 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(4.069807), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07332915402948856), 'actor_loss': np.float64(-0.9780562698841095), 'hyper_actor_loss': np.float64(1.6025632930904977e-06), 'behavior_loss': np.float64(0.39343126118183136)}

Episode step 42790, time diff 4.694304704666138, total time dif 12070.315001249313)
step: 42790 @ episode report: {'average_total_reward': np.float32(9.787779), 'reward_variance': np.float32(1.077937), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07648227885365486), 'actor_loss': np.float64(-0.98906130194664), 'hyper_actor_loss': np.float64(1.6482309888488089e-06), 'behavior_loss': np.float64(0.39525049924850464)}

Episode step 42800, time diff 4.667957305908203, total time dif 12075.00930595398)
step: 42800 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(3.1469448), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07365429177880287), 'actor_loss': np.float64(-0.9950482368469238), 'hyper_actor_loss': np.float64(1.5992180351531715e-06), 'behavior_loss': np.float64(0.39271804988384246)}

Episode step 42810, time diff 4.7058680057525635, total time dif 12079.677263259888)
step: 42810 @ episode report: {'average_total_reward': np.float32(9.524446), 'reward_variance': np.float32(1.8006375), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06349056251347066), 'actor_loss': np.float64(-0.9828212678432464), 'hyper_actor_loss': np.float64(1.611736627182836e-06), 'behavior_loss': np.float64(0.3971732914447784)}

Episode step 42820, time diff 4.645981788635254, total time dif 12084.38313126564)
step: 42820 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(1.831057), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07699621841311455), 'actor_loss': np.float64(-0.9699125528335572), 'hyper_actor_loss': np.float64(1.668684876676707e-06), 'behavior_loss': np.float64(0.3896194607019424)}

Episode step 42830, time diff 4.6764092445373535, total time dif 12089.029113054276)
step: 42830 @ episode report: {'average_total_reward': np.float32(10.3977785), 'reward_variance': np.float32(2.252514), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0826412558555603), 'actor_loss': np.float64(-1.0080808997154236), 'hyper_actor_loss': np.float64(1.628013899335201e-06), 'behavior_loss': np.float64(0.3950179010629654)}

Episode step 42840, time diff 4.626749753952026, total time dif 12093.705522298813)
step: 42840 @ episode report: {'average_total_reward': np.float32(9.812223), 'reward_variance': np.float32(3.089864), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07793587259948254), 'actor_loss': np.float64(-1.0141314387321472), 'hyper_actor_loss': np.float64(1.7739653912940411e-06), 'behavior_loss': np.float64(0.3903583139181137)}

Episode step 42850, time diff 4.647335767745972, total time dif 12098.332272052765)
step: 42850 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(2.9222674), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07312380373477936), 'actor_loss': np.float64(-0.9909432888031006), 'hyper_actor_loss': np.float64(1.6005469433366671e-06), 'behavior_loss': np.float64(0.38980940282344817)}

Episode step 42860, time diff 4.6364381313323975, total time dif 12102.97960782051)
step: 42860 @ episode report: {'average_total_reward': np.float32(8.414445), 'reward_variance': np.float32(3.5742981), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06536994129419327), 'actor_loss': np.float64(-0.972714501619339), 'hyper_actor_loss': np.float64(1.9504608303577696e-06), 'behavior_loss': np.float64(0.3952946662902832)}

Episode step 42870, time diff 4.6846089363098145, total time dif 12107.616045951843)
step: 42870 @ episode report: {'average_total_reward': np.float32(9.751112), 'reward_variance': np.float32(1.5706221), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06601665616035461), 'actor_loss': np.float64(-0.9761176526546478), 'hyper_actor_loss': np.float64(2.0210452134961087e-06), 'behavior_loss': np.float64(0.3915196269750595)}

Episode step 42880, time diff 4.810586452484131, total time dif 12112.300654888153)
step: 42880 @ episode report: {'average_total_reward': np.float32(8.851112), 'reward_variance': np.float32(3.1742272), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06434213519096374), 'actor_loss': np.float64(-0.980479496717453), 'hyper_actor_loss': np.float64(2.415549238321546e-06), 'behavior_loss': np.float64(0.39404545426368714)}

Episode step 42890, time diff 5.019286870956421, total time dif 12117.111241340637)
step: 42890 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.2356856), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07323885671794414), 'actor_loss': np.float64(-0.9893046498298645), 'hyper_actor_loss': np.float64(2.9127152856744944e-06), 'behavior_loss': np.float64(0.3901857793331146)}

Episode step 42900, time diff 4.970373630523682, total time dif 12122.130528211594)
step: 42900 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(1.5597584), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07346404213458299), 'actor_loss': np.float64(-0.9914511680603028), 'hyper_actor_loss': np.float64(3.918675383829396e-06), 'behavior_loss': np.float64(0.39241875410079957)}

Episode step 42910, time diff 4.7867608070373535, total time dif 12127.100901842117)
step: 42910 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(3.3173835), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06788161657750606), 'actor_loss': np.float64(-0.9805150210857392), 'hyper_actor_loss': np.float64(4.782336827702238e-06), 'behavior_loss': np.float64(0.39535330832004545)}

Episode step 42920, time diff 4.773758888244629, total time dif 12131.887662649155)
step: 42920 @ episode report: {'average_total_reward': np.float32(8.3144455), 'reward_variance': np.float32(1.0484214), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07113306596875191), 'actor_loss': np.float64(-0.9824828505516052), 'hyper_actor_loss': np.float64(5.805015416626702e-06), 'behavior_loss': np.float64(0.3914739191532135)}

Episode step 42930, time diff 4.7858686447143555, total time dif 12136.6614215374)
step: 42930 @ episode report: {'average_total_reward': np.float32(8.987778), 'reward_variance': np.float32(2.8644319), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06465435773134232), 'actor_loss': np.float64(-0.9777150452136993), 'hyper_actor_loss': np.float64(8.036144436118775e-06), 'behavior_loss': np.float64(0.39848569929599764)}

Episode step 42940, time diff 4.805410146713257, total time dif 12141.447290182114)
step: 42940 @ episode report: {'average_total_reward': np.float32(9.436667), 'reward_variance': np.float32(3.2330136), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07404606193304061), 'actor_loss': np.float64(-0.9804729878902435), 'hyper_actor_loss': np.float64(1.01358033134602e-05), 'behavior_loss': np.float64(0.3962561458349228)}

Episode step 42950, time diff 4.836885452270508, total time dif 12146.252700328827)
step: 42950 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(1.3198469), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07231208384037018), 'actor_loss': np.float64(-1.0031372725963592), 'hyper_actor_loss': np.float64(1.4729941540281288e-05), 'behavior_loss': np.float64(0.39449273645877836)}

Episode step 42960, time diff 4.808547735214233, total time dif 12151.089585781097)
step: 42960 @ episode report: {'average_total_reward': np.float32(9.924444), 'reward_variance': np.float32(3.370044), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06447457931935788), 'actor_loss': np.float64(-0.9777509987354278), 'hyper_actor_loss': np.float64(1.8296514099347405e-05), 'behavior_loss': np.float64(0.3944451779127121)}

Episode step 42970, time diff 4.717387676239014, total time dif 12155.898133516312)
step: 42970 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(1.672698), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07122081480920314), 'actor_loss': np.float64(-0.9714503586292267), 'hyper_actor_loss': np.float64(1.7361347090627532e-05), 'behavior_loss': np.float64(0.3928480327129364)}

Episode step 42980, time diff 4.6979570388793945, total time dif 12160.61552119255)
step: 42980 @ episode report: {'average_total_reward': np.float32(8.551111), 'reward_variance': np.float32(2.1932895), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05970025472342968), 'actor_loss': np.float64(-0.973497474193573), 'hyper_actor_loss': np.float64(1.486747532908339e-05), 'behavior_loss': np.float64(0.4014781594276428)}

Episode step 42990, time diff 4.665695428848267, total time dif 12165.31347823143)
step: 42990 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(2.5952857), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555552), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07707754820585251), 'actor_loss': np.float64(-0.9710537314414978), 'hyper_actor_loss': np.float64(1.3280726943776244e-05), 'behavior_loss': np.float64(0.39671907722949984)}

Episode step 43000, time diff 4.739899635314941, total time dif 12169.979173660278)
step: 43000 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(5.056823), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06251262724399567), 'actor_loss': np.float64(-0.9966948628425598), 'hyper_actor_loss': np.float64(1.3822254459228134e-05), 'behavior_loss': np.float64(0.3961569607257843)}

Episode step 43010, time diff 4.646880865097046, total time dif 12174.719073295593)
step: 43010 @ episode report: {'average_total_reward': np.float32(7.88), 'reward_variance': np.float32(1.9845635), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07136860080063342), 'actor_loss': np.float64(-0.9793221592903137), 'hyper_actor_loss': np.float64(1.4903622468409593e-05), 'behavior_loss': np.float64(0.39163473546504973)}

Episode step 43020, time diff 4.707167863845825, total time dif 12179.36595416069)
step: 43020 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(3.2856057), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07587270401418209), 'actor_loss': np.float64(-0.9886573374271392), 'hyper_actor_loss': np.float64(1.5162838280957658e-05), 'behavior_loss': np.float64(0.3936812043190002)}

Episode step 43030, time diff 4.779571533203125, total time dif 12184.073122024536)
step: 43030 @ episode report: {'average_total_reward': np.float32(9.114446), 'reward_variance': np.float32(1.4535077), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057806596532464026), 'actor_loss': np.float64(-0.9732031464576721), 'hyper_actor_loss': np.float64(2.0253371985745616e-05), 'behavior_loss': np.float64(0.39570901691913607)}

Episode step 43040, time diff 4.745606899261475, total time dif 12188.85269355774)
step: 43040 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(0.759364), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06850552819669246), 'actor_loss': np.float64(-0.9594439446926117), 'hyper_actor_loss': np.float64(2.8897501397295856e-05), 'behavior_loss': np.float64(0.3867706388235092)}

Episode step 43050, time diff 4.776515960693359, total time dif 12193.598300457)
step: 43050 @ episode report: {'average_total_reward': np.float32(9.500002), 'reward_variance': np.float32(0.9986175), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.655557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0636053118854761), 'actor_loss': np.float64(-0.9759071409702301), 'hyper_actor_loss': np.float64(5.410870871855877e-05), 'behavior_loss': np.float64(0.39075857400894165)}

Episode step 43060, time diff 5.092548131942749, total time dif 12198.374816417694)
step: 43060 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(5.430803), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07205055207014084), 'actor_loss': np.float64(-0.9763025343418121), 'hyper_actor_loss': np.float64(0.00020988472169847228), 'behavior_loss': np.float64(0.384737440943718)}

Episode step 43070, time diff 5.169771432876587, total time dif 12203.467364549637)
step: 43070 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(4.8570576), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06200936287641525), 'actor_loss': np.float64(-0.9946167707443238), 'hyper_actor_loss': np.float64(0.0006222457275725901), 'behavior_loss': np.float64(0.39110876321792604)}

Episode step 43080, time diff 5.215642690658569, total time dif 12208.637135982513)
step: 43080 @ episode report: {'average_total_reward': np.float32(7.8899994), 'reward_variance': np.float32(7.7629), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07106305807828903), 'actor_loss': np.float64(-0.9686503648757935), 'hyper_actor_loss': np.float64(0.0005366855592001229), 'behavior_loss': np.float64(0.39822075963020326)}

Episode step 43090, time diff 4.986333131790161, total time dif 12213.852778673172)
step: 43090 @ episode report: {'average_total_reward': np.float32(7.416667), 'reward_variance': np.float32(2.8274877), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06418128721415997), 'actor_loss': np.float64(-0.9730524897575379), 'hyper_actor_loss': np.float64(0.0005159462452866137), 'behavior_loss': np.float64(0.3812951534986496)}

Episode step 43100, time diff 5.1983489990234375, total time dif 12218.839111804962)
step: 43100 @ episode report: {'average_total_reward': np.float32(5.2233334), 'reward_variance': np.float32(1.6864555), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.0444446), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06980900652706623), 'actor_loss': np.float64(-0.99730304479599), 'hyper_actor_loss': np.float64(0.0005818481673486531), 'behavior_loss': np.float64(0.39198551774024964)}

Episode step 43110, time diff 4.996080636978149, total time dif 12224.037460803986)
step: 43110 @ episode report: {'average_total_reward': np.float32(5.545556), 'reward_variance': np.float32(0.76892495), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06041111648082733), 'actor_loss': np.float64(-0.9713618755340576), 'hyper_actor_loss': np.float64(0.0005039052834035828), 'behavior_loss': np.float64(0.3937175780534744)}

Episode step 43120, time diff 4.8919007778167725, total time dif 12229.033541440964)
step: 43120 @ episode report: {'average_total_reward': np.float32(5.447778), 'reward_variance': np.float32(2.41815), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08666889742016792), 'actor_loss': np.float64(-0.9729065716266632), 'hyper_actor_loss': np.float64(0.00032795891747809945), 'behavior_loss': np.float64(0.37665021121501924)}

Episode step 43130, time diff 4.806883335113525, total time dif 12233.92544221878)
step: 43130 @ episode report: {'average_total_reward': np.float32(6.257778), 'reward_variance': np.float32(2.1916497), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06626982577145099), 'actor_loss': np.float64(-0.9969541549682617), 'hyper_actor_loss': np.float64(0.0002468510763719678), 'behavior_loss': np.float64(0.38207573592662813)}

Episode step 43140, time diff 4.75900673866272, total time dif 12238.732325553894)
step: 43140 @ episode report: {'average_total_reward': np.float32(6.1822224), 'reward_variance': np.float32(1.940548), 'max_total_reward': np.float32(8.411111), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06685696691274642), 'actor_loss': np.float64(-0.9742867887020111), 'hyper_actor_loss': np.float64(0.00023666770139243453), 'behavior_loss': np.float64(0.38106611371040344)}

Episode step 43150, time diff 4.804677963256836, total time dif 12243.491332292557)
step: 43150 @ episode report: {'average_total_reward': np.float32(6.082222), 'reward_variance': np.float32(1.4670671), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07051614113152027), 'actor_loss': np.float64(-0.9691927134990692), 'hyper_actor_loss': np.float64(0.00022641469695372507), 'behavior_loss': np.float64(0.37105569541454314)}

Episode step 43160, time diff 4.893378019332886, total time dif 12248.296010255814)
step: 43160 @ episode report: {'average_total_reward': np.float32(5.508889), 'reward_variance': np.float32(1.2278221), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0760337095707655), 'actor_loss': np.float64(-0.9865651309490204), 'hyper_actor_loss': np.float64(0.00019341122097102926), 'behavior_loss': np.float64(0.3758713662624359)}

Episode step 43170, time diff 4.9358069896698, total time dif 12253.189388275146)
step: 43170 @ episode report: {'average_total_reward': np.float32(5.3455563), 'reward_variance': np.float32(1.6560361), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0724598452448845), 'actor_loss': np.float64(-0.9844765782356262), 'hyper_actor_loss': np.float64(0.00019816560088656843), 'behavior_loss': np.float64(0.37711303532123563)}

Episode step 43180, time diff 5.040651798248291, total time dif 12258.125195264816)
step: 43180 @ episode report: {'average_total_reward': np.float32(5.8211117), 'reward_variance': np.float32(2.859592), 'max_total_reward': np.float32(9.655557), 'min_total_reward': np.float32(4.2888894), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07790817432105542), 'actor_loss': np.float64(-0.9774354457855224), 'hyper_actor_loss': np.float64(0.00018939659057650715), 'behavior_loss': np.float64(0.37599192261695863)}

Episode step 43190, time diff 5.11931300163269, total time dif 12263.165847063065)
step: 43190 @ episode report: {'average_total_reward': np.float32(5.7088895), 'reward_variance': np.float32(1.8649585), 'max_total_reward': np.float32(8.655557), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06726621314883233), 'actor_loss': np.float64(-0.9841409206390381), 'hyper_actor_loss': np.float64(0.00018528204236645252), 'behavior_loss': np.float64(0.3689317166805267)}

Episode step 43200, time diff 5.022751808166504, total time dif 12268.285160064697)
step: 43200 @ episode report: {'average_total_reward': np.float32(5.957778), 'reward_variance': np.float32(2.1585388), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06500703059136867), 'actor_loss': np.float64(-0.9790634274482727), 'hyper_actor_loss': np.float64(0.00017641323065618054), 'behavior_loss': np.float64(0.37440710663795473)}

Episode step 43210, time diff 5.3302576541900635, total time dif 12273.307911872864)
step: 43210 @ episode report: {'average_total_reward': np.float32(6.008889), 'reward_variance': np.float32(2.5756), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06162934750318527), 'actor_loss': np.float64(-0.9595702111721038), 'hyper_actor_loss': np.float64(0.00012909252909594217), 'behavior_loss': np.float64(0.37260561883449556)}

Episode step 43220, time diff 5.207128047943115, total time dif 12278.638169527054)
step: 43220 @ episode report: {'average_total_reward': np.float32(5.584445), 'reward_variance': np.float32(1.7776839), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06028501279652119), 'actor_loss': np.float64(-0.9588121592998504), 'hyper_actor_loss': np.float64(0.00013616056530736388), 'behavior_loss': np.float64(0.362997031211853)}

Episode step 43230, time diff 4.969644784927368, total time dif 12283.845297574997)
step: 43230 @ episode report: {'average_total_reward': np.float32(5.9700003), 'reward_variance': np.float32(1.8976309), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06877687908709049), 'actor_loss': np.float64(-0.9762394487857818), 'hyper_actor_loss': np.float64(0.00011707960584317334), 'behavior_loss': np.float64(0.3744394063949585)}

Episode step 43240, time diff 5.087910175323486, total time dif 12288.814942359924)
step: 43240 @ episode report: {'average_total_reward': np.float32(5.6577783), 'reward_variance': np.float32(1.4156989), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06972287893295288), 'actor_loss': np.float64(-0.9775212645530701), 'hyper_actor_loss': np.float64(0.00011656252536340617), 'behavior_loss': np.float64(0.3691023975610733)}

Episode step 43250, time diff 4.96484375, total time dif 12293.902852535248)
step: 43250 @ episode report: {'average_total_reward': np.float32(6.057778), 'reward_variance': np.float32(1.1561434), 'max_total_reward': np.float32(7.6555557), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07588533945381641), 'actor_loss': np.float64(-0.9852795839309693), 'hyper_actor_loss': np.float64(0.00010511345317354426), 'behavior_loss': np.float64(0.37178158164024355)}

Episode step 43260, time diff 4.850989103317261, total time dif 12298.867696285248)
step: 43260 @ episode report: {'average_total_reward': np.float32(6.4822226), 'reward_variance': np.float32(1.9427462), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07127365544438362), 'actor_loss': np.float64(-0.9833018183708191), 'hyper_actor_loss': np.float64(8.223777913372033e-05), 'behavior_loss': np.float64(0.3665034830570221)}

Episode step 43270, time diff 4.9379966259002686, total time dif 12303.718685388565)
step: 43270 @ episode report: {'average_total_reward': np.float32(7.343334), 'reward_variance': np.float32(1.1900604), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06820789650082588), 'actor_loss': np.float64(-0.9827619731426239), 'hyper_actor_loss': np.float64(7.160959976317826e-05), 'behavior_loss': np.float64(0.37081745862960813)}

Episode step 43280, time diff 4.868886709213257, total time dif 12308.656682014465)
step: 43280 @ episode report: {'average_total_reward': np.float32(7.1555557), 'reward_variance': np.float32(1.3841727), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07209579907357692), 'actor_loss': np.float64(-0.9696074247360229), 'hyper_actor_loss': np.float64(6.420733225240838e-05), 'behavior_loss': np.float64(0.3719232648611069)}

Episode step 43290, time diff 4.87236213684082, total time dif 12313.525568723679)
step: 43290 @ episode report: {'average_total_reward': np.float32(6.918889), 'reward_variance': np.float32(3.4286926), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07742986679077149), 'actor_loss': np.float64(-0.9686528980731964), 'hyper_actor_loss': np.float64(4.8449566384078936e-05), 'behavior_loss': np.float64(0.3761640965938568)}

Episode step 43300, time diff 4.777735233306885, total time dif 12318.39793086052)
step: 43300 @ episode report: {'average_total_reward': np.float32(6.6555557), 'reward_variance': np.float32(1.665358), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07558261565864086), 'actor_loss': np.float64(-0.9852411985397339), 'hyper_actor_loss': np.float64(4.365414315543603e-05), 'behavior_loss': np.float64(0.37547884285449984)}

Episode step 43310, time diff 4.885485410690308, total time dif 12323.175666093826)
step: 43310 @ episode report: {'average_total_reward': np.float32(8.453334), 'reward_variance': np.float32(2.1638474), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06340145207941532), 'actor_loss': np.float64(-0.9807353973388672), 'hyper_actor_loss': np.float64(3.9356372144538906e-05), 'behavior_loss': np.float64(0.37035709619522095)}

Episode step 43320, time diff 4.928183555603027, total time dif 12328.061151504517)
step: 43320 @ episode report: {'average_total_reward': np.float32(8.453334), 'reward_variance': np.float32(3.489057), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06712375804781914), 'actor_loss': np.float64(-0.9644969403743744), 'hyper_actor_loss': np.float64(3.8847793621243906e-05), 'behavior_loss': np.float64(0.3642669677734375)}

Episode step 43330, time diff 4.934562921524048, total time dif 12332.98933506012)
step: 43330 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(1.0834371), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0730490494519472), 'actor_loss': np.float64(-0.979171884059906), 'hyper_actor_loss': np.float64(3.5405338348937224e-05), 'behavior_loss': np.float64(0.36904318630695343)}

Episode step 43340, time diff 4.828324556350708, total time dif 12337.923897981644)
step: 43340 @ episode report: {'average_total_reward': np.float32(10.461111), 'reward_variance': np.float32(2.8485494), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07281725965440274), 'actor_loss': np.float64(-0.9919250726699829), 'hyper_actor_loss': np.float64(3.658780660771299e-05), 'behavior_loss': np.float64(0.3694072157144547)}

Episode step 43350, time diff 4.7998435497283936, total time dif 12342.752222537994)
step: 43350 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(3.734637), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07421246208250523), 'actor_loss': np.float64(-0.9824653446674347), 'hyper_actor_loss': np.float64(3.503399129840545e-05), 'behavior_loss': np.float64(0.3788291811943054)}

Episode step 43360, time diff 4.984529495239258, total time dif 12347.552066087723)
step: 43360 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(2.0190237), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07051079496741294), 'actor_loss': np.float64(-0.9747377336025238), 'hyper_actor_loss': np.float64(3.6656723386840896e-05), 'behavior_loss': np.float64(0.380256724357605)}

Episode step 43370, time diff 4.815963268280029, total time dif 12352.536595582962)
step: 43370 @ episode report: {'average_total_reward': np.float32(10.248889), 'reward_variance': np.float32(2.5853143), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677387110888958), 'actor_loss': np.float64(-0.9673999547958374), 'hyper_actor_loss': np.float64(4.054503115185071e-05), 'behavior_loss': np.float64(0.3766018748283386)}

Episode step 43380, time diff 4.967864274978638, total time dif 12357.352558851242)
step: 43380 @ episode report: {'average_total_reward': np.float32(9.8977785), 'reward_variance': np.float32(5.294614), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07070041745901108), 'actor_loss': np.float64(-0.9729467093944549), 'hyper_actor_loss': np.float64(3.63377490430139e-05), 'behavior_loss': np.float64(0.37587539851665497)}

Episode step 43390, time diff 4.8042309284210205, total time dif 12362.32042312622)
step: 43390 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(4.0920606), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06651721615344286), 'actor_loss': np.float64(-0.9764018952846527), 'hyper_actor_loss': np.float64(3.0089622305240483e-05), 'behavior_loss': np.float64(0.3730490982532501)}

Episode step 43400, time diff 4.906651496887207, total time dif 12367.124654054642)
step: 43400 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(2.0386727), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07302539497613907), 'actor_loss': np.float64(-0.9759616732597352), 'hyper_actor_loss': np.float64(2.892173106374685e-05), 'behavior_loss': np.float64(0.37262898981571196)}

Episode step 43410, time diff 4.809150457382202, total time dif 12372.031305551529)
step: 43410 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(4.8041286), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07683130726218224), 'actor_loss': np.float64(-0.9858502805233001), 'hyper_actor_loss': np.float64(2.689084540179465e-05), 'behavior_loss': np.float64(0.37533597350120546)}

Episode step 43420, time diff 4.985928058624268, total time dif 12376.840456008911)
step: 43420 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.3299375), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07207925040274858), 'actor_loss': np.float64(-0.9864722311496734), 'hyper_actor_loss': np.float64(3.2149089383892716e-05), 'behavior_loss': np.float64(0.3703170567750931)}

Episode step 43430, time diff 4.852192163467407, total time dif 12381.826384067535)
step: 43430 @ episode report: {'average_total_reward': np.float32(7.8288894), 'reward_variance': np.float32(5.199216), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06775485537946224), 'actor_loss': np.float64(-0.9819672465324402), 'hyper_actor_loss': np.float64(4.671884307754226e-05), 'behavior_loss': np.float64(0.37505869269371034)}

Episode step 43440, time diff 4.778103351593018, total time dif 12386.678576231003)
step: 43440 @ episode report: {'average_total_reward': np.float32(8.502223), 'reward_variance': np.float32(4.408193), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057214895635843276), 'actor_loss': np.float64(-0.9639993369579315), 'hyper_actor_loss': np.float64(4.558164800982922e-05), 'behavior_loss': np.float64(0.3750428020954132)}

Episode step 43450, time diff 4.68429708480835, total time dif 12391.456679582596)
step: 43450 @ episode report: {'average_total_reward': np.float32(8.241112), 'reward_variance': np.float32(0.74466723), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06656955815851688), 'actor_loss': np.float64(-0.9571886718273163), 'hyper_actor_loss': np.float64(3.593566252675373e-05), 'behavior_loss': np.float64(0.3665515065193176)}

Episode step 43460, time diff 4.811680316925049, total time dif 12396.140976667404)
step: 43460 @ episode report: {'average_total_reward': np.float32(6.831111), 'reward_variance': np.float32(1.1582172), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.074252899736166), 'actor_loss': np.float64(-0.9788943290710449), 'hyper_actor_loss': np.float64(3.1358336673292794e-05), 'behavior_loss': np.float64(0.3816362112760544)}

Episode step 43470, time diff 4.784044981002808, total time dif 12400.95265698433)
step: 43470 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(2.5667214), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06848172694444657), 'actor_loss': np.float64(-0.9869141817092896), 'hyper_actor_loss': np.float64(2.8015643329126762e-05), 'behavior_loss': np.float64(0.37604298889636995)}

Episode step 43480, time diff 4.80464768409729, total time dif 12405.736701965332)
step: 43480 @ episode report: {'average_total_reward': np.float32(8.39), 'reward_variance': np.float32(3.0062828), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06920938342809677), 'actor_loss': np.float64(-0.9730014979839325), 'hyper_actor_loss': np.float64(3.105469313595677e-05), 'behavior_loss': np.float64(0.37849477529525755)}

Episode step 43490, time diff 4.778562784194946, total time dif 12410.54134964943)
step: 43490 @ episode report: {'average_total_reward': np.float32(8.065557), 'reward_variance': np.float32(2.7923076), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07249581813812256), 'actor_loss': np.float64(-0.9678183853626251), 'hyper_actor_loss': np.float64(2.9070369237160776e-05), 'behavior_loss': np.float64(0.37689964175224305)}

Episode step 43500, time diff 4.745177268981934, total time dif 12415.319912433624)
step: 43500 @ episode report: {'average_total_reward': np.float32(8.365556), 'reward_variance': np.float32(2.331863), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07189089395105838), 'actor_loss': np.float64(-0.9670789062976837), 'hyper_actor_loss': np.float64(2.697097443160601e-05), 'behavior_loss': np.float64(0.38302322924137117)}

Episode step 43510, time diff 4.679304361343384, total time dif 12420.065089702606)
step: 43510 @ episode report: {'average_total_reward': np.float32(8.241112), 'reward_variance': np.float32(3.8035824), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.053949987329542634), 'actor_loss': np.float64(-0.959880656003952), 'hyper_actor_loss': np.float64(2.4690834288776385e-05), 'behavior_loss': np.float64(0.373892480134964)}

Episode step 43520, time diff 4.675597667694092, total time dif 12424.74439406395)
step: 43520 @ episode report: {'average_total_reward': np.float32(7.828889), 'reward_variance': np.float32(1.3627455), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06586548388004303), 'actor_loss': np.float64(-0.9584466993808747), 'hyper_actor_loss': np.float64(2.5004592680488712e-05), 'behavior_loss': np.float64(0.3758257865905762)}

Episode step 43530, time diff 4.72816014289856, total time dif 12429.419991731644)
step: 43530 @ episode report: {'average_total_reward': np.float32(9.363334), 'reward_variance': np.float32(3.3976066), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.411111), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06580595523118973), 'actor_loss': np.float64(-0.9741957545280456), 'hyper_actor_loss': np.float64(2.4744785514485555e-05), 'behavior_loss': np.float64(0.3807204574346542)}

Episode step 43540, time diff 4.96928334236145, total time dif 12434.148151874542)
step: 43540 @ episode report: {'average_total_reward': np.float32(8.153334), 'reward_variance': np.float32(3.3423169), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07566408663988114), 'actor_loss': np.float64(-0.9681307911872864), 'hyper_actor_loss': np.float64(2.4674244923517107e-05), 'behavior_loss': np.float64(0.3780960917472839)}

Episode step 43550, time diff 4.69824481010437, total time dif 12439.117435216904)
step: 43550 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(0.7955321), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06360767558217048), 'actor_loss': np.float64(-0.9795883178710938), 'hyper_actor_loss': np.float64(2.572379762568744e-05), 'behavior_loss': np.float64(0.37369811832904815)}

Episode step 43560, time diff 4.751042604446411, total time dif 12443.815680027008)
step: 43560 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(1.0913142), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07150299996137618), 'actor_loss': np.float64(-0.9788387477397918), 'hyper_actor_loss': np.float64(2.4351856154680718e-05), 'behavior_loss': np.float64(0.3754457265138626)}

Episode step 43570, time diff 4.644367456436157, total time dif 12448.566722631454)
step: 43570 @ episode report: {'average_total_reward': np.float32(9.336668), 'reward_variance': np.float32(3.3645203), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07533718310296536), 'actor_loss': np.float64(-0.9730814456939697), 'hyper_actor_loss': np.float64(2.334078963031061e-05), 'behavior_loss': np.float64(0.37662437558174133)}

Episode step 43580, time diff 4.695936679840088, total time dif 12453.21109008789)
step: 43580 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(3.1533136), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0654731934890151), 'actor_loss': np.float64(-0.9771670877933503), 'hyper_actor_loss': np.float64(2.2864167658553924e-05), 'behavior_loss': np.float64(0.37791615426540376)}

Episode step 43590, time diff 4.625572204589844, total time dif 12457.90702676773)
step: 43590 @ episode report: {'average_total_reward': np.float32(8.863334), 'reward_variance': np.float32(3.2670388), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05266193151473999), 'actor_loss': np.float64(-0.9586299121379852), 'hyper_actor_loss': np.float64(2.3349349248746876e-05), 'behavior_loss': np.float64(0.3725858718156815)}

Episode step 43600, time diff 4.704589605331421, total time dif 12462.53259897232)
step: 43600 @ episode report: {'average_total_reward': np.float32(9.138889), 'reward_variance': np.float32(1.3277099), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06886095590889454), 'actor_loss': np.float64(-0.9562982141971588), 'hyper_actor_loss': np.float64(2.0793000112462322e-05), 'behavior_loss': np.float64(0.3821374446153641)}

Episode step 43610, time diff 4.788038969039917, total time dif 12467.237188577652)
step: 43610 @ episode report: {'average_total_reward': np.float32(9.014445), 'reward_variance': np.float32(1.8198045), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07829637303948403), 'actor_loss': np.float64(-0.9681310176849365), 'hyper_actor_loss': np.float64(2.286686867591925e-05), 'behavior_loss': np.float64(0.3871345043182373)}

Episode step 43620, time diff 4.615128755569458, total time dif 12472.025227546692)
step: 43620 @ episode report: {'average_total_reward': np.float32(10.122223), 'reward_variance': np.float32(2.4117532), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08316714465618133), 'actor_loss': np.float64(-0.9902911424636841), 'hyper_actor_loss': np.float64(2.3258838155015837e-05), 'behavior_loss': np.float64(0.3776958674192429)}

Episode step 43630, time diff 4.807124137878418, total time dif 12476.640356302261)
step: 43630 @ episode report: {'average_total_reward': np.float32(10.136667), 'reward_variance': np.float32(0.70410013), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07297816425561905), 'actor_loss': np.float64(-0.9965988218784332), 'hyper_actor_loss': np.float64(2.2213040028873366e-05), 'behavior_loss': np.float64(0.37942610681056976)}

Episode step 43640, time diff 4.812478542327881, total time dif 12481.44748044014)
step: 43640 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(2.7248898), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07222757488489151), 'actor_loss': np.float64(-0.9706112921237946), 'hyper_actor_loss': np.float64(2.4115967607940548e-05), 'behavior_loss': np.float64(0.380337318778038)}

Episode step 43650, time diff 4.70669150352478, total time dif 12486.259958982468)
step: 43650 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(4.0235515), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07453243471682072), 'actor_loss': np.float64(-0.9662875354290008), 'hyper_actor_loss': np.float64(2.248381406388944e-05), 'behavior_loss': np.float64(0.3892298132181168)}

Episode step 43660, time diff 4.77890682220459, total time dif 12490.966650485992)
step: 43660 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(7.5877404), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06612450759857894), 'actor_loss': np.float64(-0.9692321121692657), 'hyper_actor_loss': np.float64(2.1124316481291318e-05), 'behavior_loss': np.float64(0.3779283374547958)}

Episode step 43670, time diff 4.799603700637817, total time dif 12495.745557308197)
step: 43670 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(2.9258466), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06982896290719509), 'actor_loss': np.float64(-0.9797328174114227), 'hyper_actor_loss': np.float64(2.298199342476437e-05), 'behavior_loss': np.float64(0.3830374598503113)}

Episode step 43680, time diff 4.795159339904785, total time dif 12500.545161008835)
step: 43680 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(7.6526923), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08205167092382908), 'actor_loss': np.float64(-0.9923005044460297), 'hyper_actor_loss': np.float64(2.19585832383018e-05), 'behavior_loss': np.float64(0.3772241622209549)}

Episode step 43690, time diff 4.752222537994385, total time dif 12505.34032034874)
step: 43690 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(3.234209), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07599994577467442), 'actor_loss': np.float64(-0.9824318051338196), 'hyper_actor_loss': np.float64(2.3710982895863707e-05), 'behavior_loss': np.float64(0.386934706568718)}

Episode step 43700, time diff 4.874002695083618, total time dif 12510.092542886734)
step: 43700 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(3.626529), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07055004090070724), 'actor_loss': np.float64(-0.9686963140964509), 'hyper_actor_loss': np.float64(2.25845564273186e-05), 'behavior_loss': np.float64(0.3751653373241425)}

Episode step 43710, time diff 4.743570566177368, total time dif 12514.966545581818)
step: 43710 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(3.2893784), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08155155554413795), 'actor_loss': np.float64(-0.9883173286914826), 'hyper_actor_loss': np.float64(2.073536434181733e-05), 'behavior_loss': np.float64(0.38359355032444)}

Episode step 43720, time diff 4.823538064956665, total time dif 12519.710116147995)
step: 43720 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(1.6909494), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07635026201605796), 'actor_loss': np.float64(-0.9969212114810944), 'hyper_actor_loss': np.float64(2.1220241251285188e-05), 'behavior_loss': np.float64(0.38707554042339326)}

Episode step 43730, time diff 4.887600898742676, total time dif 12524.533654212952)
step: 43730 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(3.531847), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06765645444393158), 'actor_loss': np.float64(-0.9650120258331298), 'hyper_actor_loss': np.float64(2.0316238078521565e-05), 'behavior_loss': np.float64(0.3831609606742859)}

Episode step 43740, time diff 4.6769609451293945, total time dif 12529.421255111694)
step: 43740 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(1.5826619), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08116715475916862), 'actor_loss': np.float64(-0.9713184475898743), 'hyper_actor_loss': np.float64(2.0466474234126507e-05), 'behavior_loss': np.float64(0.3885218858718872)}

Episode step 43750, time diff 4.5680036544799805, total time dif 12534.098216056824)
step: 43750 @ episode report: {'average_total_reward': np.float32(9.563334), 'reward_variance': np.float32(3.5812354), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07535747587680816), 'actor_loss': np.float64(-0.9901983857154846), 'hyper_actor_loss': np.float64(2.363696421525674e-05), 'behavior_loss': np.float64(0.3826343774795532)}

Episode step 43760, time diff 4.849773168563843, total time dif 12538.666219711304)
step: 43760 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(4.979289), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0712475311011076), 'actor_loss': np.float64(-0.9869452476501465), 'hyper_actor_loss': np.float64(3.083845385845052e-05), 'behavior_loss': np.float64(0.38608334958553314)}

Episode step 43770, time diff 4.8640501499176025, total time dif 12543.515992879868)
step: 43770 @ episode report: {'average_total_reward': np.float32(8.63889), 'reward_variance': np.float32(1.447562), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06442826427519321), 'actor_loss': np.float64(-0.973461526632309), 'hyper_actor_loss': np.float64(2.7966717061644887e-05), 'behavior_loss': np.float64(0.39125471711158755)}

Episode step 43780, time diff 4.729143142700195, total time dif 12548.380043029785)
step: 43780 @ episode report: {'average_total_reward': np.float32(9.351112), 'reward_variance': np.float32(1.2915108), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07804951220750808), 'actor_loss': np.float64(-0.9792022049427033), 'hyper_actor_loss': np.float64(2.364797328482382e-05), 'behavior_loss': np.float64(0.38128257989883424)}

Episode step 43790, time diff 4.6414220333099365, total time dif 12553.109186172485)
step: 43790 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(0.7388905), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07387911789119243), 'actor_loss': np.float64(-0.9863017618656158), 'hyper_actor_loss': np.float64(1.8927114433608948e-05), 'behavior_loss': np.float64(0.39413788020610807)}

Episode step 43800, time diff 4.528500556945801, total time dif 12557.750608205795)
step: 43800 @ episode report: {'average_total_reward': np.float32(9.126667), 'reward_variance': np.float32(3.1608691), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05252405013889074), 'actor_loss': np.float64(-0.9625818073749542), 'hyper_actor_loss': np.float64(1.8204436855739914e-05), 'behavior_loss': np.float64(0.3967017471790314)}

Episode step 43810, time diff 4.5480732917785645, total time dif 12562.279108762741)
step: 43810 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(1.5980543), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07257243003696204), 'actor_loss': np.float64(-0.9337070047855377), 'hyper_actor_loss': np.float64(1.7049767848220654e-05), 'behavior_loss': np.float64(0.3890226572751999)}

Episode step 43820, time diff 4.62254786491394, total time dif 12566.82718205452)
step: 43820 @ episode report: {'average_total_reward': np.float32(9.000001), 'reward_variance': np.float32(1.7463953), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06645279843360186), 'actor_loss': np.float64(-0.9759306728839874), 'hyper_actor_loss': np.float64(1.6323076670232696e-05), 'behavior_loss': np.float64(0.39663325250148773)}

Episode step 43830, time diff 4.663238286972046, total time dif 12571.449729919434)
step: 43830 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(1.7307947), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06898963712155819), 'actor_loss': np.float64(-0.9775652050971985), 'hyper_actor_loss': np.float64(1.5177734530880116e-05), 'behavior_loss': np.float64(0.3985722571611404)}

Episode step 43840, time diff 4.703352928161621, total time dif 12576.112968206406)
step: 43840 @ episode report: {'average_total_reward': np.float32(9.002223), 'reward_variance': np.float32(3.375131), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057775619253516194), 'actor_loss': np.float64(-0.9472098588943482), 'hyper_actor_loss': np.float64(1.4747115437785397e-05), 'behavior_loss': np.float64(0.39012896716594697)}

Episode step 43850, time diff 4.697801351547241, total time dif 12580.816321134567)
step: 43850 @ episode report: {'average_total_reward': np.float32(8.775556), 'reward_variance': np.float32(2.072094), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06253599561750889), 'actor_loss': np.float64(-0.9577779114246369), 'hyper_actor_loss': np.float64(1.5209520188363968e-05), 'behavior_loss': np.float64(0.39448714852333067)}

Episode step 43860, time diff 4.694720983505249, total time dif 12585.514122486115)
step: 43860 @ episode report: {'average_total_reward': np.float32(8.675556), 'reward_variance': np.float32(5.3352785), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06717425771057606), 'actor_loss': np.float64(-0.9606477200984955), 'hyper_actor_loss': np.float64(1.5216978954413208e-05), 'behavior_loss': np.float64(0.39384998083114625)}

Episode step 43870, time diff 4.888904571533203, total time dif 12590.20884346962)
step: 43870 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.6150919), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07936595268547535), 'actor_loss': np.float64(-0.9768228650093078), 'hyper_actor_loss': np.float64(1.6172898631339196e-05), 'behavior_loss': np.float64(0.3883372932672501)}

Episode step 43880, time diff 4.579297065734863, total time dif 12595.097748041153)
step: 43880 @ episode report: {'average_total_reward': np.float32(9.03889), 'reward_variance': np.float32(2.6091669), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06787969656288624), 'actor_loss': np.float64(-0.9913378953933716), 'hyper_actor_loss': np.float64(1.5343833274528153e-05), 'behavior_loss': np.float64(0.39614686369895935)}

Episode step 43890, time diff 4.434080600738525, total time dif 12599.677045106888)
step: 43890 @ episode report: {'average_total_reward': np.float32(9.512223), 'reward_variance': np.float32(1.8476664), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0671766247600317), 'actor_loss': np.float64(-0.9609501004219055), 'hyper_actor_loss': np.float64(1.6258031700999707e-05), 'behavior_loss': np.float64(0.3929667741060257)}

Episode step 43900, time diff 4.411328554153442, total time dif 12604.111125707626)
step: 43900 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(2.529328), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057087738625705244), 'actor_loss': np.float64(-0.9606012523174285), 'hyper_actor_loss': np.float64(1.7175937318825164e-05), 'behavior_loss': np.float64(0.39573894441127777)}

Episode step 43910, time diff 4.434274435043335, total time dif 12608.52245426178)
step: 43910 @ episode report: {'average_total_reward': np.float32(8.790001), 'reward_variance': np.float32(2.021789), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07451480962336063), 'actor_loss': np.float64(-0.9633220672607422), 'hyper_actor_loss': np.float64(1.6872048036020715e-05), 'behavior_loss': np.float64(0.3934760093688965)}

Episode step 43920, time diff 4.354139566421509, total time dif 12612.956728696823)
step: 43920 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(1.024174), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06745892129838467), 'actor_loss': np.float64(-0.9856332540512085), 'hyper_actor_loss': np.float64(1.4929426743037767e-05), 'behavior_loss': np.float64(0.3886664867401123)}

Episode step 43930, time diff 4.404993534088135, total time dif 12617.310868263245)
step: 43930 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(2.7119012), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.055269287526607515), 'actor_loss': np.float64(-0.9768733978271484), 'hyper_actor_loss': np.float64(1.6920399230002658e-05), 'behavior_loss': np.float64(0.3938284456729889)}

Episode step 43940, time diff 4.396989583969116, total time dif 12621.715861797333)
step: 43940 @ episode report: {'average_total_reward': np.float32(9.663334), 'reward_variance': np.float32(2.0971127), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08074554577469825), 'actor_loss': np.float64(-0.9526436805725098), 'hyper_actor_loss': np.float64(1.5952447301970097e-05), 'behavior_loss': np.float64(0.4057642251253128)}

Episode step 43950, time diff 4.465185642242432, total time dif 12626.112851381302)
step: 43950 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(1.8944008), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06399360150098801), 'actor_loss': np.float64(-0.9792858123779297), 'hyper_actor_loss': np.float64(1.2800900822185213e-05), 'behavior_loss': np.float64(0.39113342463970185)}

Episode step 43960, time diff 4.431452512741089, total time dif 12630.578037023544)
step: 43960 @ episode report: {'average_total_reward': np.float32(9.236667), 'reward_variance': np.float32(5.871581), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06453296840190888), 'actor_loss': np.float64(-0.973133772611618), 'hyper_actor_loss': np.float64(1.4454823576670605e-05), 'behavior_loss': np.float64(0.39412965178489684)}

Episode step 43970, time diff 4.427999019622803, total time dif 12635.009489536285)
step: 43970 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(1.8650885), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07219622656702995), 'actor_loss': np.float64(-0.9618242383003235), 'hyper_actor_loss': np.float64(1.410178601872758e-05), 'behavior_loss': np.float64(0.3935665965080261)}

Episode step 43980, time diff 4.408976078033447, total time dif 12639.437488555908)
step: 43980 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(2.3741436), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06322066001594066), 'actor_loss': np.float64(-0.9762679278850556), 'hyper_actor_loss': np.float64(1.4585632197849917e-05), 'behavior_loss': np.float64(0.3945954144001007)}

Episode step 43990, time diff 4.413058280944824, total time dif 12643.846464633942)
step: 43990 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(2.1397223), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06569343525916338), 'actor_loss': np.float64(-0.9671192288398742), 'hyper_actor_loss': np.float64(1.4393468245543772e-05), 'behavior_loss': np.float64(0.4019968301057816)}

Episode step 44000, time diff 4.415318489074707, total time dif 12648.259522914886)
step: 44000 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(3.0465722), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06853139251470566), 'actor_loss': np.float64(-0.966411554813385), 'hyper_actor_loss': np.float64(1.3552748896472622e-05), 'behavior_loss': np.float64(0.39959522485733034)}

Episode step 44010, time diff 4.433703422546387, total time dif 12652.674841403961)
step: 44010 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(2.0888402), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08581323064863682), 'actor_loss': np.float64(-0.9792231857776642), 'hyper_actor_loss': np.float64(1.3562695767177501e-05), 'behavior_loss': np.float64(0.40413651168346404)}

Episode step 44020, time diff 4.4248366355896, total time dif 12657.108544826508)
step: 44020 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(2.3556802), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0721585888415575), 'actor_loss': np.float64(-0.9944119453430176), 'hyper_actor_loss': np.float64(1.0954651861538877e-05), 'behavior_loss': np.float64(0.40559587478637693)}

Episode step 44030, time diff 4.582926273345947, total time dif 12661.533381462097)
step: 44030 @ episode report: {'average_total_reward': np.float32(8.690001), 'reward_variance': np.float32(1.9451965), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06040001250803471), 'actor_loss': np.float64(-0.9677794694900512), 'hyper_actor_loss': np.float64(1.203843407893146e-05), 'behavior_loss': np.float64(0.3940433472394943)}

Episode step 44040, time diff 4.58815598487854, total time dif 12666.116307735443)
step: 44040 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(1.5970578), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07109695561230182), 'actor_loss': np.float64(-0.9568704247474671), 'hyper_actor_loss': np.float64(1.2337298358033877e-05), 'behavior_loss': np.float64(0.40476112961769106)}

Episode step 44050, time diff 4.6290061473846436, total time dif 12670.704463720322)
step: 44050 @ episode report: {'average_total_reward': np.float32(9.138888), 'reward_variance': np.float32(2.6075613), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07130599655210972), 'actor_loss': np.float64(-0.9742220520973206), 'hyper_actor_loss': np.float64(1.3115080764691812e-05), 'behavior_loss': np.float64(0.3965536177158356)}

Episode step 44060, time diff 4.6429831981658936, total time dif 12675.333469867706)
step: 44060 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(0.7282025), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06519217789173126), 'actor_loss': np.float64(-0.9749352037906647), 'hyper_actor_loss': np.float64(1.3208622931415448e-05), 'behavior_loss': np.float64(0.39186148941516874)}

Episode step 44070, time diff 4.620357513427734, total time dif 12679.976453065872)
step: 44070 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(3.5018322), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.059640977159142496), 'actor_loss': np.float64(-0.9651820182800293), 'hyper_actor_loss': np.float64(1.4756768177903722e-05), 'behavior_loss': np.float64(0.40059167742729185)}

Episode step 44080, time diff 4.724456787109375, total time dif 12684.5968105793)
step: 44080 @ episode report: {'average_total_reward': np.float32(9.724444), 'reward_variance': np.float32(0.95821744), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08555517382919789), 'actor_loss': np.float64(-0.9716323018074036), 'hyper_actor_loss': np.float64(1.462429227103712e-05), 'behavior_loss': np.float64(0.4009870320558548)}

Episode step 44090, time diff 4.700697660446167, total time dif 12689.32126736641)
step: 44090 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(2.1262527), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06757329925894737), 'actor_loss': np.float64(-0.9942063808441162), 'hyper_actor_loss': np.float64(1.5115594851522474e-05), 'behavior_loss': np.float64(0.4012029588222504)}

Episode step 44100, time diff 4.476387977600098, total time dif 12694.021965026855)
step: 44100 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(2.1403072), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061234169825911525), 'actor_loss': np.float64(-0.9716150045394898), 'hyper_actor_loss': np.float64(1.878862303783535e-05), 'behavior_loss': np.float64(0.3995297491550446)}

Episode step 44110, time diff 4.604095697402954, total time dif 12698.498353004456)
step: 44110 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(2.5429153), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07412784434854984), 'actor_loss': np.float64(-0.9712043404579163), 'hyper_actor_loss': np.float64(1.7556108832650353e-05), 'behavior_loss': np.float64(0.3968645393848419)}

Episode step 44120, time diff 4.652125358581543, total time dif 12703.102448701859)
step: 44120 @ episode report: {'average_total_reward': np.float32(9.624446), 'reward_variance': np.float32(2.0450566), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06683264262974262), 'actor_loss': np.float64(-0.9816189646720886), 'hyper_actor_loss': np.float64(1.69406534951122e-05), 'behavior_loss': np.float64(0.4041918843984604)}

Episode step 44130, time diff 4.683925628662109, total time dif 12707.75457406044)
step: 44130 @ episode report: {'average_total_reward': np.float32(9.687778), 'reward_variance': np.float32(4.1894693), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07506154514849186), 'actor_loss': np.float64(-0.981887698173523), 'hyper_actor_loss': np.float64(1.4196028223523171e-05), 'behavior_loss': np.float64(0.39818525314331055)}

Episode step 44140, time diff 4.61744236946106, total time dif 12712.438499689102)
step: 44140 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(1.4097545), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07514073774218559), 'actor_loss': np.float64(-0.9874419152736664), 'hyper_actor_loss': np.float64(1.1468030970718246e-05), 'behavior_loss': np.float64(0.4000440984964371)}

Episode step 44150, time diff 4.601613759994507, total time dif 12717.055942058563)
step: 44150 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(3.9195132), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(4.533333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0652821058407426), 'actor_loss': np.float64(-0.972519600391388), 'hyper_actor_loss': np.float64(1.1007211742253276e-05), 'behavior_loss': np.float64(0.4028834402561188)}

Episode step 44160, time diff 4.657710552215576, total time dif 12721.657555818558)
step: 44160 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(2.5318227), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07346901632845401), 'actor_loss': np.float64(-0.9728857278823853), 'hyper_actor_loss': np.float64(9.749920582180494e-06), 'behavior_loss': np.float64(0.402983096241951)}

Episode step 44170, time diff 4.5652148723602295, total time dif 12726.315266370773)
step: 44170 @ episode report: {'average_total_reward': np.float32(8.863334), 'reward_variance': np.float32(1.8839773), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07145782746374607), 'actor_loss': np.float64(-0.9740191876888276), 'hyper_actor_loss': np.float64(9.107650748774177e-06), 'behavior_loss': np.float64(0.40855957865715026)}

Episode step 44180, time diff 4.559168815612793, total time dif 12730.880481243134)
step: 44180 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(0.74486554), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07369609959423543), 'actor_loss': np.float64(-0.9686870574951172), 'hyper_actor_loss': np.float64(9.104280343308346e-06), 'behavior_loss': np.float64(0.4117415636777878)}

Episode step 44190, time diff 4.591606378555298, total time dif 12735.439650058746)
step: 44190 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(3.5024247), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06644593849778176), 'actor_loss': np.float64(-0.9797284185886384), 'hyper_actor_loss': np.float64(8.145733409037348e-06), 'behavior_loss': np.float64(0.4123855113983154)}

Episode step 44200, time diff 4.390145540237427, total time dif 12740.031256437302)
step: 44200 @ episode report: {'average_total_reward': np.float32(8.326668), 'reward_variance': np.float32(1.9588693), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06918799318373203), 'actor_loss': np.float64(-0.9673278748989105), 'hyper_actor_loss': np.float64(7.834807183826342e-06), 'behavior_loss': np.float64(0.41094318926334383)}

Episode step 44210, time diff 4.371302366256714, total time dif 12744.421401977539)
step: 44210 @ episode report: {'average_total_reward': np.float32(9.324445), 'reward_variance': np.float32(1.3066614), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07311786040663719), 'actor_loss': np.float64(-0.9745389938354492), 'hyper_actor_loss': np.float64(7.875900246290258e-06), 'behavior_loss': np.float64(0.40902761220932005)}

Episode step 44220, time diff 4.409581422805786, total time dif 12748.792704343796)
step: 44220 @ episode report: {'average_total_reward': np.float32(9.412222), 'reward_variance': np.float32(0.7154429), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06177912130951881), 'actor_loss': np.float64(-0.9663717925548554), 'hyper_actor_loss': np.float64(7.989511141204276e-06), 'behavior_loss': np.float64(0.4062517911195755)}

Episode step 44230, time diff 4.419214248657227, total time dif 12753.202285766602)
step: 44230 @ episode report: {'average_total_reward': np.float32(9.4366665), 'reward_variance': np.float32(2.1656556), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08053258582949638), 'actor_loss': np.float64(-0.9671855270862579), 'hyper_actor_loss': np.float64(7.562773225799901e-06), 'behavior_loss': np.float64(0.4113429248332977)}

Episode step 44240, time diff 4.472402095794678, total time dif 12757.621500015259)
step: 44240 @ episode report: {'average_total_reward': np.float32(9.9388895), 'reward_variance': np.float32(0.7638088), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06371848769485951), 'actor_loss': np.float64(-0.9755992293357849), 'hyper_actor_loss': np.float64(7.320099257412949e-06), 'behavior_loss': np.float64(0.4144886612892151)}

Episode step 44250, time diff 4.554129600524902, total time dif 12762.093902111053)
step: 44250 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(5.614816), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07752086147665978), 'actor_loss': np.float64(-0.9691151738166809), 'hyper_actor_loss': np.float64(8.050733458730975e-06), 'behavior_loss': np.float64(0.4051781058311462)}

Episode step 44260, time diff 4.368199110031128, total time dif 12766.648031711578)
step: 44260 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(0.3886469), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06643906347453594), 'actor_loss': np.float64(-0.9828118801116943), 'hyper_actor_loss': np.float64(7.40488940209616e-06), 'behavior_loss': np.float64(0.4067291855812073)}

Episode step 44270, time diff 4.412933826446533, total time dif 12771.01623082161)
step: 44270 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(4.669451), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07118562832474709), 'actor_loss': np.float64(-0.9695616364479065), 'hyper_actor_loss': np.float64(7.088529218890471e-06), 'behavior_loss': np.float64(0.4122373402118683)}

Episode step 44280, time diff 4.5732643604278564, total time dif 12775.429164648056)
step: 44280 @ episode report: {'average_total_reward': np.float32(10.322223), 'reward_variance': np.float32(3.774667), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06962327137589455), 'actor_loss': np.float64(-0.9717502415180206), 'hyper_actor_loss': np.float64(6.129744861027575e-06), 'behavior_loss': np.float64(0.40950285494327543)}

Episode step 44290, time diff 4.624778509140015, total time dif 12780.002429008484)
step: 44290 @ episode report: {'average_total_reward': np.float32(10.036667), 'reward_variance': np.float32(1.9950632), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.533335), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05822175350040197), 'actor_loss': np.float64(-0.9752264559268952), 'hyper_actor_loss': np.float64(7.125783531591878e-06), 'behavior_loss': np.float64(0.4079368233680725)}

Episode step 44300, time diff 4.585636615753174, total time dif 12784.627207517624)
step: 44300 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(1.6940272), 'max_total_reward': np.float32(11.144446), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06052249427884817), 'actor_loss': np.float64(-0.9554484248161316), 'hyper_actor_loss': np.float64(7.316282471947489e-06), 'behavior_loss': np.float64(0.4076253354549408)}

Episode step 44310, time diff 4.615940809249878, total time dif 12789.212844133377)
step: 44310 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(6.3927703), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058639897219836715), 'actor_loss': np.float64(-0.9489406466484069), 'hyper_actor_loss': np.float64(8.51949362186133e-06), 'behavior_loss': np.float64(0.40953927040100097)}

Episode step 44320, time diff 4.6679160594940186, total time dif 12793.828784942627)
step: 44320 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(5.2142344), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07130571566522122), 'actor_loss': np.float64(-0.9587707340717315), 'hyper_actor_loss': np.float64(9.256384964828613e-06), 'behavior_loss': np.float64(0.41101132035255433)}

Episode step 44330, time diff 4.631973743438721, total time dif 12798.496701002121)
step: 44330 @ episode report: {'average_total_reward': np.float32(8.502223), 'reward_variance': np.float32(1.713872), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07343413047492504), 'actor_loss': np.float64(-0.9791614949703217), 'hyper_actor_loss': np.float64(1.2057590265612816e-05), 'behavior_loss': np.float64(0.40774058997631074)}

Episode step 44340, time diff 4.421491622924805, total time dif 12803.12867474556)
step: 44340 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(1.8965256), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07017056085169315), 'actor_loss': np.float64(-0.9773277342319489), 'hyper_actor_loss': np.float64(1.3726851193496259e-05), 'behavior_loss': np.float64(0.4068419873714447)}

Episode step 44350, time diff 4.543784141540527, total time dif 12807.550166368484)
step: 44350 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(2.4555368), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06645013839006424), 'actor_loss': np.float64(-0.9646808922290802), 'hyper_actor_loss': np.float64(1.5290356805053308e-05), 'behavior_loss': np.float64(0.4006004989147186)}

Episode step 44360, time diff 4.733700275421143, total time dif 12812.093950510025)
step: 44360 @ episode report: {'average_total_reward': np.float32(8.614446), 'reward_variance': np.float32(1.5099767), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0611786425113678), 'actor_loss': np.float64(-0.9754304170608521), 'hyper_actor_loss': np.float64(1.2515766411524964e-05), 'behavior_loss': np.float64(0.4063931882381439)}

Episode step 44370, time diff 4.644923686981201, total time dif 12816.827650785446)
step: 44370 @ episode report: {'average_total_reward': np.float32(8.514445), 'reward_variance': np.float32(1.7738289), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0703360192477703), 'actor_loss': np.float64(-0.9767736911773681), 'hyper_actor_loss': np.float64(1.0047773093901924e-05), 'behavior_loss': np.float64(0.40590073466300963)}

Episode step 44380, time diff 4.6500561237335205, total time dif 12821.472574472427)
step: 44380 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(2.5638633), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07440678887069226), 'actor_loss': np.float64(-0.9624740481376648), 'hyper_actor_loss': np.float64(7.935263920444412e-06), 'behavior_loss': np.float64(0.4122709631919861)}

Episode step 44390, time diff 4.56784725189209, total time dif 12826.12263059616)
step: 44390 @ episode report: {'average_total_reward': np.float32(9.326666), 'reward_variance': np.float32(2.1863005), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07720199339091778), 'actor_loss': np.float64(-0.9819811165332795), 'hyper_actor_loss': np.float64(7.845098571124253e-06), 'behavior_loss': np.float64(0.4040810942649841)}

Episode step 44400, time diff 4.555860757827759, total time dif 12830.690477848053)
step: 44400 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(1.1398767), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07528799958527088), 'actor_loss': np.float64(-0.992248147726059), 'hyper_actor_loss': np.float64(7.008513921391568e-06), 'behavior_loss': np.float64(0.40754406452178954)}

Episode step 44410, time diff 4.570193767547607, total time dif 12835.24633860588)
step: 44410 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(2.5432153), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07381038889288902), 'actor_loss': np.float64(-0.9829371035099029), 'hyper_actor_loss': np.float64(6.963550322325318e-06), 'behavior_loss': np.float64(0.4010010212659836)}

Episode step 44420, time diff 4.513398885726929, total time dif 12839.816532373428)
step: 44420 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(3.3203762), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08620089292526245), 'actor_loss': np.float64(-0.9679385542869567), 'hyper_actor_loss': np.float64(7.429103152389871e-06), 'behavior_loss': np.float64(0.4137492895126343)}

Episode step 44430, time diff 4.465005397796631, total time dif 12844.329931259155)
step: 44430 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(3.0660355), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07305515296757221), 'actor_loss': np.float64(-0.9838040173053741), 'hyper_actor_loss': np.float64(7.02140146131569e-06), 'behavior_loss': np.float64(0.4070968598127365)}

Episode step 44440, time diff 4.6591362953186035, total time dif 12848.794936656952)
step: 44440 @ episode report: {'average_total_reward': np.float32(9.700002), 'reward_variance': np.float32(3.5640488), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06541469153016806), 'actor_loss': np.float64(-0.9698371887207031), 'hyper_actor_loss': np.float64(7.086164714564802e-06), 'behavior_loss': np.float64(0.4058717876672745)}

Episode step 44450, time diff 4.980837821960449, total time dif 12853.45407295227)
step: 44450 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(2.0663905), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0590503104031086), 'actor_loss': np.float64(-0.9610118508338928), 'hyper_actor_loss': np.float64(6.894132138768328e-06), 'behavior_loss': np.float64(0.39973506033420564)}

Episode step 44460, time diff 4.635319232940674, total time dif 12858.434910774231)
step: 44460 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(0.4648644), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06333230398595333), 'actor_loss': np.float64(-0.9764852046966552), 'hyper_actor_loss': np.float64(6.758287690900034e-06), 'behavior_loss': np.float64(0.41027402579784394)}

Episode step 44470, time diff 4.43855881690979, total time dif 12863.070230007172)
step: 44470 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(3.0106533), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06249670758843422), 'actor_loss': np.float64(-0.9760347068309784), 'hyper_actor_loss': np.float64(7.189435382315423e-06), 'behavior_loss': np.float64(0.40055533349514005)}

Episode step 44480, time diff 4.516505002975464, total time dif 12867.508788824081)
step: 44480 @ episode report: {'average_total_reward': np.float32(8.763334), 'reward_variance': np.float32(0.8140007), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06929057911038398), 'actor_loss': np.float64(-0.9646970212459565), 'hyper_actor_loss': np.float64(7.694417809034348e-06), 'behavior_loss': np.float64(0.40956553220748904)}

Episode step 44490, time diff 4.395294904708862, total time dif 12872.025293827057)
step: 44490 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(3.4461796), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677475668489933), 'actor_loss': np.float64(-0.9689002633094788), 'hyper_actor_loss': np.float64(8.771899138082517e-06), 'behavior_loss': np.float64(0.40789770781993867)}

Episode step 44500, time diff 4.522515058517456, total time dif 12876.420588731766)
step: 44500 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(2.089532), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061012473329901694), 'actor_loss': np.float64(-0.9725392997264862), 'hyper_actor_loss': np.float64(9.044025409821188e-06), 'behavior_loss': np.float64(0.4088281333446503)}

Episode step 44510, time diff 4.590054273605347, total time dif 12880.943103790283)
step: 44510 @ episode report: {'average_total_reward': np.float32(9.626668), 'reward_variance': np.float32(2.0759299), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06751719936728477), 'actor_loss': np.float64(-0.9706058919429779), 'hyper_actor_loss': np.float64(9.789723571884679e-06), 'behavior_loss': np.float64(0.4185658872127533)}

Episode step 44520, time diff 4.6766197681427, total time dif 12885.533158063889)
step: 44520 @ episode report: {'average_total_reward': np.float32(9.763334), 'reward_variance': np.float32(1.8519266), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07301241233944893), 'actor_loss': np.float64(-0.9821103692054749), 'hyper_actor_loss': np.float64(1.103575623346842e-05), 'behavior_loss': np.float64(0.404313263297081)}

Episode step 44530, time diff 4.6669557094573975, total time dif 12890.209777832031)
step: 44530 @ episode report: {'average_total_reward': np.float32(8.951111), 'reward_variance': np.float32(1.7901781), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06107209715992212), 'actor_loss': np.float64(-0.9725203096866608), 'hyper_actor_loss': np.float64(1.235555182574899e-05), 'behavior_loss': np.float64(0.41908579468727114)}

Episode step 44540, time diff 4.796366453170776, total time dif 12894.876733541489)
step: 44540 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(2.6506689), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057034990005195144), 'actor_loss': np.float64(-0.9435394406318665), 'hyper_actor_loss': np.float64(1.1288723362667951e-05), 'behavior_loss': np.float64(0.41032628118991854)}

Episode step 44550, time diff 4.681830167770386, total time dif 12899.67309999466)
step: 44550 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(1.4810481), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06043962687253952), 'actor_loss': np.float64(-0.9583243608474732), 'hyper_actor_loss': np.float64(1.1826668560388498e-05), 'behavior_loss': np.float64(0.41148305833339693)}

Episode step 44560, time diff 4.488754510879517, total time dif 12904.35493016243)
step: 44560 @ episode report: {'average_total_reward': np.float32(9.23889), 'reward_variance': np.float32(1.4238094), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06641594506800175), 'actor_loss': np.float64(-0.9752484321594238), 'hyper_actor_loss': np.float64(1.0354294954595388e-05), 'behavior_loss': np.float64(0.4148231685161591)}

Episode step 44570, time diff 4.526881694793701, total time dif 12908.84368467331)
step: 44570 @ episode report: {'average_total_reward': np.float32(9.075556), 'reward_variance': np.float32(5.2705393), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07427438870072364), 'actor_loss': np.float64(-0.9682472348213196), 'hyper_actor_loss': np.float64(1.1397278285585344e-05), 'behavior_loss': np.float64(0.42282986640930176)}

Episode step 44580, time diff 4.767400741577148, total time dif 12913.370566368103)
step: 44580 @ episode report: {'average_total_reward': np.float32(9.175557), 'reward_variance': np.float32(3.822144), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05754971317946911), 'actor_loss': np.float64(-0.9597539007663727), 'hyper_actor_loss': np.float64(1.0860607744689332e-05), 'behavior_loss': np.float64(0.41205469369888303)}

Episode step 44590, time diff 4.713797569274902, total time dif 12918.13796710968)
step: 44590 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.2426436), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07305274643003941), 'actor_loss': np.float64(-0.9691562294960022), 'hyper_actor_loss': np.float64(1.0184242182731395e-05), 'behavior_loss': np.float64(0.42199023365974425)}

Episode step 44600, time diff 4.692744493484497, total time dif 12922.851764678955)
step: 44600 @ episode report: {'average_total_reward': np.float32(8.614446), 'reward_variance': np.float32(0.48753184), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0796736553311348), 'actor_loss': np.float64(-0.9823126137256623), 'hyper_actor_loss': np.float64(8.830770320855663e-06), 'behavior_loss': np.float64(0.41844487488269805)}

Episode step 44610, time diff 4.613543272018433, total time dif 12927.54450917244)
step: 44610 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(0.78656316), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0633154820650816), 'actor_loss': np.float64(-0.9735041081905365), 'hyper_actor_loss': np.float64(7.981580347404816e-06), 'behavior_loss': np.float64(0.4246168702840805)}

Episode step 44620, time diff 4.737386465072632, total time dif 12932.158052444458)
step: 44620 @ episode report: {'average_total_reward': np.float32(9.063334), 'reward_variance': np.float32(0.9972602), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06441349014639855), 'actor_loss': np.float64(-0.9580320239067077), 'hyper_actor_loss': np.float64(6.538468414873932e-06), 'behavior_loss': np.float64(0.4189251810312271)}

Episode step 44630, time diff 4.751756429672241, total time dif 12936.89543890953)
step: 44630 @ episode report: {'average_total_reward': np.float32(8.853334), 'reward_variance': np.float32(1.6419452), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058626698516309264), 'actor_loss': np.float64(-0.9636050343513489), 'hyper_actor_loss': np.float64(5.10071731696371e-06), 'behavior_loss': np.float64(0.4179238259792328)}

Episode step 44640, time diff 4.745412349700928, total time dif 12941.647195339203)
step: 44640 @ episode report: {'average_total_reward': np.float32(9.13889), 'reward_variance': np.float32(2.8848717), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06684992648661137), 'actor_loss': np.float64(-0.9662886798381806), 'hyper_actor_loss': np.float64(4.564290770758817e-06), 'behavior_loss': np.float64(0.4198382169008255)}

Episode step 44650, time diff 4.7916882038116455, total time dif 12946.392607688904)
step: 44650 @ episode report: {'average_total_reward': np.float32(10.112223), 'reward_variance': np.float32(2.9470232), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06633213423192501), 'actor_loss': np.float64(-0.9674094021320343), 'hyper_actor_loss': np.float64(4.129811736675038e-06), 'behavior_loss': np.float64(0.4208791464567184)}

Episode step 44660, time diff 4.715097188949585, total time dif 12951.184295892715)
step: 44660 @ episode report: {'average_total_reward': np.float32(9.251111), 'reward_variance': np.float32(0.87770903), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.411111), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07589302696287632), 'actor_loss': np.float64(-0.9808621764183044), 'hyper_actor_loss': np.float64(3.881067709698982e-06), 'behavior_loss': np.float64(0.4183458060026169)}

Episode step 44670, time diff 4.704360246658325, total time dif 12955.899393081665)
step: 44670 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(2.6706038), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06754247639328241), 'actor_loss': np.float64(-0.9840248465538025), 'hyper_actor_loss': np.float64(4.045728837809293e-06), 'behavior_loss': np.float64(0.42402143180370333)}

Episode step 44680, time diff 4.666402816772461, total time dif 12960.603753328323)
step: 44680 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(2.2005684), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07431905940175057), 'actor_loss': np.float64(-0.9728359043598175), 'hyper_actor_loss': np.float64(3.6294735537012457e-06), 'behavior_loss': np.float64(0.4228490263223648)}

Episode step 44690, time diff 5.340402841567993, total time dif 12965.270156145096)
step: 44690 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(1.5454137), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07366984374821187), 'actor_loss': np.float64(-0.9747801184654236), 'hyper_actor_loss': np.float64(3.7593413708236765e-06), 'behavior_loss': np.float64(0.4245226740837097)}

Episode step 44700, time diff 4.607879400253296, total time dif 12970.610558986664)
step: 44700 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(3.0799804), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06660211943089962), 'actor_loss': np.float64(-0.9820730566978455), 'hyper_actor_loss': np.float64(3.937772976314591e-06), 'behavior_loss': np.float64(0.41604319512844085)}

Episode step 44710, time diff 4.395110607147217, total time dif 12975.218438386917)
step: 44710 @ episode report: {'average_total_reward': np.float32(10.061112), 'reward_variance': np.float32(4.0018086), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06513947434723377), 'actor_loss': np.float64(-0.9756514728069305), 'hyper_actor_loss': np.float64(4.347448225416883e-06), 'behavior_loss': np.float64(0.42188531458377837)}

Episode step 44720, time diff 4.426889181137085, total time dif 12979.613548994064)
step: 44720 @ episode report: {'average_total_reward': np.float32(9.8122225), 'reward_variance': np.float32(2.1776166), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07357141599059105), 'actor_loss': np.float64(-0.9691127300262451), 'hyper_actor_loss': np.float64(3.863789356728376e-06), 'behavior_loss': np.float64(0.4208461999893188)}

Episode step 44730, time diff 4.377588748931885, total time dif 12984.040438175201)
step: 44730 @ episode report: {'average_total_reward': np.float32(9.587778), 'reward_variance': np.float32(3.344234), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07637121677398681), 'actor_loss': np.float64(-0.9875202775001526), 'hyper_actor_loss': np.float64(4.254192958796921e-06), 'behavior_loss': np.float64(0.41706955432891846)}

Episode step 44740, time diff 4.390559673309326, total time dif 12988.418026924133)
step: 44740 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(1.6874123), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07024832479655743), 'actor_loss': np.float64(-0.9846822440624237), 'hyper_actor_loss': np.float64(5.593070409304346e-06), 'behavior_loss': np.float64(0.42062426507472994)}

Episode step 44750, time diff 4.4280173778533936, total time dif 12992.808586597443)
step: 44750 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.6641104), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06839963681995868), 'actor_loss': np.float64(-0.9560904741287232), 'hyper_actor_loss': np.float64(5.097444068269397e-06), 'behavior_loss': np.float64(0.4219993680715561)}

Episode step 44760, time diff 4.407270193099976, total time dif 12997.236603975296)
step: 44760 @ episode report: {'average_total_reward': np.float32(9.551111), 'reward_variance': np.float32(1.1289184), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06564835198223591), 'actor_loss': np.float64(-0.9675518870353699), 'hyper_actor_loss': np.float64(4.434686002241506e-06), 'behavior_loss': np.float64(0.41934763789176943)}

Episode step 44770, time diff 4.410518169403076, total time dif 13001.643874168396)
step: 44770 @ episode report: {'average_total_reward': np.float32(9.251112), 'reward_variance': np.float32(0.79242516), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06410804055631161), 'actor_loss': np.float64(-0.9776398301124573), 'hyper_actor_loss': np.float64(5.016740988139645e-06), 'behavior_loss': np.float64(0.4235470682382584)}

Episode step 44780, time diff 4.420141220092773, total time dif 13006.054392337799)
step: 44780 @ episode report: {'average_total_reward': np.float32(10.236667), 'reward_variance': np.float32(1.2749637), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06256004720926285), 'actor_loss': np.float64(-0.9607340633869171), 'hyper_actor_loss': np.float64(5.738292429668945e-06), 'behavior_loss': np.float64(0.4273879170417786)}

Episode step 44790, time diff 4.5258095264434814, total time dif 13010.474533557892)
step: 44790 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(1.7275422), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0731153842061758), 'actor_loss': np.float64(-0.9658277869224549), 'hyper_actor_loss': np.float64(5.731574265155359e-06), 'behavior_loss': np.float64(0.42228659987449646)}

Episode step 44800, time diff 4.652204990386963, total time dif 13015.000343084335)
step: 44800 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(1.3688152), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07431350648403168), 'actor_loss': np.float64(-0.990877217054367), 'hyper_actor_loss': np.float64(5.595569882643759e-06), 'behavior_loss': np.float64(0.4266146123409271)}

Episode step 44810, time diff 4.676049470901489, total time dif 13019.652548074722)
step: 44810 @ episode report: {'average_total_reward': np.float32(9.451113), 'reward_variance': np.float32(2.715264), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05409924425184727), 'actor_loss': np.float64(-0.9708488345146179), 'hyper_actor_loss': np.float64(4.8045556013676105e-06), 'behavior_loss': np.float64(0.42756321728229524)}

Episode step 44820, time diff 4.6469080448150635, total time dif 13024.328597545624)
step: 44820 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(1.7890928), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06330151297152042), 'actor_loss': np.float64(-0.9522268772125244), 'hyper_actor_loss': np.float64(3.887837306137954e-06), 'behavior_loss': np.float64(0.4222370654344559)}

Episode step 44830, time diff 4.7369704246521, total time dif 13028.975505590439)
step: 44830 @ episode report: {'average_total_reward': np.float32(9.675557), 'reward_variance': np.float32(3.4145145), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07227085791528225), 'actor_loss': np.float64(-0.9749957978725433), 'hyper_actor_loss': np.float64(3.7084462519487714e-06), 'behavior_loss': np.float64(0.4233163446187973)}

Episode step 44840, time diff 4.60558295249939, total time dif 13033.712476015091)
step: 44840 @ episode report: {'average_total_reward': np.float32(9.563334), 'reward_variance': np.float32(1.6879025), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.2888885), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06584200114011765), 'actor_loss': np.float64(-0.9833975672721863), 'hyper_actor_loss': np.float64(4.146704714003135e-06), 'behavior_loss': np.float64(0.42907857000827787)}

Episode step 44850, time diff 4.819178819656372, total time dif 13038.31805896759)
step: 44850 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(2.5821185), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06609321348369121), 'actor_loss': np.float64(-0.9622502803802491), 'hyper_actor_loss': np.float64(4.863516824116232e-06), 'behavior_loss': np.float64(0.43171792924404145)}

Episode step 44860, time diff 4.687106132507324, total time dif 13043.137237787247)
step: 44860 @ episode report: {'average_total_reward': np.float32(9.300001), 'reward_variance': np.float32(1.4475069), 'max_total_reward': np.float32(12.022223), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.083318654820323), 'actor_loss': np.float64(-0.969613927602768), 'hyper_actor_loss': np.float64(4.903615695184271e-06), 'behavior_loss': np.float64(0.4371056705713272)}

Episode step 44870, time diff 4.725414752960205, total time dif 13047.824343919754)
step: 44870 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(1.7016792), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0760790653526783), 'actor_loss': np.float64(-0.9982603192329407), 'hyper_actor_loss': np.float64(5.633840783048072e-06), 'behavior_loss': np.float64(0.4300100415945053)}

Episode step 44880, time diff 4.707365036010742, total time dif 13052.549758672714)
step: 44880 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(0.59814936), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07383340075612069), 'actor_loss': np.float64(-0.9918178200721741), 'hyper_actor_loss': np.float64(6.910960701134173e-06), 'behavior_loss': np.float64(0.42768691182136537)}

Episode step 44890, time diff 4.6032116413116455, total time dif 13057.257123708725)
step: 44890 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(5.012697), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07164962366223335), 'actor_loss': np.float64(-0.9785311102867127), 'hyper_actor_loss': np.float64(7.067085698508891e-06), 'behavior_loss': np.float64(0.43259745836257935)}

Episode step 44900, time diff 4.622952938079834, total time dif 13061.860335350037)
step: 44900 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(1.4978716), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060522424429655074), 'actor_loss': np.float64(-0.9572342514991761), 'hyper_actor_loss': np.float64(6.228199390534428e-06), 'behavior_loss': np.float64(0.4313082456588745)}

Episode step 44910, time diff 4.825579643249512, total time dif 13066.483288288116)
step: 44910 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.8954692), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057627180404961106), 'actor_loss': np.float64(-0.9563531458377839), 'hyper_actor_loss': np.float64(5.222885647526709e-06), 'behavior_loss': np.float64(0.4294252246618271)}

Episode step 44920, time diff 4.840031862258911, total time dif 13071.308867931366)
step: 44920 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(2.8390818), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0716991025954485), 'actor_loss': np.float64(-0.9668967068195343), 'hyper_actor_loss': np.float64(5.0766744607244615e-06), 'behavior_loss': np.float64(0.4342283338308334)}

Episode step 44930, time diff 4.5155627727508545, total time dif 13076.148899793625)
step: 44930 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(3.4539711), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062017297744750975), 'actor_loss': np.float64(-0.9709577918052673), 'hyper_actor_loss': np.float64(5.272607722872635e-06), 'behavior_loss': np.float64(0.43037966787815096)}

Episode step 44940, time diff 4.510244131088257, total time dif 13080.664462566376)
step: 44940 @ episode report: {'average_total_reward': np.float32(9.985557), 'reward_variance': np.float32(2.7189157), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06515441201627255), 'actor_loss': np.float64(-0.9705745935440063), 'hyper_actor_loss': np.float64(5.1137321406713456e-06), 'behavior_loss': np.float64(0.4269426941871643)}

Episode step 44950, time diff 4.699229717254639, total time dif 13085.174706697464)
step: 44950 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(2.8909438), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07496700920164585), 'actor_loss': np.float64(-0.9787912368774414), 'hyper_actor_loss': np.float64(4.692885977419792e-06), 'behavior_loss': np.float64(0.4300683379173279)}

Episode step 44960, time diff 4.702608108520508, total time dif 13089.873936414719)
step: 44960 @ episode report: {'average_total_reward': np.float32(8.98778), 'reward_variance': np.float32(1.3531717), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06306590773165226), 'actor_loss': np.float64(-0.9730533421039581), 'hyper_actor_loss': np.float64(5.0911054131574925e-06), 'behavior_loss': np.float64(0.43342081606388094)}

Episode step 44970, time diff 4.754310131072998, total time dif 13094.57654452324)
step: 44970 @ episode report: {'average_total_reward': np.float32(9.712224), 'reward_variance': np.float32(4.0052204), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0627512775361538), 'actor_loss': np.float64(-0.9555035829544067), 'hyper_actor_loss': np.float64(5.500773113453761e-06), 'behavior_loss': np.float64(0.43126703798770905)}

Episode step 44980, time diff 4.738610744476318, total time dif 13099.330854654312)
step: 44980 @ episode report: {'average_total_reward': np.float32(9.997778), 'reward_variance': np.float32(0.9824649), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06996230371296405), 'actor_loss': np.float64(-0.9625643610954284), 'hyper_actor_loss': np.float64(5.290758781484328e-06), 'behavior_loss': np.float64(0.4368429481983185)}

Episode step 44990, time diff 4.724301338195801, total time dif 13104.069465398788)
step: 44990 @ episode report: {'average_total_reward': np.float32(9.363335), 'reward_variance': np.float32(3.602593), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07075302861630917), 'actor_loss': np.float64(-0.9758991897106171), 'hyper_actor_loss': np.float64(5.644881321131834e-06), 'behavior_loss': np.float64(0.42953583896160125)}

Episode step 45000, time diff 4.748554944992065, total time dif 13108.793766736984)
step: 45000 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(3.180842), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08152273446321487), 'actor_loss': np.float64(-0.977232015132904), 'hyper_actor_loss': np.float64(6.493117098216317e-06), 'behavior_loss': np.float64(0.43666580617427825)}

Episode step 45010, time diff 4.64504599571228, total time dif 13113.542321681976)
step: 45010 @ episode report: {'average_total_reward': np.float32(8.714445), 'reward_variance': np.float32(2.5488906), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07037623710930348), 'actor_loss': np.float64(-0.9862608015537262), 'hyper_actor_loss': np.float64(6.929289065737976e-06), 'behavior_loss': np.float64(0.4335968345403671)}

Episode step 45020, time diff 4.823616981506348, total time dif 13118.187367677689)
step: 45020 @ episode report: {'average_total_reward': np.float32(9.175556), 'reward_variance': np.float32(4.292489), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07487821280956268), 'actor_loss': np.float64(-0.967295914888382), 'hyper_actor_loss': np.float64(8.821206392894965e-06), 'behavior_loss': np.float64(0.4380639284849167)}

Episode step 45030, time diff 4.691316843032837, total time dif 13123.010984659195)
step: 45030 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(2.049131), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06568034254014492), 'actor_loss': np.float64(-0.9641829848289489), 'hyper_actor_loss': np.float64(8.376291452805162e-06), 'behavior_loss': np.float64(0.4331983655691147)}

Episode step 45040, time diff 4.717498540878296, total time dif 13127.702301502228)
step: 45040 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.8788943), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0868883904069662), 'actor_loss': np.float64(-0.9736154198646545), 'hyper_actor_loss': np.float64(9.03282762010349e-06), 'behavior_loss': np.float64(0.43172337412834166)}

Episode step 45050, time diff 4.707605838775635, total time dif 13132.419800043106)
step: 45050 @ episode report: {'average_total_reward': np.float32(9.54889), 'reward_variance': np.float32(5.9496603), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08190409801900386), 'actor_loss': np.float64(-1.010232138633728), 'hyper_actor_loss': np.float64(9.20762422538246e-06), 'behavior_loss': np.float64(0.426128289103508)}

Episode step 45060, time diff 4.802841663360596, total time dif 13137.127405881882)
step: 45060 @ episode report: {'average_total_reward': np.float32(9.075557), 'reward_variance': np.float32(1.9627359), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07598980478942394), 'actor_loss': np.float64(-1.0039307117462157), 'hyper_actor_loss': np.float64(8.814651118882466e-06), 'behavior_loss': np.float64(0.42949080765247344)}

Episode step 45070, time diff 4.698362827301025, total time dif 13141.930247545242)
step: 45070 @ episode report: {'average_total_reward': np.float32(10.212223), 'reward_variance': np.float32(3.70085), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06908804439008236), 'actor_loss': np.float64(-0.9587791085243225), 'hyper_actor_loss': np.float64(8.645065190648892e-06), 'behavior_loss': np.float64(0.4291690766811371)}

Episode step 45080, time diff 4.6468751430511475, total time dif 13146.628610372543)
step: 45080 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(5.3712096), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.411112), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058169583044946196), 'actor_loss': np.float64(-0.9465428113937377), 'hyper_actor_loss': np.float64(8.316669755004113e-06), 'behavior_loss': np.float64(0.4213314920663834)}

Episode step 45090, time diff 4.586428642272949, total time dif 13151.275485515594)
step: 45090 @ episode report: {'average_total_reward': np.float32(8.826667), 'reward_variance': np.float32(4.9647956), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06969250366091728), 'actor_loss': np.float64(-0.9625119507312775), 'hyper_actor_loss': np.float64(8.367474993065117e-06), 'behavior_loss': np.float64(0.4236657530069351)}

Episode step 45100, time diff 4.493448495864868, total time dif 13155.861914157867)
step: 45100 @ episode report: {'average_total_reward': np.float32(9.212222), 'reward_variance': np.float32(1.2633941), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07575893439352513), 'actor_loss': np.float64(-0.9873479068279266), 'hyper_actor_loss': np.float64(8.991428330773488e-06), 'behavior_loss': np.float64(0.42580932974815366)}

Episode step 45110, time diff 4.582233667373657, total time dif 13160.355362653732)
step: 45110 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.0285726), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06924915574491024), 'actor_loss': np.float64(-0.9879590809345246), 'hyper_actor_loss': np.float64(1.1400925359339453e-05), 'behavior_loss': np.float64(0.42274574041366575)}

Episode step 45120, time diff 4.375669240951538, total time dif 13164.937596321106)
step: 45120 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(3.6571622), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0671640146523714), 'actor_loss': np.float64(-0.9669115602970123), 'hyper_actor_loss': np.float64(1.2867915575043299e-05), 'behavior_loss': np.float64(0.42697805166244507)}

Episode step 45130, time diff 4.708040714263916, total time dif 13169.313265562057)
step: 45130 @ episode report: {'average_total_reward': np.float32(8.702223), 'reward_variance': np.float32(2.290835), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07727042362093925), 'actor_loss': np.float64(-0.9596685290336608), 'hyper_actor_loss': np.float64(1.3983671033201972e-05), 'behavior_loss': np.float64(0.4221291482448578)}

Episode step 45140, time diff 4.648989200592041, total time dif 13174.021306276321)
step: 45140 @ episode report: {'average_total_reward': np.float32(9.114446), 'reward_variance': np.float32(1.7572601), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07270854637026787), 'actor_loss': np.float64(-0.9730823576450348), 'hyper_actor_loss': np.float64(1.4566827576345532e-05), 'behavior_loss': np.float64(0.4209627538919449)}

Episode step 45150, time diff 4.527562141418457, total time dif 13178.670295476913)
step: 45150 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(7.801394), 'max_total_reward': np.float32(16.755556), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06293136421591043), 'actor_loss': np.float64(-0.9770652472972869), 'hyper_actor_loss': np.float64(1.6997089005599264e-05), 'behavior_loss': np.float64(0.4209233790636063)}

Episode step 45160, time diff 4.419724464416504, total time dif 13183.197857618332)
step: 45160 @ episode report: {'average_total_reward': np.float32(8.390001), 'reward_variance': np.float32(2.214258), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0728037029504776), 'actor_loss': np.float64(-0.9695124506950379), 'hyper_actor_loss': np.float64(1.8213219482277056e-05), 'behavior_loss': np.float64(0.41249324977397916)}

Episode step 45170, time diff 4.4236321449279785, total time dif 13187.617582082748)
step: 45170 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(1.8493137), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06761467382311821), 'actor_loss': np.float64(-0.9737906634807587), 'hyper_actor_loss': np.float64(2.0843644051637966e-05), 'behavior_loss': np.float64(0.41627072989940644)}

Episode step 45180, time diff 4.557177782058716, total time dif 13192.041214227676)
step: 45180 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(3.5482917), 'max_total_reward': np.float32(13.144445), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07045897245407104), 'actor_loss': np.float64(-0.969234949350357), 'hyper_actor_loss': np.float64(2.280442295159446e-05), 'behavior_loss': np.float64(0.4108491688966751)}

Episode step 45190, time diff 4.489032030105591, total time dif 13196.598392009735)
step: 45190 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(1.0615866), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0672295480966568), 'actor_loss': np.float64(-0.9855345606803894), 'hyper_actor_loss': np.float64(2.687264313863125e-05), 'behavior_loss': np.float64(0.40293340384960175)}

Episode step 45200, time diff 4.475226402282715, total time dif 13201.08742403984)
step: 45200 @ episode report: {'average_total_reward': np.float32(10.397779), 'reward_variance': np.float32(3.06002), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07052856609225273), 'actor_loss': np.float64(-0.9728229701519012), 'hyper_actor_loss': np.float64(3.084761519858148e-05), 'behavior_loss': np.float64(0.4098106861114502)}

Episode step 45210, time diff 4.497693300247192, total time dif 13205.562650442123)
step: 45210 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(1.8999065), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.065659886226058), 'actor_loss': np.float64(-0.9655508399009705), 'hyper_actor_loss': np.float64(3.237268774682889e-05), 'behavior_loss': np.float64(0.40263161063194275)}

Episode step 45220, time diff 4.455981969833374, total time dif 13210.06034374237)
step: 45220 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(2.3496993), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07390104383230209), 'actor_loss': np.float64(-0.9715161561965943), 'hyper_actor_loss': np.float64(3.9666222437517715e-05), 'behavior_loss': np.float64(0.40262394547462466)}

Episode step 45230, time diff 4.488336801528931, total time dif 13214.516325712204)
step: 45230 @ episode report: {'average_total_reward': np.float32(9.5633335), 'reward_variance': np.float32(3.5752614), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06880677528679371), 'actor_loss': np.float64(-0.9851818919181824), 'hyper_actor_loss': np.float64(4.634023935068399e-05), 'behavior_loss': np.float64(0.3884717494249344)}

Episode step 45240, time diff 4.482761859893799, total time dif 13219.004662513733)
step: 45240 @ episode report: {'average_total_reward': np.float32(8.490001), 'reward_variance': np.float32(2.0750732), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06872960925102234), 'actor_loss': np.float64(-0.9917641580104828), 'hyper_actor_loss': np.float64(5.0828301027650014e-05), 'behavior_loss': np.float64(0.3887231558561325)}

Episode step 45250, time diff 4.4448230266571045, total time dif 13223.487424373627)
step: 45250 @ episode report: {'average_total_reward': np.float32(9.102223), 'reward_variance': np.float32(1.6915016), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07341162413358689), 'actor_loss': np.float64(-0.9706241726875305), 'hyper_actor_loss': np.float64(5.4381450900109486e-05), 'behavior_loss': np.float64(0.3833142817020416)}

Episode step 45260, time diff 4.45504093170166, total time dif 13227.932247400284)
step: 45260 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(0.837142), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07114496007561684), 'actor_loss': np.float64(-0.9818541765213012), 'hyper_actor_loss': np.float64(5.43230191397015e-05), 'behavior_loss': np.float64(0.38600738942623136)}

Episode step 45270, time diff 4.7450830936431885, total time dif 13232.387288331985)
step: 45270 @ episode report: {'average_total_reward': np.float32(9.512224), 'reward_variance': np.float32(4.180381), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07284909151494504), 'actor_loss': np.float64(-0.9663508594036102), 'hyper_actor_loss': np.float64(5.6574587142677045e-05), 'behavior_loss': np.float64(0.38755378425121306)}

Episode step 45280, time diff 4.7522547245025635, total time dif 13237.132371425629)
step: 45280 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(2.3951657), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06871546171605587), 'actor_loss': np.float64(-0.9666528701782227), 'hyper_actor_loss': np.float64(6.249675279832446e-05), 'behavior_loss': np.float64(0.3803060382604599)}

Episode step 45290, time diff 4.766388416290283, total time dif 13241.884626150131)
step: 45290 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(1.9258522), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06377088520675897), 'actor_loss': np.float64(-0.9715211749076843), 'hyper_actor_loss': np.float64(6.55266863759607e-05), 'behavior_loss': np.float64(0.37690011560916903)}

Episode step 45300, time diff 4.833914756774902, total time dif 13246.651014566422)
step: 45300 @ episode report: {'average_total_reward': np.float32(8.951113), 'reward_variance': np.float32(1.8420547), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07028324361890555), 'actor_loss': np.float64(-0.9680018782615661), 'hyper_actor_loss': np.float64(7.399424503091722e-05), 'behavior_loss': np.float64(0.38159630000591277)}

Episode step 45310, time diff 4.854386568069458, total time dif 13251.484929323196)
step: 45310 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.6551423), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06901321038603783), 'actor_loss': np.float64(-0.9717994749546051), 'hyper_actor_loss': np.float64(7.966051125549712e-05), 'behavior_loss': np.float64(0.37052481472492216)}

Episode step 45320, time diff 4.634756088256836, total time dif 13256.339315891266)
step: 45320 @ episode report: {'average_total_reward': np.float32(10.061112), 'reward_variance': np.float32(2.545413), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06755127683281899), 'actor_loss': np.float64(-0.9817182779312134), 'hyper_actor_loss': np.float64(8.986678076325916e-05), 'behavior_loss': np.float64(0.35900442898273466)}

Episode step 45330, time diff 4.59623646736145, total time dif 13260.974071979523)
step: 45330 @ episode report: {'average_total_reward': np.float32(9.275556), 'reward_variance': np.float32(1.7517478), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0704235840588808), 'actor_loss': np.float64(-0.983346289396286), 'hyper_actor_loss': np.float64(0.00010102479573106393), 'behavior_loss': np.float64(0.3645223766565323)}

Episode step 45340, time diff 4.802101135253906, total time dif 13265.570308446884)
step: 45340 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(1.6903999), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07186112217605115), 'actor_loss': np.float64(-0.9782890498638153), 'hyper_actor_loss': np.float64(0.00010624051064951345), 'behavior_loss': np.float64(0.3643308639526367)}

Episode step 45350, time diff 4.912130355834961, total time dif 13270.372409582138)
step: 45350 @ episode report: {'average_total_reward': np.float32(9.875555), 'reward_variance': np.float32(2.9331057), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05154404230415821), 'actor_loss': np.float64(-0.9627205193042755), 'hyper_actor_loss': np.float64(0.00010368724542786367), 'behavior_loss': np.float64(0.35920038223266604)}

Episode step 45360, time diff 4.655789375305176, total time dif 13275.284539937973)
step: 45360 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(2.1005054), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555552), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07466260641813278), 'actor_loss': np.float64(-0.9569433689117431), 'hyper_actor_loss': np.float64(0.00010034073930000886), 'behavior_loss': np.float64(0.3531917124986649)}

Episode step 45370, time diff 4.630566596984863, total time dif 13279.940329313278)
step: 45370 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.1552544), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060243146121501924), 'actor_loss': np.float64(-0.9706700325012207), 'hyper_actor_loss': np.float64(9.982110641431064e-05), 'behavior_loss': np.float64(0.3624099791049957)}

Episode step 45380, time diff 4.56569242477417, total time dif 13284.570895910263)
step: 45380 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(1.5791805), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0652022384107113), 'actor_loss': np.float64(-0.9663536787033081), 'hyper_actor_loss': np.float64(0.0001142142849857919), 'behavior_loss': np.float64(0.36284170746803285)}

Episode step 45390, time diff 4.693282604217529, total time dif 13289.136588335037)
step: 45390 @ episode report: {'average_total_reward': np.float32(9.48778), 'reward_variance': np.float32(1.7398388), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06280111782252788), 'actor_loss': np.float64(-0.9666324734687806), 'hyper_actor_loss': np.float64(7.764988331473432e-05), 'behavior_loss': np.float64(0.3644521087408066)}

Episode step 45400, time diff 4.734846591949463, total time dif 13293.829870939255)
step: 45400 @ episode report: {'average_total_reward': np.float32(9.675557), 'reward_variance': np.float32(2.6529095), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08325706496834755), 'actor_loss': np.float64(-0.9884828865528107), 'hyper_actor_loss': np.float64(7.359961600741371e-05), 'behavior_loss': np.float64(0.37242862284183503)}

Episode step 45410, time diff 4.703667402267456, total time dif 13298.564717531204)
step: 45410 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(4.1607857), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06167972106486559), 'actor_loss': np.float64(-0.986844676733017), 'hyper_actor_loss': np.float64(6.706228232360445e-05), 'behavior_loss': np.float64(0.36198942065238954)}

Episode step 45420, time diff 4.619760990142822, total time dif 13303.268384933472)
step: 45420 @ episode report: {'average_total_reward': np.float32(9.051112), 'reward_variance': np.float32(1.7552398), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0688237089663744), 'actor_loss': np.float64(-0.9602754712104797), 'hyper_actor_loss': np.float64(6.199850540724582e-05), 'behavior_loss': np.float64(0.37254790365695956)}

Episode step 45430, time diff 4.654242277145386, total time dif 13307.888145923615)
step: 45430 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(2.6659963), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08043803945183754), 'actor_loss': np.float64(-0.9671594560146332), 'hyper_actor_loss': np.float64(6.13233307376504e-05), 'behavior_loss': np.float64(0.36426492035388947)}

Episode step 45440, time diff 4.621318578720093, total time dif 13312.54238820076)
step: 45440 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.886076), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058374589681625365), 'actor_loss': np.float64(-0.9873387336730957), 'hyper_actor_loss': np.float64(5.856222269358113e-05), 'behavior_loss': np.float64(0.3636716574430466)}

Episode step 45450, time diff 4.699106931686401, total time dif 13317.16370677948)
step: 45450 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(1.1077731), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07725375220179558), 'actor_loss': np.float64(-0.977592670917511), 'hyper_actor_loss': np.float64(5.6477128964615986e-05), 'behavior_loss': np.float64(0.36378092765808107)}

Episode step 45460, time diff 4.612436532974243, total time dif 13321.862813711166)
step: 45460 @ episode report: {'average_total_reward': np.float32(8.141111), 'reward_variance': np.float32(2.2281005), 'max_total_reward': np.float32(10.777779), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06415907517075539), 'actor_loss': np.float64(-0.9706506192684173), 'hyper_actor_loss': np.float64(5.778986633231398e-05), 'behavior_loss': np.float64(0.3636747419834137)}

Episode step 45470, time diff 4.71871542930603, total time dif 13326.47525024414)
step: 45470 @ episode report: {'average_total_reward': np.float32(8.490001), 'reward_variance': np.float32(1.1418875), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06735185086727143), 'actor_loss': np.float64(-0.9763598442077637), 'hyper_actor_loss': np.float64(5.614736292045563e-05), 'behavior_loss': np.float64(0.35949409008026123)}

Episode step 45480, time diff 4.944171667098999, total time dif 13331.193965673447)
step: 45480 @ episode report: {'average_total_reward': np.float32(9.175555), 'reward_variance': np.float32(2.6859703), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333343), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06631873808801174), 'actor_loss': np.float64(-0.9868331253528595), 'hyper_actor_loss': np.float64(5.629806837532669e-05), 'behavior_loss': np.float64(0.356927627325058)}

Episode step 45490, time diff 4.824280261993408, total time dif 13336.138137340546)
step: 45490 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(2.9950914), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07142497338354588), 'actor_loss': np.float64(-0.9761730611324311), 'hyper_actor_loss': np.float64(5.084030199213885e-05), 'behavior_loss': np.float64(0.3716247737407684)}

Episode step 45500, time diff 4.910822868347168, total time dif 13340.962417602539)
step: 45500 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(1.8176098), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07708248347043992), 'actor_loss': np.float64(-0.9718645632266998), 'hyper_actor_loss': np.float64(5.266045955067966e-05), 'behavior_loss': np.float64(0.36933145225048064)}

Episode step 45510, time diff 4.914602518081665, total time dif 13345.873240470886)
step: 45510 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.4426428), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06875905208289623), 'actor_loss': np.float64(-0.9799148499965668), 'hyper_actor_loss': np.float64(5.192825847188942e-05), 'behavior_loss': np.float64(0.36466139256954194)}

Episode step 45520, time diff 4.849916934967041, total time dif 13350.787842988968)
step: 45520 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(4.0272455), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07852882407605648), 'actor_loss': np.float64(-0.9809589445590973), 'hyper_actor_loss': np.float64(5.761339598393533e-05), 'behavior_loss': np.float64(0.3715508908033371)}

Episode step 45530, time diff 4.890155792236328, total time dif 13355.637759923935)
step: 45530 @ episode report: {'average_total_reward': np.float32(8.73889), 'reward_variance': np.float32(2.939439), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0636006772518158), 'actor_loss': np.float64(-0.9736809313297272), 'hyper_actor_loss': np.float64(6.593477046408225e-05), 'behavior_loss': np.float64(0.3792222052812576)}

Episode step 45540, time diff 4.863715171813965, total time dif 13360.527915716171)
step: 45540 @ episode report: {'average_total_reward': np.float32(8.465555), 'reward_variance': np.float32(2.5915914), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061748267337679866), 'actor_loss': np.float64(-0.9605093777179718), 'hyper_actor_loss': np.float64(4.9683112229104155e-05), 'behavior_loss': np.float64(0.3610927611589432)}

Episode step 45550, time diff 4.836739540100098, total time dif 13365.391630887985)
step: 45550 @ episode report: {'average_total_reward': np.float32(9.5244465), 'reward_variance': np.float32(4.2919703), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06824674271047115), 'actor_loss': np.float64(-0.972138649225235), 'hyper_actor_loss': np.float64(4.4655268720816824e-05), 'behavior_loss': np.float64(0.3765920281410217)}

Episode step 45560, time diff 4.763625860214233, total time dif 13370.228370428085)
step: 45560 @ episode report: {'average_total_reward': np.float32(9.624445), 'reward_variance': np.float32(1.5168598), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07660389170050622), 'actor_loss': np.float64(-0.9716806769371032), 'hyper_actor_loss': np.float64(4.4334877748042346e-05), 'behavior_loss': np.float64(0.37778526544570923)}

Episode step 45570, time diff 4.733356952667236, total time dif 13374.9919962883)
step: 45570 @ episode report: {'average_total_reward': np.float32(9.077779), 'reward_variance': np.float32(0.99550647), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06799779571592808), 'actor_loss': np.float64(-0.9715700685977936), 'hyper_actor_loss': np.float64(4.098225290363189e-05), 'behavior_loss': np.float64(0.37770653069019317)}

Episode step 45580, time diff 4.903806447982788, total time dif 13379.725353240967)
step: 45580 @ episode report: {'average_total_reward': np.float32(8.128889), 'reward_variance': np.float32(2.4443011), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07216258831322193), 'actor_loss': np.float64(-0.9743478536605835), 'hyper_actor_loss': np.float64(3.723374393302947e-05), 'behavior_loss': np.float64(0.37757771611213686)}

Episode step 45590, time diff 4.80862021446228, total time dif 13384.62915968895)
step: 45590 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(3.4740353), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07328981459140778), 'actor_loss': np.float64(-0.9740125417709351), 'hyper_actor_loss': np.float64(3.622107033152133e-05), 'behavior_loss': np.float64(0.37020750641822814)}

Episode step 45600, time diff 4.693660020828247, total time dif 13389.437779903412)
step: 45600 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(2.20556), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0833709791302681), 'actor_loss': np.float64(-0.9859169065952301), 'hyper_actor_loss': np.float64(3.995434453827329e-05), 'behavior_loss': np.float64(0.3772712558507919)}

Episode step 45610, time diff 4.626417636871338, total time dif 13394.13143992424)
step: 45610 @ episode report: {'average_total_reward': np.float32(9.324446), 'reward_variance': np.float32(0.57846385), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06925766728818417), 'actor_loss': np.float64(-0.9910840868949891), 'hyper_actor_loss': np.float64(4.055570825585164e-05), 'behavior_loss': np.float64(0.3720309525728226)}

Episode step 45620, time diff 4.648260593414307, total time dif 13398.757857561111)
step: 45620 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(2.2348597), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07876385450363159), 'actor_loss': np.float64(-0.9866616547107696), 'hyper_actor_loss': np.float64(3.656845874502324e-05), 'behavior_loss': np.float64(0.3702614963054657)}

Episode step 45630, time diff 4.662830829620361, total time dif 13403.406118154526)
step: 45630 @ episode report: {'average_total_reward': np.float32(8.477778), 'reward_variance': np.float32(0.77758), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07305503040552139), 'actor_loss': np.float64(-0.9847615599632263), 'hyper_actor_loss': np.float64(3.8163498720678037e-05), 'behavior_loss': np.float64(0.3758621603250504)}

Episode step 45640, time diff 4.645890951156616, total time dif 13408.068948984146)
step: 45640 @ episode report: {'average_total_reward': np.float32(8.465556), 'reward_variance': np.float32(4.888901), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06605179123580456), 'actor_loss': np.float64(-0.9811356604099274), 'hyper_actor_loss': np.float64(3.7916119254077785e-05), 'behavior_loss': np.float64(0.36730264127254486)}

Episode step 45650, time diff 4.693723678588867, total time dif 13412.714839935303)
step: 45650 @ episode report: {'average_total_reward': np.float32(9.563334), 'reward_variance': np.float32(1.3417791), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06625020317733288), 'actor_loss': np.float64(-0.9627762317657471), 'hyper_actor_loss': np.float64(3.677556651382474e-05), 'behavior_loss': np.float64(0.3723648369312286)}

Episode step 45660, time diff 4.90013861656189, total time dif 13417.408563613892)
step: 45660 @ episode report: {'average_total_reward': np.float32(8.751112), 'reward_variance': np.float32(3.3552146), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0746340923011303), 'actor_loss': np.float64(-0.9662663578987122), 'hyper_actor_loss': np.float64(3.566621071513509e-05), 'behavior_loss': np.float64(0.36888849139213564)}

Episode step 45670, time diff 4.648224592208862, total time dif 13422.308702230453)
step: 45670 @ episode report: {'average_total_reward': np.float32(8.975555), 'reward_variance': np.float32(2.8679214), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07587076835334301), 'actor_loss': np.float64(-0.9871409475803375), 'hyper_actor_loss': np.float64(3.495850778563181e-05), 'behavior_loss': np.float64(0.3794875741004944)}

Episode step 45680, time diff 4.841746091842651, total time dif 13426.956926822662)
step: 45680 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(1.6694866), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06448173150420189), 'actor_loss': np.float64(-0.9838369190692902), 'hyper_actor_loss': np.float64(3.786206871154718e-05), 'behavior_loss': np.float64(0.3663745164871216)}

Episode step 45690, time diff 4.771209955215454, total time dif 13431.798672914505)
step: 45690 @ episode report: {'average_total_reward': np.float32(9.1), 'reward_variance': np.float32(1.4742472), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07651090472936631), 'actor_loss': np.float64(-0.9707080066204071), 'hyper_actor_loss': np.float64(3.516660162858898e-05), 'behavior_loss': np.float64(0.3731444299221039)}

Episode step 45700, time diff 4.853724718093872, total time dif 13436.56988286972)
step: 45700 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(1.7461529), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06085660494863987), 'actor_loss': np.float64(-0.9721799492835999), 'hyper_actor_loss': np.float64(4.141597164561972e-05), 'behavior_loss': np.float64(0.3744950175285339)}

Episode step 45710, time diff 4.983944654464722, total time dif 13441.423607587814)
step: 45710 @ episode report: {'average_total_reward': np.float32(8.902224), 'reward_variance': np.float32(3.34044), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06608812883496284), 'actor_loss': np.float64(-0.964989322423935), 'hyper_actor_loss': np.float64(4.401707192300819e-05), 'behavior_loss': np.float64(0.36427621841430663)}

Episode step 45720, time diff 4.93996262550354, total time dif 13446.407552242279)
step: 45720 @ episode report: {'average_total_reward': np.float32(8.077778), 'reward_variance': np.float32(2.1457038), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06341574974358082), 'actor_loss': np.float64(-0.9745745539665223), 'hyper_actor_loss': np.float64(4.266590512997936e-05), 'behavior_loss': np.float64(0.3607128232717514)}

Episode step 45730, time diff 4.849629163742065, total time dif 13451.347514867783)
step: 45730 @ episode report: {'average_total_reward': np.float32(9.212223), 'reward_variance': np.float32(2.864925), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07918249331414699), 'actor_loss': np.float64(-0.9760146975517273), 'hyper_actor_loss': np.float64(4.37705861259019e-05), 'behavior_loss': np.float64(0.3671872466802597)}

Episode step 45740, time diff 4.883224725723267, total time dif 13456.197144031525)
step: 45740 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(1.1713147), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06924059167504311), 'actor_loss': np.float64(-0.9858733773231506), 'hyper_actor_loss': np.float64(4.2481086347834206e-05), 'behavior_loss': np.float64(0.363549131155014)}

Episode step 45750, time diff 4.911277532577515, total time dif 13461.080368757248)
step: 45750 @ episode report: {'average_total_reward': np.float32(9.6), 'reward_variance': np.float32(5.0704446), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0711115462705493), 'actor_loss': np.float64(-0.9847172915935516), 'hyper_actor_loss': np.float64(4.632685195247177e-05), 'behavior_loss': np.float64(0.35930727124214173)}

Episode step 45760, time diff 4.847370624542236, total time dif 13465.991646289825)
step: 45760 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(3.0160444), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07872103974223137), 'actor_loss': np.float64(-0.9762137591838836), 'hyper_actor_loss': np.float64(4.549723817035556e-05), 'behavior_loss': np.float64(0.36764066219329833)}

Episode step 45770, time diff 4.851170778274536, total time dif 13470.839016914368)
step: 45770 @ episode report: {'average_total_reward': np.float32(9.163334), 'reward_variance': np.float32(1.4211867), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06680313795804978), 'actor_loss': np.float64(-0.9819675922393799), 'hyper_actor_loss': np.float64(5.0953994650626556e-05), 'behavior_loss': np.float64(0.36140728890895846)}

Episode step 45780, time diff 4.7588207721710205, total time dif 13475.690187692642)
step: 45780 @ episode report: {'average_total_reward': np.float32(9.3122225), 'reward_variance': np.float32(2.8845048), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06263230144977569), 'actor_loss': np.float64(-0.9699970960617066), 'hyper_actor_loss': np.float64(5.251690890872851e-05), 'behavior_loss': np.float64(0.3537636250257492)}

Episode step 45790, time diff 4.7545859813690186, total time dif 13480.449008464813)
step: 45790 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(4.9323034), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05544126406311989), 'actor_loss': np.float64(-0.9678386390209198), 'hyper_actor_loss': np.float64(5.10063244291814e-05), 'behavior_loss': np.float64(0.353116637468338)}

Episode step 45800, time diff 4.998227596282959, total time dif 13485.203594446182)
step: 45800 @ episode report: {'average_total_reward': np.float32(8.677778), 'reward_variance': np.float32(4.7339754), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07631458342075348), 'actor_loss': np.float64(-0.972253966331482), 'hyper_actor_loss': np.float64(5.16440202773083e-05), 'behavior_loss': np.float64(0.3558744937181473)}

Episode step 45810, time diff 4.960587024688721, total time dif 13490.201822042465)
step: 45810 @ episode report: {'average_total_reward': np.float32(9.224444), 'reward_variance': np.float32(1.875576), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08116191774606704), 'actor_loss': np.float64(-0.9929428339004517), 'hyper_actor_loss': np.float64(4.900632666249294e-05), 'behavior_loss': np.float64(0.3625201344490051)}

Episode step 45820, time diff 4.936474084854126, total time dif 13495.162409067154)
step: 45820 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(4.600353), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06340811103582382), 'actor_loss': np.float64(-0.981221330165863), 'hyper_actor_loss': np.float64(4.791455357917584e-05), 'behavior_loss': np.float64(0.367595911026001)}

Episode step 45830, time diff 4.865685939788818, total time dif 13500.098883152008)
step: 45830 @ episode report: {'average_total_reward': np.float32(9.675555), 'reward_variance': np.float32(4.492366), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06657139845192432), 'actor_loss': np.float64(-0.9513325035572052), 'hyper_actor_loss': np.float64(4.792216022906359e-05), 'behavior_loss': np.float64(0.3621251881122589)}

Episode step 45840, time diff 4.94588828086853, total time dif 13504.964569091797)
step: 45840 @ episode report: {'average_total_reward': np.float32(8.902224), 'reward_variance': np.float32(2.5084896), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07780418805778026), 'actor_loss': np.float64(-0.9779496610164642), 'hyper_actor_loss': np.float64(4.601786131388508e-05), 'behavior_loss': np.float64(0.36124168038368226)}

Episode step 45850, time diff 4.949974775314331, total time dif 13509.910457372665)
step: 45850 @ episode report: {'average_total_reward': np.float32(9.9366665), 'reward_variance': np.float32(3.083508), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07168644890189171), 'actor_loss': np.float64(-1.0003449320793152), 'hyper_actor_loss': np.float64(4.7739897854626176e-05), 'behavior_loss': np.float64(0.3518560975790024)}

Episode step 45860, time diff 4.926949739456177, total time dif 13514.86043214798)
step: 45860 @ episode report: {'average_total_reward': np.float32(9.551111), 'reward_variance': np.float32(1.3887705), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06302440539002419), 'actor_loss': np.float64(-0.9669782638549804), 'hyper_actor_loss': np.float64(4.7888821063679644e-05), 'behavior_loss': np.float64(0.35060147047042844)}

Episode step 45870, time diff 4.777821779251099, total time dif 13519.787381887436)
step: 45870 @ episode report: {'average_total_reward': np.float32(8.314445), 'reward_variance': np.float32(3.5781505), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06880554296076298), 'actor_loss': np.float64(-0.9614867985248565), 'hyper_actor_loss': np.float64(4.981186903023627e-05), 'behavior_loss': np.float64(0.3602064460515976)}

Episode step 45880, time diff 4.956255912780762, total time dif 13524.565203666687)
step: 45880 @ episode report: {'average_total_reward': np.float32(9.9), 'reward_variance': np.float32(3.313778), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06295659802854062), 'actor_loss': np.float64(-0.9677266418933869), 'hyper_actor_loss': np.float64(4.5011066322331314e-05), 'behavior_loss': np.float64(0.3516930937767029)}

Episode step 45890, time diff 4.914954423904419, total time dif 13529.521459579468)
step: 45890 @ episode report: {'average_total_reward': np.float32(8.526668), 'reward_variance': np.float32(1.1711161), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06695175319910049), 'actor_loss': np.float64(-0.9621724724769593), 'hyper_actor_loss': np.float64(5.0849263061536476e-05), 'behavior_loss': np.float64(0.3583527535200119)}

Episode step 45900, time diff 4.9911208152771, total time dif 13534.436414003372)
step: 45900 @ episode report: {'average_total_reward': np.float32(9.375555), 'reward_variance': np.float32(1.2060201), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0556173175573349), 'actor_loss': np.float64(-0.9708015382289886), 'hyper_actor_loss': np.float64(5.574169626925141e-05), 'behavior_loss': np.float64(0.354792046546936)}

Episode step 45910, time diff 5.011780500411987, total time dif 13539.42753481865)
step: 45910 @ episode report: {'average_total_reward': np.float32(9.138888), 'reward_variance': np.float32(2.1322277), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08926806934177875), 'actor_loss': np.float64(-0.9814639866352082), 'hyper_actor_loss': np.float64(5.8776048172148875e-05), 'behavior_loss': np.float64(0.35323837399482727)}

Episode step 45920, time diff 4.9665749073028564, total time dif 13544.439315319061)
step: 45920 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.6520805), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07476051189005375), 'actor_loss': np.float64(-1.0018615245819091), 'hyper_actor_loss': np.float64(6.287155410973355e-05), 'behavior_loss': np.float64(0.3569892406463623)}

Episode step 45930, time diff 4.951744079589844, total time dif 13549.405890226364)
step: 45930 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(1.4231958), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.057138781808316705), 'actor_loss': np.float64(-0.9603552222251892), 'hyper_actor_loss': np.float64(7.219708277261816e-05), 'behavior_loss': np.float64(0.3495194882154465)}

Episode step 45940, time diff 4.961325407028198, total time dif 13554.357634305954)
step: 45940 @ episode report: {'average_total_reward': np.float32(8.926668), 'reward_variance': np.float32(2.3781288), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06723972540348769), 'actor_loss': np.float64(-0.949865746498108), 'hyper_actor_loss': np.float64(7.189712778199464e-05), 'behavior_loss': np.float64(0.34828523695468905)}

Episode step 45950, time diff 5.05246639251709, total time dif 13559.318959712982)
step: 45950 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(1.7131407), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06852456219494343), 'actor_loss': np.float64(-0.9850271821022034), 'hyper_actor_loss': np.float64(7.353422188316472e-05), 'behavior_loss': np.float64(0.34310248792171477)}

Episode step 45960, time diff 5.0025904178619385, total time dif 13564.3714261055)
step: 45960 @ episode report: {'average_total_reward': np.float32(8.614445), 'reward_variance': np.float32(1.4127424), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06680791452527046), 'actor_loss': np.float64(-0.9944297254085541), 'hyper_actor_loss': np.float64(7.88659519457724e-05), 'behavior_loss': np.float64(0.3376614272594452)}

Episode step 45970, time diff 5.012540340423584, total time dif 13569.374016523361)
step: 45970 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(1.5472544), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0687404066324234), 'actor_loss': np.float64(-0.9741821110248565), 'hyper_actor_loss': np.float64(8.26481540570967e-05), 'behavior_loss': np.float64(0.34489186108112335)}

Episode step 45980, time diff 4.9479615688323975, total time dif 13574.386556863785)
step: 45980 @ episode report: {'average_total_reward': np.float32(9.287779), 'reward_variance': np.float32(3.6656654), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05587329249829054), 'actor_loss': np.float64(-0.9606732130050659), 'hyper_actor_loss': np.float64(8.150938592734746e-05), 'behavior_loss': np.float64(0.33315182030200957)}

Episode step 45990, time diff 4.60733962059021, total time dif 13579.334518432617)
step: 45990 @ episode report: {'average_total_reward': np.float32(9.887778), 'reward_variance': np.float32(1.6667023), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062141460552811625), 'actor_loss': np.float64(-0.9579479813575744), 'hyper_actor_loss': np.float64(8.166267944034188e-05), 'behavior_loss': np.float64(0.3407309889793396)}

Episode step 46000, time diff 4.5921790599823, total time dif 13583.941858053207)
step: 46000 @ episode report: {'average_total_reward': np.float32(9.973333), 'reward_variance': np.float32(2.5772145), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07670762799680234), 'actor_loss': np.float64(-0.9803796172142029), 'hyper_actor_loss': np.float64(7.928081831778399e-05), 'behavior_loss': np.float64(0.3314434677362442)}

Episode step 46010, time diff 4.7420876026153564, total time dif 13588.53403711319)
step: 46010 @ episode report: {'average_total_reward': np.float32(9.287778), 'reward_variance': np.float32(2.6142583), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07351046316325664), 'actor_loss': np.float64(-0.9990775942802429), 'hyper_actor_loss': np.float64(8.100348859443329e-05), 'behavior_loss': np.float64(0.3397708237171173)}

Episode step 46020, time diff 4.566960096359253, total time dif 13593.276124715805)
step: 46020 @ episode report: {'average_total_reward': np.float32(9.74889), 'reward_variance': np.float32(2.9567704), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06881304085254669), 'actor_loss': np.float64(-0.9888958096504211), 'hyper_actor_loss': np.float64(8.533277505193838e-05), 'behavior_loss': np.float64(0.3352820247411728)}

Episode step 46030, time diff 4.600897312164307, total time dif 13597.843084812164)
step: 46030 @ episode report: {'average_total_reward': np.float32(8.626668), 'reward_variance': np.float32(1.8395357), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06151586212217808), 'actor_loss': np.float64(-0.9708683431148529), 'hyper_actor_loss': np.float64(8.575639731134288e-05), 'behavior_loss': np.float64(0.33607388138771055)}

Episode step 46040, time diff 4.665990114212036, total time dif 13602.443982124329)
step: 46040 @ episode report: {'average_total_reward': np.float32(9.8), 'reward_variance': np.float32(1.8591115), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.533333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06483134366571904), 'actor_loss': np.float64(-0.9596083760261536), 'hyper_actor_loss': np.float64(8.230460152844899e-05), 'behavior_loss': np.float64(0.3391597867012024)}

Episode step 46050, time diff 4.539875030517578, total time dif 13607.10997223854)
step: 46050 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(2.1183758), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07251132614910602), 'actor_loss': np.float64(-0.9781387150287628), 'hyper_actor_loss': np.float64(9.699716756585985e-05), 'behavior_loss': np.float64(0.3375884681940079)}

Episode step 46060, time diff 4.4816319942474365, total time dif 13611.649847269058)
step: 46060 @ episode report: {'average_total_reward': np.float32(9.524445), 'reward_variance': np.float32(2.4171062), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06502713300287724), 'actor_loss': np.float64(-0.9728786230087281), 'hyper_actor_loss': np.float64(0.00011363154044374824), 'behavior_loss': np.float64(0.3439475387334824)}

Episode step 46070, time diff 4.548069000244141, total time dif 13616.131479263306)
step: 46070 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(3.2999504), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07869326025247574), 'actor_loss': np.float64(-0.9664079487323761), 'hyper_actor_loss': np.float64(0.00012078726504114457), 'behavior_loss': np.float64(0.34074864387512205)}

Episode step 46080, time diff 4.934865236282349, total time dif 13620.67954826355)
step: 46080 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(1.8669679), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06279089003801346), 'actor_loss': np.float64(-0.984609317779541), 'hyper_actor_loss': np.float64(0.00012669242933043278), 'behavior_loss': np.float64(0.32410500943660736)}

Episode step 46090, time diff 4.916248083114624, total time dif 13625.614413499832)
step: 46090 @ episode report: {'average_total_reward': np.float32(8.29), 'reward_variance': np.float32(0.57329535), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06481628715991974), 'actor_loss': np.float64(-0.9825754702091217), 'hyper_actor_loss': np.float64(0.00017050807364284992), 'behavior_loss': np.float64(0.3253480106592178)}

Episode step 46100, time diff 4.692458629608154, total time dif 13630.530661582947)
step: 46100 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(4.0398817), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.068663932941854), 'actor_loss': np.float64(-0.9811294436454773), 'hyper_actor_loss': np.float64(0.00017621139995753764), 'behavior_loss': np.float64(0.33514882922172545)}

Episode step 46110, time diff 4.6173810958862305, total time dif 13635.223120212555)
step: 46110 @ episode report: {'average_total_reward': np.float32(10.112223), 'reward_variance': np.float32(3.3789742), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07822496853768826), 'actor_loss': np.float64(-0.9811055719852447), 'hyper_actor_loss': np.float64(0.00019561915833037346), 'behavior_loss': np.float64(0.32875838279724123)}

Episode step 46120, time diff 4.659028768539429, total time dif 13639.840501308441)
step: 46120 @ episode report: {'average_total_reward': np.float32(7.7655554), 'reward_variance': np.float32(3.3964305), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07414485812187195), 'actor_loss': np.float64(-0.9898449778556824), 'hyper_actor_loss': np.float64(0.00021227538964012637), 'behavior_loss': np.float64(0.33678342700004577)}

Episode step 46130, time diff 4.717388153076172, total time dif 13644.49953007698)
step: 46130 @ episode report: {'average_total_reward': np.float32(5.87), 'reward_variance': np.float32(1.6260755), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08410862814635038), 'actor_loss': np.float64(-0.9856141090393067), 'hyper_actor_loss': np.float64(0.00021307507995516062), 'behavior_loss': np.float64(0.3485706239938736)}

Episode step 46140, time diff 4.603216886520386, total time dif 13649.216918230057)
step: 46140 @ episode report: {'average_total_reward': np.float32(3.5622222), 'reward_variance': np.float32(0.63800496), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07151528559625149), 'actor_loss': np.float64(-0.9901449680328369), 'hyper_actor_loss': np.float64(0.0002506787393940613), 'behavior_loss': np.float64(0.3370732307434082)}

Episode step 46150, time diff 4.8970983028411865, total time dif 13653.820135116577)
step: 46150 @ episode report: {'average_total_reward': np.float32(5.1477776), 'reward_variance': np.float32(1.7508409), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06264782845973968), 'actor_loss': np.float64(-0.9780645489692688), 'hyper_actor_loss': np.float64(0.00023963023704709485), 'behavior_loss': np.float64(0.327818301320076)}

Episode step 46160, time diff 4.801882982254028, total time dif 13658.717233419418)
step: 46160 @ episode report: {'average_total_reward': np.float32(5.26), 'reward_variance': np.float32(0.6773876), 'max_total_reward': np.float32(6.6555557), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07250721417367459), 'actor_loss': np.float64(-0.9639708280563355), 'hyper_actor_loss': np.float64(0.0001508486850070767), 'behavior_loss': np.float64(0.34755179584026336)}

Episode step 46170, time diff 4.5878212451934814, total time dif 13663.519116401672)
step: 46170 @ episode report: {'average_total_reward': np.float32(5.545556), 'reward_variance': np.float32(1.1903818), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07197995223104954), 'actor_loss': np.float64(-0.975165456533432), 'hyper_actor_loss': np.float64(0.00012318688677623869), 'behavior_loss': np.float64(0.33245974481105806)}

Episode step 46180, time diff 4.42492413520813, total time dif 13668.106937646866)
step: 46180 @ episode report: {'average_total_reward': np.float32(7.767779), 'reward_variance': np.float32(3.0685058), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07384358048439026), 'actor_loss': np.float64(-0.9809535384178162), 'hyper_actor_loss': np.float64(0.00010493425725144334), 'behavior_loss': np.float64(0.34528751075267794)}

Episode step 46190, time diff 4.315626859664917, total time dif 13672.531861782074)
step: 46190 @ episode report: {'average_total_reward': np.float32(6.106667), 'reward_variance': np.float32(2.2411656), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06599192284047603), 'actor_loss': np.float64(-0.9686834454536438), 'hyper_actor_loss': np.float64(0.00010983113606926054), 'behavior_loss': np.float64(0.34357677698135375)}

Episode step 46200, time diff 4.362610340118408, total time dif 13676.847488641739)
step: 46200 @ episode report: {'average_total_reward': np.float32(5.7844443), 'reward_variance': np.float32(1.7262024), 'max_total_reward': np.float32(7.7777777), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(7.3), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06336641926318407), 'actor_loss': np.float64(-0.9501468598842621), 'hyper_actor_loss': np.float64(0.0001086893811589107), 'behavior_loss': np.float64(0.3477961480617523)}

Episode step 46210, time diff 4.349084377288818, total time dif 13681.210098981857)
step: 46210 @ episode report: {'average_total_reward': np.float32(5.3088894), 'reward_variance': np.float32(1.0987357), 'max_total_reward': np.float32(7.411112), 'min_total_reward': np.float32(4.166667), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0638337928801775), 'actor_loss': np.float64(-0.9578413665294647), 'hyper_actor_loss': np.float64(0.00010149340159841813), 'behavior_loss': np.float64(0.34141603112220764)}

Episode step 46220, time diff 4.424425840377808, total time dif 13685.559183359146)
step: 46220 @ episode report: {'average_total_reward': np.float32(5.6700006), 'reward_variance': np.float32(1.4903228), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07131623886525632), 'actor_loss': np.float64(-0.9728949010372162), 'hyper_actor_loss': np.float64(9.804812871152535e-05), 'behavior_loss': np.float64(0.34511401057243346)}

Episode step 46230, time diff 4.3424248695373535, total time dif 13689.983609199524)
step: 46230 @ episode report: {'average_total_reward': np.float32(5.6844444), 'reward_variance': np.float32(1.892869), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07302733659744262), 'actor_loss': np.float64(-0.9673061549663544), 'hyper_actor_loss': np.float64(0.00010022490096162073), 'behavior_loss': np.float64(0.35834642946720124)}

Episode step 46240, time diff 4.361996650695801, total time dif 13694.326034069061)
step: 46240 @ episode report: {'average_total_reward': np.float32(5.321111), 'reward_variance': np.float32(1.6188259), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(6.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07318178806453943), 'actor_loss': np.float64(-0.9679970562458038), 'hyper_actor_loss': np.float64(9.623902078601532e-05), 'behavior_loss': np.float64(0.3340787649154663)}

Episode step 46250, time diff 4.406116008758545, total time dif 13698.688030719757)
step: 46250 @ episode report: {'average_total_reward': np.float32(6.1066675), 'reward_variance': np.float32(0.9328941), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.5), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07705202549695969), 'actor_loss': np.float64(-0.9850335597991944), 'hyper_actor_loss': np.float64(0.00010161118261748925), 'behavior_loss': np.float64(0.33820618093013766)}

Episode step 46260, time diff 4.389177083969116, total time dif 13703.094146728516)
step: 46260 @ episode report: {'average_total_reward': np.float32(6.631111), 'reward_variance': np.float32(2.078711), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06378806047141553), 'actor_loss': np.float64(-0.9791921079158783), 'hyper_actor_loss': np.float64(9.619561678846367e-05), 'behavior_loss': np.float64(0.3420484125614166)}

Episode step 46270, time diff 4.306053400039673, total time dif 13707.483323812485)
step: 46270 @ episode report: {'average_total_reward': np.float32(5.957778), 'reward_variance': np.float32(1.6547858), 'max_total_reward': np.float32(7.777778), 'min_total_reward': np.float32(3.411111), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06579149551689625), 'actor_loss': np.float64(-0.9570520102977753), 'hyper_actor_loss': np.float64(0.00010037190295406618), 'behavior_loss': np.float64(0.32482013702392576)}

Episode step 46280, time diff 4.366299152374268, total time dif 13711.789377212524)
step: 46280 @ episode report: {'average_total_reward': np.float32(5.6333337), 'reward_variance': np.float32(1.5339015), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.1), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06946560889482498), 'actor_loss': np.float64(-0.9712871313095093), 'hyper_actor_loss': np.float64(9.541383260511794e-05), 'behavior_loss': np.float64(0.3264969944953918)}

Episode step 46290, time diff 4.349720239639282, total time dif 13716.155676364899)
step: 46290 @ episode report: {'average_total_reward': np.float32(6.27), 'reward_variance': np.float32(3.01973), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07087519615888596), 'actor_loss': np.float64(-0.975658369064331), 'hyper_actor_loss': np.float64(0.00010267130346619524), 'behavior_loss': np.float64(0.3385711580514908)}

Episode step 46300, time diff 4.283626556396484, total time dif 13720.505396604538)
step: 46300 @ episode report: {'average_total_reward': np.float32(6.408889), 'reward_variance': np.float32(3.2032795), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07570865899324417), 'actor_loss': np.float64(-0.9773019969463348), 'hyper_actor_loss': np.float64(8.926453592721373e-05), 'behavior_loss': np.float64(0.33544681668281556)}

Episode step 46310, time diff 4.2200927734375, total time dif 13724.789023160934)
step: 46310 @ episode report: {'average_total_reward': np.float32(5.708889), 'reward_variance': np.float32(1.9766865), 'max_total_reward': np.float32(7.5333343), 'min_total_reward': np.float32(3.2888894), 'average_n_step': np.float32(7.2), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07218137420713902), 'actor_loss': np.float64(-0.9787911951541901), 'hyper_actor_loss': np.float64(8.31334116810467e-05), 'behavior_loss': np.float64(0.33869004249572754)}

Episode step 46320, time diff 4.274430751800537, total time dif 13729.009115934372)
step: 46320 @ episode report: {'average_total_reward': np.float32(6.2333336), 'reward_variance': np.float32(1.9546669), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07014028206467629), 'actor_loss': np.float64(-0.9688677608966827), 'hyper_actor_loss': np.float64(8.402112725889311e-05), 'behavior_loss': np.float64(0.3416608333587646)}

Episode step 46330, time diff 4.526408910751343, total time dif 13733.283546686172)
step: 46330 @ episode report: {'average_total_reward': np.float32(5.9944444), 'reward_variance': np.float32(2.2343023), 'max_total_reward': np.float32(8.9), 'min_total_reward': np.float32(3.288889), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06805453263223171), 'actor_loss': np.float64(-0.9669770956039428), 'hyper_actor_loss': np.float64(8.001475143828429e-05), 'behavior_loss': np.float64(0.3324112743139267)}

Episode step 46340, time diff 4.3312718868255615, total time dif 13737.809955596924)
step: 46340 @ episode report: {'average_total_reward': np.float32(6.3944445), 'reward_variance': np.float32(1.1956856), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06675401069223881), 'actor_loss': np.float64(-0.9692085325717926), 'hyper_actor_loss': np.float64(8.839430156513118e-05), 'behavior_loss': np.float64(0.3460050016641617)}

Episode step 46350, time diff 4.332536220550537, total time dif 13742.14122748375)
step: 46350 @ episode report: {'average_total_reward': np.float32(6.631111), 'reward_variance': np.float32(1.806909), 'max_total_reward': np.float32(7.7777786), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07391630001366138), 'actor_loss': np.float64(-0.9669346511363983), 'hyper_actor_loss': np.float64(8.024070775718428e-05), 'behavior_loss': np.float64(0.35010230243206025)}

Episode step 46360, time diff 4.382575273513794, total time dif 13746.4737637043)
step: 46360 @ episode report: {'average_total_reward': np.float32(6.8944445), 'reward_variance': np.float32(1.0698582), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(5.2888894), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07478275150060654), 'actor_loss': np.float64(-0.9661067187786102), 'hyper_actor_loss': np.float64(7.185678477981128e-05), 'behavior_loss': np.float64(0.3331997185945511)}

Episode step 46370, time diff 4.290358066558838, total time dif 13750.856338977814)
step: 46370 @ episode report: {'average_total_reward': np.float32(5.9700003), 'reward_variance': np.float32(1.6731861), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(7.4), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0703969981521368), 'actor_loss': np.float64(-0.9699088275432587), 'hyper_actor_loss': np.float64(6.66999851091532e-05), 'behavior_loss': np.float64(0.3380850374698639)}

Episode step 46380, time diff 4.373943328857422, total time dif 13755.146697044373)
step: 46380 @ episode report: {'average_total_reward': np.float32(6.6555557), 'reward_variance': np.float32(3.7736297), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(3.4111109), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0652403187006712), 'actor_loss': np.float64(-0.9694100499153138), 'hyper_actor_loss': np.float64(8.243258635047823e-05), 'behavior_loss': np.float64(0.33508090674877167)}

Episode step 46390, time diff 4.307535648345947, total time dif 13759.52064037323)
step: 46390 @ episode report: {'average_total_reward': np.float32(6.231111), 'reward_variance': np.float32(1.876242), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06763659790158272), 'actor_loss': np.float64(-0.9713594615459442), 'hyper_actor_loss': np.float64(7.105131044227164e-05), 'behavior_loss': np.float64(0.3379016757011414)}

Episode step 46400, time diff 4.32360315322876, total time dif 13763.828176021576)
step: 46400 @ episode report: {'average_total_reward': np.float32(6.37), 'reward_variance': np.float32(4.552964), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07425431311130523), 'actor_loss': np.float64(-0.9757634043693543), 'hyper_actor_loss': np.float64(7.11605043761665e-05), 'behavior_loss': np.float64(0.34807154834270476)}

Episode step 46410, time diff 4.411741495132446, total time dif 13768.151779174805)
step: 46410 @ episode report: {'average_total_reward': np.float32(7.3188896), 'reward_variance': np.float32(1.7986434), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06205625683069229), 'actor_loss': np.float64(-0.9681979179382324), 'hyper_actor_loss': np.float64(6.77056290442124e-05), 'behavior_loss': np.float64(0.3435844838619232)}

Episode step 46420, time diff 4.4224817752838135, total time dif 13772.563520669937)
step: 46420 @ episode report: {'average_total_reward': np.float32(7.9288893), 'reward_variance': np.float32(4.238079), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06882179901003838), 'actor_loss': np.float64(-0.9498768091201782), 'hyper_actor_loss': np.float64(8.284593059215695e-05), 'behavior_loss': np.float64(0.347649821639061)}

Episode step 46430, time diff 4.4078991413116455, total time dif 13776.986002445221)
step: 46430 @ episode report: {'average_total_reward': np.float32(8.526667), 'reward_variance': np.float32(1.9816097), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06845827139914036), 'actor_loss': np.float64(-0.9674165785312653), 'hyper_actor_loss': np.float64(8.440372912446036e-05), 'behavior_loss': np.float64(0.33449828922748565)}

Episode step 46440, time diff 4.430361747741699, total time dif 13781.393901586533)
step: 46440 @ episode report: {'average_total_reward': np.float32(9.5), 'reward_variance': np.float32(2.4225929), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08152657523751258), 'actor_loss': np.float64(-0.9792842447757721), 'hyper_actor_loss': np.float64(6.735856441082433e-05), 'behavior_loss': np.float64(0.3382096588611603)}

Episode step 46450, time diff 4.409083127975464, total time dif 13785.824263334274)
step: 46450 @ episode report: {'average_total_reward': np.float32(9.487779), 'reward_variance': np.float32(2.2231221), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.411112), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06116928830742836), 'actor_loss': np.float64(-0.9829930305480957), 'hyper_actor_loss': np.float64(6.717488940921612e-05), 'behavior_loss': np.float64(0.3353860110044479)}

Episode step 46460, time diff 4.415072917938232, total time dif 13790.23334646225)
step: 46460 @ episode report: {'average_total_reward': np.float32(9.051111), 'reward_variance': np.float32(3.7398314), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06822500526905059), 'actor_loss': np.float64(-0.9601737499237061), 'hyper_actor_loss': np.float64(6.080691928218584e-05), 'behavior_loss': np.float64(0.3302744060754776)}

Episode step 46470, time diff 4.416651725769043, total time dif 13794.648419380188)
step: 46470 @ episode report: {'average_total_reward': np.float32(8.714444), 'reward_variance': np.float32(2.931952), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07164530046284198), 'actor_loss': np.float64(-0.9677666664123535), 'hyper_actor_loss': np.float64(6.393311014107894e-05), 'behavior_loss': np.float64(0.3426035702228546)}

Episode step 46480, time diff 4.384359836578369, total time dif 13799.065071105957)
step: 46480 @ episode report: {'average_total_reward': np.float32(9.375557), 'reward_variance': np.float32(2.1282423), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06631665267050266), 'actor_loss': np.float64(-0.9767308831214905), 'hyper_actor_loss': np.float64(5.8424157032277435e-05), 'behavior_loss': np.float64(0.3474740505218506)}

Episode step 46490, time diff 4.608808755874634, total time dif 13803.449430942535)
step: 46490 @ episode report: {'average_total_reward': np.float32(9.2), 'reward_variance': np.float32(3.2903705), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06986593082547188), 'actor_loss': np.float64(-0.9690335690975189), 'hyper_actor_loss': np.float64(5.91238454944687e-05), 'behavior_loss': np.float64(0.3378801286220551)}

Episode step 46500, time diff 4.478419780731201, total time dif 13808.05823969841)
step: 46500 @ episode report: {'average_total_reward': np.float32(8.665556), 'reward_variance': np.float32(2.4500356), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06503266543149948), 'actor_loss': np.float64(-0.9599982321262359), 'hyper_actor_loss': np.float64(6.112454502726905e-05), 'behavior_loss': np.float64(0.33483001589775085)}

Episode step 46510, time diff 4.475311040878296, total time dif 13812.536659479141)
step: 46510 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(1.612104), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06832499280571938), 'actor_loss': np.float64(-0.9728028535842895), 'hyper_actor_loss': np.float64(6.451242334151174e-05), 'behavior_loss': np.float64(0.33494617938995364)}

Episode step 46520, time diff 4.458672285079956, total time dif 13817.01197052002)
step: 46520 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(4.1174393), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06488406881690026), 'actor_loss': np.float64(-0.9733516693115234), 'hyper_actor_loss': np.float64(5.724460424971767e-05), 'behavior_loss': np.float64(0.35726250410079957)}

Episode step 46530, time diff 4.394885301589966, total time dif 13821.4706428051)
step: 46530 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(2.6976993), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.2888894), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06535795908421278), 'actor_loss': np.float64(-0.9465693175792694), 'hyper_actor_loss': np.float64(5.816346820211038e-05), 'behavior_loss': np.float64(0.34308664202690126)}

Episode step 46540, time diff 4.4633872509002686, total time dif 13825.86552810669)
step: 46540 @ episode report: {'average_total_reward': np.float32(8.502223), 'reward_variance': np.float32(0.7886621), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0668547187000513), 'actor_loss': np.float64(-0.9619271934032441), 'hyper_actor_loss': np.float64(6.359983599395491e-05), 'behavior_loss': np.float64(0.3436157673597336)}

Episode step 46550, time diff 4.4298975467681885, total time dif 13830.32891535759)
step: 46550 @ episode report: {'average_total_reward': np.float32(8.93889), 'reward_variance': np.float32(1.5683264), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08302210420370101), 'actor_loss': np.float64(-0.9835415363311768), 'hyper_actor_loss': np.float64(5.3820499670109714e-05), 'behavior_loss': np.float64(0.35569722950458527)}

Episode step 46560, time diff 4.503477573394775, total time dif 13834.758812904358)
step: 46560 @ episode report: {'average_total_reward': np.float32(8.526668), 'reward_variance': np.float32(2.488351), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07304139360785485), 'actor_loss': np.float64(-0.987319815158844), 'hyper_actor_loss': np.float64(4.6266214485513044e-05), 'behavior_loss': np.float64(0.3533875674009323)}

Episode step 46570, time diff 4.472848176956177, total time dif 13839.262290477753)
step: 46570 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(3.046969), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07969111762940884), 'actor_loss': np.float64(-0.9753520488739014), 'hyper_actor_loss': np.float64(4.798217705683783e-05), 'behavior_loss': np.float64(0.35454323291778567)}

Episode step 46580, time diff 4.507324934005737, total time dif 13843.735138654709)
step: 46580 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(1.2345388), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07433768026530743), 'actor_loss': np.float64(-0.9793817818164825), 'hyper_actor_loss': np.float64(4.9810847485787235e-05), 'behavior_loss': np.float64(0.34574654400348664)}

Episode step 46590, time diff 4.522153854370117, total time dif 13848.242463588715)
step: 46590 @ episode report: {'average_total_reward': np.float32(9.226667), 'reward_variance': np.float32(1.4678327), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0697506945580244), 'actor_loss': np.float64(-0.9751122295856476), 'hyper_actor_loss': np.float64(5.5494445768999866e-05), 'behavior_loss': np.float64(0.36130947768688204)}

Episode step 46600, time diff 4.493892669677734, total time dif 13852.764617443085)
step: 46600 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(5.0451064), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07109832391142845), 'actor_loss': np.float64(-0.9694320976734161), 'hyper_actor_loss': np.float64(5.960960661468562e-05), 'behavior_loss': np.float64(0.36084073781967163)}

Episode step 46610, time diff 4.544403791427612, total time dif 13857.258510112762)
step: 46610 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(6.7056174), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05947438254952431), 'actor_loss': np.float64(-0.9652167975902557), 'hyper_actor_loss': np.float64(7.576511634397321e-05), 'behavior_loss': np.float64(0.3646067500114441)}

Episode step 46620, time diff 4.509381055831909, total time dif 13861.80291390419)
step: 46620 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(3.247477), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07527921162545681), 'actor_loss': np.float64(-0.9658922016620636), 'hyper_actor_loss': np.float64(0.00010910509154200554), 'behavior_loss': np.float64(0.3810630291700363)}

Episode step 46630, time diff 4.574720621109009, total time dif 13866.312294960022)
step: 46630 @ episode report: {'average_total_reward': np.float32(8.963334), 'reward_variance': np.float32(1.3049889), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07003872841596603), 'actor_loss': np.float64(-0.9719902455806733), 'hyper_actor_loss': np.float64(0.0001274835398362484), 'behavior_loss': np.float64(0.3821346253156662)}

Episode step 46640, time diff 4.64711332321167, total time dif 13870.887015581131)
step: 46640 @ episode report: {'average_total_reward': np.float32(8.951112), 'reward_variance': np.float32(0.637536), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06331422030925751), 'actor_loss': np.float64(-0.9692818582057953), 'hyper_actor_loss': np.float64(0.00013422502452158368), 'behavior_loss': np.float64(0.3856803923845291)}

Episode step 46650, time diff 4.644181728363037, total time dif 13875.534128904343)
step: 46650 @ episode report: {'average_total_reward': np.float32(9.400001), 'reward_variance': np.float32(1.2879262), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07295147180557252), 'actor_loss': np.float64(-0.960248875617981), 'hyper_actor_loss': np.float64(0.00017106643208535387), 'behavior_loss': np.float64(0.40020073056221006)}

Episode step 46660, time diff 4.85550332069397, total time dif 13880.178310632706)
step: 46660 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(0.78267163), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0656741812825203), 'actor_loss': np.float64(-0.9721570611000061), 'hyper_actor_loss': np.float64(0.00018494082760298625), 'behavior_loss': np.float64(0.4169414401054382)}

Episode step 46670, time diff 4.676840305328369, total time dif 13885.0338139534)
step: 46670 @ episode report: {'average_total_reward': np.float32(9.548889), 'reward_variance': np.float32(2.725141), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07857266180217266), 'actor_loss': np.float64(-0.9695035576820373), 'hyper_actor_loss': np.float64(0.000170579829136841), 'behavior_loss': np.float64(0.4258462846279144)}

Episode step 46680, time diff 4.730379581451416, total time dif 13889.710654258728)
step: 46680 @ episode report: {'average_total_reward': np.float32(9.475555), 'reward_variance': np.float32(1.7859701), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07172856889665127), 'actor_loss': np.float64(-0.9801515817642212), 'hyper_actor_loss': np.float64(0.00015980539319571108), 'behavior_loss': np.float64(0.4337619423866272)}

Episode step 46690, time diff 4.8506999015808105, total time dif 13894.44103384018)
step: 46690 @ episode report: {'average_total_reward': np.float32(10.34889), 'reward_variance': np.float32(2.9880548), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.2888894), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06118274405598641), 'actor_loss': np.float64(-0.9631230533123016), 'hyper_actor_loss': np.float64(0.00017429075232939796), 'behavior_loss': np.float64(0.459575417637825)}

Episode step 46700, time diff 4.849031209945679, total time dif 13899.29173374176)
step: 46700 @ episode report: {'average_total_reward': np.float32(9.536668), 'reward_variance': np.float32(3.9987426), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06435346379876136), 'actor_loss': np.float64(-0.9615331709384918), 'hyper_actor_loss': np.float64(0.00028100202762288974), 'behavior_loss': np.float64(0.47048054039478304)}

Episode step 46710, time diff 4.739353895187378, total time dif 13904.140764951706)
step: 46710 @ episode report: {'average_total_reward': np.float32(9.275557), 'reward_variance': np.float32(3.0495255), 'max_total_reward': np.float32(13.0222225), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06806535944342613), 'actor_loss': np.float64(-0.9794620335102081), 'hyper_actor_loss': np.float64(0.0003886351740220562), 'behavior_loss': np.float64(0.49189729392528536)}

Episode step 46720, time diff 4.683169841766357, total time dif 13908.880118846893)
step: 46720 @ episode report: {'average_total_reward': np.float32(11.2322235), 'reward_variance': np.float32(2.639987), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06451525464653969), 'actor_loss': np.float64(-0.9573665201663971), 'hyper_actor_loss': np.float64(0.0005428875010693446), 'behavior_loss': np.float64(0.5156936347484589)}

Episode step 46730, time diff 4.61715030670166, total time dif 13913.56328868866)
step: 46730 @ episode report: {'average_total_reward': np.float32(11.320001), 'reward_variance': np.float32(2.072465), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07428082600235938), 'actor_loss': np.float64(-0.9699773371219635), 'hyper_actor_loss': np.float64(0.0007195731333922595), 'behavior_loss': np.float64(0.535324627161026)}

Episode step 46740, time diff 4.666341543197632, total time dif 13918.180438995361)
step: 46740 @ episode report: {'average_total_reward': np.float32(11.307778), 'reward_variance': np.float32(1.4980257), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07025451101362705), 'actor_loss': np.float64(-0.9874892115592957), 'hyper_actor_loss': np.float64(0.0007339918112847954), 'behavior_loss': np.float64(0.536772483587265)}

Episode step 46750, time diff 4.65596342086792, total time dif 13922.846780538559)
step: 46750 @ episode report: {'average_total_reward': np.float32(10.222223), 'reward_variance': np.float32(3.04037), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05806393679231405), 'actor_loss': np.float64(-0.983422440290451), 'hyper_actor_loss': np.float64(0.0006589655473362655), 'behavior_loss': np.float64(0.5408732473850251)}

Episode step 46760, time diff 4.717115163803101, total time dif 13927.502743959427)
step: 46760 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(3.7449393), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07221799641847611), 'actor_loss': np.float64(-0.9638137757778168), 'hyper_actor_loss': np.float64(0.0006272107886616141), 'behavior_loss': np.float64(0.5398065268993377)}

Episode step 46770, time diff 4.6695215702056885, total time dif 13932.21985912323)
step: 46770 @ episode report: {'average_total_reward': np.float32(9.861113), 'reward_variance': np.float32(3.5773644), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06709610000252723), 'actor_loss': np.float64(-0.9686079084873199), 'hyper_actor_loss': np.float64(0.0005907015351112932), 'behavior_loss': np.float64(0.5431709885597229)}

Episode step 46780, time diff 4.645665168762207, total time dif 13936.889380693436)
step: 46780 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(3.2631373), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06573605164885521), 'actor_loss': np.float64(-0.9781269729137421), 'hyper_actor_loss': np.float64(0.000563884456641972), 'behavior_loss': np.float64(0.5427510678768158)}

Episode step 46790, time diff 4.623580455780029, total time dif 13941.535045862198)
step: 46790 @ episode report: {'average_total_reward': np.float32(10.085556), 'reward_variance': np.float32(2.342915), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08080520331859589), 'actor_loss': np.float64(-0.9621989667415619), 'hyper_actor_loss': np.float64(0.0005294268776196986), 'behavior_loss': np.float64(0.5421215415000915)}

Episode step 46800, time diff 4.674175262451172, total time dif 13946.158626317978)
step: 46800 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.8341248), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07532636299729348), 'actor_loss': np.float64(-0.9794400095939636), 'hyper_actor_loss': np.float64(0.0005226448702160269), 'behavior_loss': np.float64(0.5453136622905731)}

Episode step 46810, time diff 4.750478506088257, total time dif 13950.832801580429)
step: 46810 @ episode report: {'average_total_reward': np.float32(8.863334), 'reward_variance': np.float32(1.1557789), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06521893180906772), 'actor_loss': np.float64(-0.9820640563964844), 'hyper_actor_loss': np.float64(0.0005507427733391524), 'behavior_loss': np.float64(0.5447163462638855)}

Episode step 46820, time diff 5.026267051696777, total time dif 13955.583280086517)
step: 46820 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(2.1484017), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0729675006121397), 'actor_loss': np.float64(-0.9664803624153138), 'hyper_actor_loss': np.float64(0.0007617132738232613), 'behavior_loss': np.float64(0.5595563352108002)}

Episode step 46830, time diff 5.221048355102539, total time dif 13960.609547138214)
step: 46830 @ episode report: {'average_total_reward': np.float32(7.9800005), 'reward_variance': np.float32(10.352489), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(1.0444446), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(3.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07453469559550285), 'actor_loss': np.float64(-0.9755819916725159), 'hyper_actor_loss': np.float64(0.0014188610250130296), 'behavior_loss': np.float64(0.5970341265201569)}

Episode step 46840, time diff 5.130807161331177, total time dif 13965.830595493317)
step: 46840 @ episode report: {'average_total_reward': np.float32(4.5111113), 'reward_variance': np.float32(1.6027162), 'max_total_reward': np.float32(6.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(6.1), 'max_n_step': np.float32(8.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06620354913175105), 'actor_loss': np.float64(-0.993857353925705), 'hyper_actor_loss': np.float64(0.0010038373060524465), 'behavior_loss': np.float64(0.5859110236167908)}

Episode step 46850, time diff 5.072330713272095, total time dif 13970.961402654648)
step: 46850 @ episode report: {'average_total_reward': np.float32(3.2422223), 'reward_variance': np.float32(0.82992095), 'max_total_reward': np.float32(5.1666665), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07961631417274476), 'actor_loss': np.float64(-0.9877524852752686), 'hyper_actor_loss': np.float64(0.0009939260664395988), 'behavior_loss': np.float64(0.6007338285446167)}

Episode step 46860, time diff 4.990029573440552, total time dif 13976.03373336792)
step: 46860 @ episode report: {'average_total_reward': np.float32(2.9544444), 'reward_variance': np.float32(0.1958383), 'max_total_reward': np.float32(3.411111), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07630016952753067), 'actor_loss': np.float64(-0.9799654304981231), 'hyper_actor_loss': np.float64(0.0008420404861681164), 'behavior_loss': np.float64(0.6020819127559662)}

Episode step 46870, time diff 4.968086004257202, total time dif 13981.02376294136)
step: 46870 @ episode report: {'average_total_reward': np.float32(3.3277779), 'reward_variance': np.float32(1.3847224), 'max_total_reward': np.float32(5.533334), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(5.1), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06824933364987373), 'actor_loss': np.float64(-0.9691033840179444), 'hyper_actor_loss': np.float64(0.0006545983196701854), 'behavior_loss': np.float64(0.5956150650978088)}

Episode step 46880, time diff 4.916085243225098, total time dif 13985.991848945618)
step: 46880 @ episode report: {'average_total_reward': np.float32(3.3788886), 'reward_variance': np.float32(0.46265307), 'max_total_reward': np.float32(4.411111), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07354906350374221), 'actor_loss': np.float64(-0.9803655803203583), 'hyper_actor_loss': np.float64(0.0005794755939859897), 'behavior_loss': np.float64(0.5980275928974151)}

Episode step 46890, time diff 4.9081737995147705, total time dif 13990.907934188843)
step: 46890 @ episode report: {'average_total_reward': np.float32(2.9033334), 'reward_variance': np.float32(0.19819883), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(2.1666667), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07851722985506057), 'actor_loss': np.float64(-0.9743891835212708), 'hyper_actor_loss': np.float64(0.0004750683350721374), 'behavior_loss': np.float64(0.597835648059845)}

Episode step 46900, time diff 4.85881233215332, total time dif 13995.816107988358)
step: 46900 @ episode report: {'average_total_reward': np.float32(2.9300003), 'reward_variance': np.float32(0.1908408), 'max_total_reward': np.float32(3.2888892), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07340032793581486), 'actor_loss': np.float64(-0.9685748994350434), 'hyper_actor_loss': np.float64(0.0003783940046560019), 'behavior_loss': np.float64(0.5978016197681427)}

Episode step 46910, time diff 4.895867347717285, total time dif 14000.67492032051)
step: 46910 @ episode report: {'average_total_reward': np.float32(3.178889), 'reward_variance': np.float32(0.5749001), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(1.8000001), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06898283921182155), 'actor_loss': np.float64(-0.9685868144035339), 'hyper_actor_loss': np.float64(0.00034783844312187285), 'behavior_loss': np.float64(0.6000743269920349)}

Episode step 46920, time diff 4.864354372024536, total time dif 14005.570787668228)
step: 46920 @ episode report: {'average_total_reward': np.float32(2.9277778), 'reward_variance': np.float32(0.52617913), 'max_total_reward': np.float32(4.533334), 'min_total_reward': np.float32(2.0444446), 'average_n_step': np.float32(4.7), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07923090010881424), 'actor_loss': np.float64(-0.9533910989761353), 'hyper_actor_loss': np.float64(0.00032248510979115963), 'behavior_loss': np.float64(0.6025991797447204)}

Episode step 46930, time diff 4.9042909145355225, total time dif 14010.435142040253)
step: 46930 @ episode report: {'average_total_reward': np.float32(3.178889), 'reward_variance': np.float32(0.3918383), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.166667), 'average_n_step': np.float32(5.0), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0700124841183424), 'actor_loss': np.float64(-0.9665842711925506), 'hyper_actor_loss': np.float64(0.0003057680500205606), 'behavior_loss': np.float64(0.6049931704998016)}

Episode step 46940, time diff 4.7987220287323, total time dif 14015.339432954788)
step: 46940 @ episode report: {'average_total_reward': np.float32(3.4033337), 'reward_variance': np.float32(0.21528515), 'max_total_reward': np.float32(4.288889), 'min_total_reward': np.float32(2.9222224), 'average_n_step': np.float32(5.2), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07423949465155602), 'actor_loss': np.float64(-0.965306556224823), 'hyper_actor_loss': np.float64(0.0002972870832309127), 'behavior_loss': np.float64(0.6093035519123078)}

Episode step 46950, time diff 4.785559177398682, total time dif 14020.13815498352)
step: 46950 @ episode report: {'average_total_reward': np.float32(3.0033336), 'reward_variance': np.float32(0.71795195), 'max_total_reward': np.float32(4.4111114), 'min_total_reward': np.float32(1.9222223), 'average_n_step': np.float32(4.8), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06092798560857773), 'actor_loss': np.float64(-0.9583032846450805), 'hyper_actor_loss': np.float64(0.0002843497000867501), 'behavior_loss': np.float64(0.6099554479122162)}

Episode step 46960, time diff 4.734662771224976, total time dif 14024.92371416092)
step: 46960 @ episode report: {'average_total_reward': np.float32(3.0666668), 'reward_variance': np.float32(0.32286438), 'max_total_reward': np.float32(4.411112), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(6.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0720131330192089), 'actor_loss': np.float64(-0.9489731073379517), 'hyper_actor_loss': np.float64(0.00025351620861329137), 'behavior_loss': np.float64(0.6078804075717926)}

Episode step 46970, time diff 4.7048659324646, total time dif 14029.658376932144)
step: 46970 @ episode report: {'average_total_reward': np.float32(3.188889), 'reward_variance': np.float32(0.11091362), 'max_total_reward': np.float32(3.4111114), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(4.9), 'max_n_step': np.float32(5.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07392094694077969), 'actor_loss': np.float64(-0.9637531578540802), 'hyper_actor_loss': np.float64(0.0002462927106535062), 'behavior_loss': np.float64(0.6104298233985901)}

Episode step 46980, time diff 4.864243984222412, total time dif 14034.363242864609)
step: 46980 @ episode report: {'average_total_reward': np.float32(3.7011113), 'reward_variance': np.float32(0.69838154), 'max_total_reward': np.float32(5.166667), 'min_total_reward': np.float32(2.288889), 'average_n_step': np.float32(5.4), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(4.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0843700621277094), 'actor_loss': np.float64(-0.9747256457805633), 'hyper_actor_loss': np.float64(0.00023345534136751667), 'behavior_loss': np.float64(0.6074553549289703)}

Episode step 46990, time diff 4.798420667648315, total time dif 14039.227486848831)
step: 46990 @ episode report: {'average_total_reward': np.float32(3.6255555), 'reward_variance': np.float32(0.42155686), 'max_total_reward': np.float32(5.2888894), 'min_total_reward': np.float32(3.166667), 'average_n_step': np.float32(5.3), 'max_n_step': np.float32(7.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07673338949680328), 'actor_loss': np.float64(-0.9780120849609375), 'hyper_actor_loss': np.float64(0.0002779550341074355), 'behavior_loss': np.float64(0.6019966065883636)}

Episode step 47000, time diff 4.784551382064819, total time dif 14044.02590751648)
step: 47000 @ episode report: {'average_total_reward': np.float32(5.2211113), 'reward_variance': np.float32(1.4204806), 'max_total_reward': np.float32(7.6555567), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(6.7), 'max_n_step': np.float32(9.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07205367349088192), 'actor_loss': np.float64(-0.96127849817276), 'hyper_actor_loss': np.float64(0.0003794518008362502), 'behavior_loss': np.float64(0.5936680555343627)}

Episode step 47010, time diff 4.786224842071533, total time dif 14048.810458898544)
step: 47010 @ episode report: {'average_total_reward': np.float32(6.6699996), 'reward_variance': np.float32(2.2489395), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(4.288889), 'average_n_step': np.float32(8.1), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0683253776282072), 'actor_loss': np.float64(-0.9389156222343444), 'hyper_actor_loss': np.float64(0.00046493002155330033), 'behavior_loss': np.float64(0.5829860746860505)}

Episode step 47020, time diff 4.710952043533325, total time dif 14053.596683740616)
step: 47020 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.2011406), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07496690526604652), 'actor_loss': np.float64(-0.9554474353790283), 'hyper_actor_loss': np.float64(0.0003326173755340278), 'behavior_loss': np.float64(0.5728598654270172)}

Episode step 47030, time diff 4.643585443496704, total time dif 14058.30763578415)
step: 47030 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(3.9302926), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08023457787930965), 'actor_loss': np.float64(-0.9893680036067962), 'hyper_actor_loss': np.float64(0.0002841518784407526), 'behavior_loss': np.float64(0.5671780407428741)}

Episode step 47040, time diff 4.646197557449341, total time dif 14062.951221227646)
step: 47040 @ episode report: {'average_total_reward': np.float32(9.387777), 'reward_variance': np.float32(4.5594187), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08176352940499783), 'actor_loss': np.float64(-0.9876894235610962), 'hyper_actor_loss': np.float64(0.000250985829916317), 'behavior_loss': np.float64(0.5647082686424255)}

Episode step 47050, time diff 4.6032185554504395, total time dif 14067.597418785095)
step: 47050 @ episode report: {'average_total_reward': np.float32(9.200001), 'reward_variance': np.float32(3.5756543), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06892889365553856), 'actor_loss': np.float64(-0.9610621631145477), 'hyper_actor_loss': np.float64(0.00024539409059798345), 'behavior_loss': np.float64(0.5600136756896973)}

Episode step 47060, time diff 4.632892370223999, total time dif 14072.200637340546)
step: 47060 @ episode report: {'average_total_reward': np.float32(10.309999), 'reward_variance': np.float32(2.7648752), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0632904402911663), 'actor_loss': np.float64(-0.9563360691070557), 'hyper_actor_loss': np.float64(0.00026579315890558065), 'behavior_loss': np.float64(0.5607455968856812)}

Episode step 47070, time diff 4.678735256195068, total time dif 14076.83352971077)
step: 47070 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(3.7029386), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06798675656318665), 'actor_loss': np.float64(-0.947486275434494), 'hyper_actor_loss': np.float64(0.0003018970397533849), 'behavior_loss': np.float64(0.5586213886737823)}

Episode step 47080, time diff 4.609589576721191, total time dif 14081.512264966965)
step: 47080 @ episode report: {'average_total_reward': np.float32(9.910001), 'reward_variance': np.float32(4.064061), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08082582429051399), 'actor_loss': np.float64(-0.958311915397644), 'hyper_actor_loss': np.float64(0.0003373062936589122), 'behavior_loss': np.float64(0.5545803248882294)}

Episode step 47090, time diff 4.647416353225708, total time dif 14086.121854543686)
step: 47090 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(2.0935366), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0781304720789194), 'actor_loss': np.float64(-0.9714211583137512), 'hyper_actor_loss': np.float64(0.00028071836422896015), 'behavior_loss': np.float64(0.5523075342178345)}

Episode step 47100, time diff 4.642255067825317, total time dif 14090.769270896912)
step: 47100 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(1.2586284), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07022679410874844), 'actor_loss': np.float64(-0.9741672098636627), 'hyper_actor_loss': np.float64(0.00021677708864444866), 'behavior_loss': np.float64(0.5520743072032929)}

Episode step 47110, time diff 4.557105302810669, total time dif 14095.411525964737)
step: 47110 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(4.72502), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07528039701282978), 'actor_loss': np.float64(-0.9844810128211975), 'hyper_actor_loss': np.float64(0.0001764789136359468), 'behavior_loss': np.float64(0.5552211940288544)}

Episode step 47120, time diff 4.656427621841431, total time dif 14099.968631267548)
step: 47120 @ episode report: {'average_total_reward': np.float32(10.622223), 'reward_variance': np.float32(2.0335555), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08439427316188812), 'actor_loss': np.float64(-0.9731069505214691), 'hyper_actor_loss': np.float64(0.00015310120506910606), 'behavior_loss': np.float64(0.553559559583664)}

Episode step 47130, time diff 4.615867853164673, total time dif 14104.625058889389)
step: 47130 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(6.8720946), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.09026933163404464), 'actor_loss': np.float64(-0.9895975410938262), 'hyper_actor_loss': np.float64(0.0001729818672174588), 'behavior_loss': np.float64(0.5524671316146851)}

Episode step 47140, time diff 4.821384429931641, total time dif 14109.240926742554)
step: 47140 @ episode report: {'average_total_reward': np.float32(9.263333), 'reward_variance': np.float32(2.972767), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07054433338344097), 'actor_loss': np.float64(-0.9885356962680817), 'hyper_actor_loss': np.float64(0.0001903726311866194), 'behavior_loss': np.float64(0.5545650243759155)}

Episode step 47150, time diff 4.772843360900879, total time dif 14114.062311172485)
step: 47150 @ episode report: {'average_total_reward': np.float32(9.912222), 'reward_variance': np.float32(1.2941344), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07117719948291779), 'actor_loss': np.float64(-0.9675258994102478), 'hyper_actor_loss': np.float64(0.00023297570733120664), 'behavior_loss': np.float64(0.5523058772087097)}

Episode step 47160, time diff 4.803164720535278, total time dif 14118.835154533386)
step: 47160 @ episode report: {'average_total_reward': np.float32(10.846667), 'reward_variance': np.float32(5.102959), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07326219789683819), 'actor_loss': np.float64(-0.968657648563385), 'hyper_actor_loss': np.float64(0.00029125759319867937), 'behavior_loss': np.float64(0.5596518635749816)}

Episode step 47170, time diff 4.8830718994140625, total time dif 14123.638319253922)
step: 47170 @ episode report: {'average_total_reward': np.float32(9.748889), 'reward_variance': np.float32(2.7951653), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07549933455884457), 'actor_loss': np.float64(-0.9805058658123016), 'hyper_actor_loss': np.float64(0.00038950310263317076), 'behavior_loss': np.float64(0.5609267890453339)}

Episode step 47180, time diff 4.893128395080566, total time dif 14128.521391153336)
step: 47180 @ episode report: {'average_total_reward': np.float32(11.120001), 'reward_variance': np.float32(4.2842917), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06593077443540096), 'actor_loss': np.float64(-0.9587607383728027), 'hyper_actor_loss': np.float64(0.0005059558723587542), 'behavior_loss': np.float64(0.5636590719223022)}

Episode step 47190, time diff 4.8620476722717285, total time dif 14133.414519548416)
step: 47190 @ episode report: {'average_total_reward': np.float32(12.530002), 'reward_variance': np.float32(2.1434088), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(11.022223), 'average_n_step': np.float32(13.3), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(12.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0744920589029789), 'actor_loss': np.float64(-0.9520279586315155), 'hyper_actor_loss': np.float64(0.0005166028277017176), 'behavior_loss': np.float64(0.5591328322887421)}

Episode step 47200, time diff 4.803154230117798, total time dif 14138.276567220688)
step: 47200 @ episode report: {'average_total_reward': np.float32(10.173334), 'reward_variance': np.float32(2.9567704), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08214101754128933), 'actor_loss': np.float64(-0.9792672872543335), 'hyper_actor_loss': np.float64(0.00047116225468926134), 'behavior_loss': np.float64(0.5580282807350159)}

Episode step 47210, time diff 4.804598331451416, total time dif 14143.079721450806)
step: 47210 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(2.1183765), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08314641155302524), 'actor_loss': np.float64(-0.9855059027671814), 'hyper_actor_loss': np.float64(0.0003987369011156261), 'behavior_loss': np.float64(0.554840749502182)}

Episode step 47220, time diff 4.830233573913574, total time dif 14147.884319782257)
step: 47220 @ episode report: {'average_total_reward': np.float32(11.86889), 'reward_variance': np.float32(2.8585384), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(10.0222225), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06300470344722271), 'actor_loss': np.float64(-0.9599391460418701), 'hyper_actor_loss': np.float64(0.000302856856433209), 'behavior_loss': np.float64(0.5544414937496185)}

Episode step 47230, time diff 4.758472681045532, total time dif 14152.71455335617)
step: 47230 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(4.4562044), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0781767500564456), 'actor_loss': np.float64(-0.9529945611953735), 'hyper_actor_loss': np.float64(0.00015243503585224972), 'behavior_loss': np.float64(0.5425249397754669)}

Episode step 47240, time diff 4.690744638442993, total time dif 14157.473026037216)
step: 47240 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(2.4853625), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08574585765600204), 'actor_loss': np.float64(-0.9859367072582245), 'hyper_actor_loss': np.float64(9.230398791260086e-05), 'behavior_loss': np.float64(0.5369020879268647)}

Episode step 47250, time diff 4.6757824420928955, total time dif 14162.16377067566)
step: 47250 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(3.3297737), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0725886769592762), 'actor_loss': np.float64(-0.9935546100139618), 'hyper_actor_loss': np.float64(6.872404701425694e-05), 'behavior_loss': np.float64(0.5377955555915832)}

Episode step 47260, time diff 4.632034063339233, total time dif 14166.839553117752)
step: 47260 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.5707712), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08303582482039928), 'actor_loss': np.float64(-0.9709372758865357), 'hyper_actor_loss': np.float64(5.3919589117867875e-05), 'behavior_loss': np.float64(0.5352608859539032)}

Episode step 47270, time diff 4.708226203918457, total time dif 14171.471587181091)
step: 47270 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(1.8844258), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0734501402825117), 'actor_loss': np.float64(-0.9836901009082795), 'hyper_actor_loss': np.float64(4.3120452028233557e-05), 'behavior_loss': np.float64(0.535009217262268)}

Episode step 47280, time diff 4.629570960998535, total time dif 14176.17981338501)
step: 47280 @ episode report: {'average_total_reward': np.float32(10.983334), 'reward_variance': np.float32(3.352921), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07249760590493678), 'actor_loss': np.float64(-0.9666643857955932), 'hyper_actor_loss': np.float64(3.5982377994514536e-05), 'behavior_loss': np.float64(0.5324548721313477)}

Episode step 47290, time diff 4.627930641174316, total time dif 14180.809384346008)
step: 47290 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(1.5901781), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07257302738726139), 'actor_loss': np.float64(-0.9734912633895874), 'hyper_actor_loss': np.float64(3.08189204588416e-05), 'behavior_loss': np.float64(0.5332923650741577)}

Episode step 47300, time diff 4.848539590835571, total time dif 14185.437314987183)
step: 47300 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(1.8596312), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06516341306269169), 'actor_loss': np.float64(-0.9548750758171082), 'hyper_actor_loss': np.float64(3.1797753217688295e-05), 'behavior_loss': np.float64(0.5352529525756836)}

Episode step 47310, time diff 4.626237630844116, total time dif 14190.285854578018)
step: 47310 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(2.5104012), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07764836102724075), 'actor_loss': np.float64(-0.962791132926941), 'hyper_actor_loss': np.float64(4.159932068432681e-05), 'behavior_loss': np.float64(0.527620667219162)}

Episode step 47320, time diff 4.59075665473938, total time dif 14194.912092208862)
step: 47320 @ episode report: {'average_total_reward': np.float32(10.361112), 'reward_variance': np.float32(2.681192), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06451075151562691), 'actor_loss': np.float64(-0.9692712128162384), 'hyper_actor_loss': np.float64(5.3879459665040484e-05), 'behavior_loss': np.float64(0.5291797161102295)}

Episode step 47330, time diff 4.6450355052948, total time dif 14199.502848863602)
step: 47330 @ episode report: {'average_total_reward': np.float32(10.124445), 'reward_variance': np.float32(1.8942913), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06750617269426584), 'actor_loss': np.float64(-0.9523948013782502), 'hyper_actor_loss': np.float64(6.193015615281183e-05), 'behavior_loss': np.float64(0.5260217249393463)}

Episode step 47340, time diff 4.577877521514893, total time dif 14204.147884368896)
step: 47340 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(2.1513896), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08460639081895352), 'actor_loss': np.float64(-0.9886277973651886), 'hyper_actor_loss': np.float64(6.669810536550358e-05), 'behavior_loss': np.float64(0.5279888272285461)}

Episode step 47350, time diff 4.633744478225708, total time dif 14208.725761890411)
step: 47350 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(4.111567), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07041627503931522), 'actor_loss': np.float64(-0.9799973547458649), 'hyper_actor_loss': np.float64(6.60366153169889e-05), 'behavior_loss': np.float64(0.5247360050678254)}

Episode step 47360, time diff 4.62849760055542, total time dif 14213.359506368637)
step: 47360 @ episode report: {'average_total_reward': np.float32(10.473333), 'reward_variance': np.float32(3.7309685), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06987321302294731), 'actor_loss': np.float64(-0.9751980185508728), 'hyper_actor_loss': np.float64(7.664604345336556e-05), 'behavior_loss': np.float64(0.5246036887168884)}

Episode step 47370, time diff 4.585509538650513, total time dif 14217.988003969193)
step: 47370 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(3.720426), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07994319535791874), 'actor_loss': np.float64(-0.9907739043235779), 'hyper_actor_loss': np.float64(7.096438348526135e-05), 'behavior_loss': np.float64(0.5235773921012878)}

Episode step 47380, time diff 4.568662405014038, total time dif 14222.573513507843)
step: 47380 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(2.2416897), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06699489541351795), 'actor_loss': np.float64(-0.9922371625900268), 'hyper_actor_loss': np.float64(5.328019979060628e-05), 'behavior_loss': np.float64(0.5292418897151947)}

Episode step 47390, time diff 4.638288736343384, total time dif 14227.142175912857)
step: 47390 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.225705), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06884171459823847), 'actor_loss': np.float64(-0.9624509632587432), 'hyper_actor_loss': np.float64(4.101462473045103e-05), 'behavior_loss': np.float64(0.5259681940078735)}

Episode step 47400, time diff 4.586361408233643, total time dif 14231.7804646492)
step: 47400 @ episode report: {'average_total_reward': np.float32(9.824446), 'reward_variance': np.float32(2.008538), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07117967680096626), 'actor_loss': np.float64(-0.9817968666553497), 'hyper_actor_loss': np.float64(3.459907275100704e-05), 'behavior_loss': np.float64(0.5239158749580384)}

Episode step 47410, time diff 4.596460342407227, total time dif 14236.366826057434)
step: 47410 @ episode report: {'average_total_reward': np.float32(11.007779), 'reward_variance': np.float32(5.1203475), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06418168693780898), 'actor_loss': np.float64(-0.9884111940860748), 'hyper_actor_loss': np.float64(3.0095728288870304e-05), 'behavior_loss': np.float64(0.5260901331901551)}

Episode step 47420, time diff 4.574818134307861, total time dif 14240.963286399841)
step: 47420 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(3.2539616), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06767329685389996), 'actor_loss': np.float64(-0.9538873076438904), 'hyper_actor_loss': np.float64(2.92811890176381e-05), 'behavior_loss': np.float64(0.5257446527481079)}

Episode step 47430, time diff 4.693564414978027, total time dif 14245.53810453415)
step: 47430 @ episode report: {'average_total_reward': np.float32(11.544446), 'reward_variance': np.float32(2.5098767), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07167745977640153), 'actor_loss': np.float64(-0.9673047423362732), 'hyper_actor_loss': np.float64(2.9959873972984497e-05), 'behavior_loss': np.float64(0.5241514682769776)}

Episode step 47440, time diff 4.627846717834473, total time dif 14250.231668949127)
step: 47440 @ episode report: {'average_total_reward': np.float32(11.7322235), 'reward_variance': np.float32(1.739838), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(9.900002), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06759794130921364), 'actor_loss': np.float64(-0.9864260375499725), 'hyper_actor_loss': np.float64(2.866350314434385e-05), 'behavior_loss': np.float64(0.523059856891632)}

Episode step 47450, time diff 4.684042692184448, total time dif 14254.859515666962)
step: 47450 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(2.1137795), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06533398814499378), 'actor_loss': np.float64(-0.9620676457881927), 'hyper_actor_loss': np.float64(2.8672468943113927e-05), 'behavior_loss': np.float64(0.522934740781784)}

Episode step 47460, time diff 4.603219747543335, total time dif 14259.543558359146)
step: 47460 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(0.8650631), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07577640861272812), 'actor_loss': np.float64(-0.9737426340579987), 'hyper_actor_loss': np.float64(3.4449122176738455e-05), 'behavior_loss': np.float64(0.5241408050060272)}

Episode step 47470, time diff 4.803935766220093, total time dif 14264.14677810669)
step: 47470 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(3.217057), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677872434258461), 'actor_loss': np.float64(-0.9930061995983124), 'hyper_actor_loss': np.float64(3.400748955755262e-05), 'behavior_loss': np.float64(0.5245664596557618)}

Episode step 47480, time diff 4.727946996688843, total time dif 14268.95071387291)
step: 47480 @ episode report: {'average_total_reward': np.float32(10.185555), 'reward_variance': np.float32(3.731507), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062083658017218116), 'actor_loss': np.float64(-0.9600487053394318), 'hyper_actor_loss': np.float64(3.1612824386684224e-05), 'behavior_loss': np.float64(0.515493381023407)}

Episode step 47490, time diff 4.6551971435546875, total time dif 14273.678660869598)
step: 47490 @ episode report: {'average_total_reward': np.float32(11.283335), 'reward_variance': np.float32(2.5513148), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07500194385647774), 'actor_loss': np.float64(-0.9869655787944793), 'hyper_actor_loss': np.float64(4.377600271254778e-05), 'behavior_loss': np.float64(0.5247511208057404)}

Episode step 47500, time diff 4.608710050582886, total time dif 14278.333858013153)
step: 47500 @ episode report: {'average_total_reward': np.float32(10.934444), 'reward_variance': np.float32(2.2489493), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08272646814584732), 'actor_loss': np.float64(-0.9885396540164948), 'hyper_actor_loss': np.float64(4.6363903311430474e-05), 'behavior_loss': np.float64(0.5259792327880859)}

Episode step 47510, time diff 4.696691036224365, total time dif 14282.942568063736)
step: 47510 @ episode report: {'average_total_reward': np.float32(10.224444), 'reward_variance': np.float32(1.1897482), 'max_total_reward': np.float32(11.777778), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.065128780528903), 'actor_loss': np.float64(-0.9762145102024078), 'hyper_actor_loss': np.float64(3.5818378091789785e-05), 'behavior_loss': np.float64(0.5195436358451844)}

Episode step 47520, time diff 4.652162075042725, total time dif 14287.63925909996)
step: 47520 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(1.0589931), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07488960511982441), 'actor_loss': np.float64(-0.9675959289073944), 'hyper_actor_loss': np.float64(3.312579301564256e-05), 'behavior_loss': np.float64(0.5146140575408935)}

Episode step 47530, time diff 4.6804516315460205, total time dif 14292.291421175003)
step: 47530 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(1.3243568), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07042403668165206), 'actor_loss': np.float64(-0.9846857726573944), 'hyper_actor_loss': np.float64(3.7600409268634397e-05), 'behavior_loss': np.float64(0.5163123428821563)}

Episode step 47540, time diff 4.623778343200684, total time dif 14296.971872806549)
step: 47540 @ episode report: {'average_total_reward': np.float32(10.671112), 'reward_variance': np.float32(1.8213885), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0729966226965189), 'actor_loss': np.float64(-0.9758677065372467), 'hyper_actor_loss': np.float64(3.46185468515614e-05), 'behavior_loss': np.float64(0.516001182794571)}

Episode step 47550, time diff 4.653141260147095, total time dif 14301.59565114975)
step: 47550 @ episode report: {'average_total_reward': np.float32(11.083334), 'reward_variance': np.float32(1.8217342), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06512699089944363), 'actor_loss': np.float64(-0.9786256432533265), 'hyper_actor_loss': np.float64(3.758201019081753e-05), 'behavior_loss': np.float64(0.5118118107318879)}

Episode step 47560, time diff 4.613661766052246, total time dif 14306.248792409897)
step: 47560 @ episode report: {'average_total_reward': np.float32(12.891111), 'reward_variance': np.float32(2.197649), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(10.022223), 'average_n_step': np.float32(13.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06385492496192455), 'actor_loss': np.float64(-0.9663272440433502), 'hyper_actor_loss': np.float64(4.4714444811688736e-05), 'behavior_loss': np.float64(0.5152739405632019)}

Episode step 47570, time diff 4.632508039474487, total time dif 14310.86245417595)
step: 47570 @ episode report: {'average_total_reward': np.float32(11.744445), 'reward_variance': np.float32(4.363284), 'max_total_reward': np.float32(15.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07026928514242173), 'actor_loss': np.float64(-0.9568906605243683), 'hyper_actor_loss': np.float64(5.12392507516779e-05), 'behavior_loss': np.float64(0.5105136573314667)}

Episode step 47580, time diff 4.584694147109985, total time dif 14315.494962215424)
step: 47580 @ episode report: {'average_total_reward': np.float32(9.7), 'reward_variance': np.float32(1.1186172), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06789325922727585), 'actor_loss': np.float64(-0.9809993505477905), 'hyper_actor_loss': np.float64(4.508093043114059e-05), 'behavior_loss': np.float64(0.5132541954517365)}

Episode step 47590, time diff 4.6120030879974365, total time dif 14320.079656362534)
step: 47590 @ episode report: {'average_total_reward': np.float32(10.285555), 'reward_variance': np.float32(4.5474577), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0700879018753767), 'actor_loss': np.float64(-0.9751466274261474), 'hyper_actor_loss': np.float64(3.981755398854148e-05), 'behavior_loss': np.float64(0.5134107768535614)}

Episode step 47600, time diff 4.62996506690979, total time dif 14324.691659450531)
step: 47600 @ episode report: {'average_total_reward': np.float32(9.500001), 'reward_variance': np.float32(3.0959277), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07189505062997341), 'actor_loss': np.float64(-0.9733781933784484), 'hyper_actor_loss': np.float64(3.459447798377368e-05), 'behavior_loss': np.float64(0.5082548201084137)}

Episode step 47610, time diff 4.598657131195068, total time dif 14329.32162451744)
step: 47610 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(3.5430622), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08230161704123021), 'actor_loss': np.float64(-0.9862396001815796), 'hyper_actor_loss': np.float64(3.223821349820355e-05), 'behavior_loss': np.float64(0.510757964849472)}

Episode step 47620, time diff 4.614478349685669, total time dif 14333.920281648636)
step: 47620 @ episode report: {'average_total_reward': np.float32(10.907779), 'reward_variance': np.float32(3.6961987), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07496759817004203), 'actor_loss': np.float64(-0.9922406077384949), 'hyper_actor_loss': np.float64(3.08825710817473e-05), 'behavior_loss': np.float64(0.5077537834644318)}

Episode step 47630, time diff 4.61151647567749, total time dif 14338.534759998322)
step: 47630 @ episode report: {'average_total_reward': np.float32(10.185556), 'reward_variance': np.float32(1.4097544), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06086770109832287), 'actor_loss': np.float64(-0.9796140730381012), 'hyper_actor_loss': np.float64(2.7435249648988247e-05), 'behavior_loss': np.float64(0.5043617397546768)}

Episode step 47640, time diff 4.766155481338501, total time dif 14343.146276473999)
step: 47640 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(1.3746963), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08543914370238781), 'actor_loss': np.float64(-0.978807246685028), 'hyper_actor_loss': np.float64(3.4049097303068264e-05), 'behavior_loss': np.float64(0.5009569734334945)}

Episode step 47650, time diff 4.556215524673462, total time dif 14347.912431955338)
step: 47650 @ episode report: {'average_total_reward': np.float32(11.46889), 'reward_variance': np.float32(2.03718), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0748130053281784), 'actor_loss': np.float64(-1.0123363971710204), 'hyper_actor_loss': np.float64(3.723141308000777e-05), 'behavior_loss': np.float64(0.5034110337495804)}

Episode step 47660, time diff 4.6588640213012695, total time dif 14352.468647480011)
step: 47660 @ episode report: {'average_total_reward': np.float32(12.068889), 'reward_variance': np.float32(3.4050822), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07369553744792938), 'actor_loss': np.float64(-0.9875135838985443), 'hyper_actor_loss': np.float64(3.892506792908534e-05), 'behavior_loss': np.float64(0.5007252126932145)}

Episode step 47670, time diff 4.589359760284424, total time dif 14357.127511501312)
step: 47670 @ episode report: {'average_total_reward': np.float32(10.983335), 'reward_variance': np.float32(5.9295373), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07400980442762375), 'actor_loss': np.float64(-0.9804706513881684), 'hyper_actor_loss': np.float64(3.903729739249684e-05), 'behavior_loss': np.float64(0.5016537189483643)}

Episode step 47680, time diff 4.608194589614868, total time dif 14361.716871261597)
step: 47680 @ episode report: {'average_total_reward': np.float32(10.246668), 'reward_variance': np.float32(2.468392), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07924097776412964), 'actor_loss': np.float64(-0.9960130214691162), 'hyper_actor_loss': np.float64(3.489757909846958e-05), 'behavior_loss': np.float64(0.5020806699991226)}

Episode step 47690, time diff 4.570134162902832, total time dif 14366.325065851212)
step: 47690 @ episode report: {'average_total_reward': np.float32(11.993335), 'reward_variance': np.float32(1.7218319), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(9.900002), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07415433377027511), 'actor_loss': np.float64(-0.9966707706451416), 'hyper_actor_loss': np.float64(3.0969103681854905e-05), 'behavior_loss': np.float64(0.5036086112260818)}

Episode step 47700, time diff 4.575433969497681, total time dif 14370.895200014114)
step: 47700 @ episode report: {'average_total_reward': np.float32(11.607779), 'reward_variance': np.float32(2.6773846), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07483904678374528), 'actor_loss': np.float64(-0.9821747899055481), 'hyper_actor_loss': np.float64(3.341856172482949e-05), 'behavior_loss': np.float64(0.49890572428703306)}

Episode step 47710, time diff 4.655121088027954, total time dif 14375.470633983612)
step: 47710 @ episode report: {'average_total_reward': np.float32(10.585556), 'reward_variance': np.float32(3.3400753), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08407399728894234), 'actor_loss': np.float64(-0.9873972713947297), 'hyper_actor_loss': np.float64(3.702970316226129e-05), 'behavior_loss': np.float64(0.49872898161411283)}

Episode step 47720, time diff 4.614292144775391, total time dif 14380.12575507164)
step: 47720 @ episode report: {'average_total_reward': np.float32(9.587779), 'reward_variance': np.float32(2.7776406), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06368251591920852), 'actor_loss': np.float64(-0.9954989612102508), 'hyper_actor_loss': np.float64(3.421459368837532e-05), 'behavior_loss': np.float64(0.49276362657546996)}

Episode step 47730, time diff 4.6083362102508545, total time dif 14384.740047216415)
step: 47730 @ episode report: {'average_total_reward': np.float32(12.854446), 'reward_variance': np.float32(2.89048), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(13.6), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07799947373569012), 'actor_loss': np.float64(-0.9968198239803314), 'hyper_actor_loss': np.float64(3.215514188923407e-05), 'behavior_loss': np.float64(0.49599337577819824)}

Episode step 47740, time diff 4.644300222396851, total time dif 14389.348383426666)
step: 47740 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(0.6424249), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(9.777778), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06807210706174374), 'actor_loss': np.float64(-0.995938903093338), 'hyper_actor_loss': np.float64(3.562010715540964e-05), 'behavior_loss': np.float64(0.4923556923866272)}

Episode step 47750, time diff 4.620528221130371, total time dif 14393.992683649063)
step: 47750 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.9053082), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08205367773771285), 'actor_loss': np.float64(-0.9817058324813843), 'hyper_actor_loss': np.float64(4.3902180550503545e-05), 'behavior_loss': np.float64(0.4985199153423309)}

Episode step 47760, time diff 4.6153178215026855, total time dif 14398.613211870193)
step: 47760 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(2.3467112), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07633656710386276), 'actor_loss': np.float64(-0.9875430524349212), 'hyper_actor_loss': np.float64(4.9137269525090235e-05), 'behavior_loss': np.float64(0.4963474482297897)}

Episode step 47770, time diff 4.7051920890808105, total time dif 14403.228529691696)
step: 47770 @ episode report: {'average_total_reward': np.float32(10.85889), 'reward_variance': np.float32(3.3240008), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06808057315647602), 'actor_loss': np.float64(-0.9878307163715363), 'hyper_actor_loss': np.float64(4.90035155962687e-05), 'behavior_loss': np.float64(0.4928939461708069)}

Episode step 47780, time diff 4.747668743133545, total time dif 14407.933721780777)
step: 47780 @ episode report: {'average_total_reward': np.float32(11.183333), 'reward_variance': np.float32(3.772377), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0722048245370388), 'actor_loss': np.float64(-0.9755187690258026), 'hyper_actor_loss': np.float64(5.694447063433472e-05), 'behavior_loss': np.float64(0.4960095942020416)}

Episode step 47790, time diff 4.687269449234009, total time dif 14412.68139052391)
step: 47790 @ episode report: {'average_total_reward': np.float32(11.207779), 'reward_variance': np.float32(0.9383963), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06335245929658413), 'actor_loss': np.float64(-0.9731253206729888), 'hyper_actor_loss': np.float64(6.630539937759749e-05), 'behavior_loss': np.float64(0.49404671490192414)}

Episode step 47800, time diff 4.692582130432129, total time dif 14417.368659973145)
step: 47800 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(5.629154), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06815504059195518), 'actor_loss': np.float64(-0.9695360362529755), 'hyper_actor_loss': np.float64(6.065979650884401e-05), 'behavior_loss': np.float64(0.4955666780471802)}

Episode step 47810, time diff 4.8224945068359375, total time dif 14422.061242103577)
step: 47810 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(0.81571466), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.900002), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08549902699887753), 'actor_loss': np.float64(-0.9857755124568939), 'hyper_actor_loss': np.float64(5.031811087974347e-05), 'behavior_loss': np.float64(0.49095745384693146)}

Episode step 47820, time diff 4.589352607727051, total time dif 14426.883736610413)
step: 47820 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.3133888), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07987409867346287), 'actor_loss': np.float64(-1.0075159311294555), 'hyper_actor_loss': np.float64(3.7231687201710884e-05), 'behavior_loss': np.float64(0.49343679249286654)}

Episode step 47830, time diff 4.621586322784424, total time dif 14431.47308921814)
step: 47830 @ episode report: {'average_total_reward': np.float32(10.795557), 'reward_variance': np.float32(3.2526715), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08512384369969368), 'actor_loss': np.float64(-0.9908956110477447), 'hyper_actor_loss': np.float64(2.6640591750037855e-05), 'behavior_loss': np.float64(0.48969101905822754)}

Episode step 47840, time diff 4.588892459869385, total time dif 14436.094675540924)
step: 47840 @ episode report: {'average_total_reward': np.float32(11.86889), 'reward_variance': np.float32(4.8022165), 'max_total_reward': np.float32(16.633333), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(17.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07489248737692833), 'actor_loss': np.float64(-0.992059737443924), 'hyper_actor_loss': np.float64(2.4584843413322232e-05), 'behavior_loss': np.float64(0.4866421282291412)}

Episode step 47850, time diff 4.611105442047119, total time dif 14440.683568000793)
step: 47850 @ episode report: {'average_total_reward': np.float32(11.095556), 'reward_variance': np.float32(4.2994375), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05100969281047583), 'actor_loss': np.float64(-0.9712393522262573), 'hyper_actor_loss': np.float64(2.427176841592882e-05), 'behavior_loss': np.float64(0.48774268925189973)}

Episode step 47860, time diff 4.614792108535767, total time dif 14445.29467344284)
step: 47860 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(2.7472212), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07725106291472912), 'actor_loss': np.float64(-0.9481245756149292), 'hyper_actor_loss': np.float64(2.6352046188549138e-05), 'behavior_loss': np.float64(0.4930447727441788)}

Episode step 47870, time diff 4.542083740234375, total time dif 14449.909465551376)
step: 47870 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(2.5667918), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06897293590009212), 'actor_loss': np.float64(-0.9629075706005097), 'hyper_actor_loss': np.float64(2.571410186646972e-05), 'behavior_loss': np.float64(0.49122112691402436)}

Episode step 47880, time diff 4.545097589492798, total time dif 14454.45154929161)
step: 47880 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(5.100012), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06864921562373638), 'actor_loss': np.float64(-0.9836270809173584), 'hyper_actor_loss': np.float64(2.535471248847898e-05), 'behavior_loss': np.float64(0.4872130811214447)}

Episode step 47890, time diff 4.6281867027282715, total time dif 14458.996646881104)
step: 47890 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.2092457), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07649630345404149), 'actor_loss': np.float64(-0.9857315063476563), 'hyper_actor_loss': np.float64(2.4988296172523405e-05), 'behavior_loss': np.float64(0.48954635560512544)}

Episode step 47900, time diff 4.611537456512451, total time dif 14463.624833583832)
step: 47900 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(4.333986), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08242415487766266), 'actor_loss': np.float64(-0.991471242904663), 'hyper_actor_loss': np.float64(2.732094271777896e-05), 'behavior_loss': np.float64(0.49304035007953645)}

Episode step 47910, time diff 4.67080283164978, total time dif 14468.236371040344)
step: 47910 @ episode report: {'average_total_reward': np.float32(11.295557), 'reward_variance': np.float32(2.3188443), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06642237231135369), 'actor_loss': np.float64(-0.9889954805374146), 'hyper_actor_loss': np.float64(3.872266170219518e-05), 'behavior_loss': np.float64(0.48564260005950927)}

Episode step 47920, time diff 4.7798237800598145, total time dif 14472.907173871994)
step: 47920 @ episode report: {'average_total_reward': np.float32(10.197779), 'reward_variance': np.float32(4.0161433), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0644317165017128), 'actor_loss': np.float64(-0.9733731985092163), 'hyper_actor_loss': np.float64(5.988384364172816e-05), 'behavior_loss': np.float64(0.4929483860731125)}

Episode step 47930, time diff 4.635949373245239, total time dif 14477.686997652054)
step: 47930 @ episode report: {'average_total_reward': np.float32(10.971112), 'reward_variance': np.float32(3.3533149), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06973705291748047), 'actor_loss': np.float64(-0.9714251279830932), 'hyper_actor_loss': np.float64(6.346477639453951e-05), 'behavior_loss': np.float64(0.4862035483121872)}

Episode step 47940, time diff 4.691575527191162, total time dif 14482.322947025299)
step: 47940 @ episode report: {'average_total_reward': np.float32(10.871111), 'reward_variance': np.float32(2.0589929), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0804816510528326), 'actor_loss': np.float64(-0.9776612758636475), 'hyper_actor_loss': np.float64(5.3860812840866856e-05), 'behavior_loss': np.float64(0.48983804881572723)}

Episode step 47950, time diff 4.665051460266113, total time dif 14487.01452255249)
step: 47950 @ episode report: {'average_total_reward': np.float32(10.285556), 'reward_variance': np.float32(2.8940516), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06676466017961502), 'actor_loss': np.float64(-0.9946077942848206), 'hyper_actor_loss': np.float64(4.740426375064999e-05), 'behavior_loss': np.float64(0.4803192228078842)}

Episode step 47960, time diff 4.634088516235352, total time dif 14491.679574012756)
step: 47960 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(1.7851902), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07592580020427704), 'actor_loss': np.float64(-0.9881738066673279), 'hyper_actor_loss': np.float64(4.242007162247319e-05), 'behavior_loss': np.float64(0.4856789469718933)}

Episode step 47970, time diff 4.598794937133789, total time dif 14496.313662528992)
step: 47970 @ episode report: {'average_total_reward': np.float32(11.532223), 'reward_variance': np.float32(3.3010736), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(12.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07614735402166843), 'actor_loss': np.float64(-0.9879307746887207), 'hyper_actor_loss': np.float64(3.95439248677576e-05), 'behavior_loss': np.float64(0.4857538491487503)}

Episode step 47980, time diff 4.894180774688721, total time dif 14500.912457466125)
step: 47980 @ episode report: {'average_total_reward': np.float32(10.783335), 'reward_variance': np.float32(3.8395126), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07901759631931782), 'actor_loss': np.float64(-0.9929097056388855), 'hyper_actor_loss': np.float64(3.912915344699286e-05), 'behavior_loss': np.float64(0.48253896832466125)}

Episode step 47990, time diff 4.735069990158081, total time dif 14505.806638240814)
step: 47990 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(3.7766716), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06765098609030247), 'actor_loss': np.float64(-0.98192178606987), 'hyper_actor_loss': np.float64(4.7228163384716024e-05), 'behavior_loss': np.float64(0.4772179782390594)}

Episode step 48000, time diff 4.695464849472046, total time dif 14510.541708230972)
step: 48000 @ episode report: {'average_total_reward': np.float32(11.768889), 'reward_variance': np.float32(2.0006373), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06997326165437698), 'actor_loss': np.float64(-0.9813029706478119), 'hyper_actor_loss': np.float64(4.0931428884505294e-05), 'behavior_loss': np.float64(0.48461792767047884)}

Episode step 48010, time diff 4.6967480182647705, total time dif 14515.237173080444)
step: 48010 @ episode report: {'average_total_reward': np.float32(10.410001), 'reward_variance': np.float32(3.1639123), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07242951281368733), 'actor_loss': np.float64(-0.988774985074997), 'hyper_actor_loss': np.float64(4.8622043323121034e-05), 'behavior_loss': np.float64(0.4742270678281784)}

Episode step 48020, time diff 4.691148281097412, total time dif 14519.93392109871)
step: 48020 @ episode report: {'average_total_reward': np.float32(11.86889), 'reward_variance': np.float32(1.7687362), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(9.9), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06927525028586387), 'actor_loss': np.float64(-0.9911245703697205), 'hyper_actor_loss': np.float64(5.6229882829939015e-05), 'behavior_loss': np.float64(0.4783612072467804)}

Episode step 48030, time diff 4.7487452030181885, total time dif 14524.625069379807)
step: 48030 @ episode report: {'average_total_reward': np.float32(11.9811125), 'reward_variance': np.float32(4.0138035), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07145717591047288), 'actor_loss': np.float64(-0.9806169390678405), 'hyper_actor_loss': np.float64(5.5921321109053676e-05), 'behavior_loss': np.float64(0.48428726494312285)}

Episode step 48040, time diff 4.809669494628906, total time dif 14529.373814582825)
step: 48040 @ episode report: {'average_total_reward': np.float32(11.320001), 'reward_variance': np.float32(3.5288594), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07345674745738506), 'actor_loss': np.float64(-0.9781851768493652), 'hyper_actor_loss': np.float64(5.6268717162311074e-05), 'behavior_loss': np.float64(0.48050072491168977)}

Episode step 48050, time diff 4.788719415664673, total time dif 14534.183484077454)
step: 48050 @ episode report: {'average_total_reward': np.float32(11.42), 'reward_variance': np.float32(3.225132), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05771256498992443), 'actor_loss': np.float64(-0.9727115273475647), 'hyper_actor_loss': np.float64(5.103195180709008e-05), 'behavior_loss': np.float64(0.4772789657115936)}

Episode step 48060, time diff 4.776259899139404, total time dif 14538.972203493118)
step: 48060 @ episode report: {'average_total_reward': np.float32(11.171112), 'reward_variance': np.float32(3.7502284), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07319297194480896), 'actor_loss': np.float64(-0.9671285688877106), 'hyper_actor_loss': np.float64(5.6481279534637e-05), 'behavior_loss': np.float64(0.47382586896419526)}

Episode step 48070, time diff 4.704461336135864, total time dif 14543.748463392258)
step: 48070 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(2.6551425), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07367147356271744), 'actor_loss': np.float64(-0.9830223858356476), 'hyper_actor_loss': np.float64(6.358241589623503e-05), 'behavior_loss': np.float64(0.4760417342185974)}

Episode step 48080, time diff 4.745394945144653, total time dif 14548.452924728394)
step: 48080 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(3.471412), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.777779), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07326227836310864), 'actor_loss': np.float64(-0.9865600049495697), 'hyper_actor_loss': np.float64(5.725117844121996e-05), 'behavior_loss': np.float64(0.4749316036701202)}

Episode step 48090, time diff 4.760681390762329, total time dif 14553.198319673538)
step: 48090 @ episode report: {'average_total_reward': np.float32(10.558889), 'reward_variance': np.float32(2.5566936), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07035458534955978), 'actor_loss': np.float64(-0.9797206997871399), 'hyper_actor_loss': np.float64(5.377081615733914e-05), 'behavior_loss': np.float64(0.4723219692707062)}

Episode step 48100, time diff 4.719758987426758, total time dif 14557.9590010643)
step: 48100 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(2.4252107), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.062364180013537406), 'actor_loss': np.float64(-0.9794303834438324), 'hyper_actor_loss': np.float64(4.228051511745434e-05), 'behavior_loss': np.float64(0.47097540497779844)}

Episode step 48110, time diff 4.71123194694519, total time dif 14562.678760051727)
step: 48110 @ episode report: {'average_total_reward': np.float32(10.534445), 'reward_variance': np.float32(2.3334432), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07356684915721416), 'actor_loss': np.float64(-0.981367027759552), 'hyper_actor_loss': np.float64(3.229047688364517e-05), 'behavior_loss': np.float64(0.4689182758331299)}

Episode step 48120, time diff 4.70052695274353, total time dif 14567.389991998672)
step: 48120 @ episode report: {'average_total_reward': np.float32(9.873334), 'reward_variance': np.float32(1.6743752), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07626403830945491), 'actor_loss': np.float64(-0.9947906911373139), 'hyper_actor_loss': np.float64(3.3563711440365296e-05), 'behavior_loss': np.float64(0.4674079269170761)}

Episode step 48130, time diff 4.690978765487671, total time dif 14572.090518951416)
step: 48130 @ episode report: {'average_total_reward': np.float32(9.861112), 'reward_variance': np.float32(1.9593647), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06873428076505661), 'actor_loss': np.float64(-0.985122162103653), 'hyper_actor_loss': np.float64(3.4887109723058526e-05), 'behavior_loss': np.float64(0.4722199112176895)}

Episode step 48140, time diff 4.8852105140686035, total time dif 14576.781497716904)
step: 48140 @ episode report: {'average_total_reward': np.float32(10.995557), 'reward_variance': np.float32(6.068993), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.067270902171731), 'actor_loss': np.float64(-0.9782730162143707), 'hyper_actor_loss': np.float64(4.035236524941865e-05), 'behavior_loss': np.float64(0.4658660441637039)}

Episode step 48150, time diff 4.689134836196899, total time dif 14581.666708230972)
step: 48150 @ episode report: {'average_total_reward': np.float32(10.646668), 'reward_variance': np.float32(2.4719706), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08555807024240494), 'actor_loss': np.float64(-0.9879353404045105), 'hyper_actor_loss': np.float64(4.998896074539516e-05), 'behavior_loss': np.float64(0.4696211308240891)}

Episode step 48160, time diff 4.707003593444824, total time dif 14586.35584306717)
step: 48160 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(4.103921), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08238422274589538), 'actor_loss': np.float64(-1.0079281091690064), 'hyper_actor_loss': np.float64(4.502336669247598e-05), 'behavior_loss': np.float64(0.4681712746620178)}

Episode step 48170, time diff 4.633120536804199, total time dif 14591.062846660614)
step: 48170 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(3.606094), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07372150085866451), 'actor_loss': np.float64(-0.998407244682312), 'hyper_actor_loss': np.float64(3.3862675991258584e-05), 'behavior_loss': np.float64(0.4575799435377121)}

Episode step 48180, time diff 4.686960697174072, total time dif 14595.695967197418)
step: 48180 @ episode report: {'average_total_reward': np.float32(11.183334), 'reward_variance': np.float32(4.2626476), 'max_total_reward': np.float32(14.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07362662330269813), 'actor_loss': np.float64(-0.9877562642097473), 'hyper_actor_loss': np.float64(3.0111928572296166e-05), 'behavior_loss': np.float64(0.45827185213565824)}

Episode step 48190, time diff 4.714255094528198, total time dif 14600.382927894592)
step: 48190 @ episode report: {'average_total_reward': np.float32(10.210001), 'reward_variance': np.float32(2.039098), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05752267986536026), 'actor_loss': np.float64(-0.9862131536006927), 'hyper_actor_loss': np.float64(2.7519321884028612e-05), 'behavior_loss': np.float64(0.46325822472572326)}

Episode step 48200, time diff 4.63219428062439, total time dif 14605.09718298912)
step: 48200 @ episode report: {'average_total_reward': np.float32(10.334445), 'reward_variance': np.float32(5.2659864), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07631031684577465), 'actor_loss': np.float64(-0.9891534149646759), 'hyper_actor_loss': np.float64(2.7010971462004817e-05), 'behavior_loss': np.float64(0.459963783621788)}

Episode step 48210, time diff 4.584494352340698, total time dif 14609.729377269745)
step: 48210 @ episode report: {'average_total_reward': np.float32(10.771112), 'reward_variance': np.float32(1.4748696), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07088727951049804), 'actor_loss': np.float64(-0.9950683236122131), 'hyper_actor_loss': np.float64(2.7174812930752524e-05), 'behavior_loss': np.float64(0.45860075056552885)}

Episode step 48220, time diff 4.636785507202148, total time dif 14614.313871622086)
step: 48220 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(0.7587667), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(9.900002), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06688288692384958), 'actor_loss': np.float64(-0.9856448352336884), 'hyper_actor_loss': np.float64(2.661347643879708e-05), 'behavior_loss': np.float64(0.4559546023607254)}

Episode step 48230, time diff 4.667396306991577, total time dif 14618.950657129288)
step: 48230 @ episode report: {'average_total_reward': np.float32(11.66889), 'reward_variance': np.float32(2.491847), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0601313091814518), 'actor_loss': np.float64(-0.9713636577129364), 'hyper_actor_loss': np.float64(2.9533128690673037e-05), 'behavior_loss': np.float64(0.46010535657405854)}

Episode step 48240, time diff 4.678134918212891, total time dif 14623.61805343628)
step: 48240 @ episode report: {'average_total_reward': np.float32(10.546667), 'reward_variance': np.float32(4.1921434), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08284987062215805), 'actor_loss': np.float64(-0.9912220239639282), 'hyper_actor_loss': np.float64(2.7749321088776923e-05), 'behavior_loss': np.float64(0.46041313111782073)}

Episode step 48250, time diff 4.640549659729004, total time dif 14628.296188354492)
step: 48250 @ episode report: {'average_total_reward': np.float32(10.485556), 'reward_variance': np.float32(4.153236), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08096237778663636), 'actor_loss': np.float64(-1.0090049743652343), 'hyper_actor_loss': np.float64(2.3842413247621154e-05), 'behavior_loss': np.float64(0.45959711670875547)}

Episode step 48260, time diff 4.713892936706543, total time dif 14632.936738014221)
step: 48260 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.6257148), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08405746184289456), 'actor_loss': np.float64(-0.995435357093811), 'hyper_actor_loss': np.float64(2.4638307877467014e-05), 'behavior_loss': np.float64(0.46432882845401763)}

Episode step 48270, time diff 4.735100030899048, total time dif 14637.650630950928)
step: 48270 @ episode report: {'average_total_reward': np.float32(11.681111), 'reward_variance': np.float32(1.549187), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(10.022222), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0647595390677452), 'actor_loss': np.float64(-0.9778485178947449), 'hyper_actor_loss': np.float64(2.6304105267627166e-05), 'behavior_loss': np.float64(0.46001286804676056)}

Episode step 48280, time diff 4.684692621231079, total time dif 14642.385730981827)
step: 48280 @ episode report: {'average_total_reward': np.float32(10.783334), 'reward_variance': np.float32(2.8240304), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07328997030854226), 'actor_loss': np.float64(-0.9715786695480346), 'hyper_actor_loss': np.float64(2.9966727925057058e-05), 'behavior_loss': np.float64(0.4541428327560425)}

Episode step 48290, time diff 4.747668504714966, total time dif 14647.070423603058)
step: 48290 @ episode report: {'average_total_reward': np.float32(11.820001), 'reward_variance': np.float32(4.3160195), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(12.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08126155398786068), 'actor_loss': np.float64(-0.996324497461319), 'hyper_actor_loss': np.float64(2.7632795536192134e-05), 'behavior_loss': np.float64(0.45901529788970946)}

Episode step 48300, time diff 4.678218126296997, total time dif 14651.818092107773)
step: 48300 @ episode report: {'average_total_reward': np.float32(11.307779), 'reward_variance': np.float32(4.9908915), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07189086526632309), 'actor_loss': np.float64(-0.998149448633194), 'hyper_actor_loss': np.float64(2.776241617539199e-05), 'behavior_loss': np.float64(0.4578486800193787)}

Episode step 48310, time diff 4.906293869018555, total time dif 14656.49631023407)
step: 48310 @ episode report: {'average_total_reward': np.float32(10.65889), 'reward_variance': np.float32(3.0156808), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07490895316004753), 'actor_loss': np.float64(-0.9734529197216034), 'hyper_actor_loss': np.float64(3.81726957130013e-05), 'behavior_loss': np.float64(0.4556077539920807)}

Episode step 48320, time diff 4.721532106399536, total time dif 14661.402604103088)
step: 48320 @ episode report: {'average_total_reward': np.float32(10.6466675), 'reward_variance': np.float32(1.0479963), 'max_total_reward': np.float32(12.144446), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0762631069868803), 'actor_loss': np.float64(-0.9888442873954773), 'hyper_actor_loss': np.float64(6.377791578415781e-05), 'behavior_loss': np.float64(0.4402959644794464)}

Episode step 48330, time diff 4.716116905212402, total time dif 14666.124136209488)
step: 48330 @ episode report: {'average_total_reward': np.float32(10.297778), 'reward_variance': np.float32(3.4445882), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07381824962794781), 'actor_loss': np.float64(-1.0006576955318451), 'hyper_actor_loss': np.float64(8.210620435420424e-05), 'behavior_loss': np.float64(0.4408399909734726)}

Episode step 48340, time diff 4.782773017883301, total time dif 14670.8402531147)
step: 48340 @ episode report: {'average_total_reward': np.float32(9.961111), 'reward_variance': np.float32(2.5254877), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08180109374225139), 'actor_loss': np.float64(-1.0027931392192841), 'hyper_actor_loss': np.float64(8.192120658350177e-05), 'behavior_loss': np.float64(0.4412795573472977)}

Episode step 48350, time diff 4.805981397628784, total time dif 14675.623026132584)
step: 48350 @ episode report: {'average_total_reward': np.float32(9.512222), 'reward_variance': np.float32(4.4487276), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07575787082314492), 'actor_loss': np.float64(-1.0020383715629577), 'hyper_actor_loss': np.float64(8.29784170491621e-05), 'behavior_loss': np.float64(0.4422444373369217)}

Episode step 48360, time diff 4.781074285507202, total time dif 14680.429007530212)
step: 48360 @ episode report: {'average_total_reward': np.float32(8.292223), 'reward_variance': np.float32(1.021581), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(6.6555567), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07730066254734994), 'actor_loss': np.float64(-0.9870921492576599), 'hyper_actor_loss': np.float64(7.168592055677437e-05), 'behavior_loss': np.float64(0.4476127952337265)}

Episode step 48370, time diff 4.859199047088623, total time dif 14685.21008181572)
step: 48370 @ episode report: {'average_total_reward': np.float32(7.1922226), 'reward_variance': np.float32(4.878915), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07107135467231274), 'actor_loss': np.float64(-0.9878147065639495), 'hyper_actor_loss': np.float64(7.080162831698544e-05), 'behavior_loss': np.float64(0.44692321717739103)}

Episode step 48380, time diff 4.868044137954712, total time dif 14690.069280862808)
step: 48380 @ episode report: {'average_total_reward': np.float32(7.2922225), 'reward_variance': np.float32(1.7408159), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07295089662075042), 'actor_loss': np.float64(-0.9836547076702118), 'hyper_actor_loss': np.float64(5.576643306994811e-05), 'behavior_loss': np.float64(0.4425561040639877)}

Episode step 48390, time diff 4.761402606964111, total time dif 14694.937325000763)
step: 48390 @ episode report: {'average_total_reward': np.float32(6.88), 'reward_variance': np.float32(2.3372054), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(3.4111114), 'average_n_step': np.float32(8.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07081629298627376), 'actor_loss': np.float64(-0.9864736795425415), 'hyper_actor_loss': np.float64(4.865873379458208e-05), 'behavior_loss': np.float64(0.43582634925842284)}

Episode step 48400, time diff 4.74141526222229, total time dif 14699.698727607727)
step: 48400 @ episode report: {'average_total_reward': np.float32(6.482222), 'reward_variance': np.float32(4.274005), 'max_total_reward': np.float32(10.777778), 'min_total_reward': np.float32(4.1666665), 'average_n_step': np.float32(7.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.061476053670048714), 'actor_loss': np.float64(-0.9898718357086181), 'hyper_actor_loss': np.float64(4.069372662343085e-05), 'behavior_loss': np.float64(0.4308259129524231)}

Episode step 48410, time diff 4.691472053527832, total time dif 14704.44014286995)
step: 48410 @ episode report: {'average_total_reward': np.float32(8.228889), 'reward_variance': np.float32(3.3997822), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07737372778356075), 'actor_loss': np.float64(-0.9888509213924408), 'hyper_actor_loss': np.float64(3.947455988964066e-05), 'behavior_loss': np.float64(0.4323679685592651)}

Episode step 48420, time diff 4.637566328048706, total time dif 14709.131614923477)
step: 48420 @ episode report: {'average_total_reward': np.float32(6.582222), 'reward_variance': np.float32(0.9133134), 'max_total_reward': np.float32(8.777778), 'min_total_reward': np.float32(5.288889), 'average_n_step': np.float32(8.0), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06963823288679123), 'actor_loss': np.float64(-0.9954208493232727), 'hyper_actor_loss': np.float64(4.1935080298571846e-05), 'behavior_loss': np.float64(0.4350017815828323)}

Episode step 48430, time diff 4.6740782260894775, total time dif 14713.769181251526)
step: 48430 @ episode report: {'average_total_reward': np.float32(8.477778), 'reward_variance': np.float32(2.0968146), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07098559997975826), 'actor_loss': np.float64(-0.9886914312839508), 'hyper_actor_loss': np.float64(3.895822192134801e-05), 'behavior_loss': np.float64(0.43156246542930604)}

Episode step 48440, time diff 4.622361183166504, total time dif 14718.443259477615)
step: 48440 @ episode report: {'average_total_reward': np.float32(6.1822224), 'reward_variance': np.float32(1.8383259), 'max_total_reward': np.float32(8.655556), 'min_total_reward': np.float32(3.2888892), 'average_n_step': np.float32(7.6), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07855283692479134), 'actor_loss': np.float64(-0.9868519604206085), 'hyper_actor_loss': np.float64(3.188096761732595e-05), 'behavior_loss': np.float64(0.4301418364048004)}

Episode step 48450, time diff 4.553144931793213, total time dif 14723.065620660782)
step: 48450 @ episode report: {'average_total_reward': np.float32(8.02889), 'reward_variance': np.float32(0.52820265), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07151225730776786), 'actor_loss': np.float64(-0.9924839079380036), 'hyper_actor_loss': np.float64(3.509873859002255e-05), 'behavior_loss': np.float64(0.42992241084575655)}

Episode step 48460, time diff 4.48924994468689, total time dif 14727.618765592575)
step: 48460 @ episode report: {'average_total_reward': np.float32(6.9311113), 'reward_variance': np.float32(1.1093535), 'max_total_reward': np.float32(8.777779), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(8.3), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08088022321462632), 'actor_loss': np.float64(-1.001792949438095), 'hyper_actor_loss': np.float64(3.398426779313013e-05), 'behavior_loss': np.float64(0.4281891852617264)}

Episode step 48470, time diff 4.672702789306641, total time dif 14732.108015537262)
step: 48470 @ episode report: {'average_total_reward': np.float32(7.131111), 'reward_variance': np.float32(3.3681684), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06807441748678685), 'actor_loss': np.float64(-1.00157750248909), 'hyper_actor_loss': np.float64(3.104018069279846e-05), 'behavior_loss': np.float64(0.4236818253993988)}

Episode step 48480, time diff 4.50154709815979, total time dif 14736.780718326569)
step: 48480 @ episode report: {'average_total_reward': np.float32(7.2555566), 'reward_variance': np.float32(1.7661982), 'max_total_reward': np.float32(9.777779), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(8.6), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0695902906358242), 'actor_loss': np.float64(-0.9675940155982972), 'hyper_actor_loss': np.float64(3.061068382521625e-05), 'behavior_loss': np.float64(0.43727867603302)}

Episode step 48490, time diff 4.480141639709473, total time dif 14741.282265424728)
step: 48490 @ episode report: {'average_total_reward': np.float32(8.702222), 'reward_variance': np.float32(2.0643902), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(6.2888894), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06444830037653446), 'actor_loss': np.float64(-0.9697078049182892), 'hyper_actor_loss': np.float64(2.7340071756043472e-05), 'behavior_loss': np.float64(0.42789346575737)}

Episode step 48500, time diff 4.5513529777526855, total time dif 14745.762407064438)
step: 48500 @ episode report: {'average_total_reward': np.float32(8.365557), 'reward_variance': np.float32(3.8136902), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0754009686410427), 'actor_loss': np.float64(-0.9851821601390839), 'hyper_actor_loss': np.float64(3.155639860779047e-05), 'behavior_loss': np.float64(0.4249627888202667)}

Episode step 48510, time diff 4.4702043533325195, total time dif 14750.31376004219)
step: 48510 @ episode report: {'average_total_reward': np.float32(7.916667), 'reward_variance': np.float32(1.7054138), 'max_total_reward': np.float32(10.022222), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08491960614919662), 'actor_loss': np.float64(-1.008812576532364), 'hyper_actor_loss': np.float64(3.0253441582317465e-05), 'behavior_loss': np.float64(0.4287849485874176)}

Episode step 48520, time diff 4.45193886756897, total time dif 14754.783964395523)
step: 48520 @ episode report: {'average_total_reward': np.float32(9.94889), 'reward_variance': np.float32(3.8395867), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555552), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0734588710591197), 'actor_loss': np.float64(-0.9982327699661255), 'hyper_actor_loss': np.float64(2.7672227224684322e-05), 'behavior_loss': np.float64(0.43112810850143435)}

Episode step 48530, time diff 4.415097236633301, total time dif 14759.235903263092)
step: 48530 @ episode report: {'average_total_reward': np.float32(8.763333), 'reward_variance': np.float32(8.148842), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07507616467773914), 'actor_loss': np.float64(-0.9759400129318238), 'hyper_actor_loss': np.float64(2.8284653490118215e-05), 'behavior_loss': np.float64(0.42061815559864046)}

Episode step 48540, time diff 4.435847043991089, total time dif 14763.651000499725)
step: 48540 @ episode report: {'average_total_reward': np.float32(8.602223), 'reward_variance': np.float32(1.0464888), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060718859359622), 'actor_loss': np.float64(-0.9768303334712982), 'hyper_actor_loss': np.float64(2.9201319193816744e-05), 'behavior_loss': np.float64(0.42903130650520327)}

Episode step 48550, time diff 4.469521522521973, total time dif 14768.086847543716)
step: 48550 @ episode report: {'average_total_reward': np.float32(8.838889), 'reward_variance': np.float32(2.5225003), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07776506505906582), 'actor_loss': np.float64(-0.9673979699611663), 'hyper_actor_loss': np.float64(2.8359958196233493e-05), 'behavior_loss': np.float64(0.42637493312358854)}

Episode step 48560, time diff 4.673449993133545, total time dif 14772.556369066238)
step: 48560 @ episode report: {'average_total_reward': np.float32(8.09), 'reward_variance': np.float32(2.0738876), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07514723725616931), 'actor_loss': np.float64(-0.9838459908962249), 'hyper_actor_loss': np.float64(2.9003547388128938e-05), 'behavior_loss': np.float64(0.4303319871425629)}

Episode step 48570, time diff 4.54650616645813, total time dif 14777.229819059372)
step: 48570 @ episode report: {'average_total_reward': np.float32(7.0311112), 'reward_variance': np.float32(2.834045), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.4), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0677170641720295), 'actor_loss': np.float64(-0.9964139461517334), 'hyper_actor_loss': np.float64(2.6940298630506733e-05), 'behavior_loss': np.float64(0.4303048253059387)}

Episode step 48580, time diff 4.522564888000488, total time dif 14781.77632522583)
step: 48580 @ episode report: {'average_total_reward': np.float32(9.5611105), 'reward_variance': np.float32(4.8264017), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08133867010474205), 'actor_loss': np.float64(-0.9847330212593078), 'hyper_actor_loss': np.float64(3.371127477294067e-05), 'behavior_loss': np.float64(0.42914568781852724)}

Episode step 48590, time diff 4.615480899810791, total time dif 14786.29889011383)
step: 48590 @ episode report: {'average_total_reward': np.float32(8.826668), 'reward_variance': np.float32(2.021585), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0730398215353489), 'actor_loss': np.float64(-0.9813811242580414), 'hyper_actor_loss': np.float64(3.0090986729192083e-05), 'behavior_loss': np.float64(0.4334360480308533)}

Episode step 48600, time diff 4.605143785476685, total time dif 14790.914371013641)
step: 48600 @ episode report: {'average_total_reward': np.float32(8.83889), 'reward_variance': np.float32(5.0961294), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(3.1666667), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(5.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06942004263401032), 'actor_loss': np.float64(-0.9843107461929321), 'hyper_actor_loss': np.float64(2.7866973687196152e-05), 'behavior_loss': np.float64(0.4338680863380432)}

Episode step 48610, time diff 4.436269998550415, total time dif 14795.519514799118)
step: 48610 @ episode report: {'average_total_reward': np.float32(9.151112), 'reward_variance': np.float32(3.6928697), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07835248187184334), 'actor_loss': np.float64(-0.990372508764267), 'hyper_actor_loss': np.float64(2.7978004436590708e-05), 'behavior_loss': np.float64(0.4300911039113998)}

Episode step 48620, time diff 4.571354627609253, total time dif 14799.955784797668)
step: 48620 @ episode report: {'average_total_reward': np.float32(7.8922224), 'reward_variance': np.float32(2.1659274), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06888077780604362), 'actor_loss': np.float64(-0.9920700252056122), 'hyper_actor_loss': np.float64(2.6336979681218507e-05), 'behavior_loss': np.float64(0.4270654171705246)}

Episode step 48630, time diff 4.929161071777344, total time dif 14804.527139425278)
step: 48630 @ episode report: {'average_total_reward': np.float32(8.902223), 'reward_variance': np.float32(3.4491806), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06669469550251961), 'actor_loss': np.float64(-0.9853089392185211), 'hyper_actor_loss': np.float64(2.736901897151256e-05), 'behavior_loss': np.float64(0.42252193093299867)}

Episode step 48640, time diff 4.954503536224365, total time dif 14809.456300497055)
step: 48640 @ episode report: {'average_total_reward': np.float32(8.290001), 'reward_variance': np.float32(3.1334443), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(4.5333333), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07142709754407406), 'actor_loss': np.float64(-0.9758675754070282), 'hyper_actor_loss': np.float64(2.5886217372317333e-05), 'behavior_loss': np.float64(0.4338353991508484)}

Episode step 48650, time diff 4.4802587032318115, total time dif 14814.41080403328)
step: 48650 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(5.2262983), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08265895396471024), 'actor_loss': np.float64(-0.982545804977417), 'hyper_actor_loss': np.float64(2.8038146047038028e-05), 'behavior_loss': np.float64(0.43013682663440705)}

Episode step 48660, time diff 4.409555912017822, total time dif 14818.891062736511)
step: 48660 @ episode report: {'average_total_reward': np.float32(7.6800013), 'reward_variance': np.float32(1.4403162), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07797646932303906), 'actor_loss': np.float64(-1.0023599207401275), 'hyper_actor_loss': np.float64(2.6250260998494922e-05), 'behavior_loss': np.float64(0.424937704205513)}

Episode step 48670, time diff 4.414413213729858, total time dif 14823.300618648529)
step: 48670 @ episode report: {'average_total_reward': np.float32(8.514444), 'reward_variance': np.float32(4.0711374), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07742189019918441), 'actor_loss': np.float64(-0.9983301520347595), 'hyper_actor_loss': np.float64(2.401316542091081e-05), 'behavior_loss': np.float64(0.42216126024723055)}

Episode step 48680, time diff 4.3997907638549805, total time dif 14827.715031862259)
step: 48680 @ episode report: {'average_total_reward': np.float32(8.477778), 'reward_variance': np.float32(2.8554325), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07008288726210594), 'actor_loss': np.float64(-0.9808838605880738), 'hyper_actor_loss': np.float64(2.4173940983018837e-05), 'behavior_loss': np.float64(0.42440112829208376)}

Episode step 48690, time diff 4.452855825424194, total time dif 14832.114822626114)
step: 48690 @ episode report: {'average_total_reward': np.float32(8.402223), 'reward_variance': np.float32(2.7348096), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06971994005143642), 'actor_loss': np.float64(-0.9784101724624634), 'hyper_actor_loss': np.float64(2.350438680878142e-05), 'behavior_loss': np.float64(0.4332687705755234)}

Episode step 48700, time diff 4.657171249389648, total time dif 14836.567678451538)
step: 48700 @ episode report: {'average_total_reward': np.float32(9.575557), 'reward_variance': np.float32(2.6950326), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06010105144232512), 'actor_loss': np.float64(-0.970890200138092), 'hyper_actor_loss': np.float64(2.304417266714154e-05), 'behavior_loss': np.float64(0.42500976622104647)}

Episode step 48710, time diff 4.754303455352783, total time dif 14841.224849700928)
step: 48710 @ episode report: {'average_total_reward': np.float32(9.661112), 'reward_variance': np.float32(4.1462536), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07500675395131111), 'actor_loss': np.float64(-0.9684682786464691), 'hyper_actor_loss': np.float64(2.38569136854494e-05), 'behavior_loss': np.float64(0.42389976978302)}

Episode step 48720, time diff 4.720260381698608, total time dif 14845.97915315628)
step: 48720 @ episode report: {'average_total_reward': np.float32(9.35111), 'reward_variance': np.float32(2.2745726), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07314136847853661), 'actor_loss': np.float64(-0.9955070436000824), 'hyper_actor_loss': np.float64(2.880262163671432e-05), 'behavior_loss': np.float64(0.42862275540828704)}

Episode step 48730, time diff 4.652923822402954, total time dif 14850.69941353798)
step: 48730 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.3620296), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06128219850361347), 'actor_loss': np.float64(-0.9828926503658295), 'hyper_actor_loss': np.float64(2.414528134977445e-05), 'behavior_loss': np.float64(0.42245352566242217)}

Episode step 48740, time diff 4.519449472427368, total time dif 14855.352337360382)
step: 48740 @ episode report: {'average_total_reward': np.float32(10.036668), 'reward_variance': np.float32(4.2041), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07051289677619935), 'actor_loss': np.float64(-0.9687061011791229), 'hyper_actor_loss': np.float64(2.4081404808384833e-05), 'behavior_loss': np.float64(0.4215199828147888)}

Episode step 48750, time diff 4.638574600219727, total time dif 14859.87178683281)
step: 48750 @ episode report: {'average_total_reward': np.float32(9.9), 'reward_variance': np.float32(1.6683455), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0798299103975296), 'actor_loss': np.float64(-0.9950875103473663), 'hyper_actor_loss': np.float64(2.3260915622813626e-05), 'behavior_loss': np.float64(0.4213467389345169)}

Episode step 48760, time diff 4.529862642288208, total time dif 14864.51036143303)
step: 48760 @ episode report: {'average_total_reward': np.float32(8.453334), 'reward_variance': np.float32(2.5853043), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08602760136127471), 'actor_loss': np.float64(-1.0056779503822326), 'hyper_actor_loss': np.float64(2.4056391885096674e-05), 'behavior_loss': np.float64(0.4312931329011917)}

Episode step 48770, time diff 4.5898988246917725, total time dif 14869.040224075317)
step: 48770 @ episode report: {'average_total_reward': np.float32(8.726667), 'reward_variance': np.float32(3.8161538), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06245954744517803), 'actor_loss': np.float64(-0.991042286157608), 'hyper_actor_loss': np.float64(2.333666579943383e-05), 'behavior_loss': np.float64(0.4219251453876495)}

Episode step 48780, time diff 4.58171820640564, total time dif 14873.63012290001)
step: 48780 @ episode report: {'average_total_reward': np.float32(10.048889), 'reward_variance': np.float32(2.5781274), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08515589088201522), 'actor_loss': np.float64(-0.9758416295051575), 'hyper_actor_loss': np.float64(2.4173112433345522e-05), 'behavior_loss': np.float64(0.42770549952983855)}

Episode step 48790, time diff 4.916089057922363, total time dif 14878.211841106415)
step: 48790 @ episode report: {'average_total_reward': np.float32(8.590001), 'reward_variance': np.float32(0.22109738), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07369996197521686), 'actor_loss': np.float64(-0.9856351494789124), 'hyper_actor_loss': np.float64(2.3168100960901937e-05), 'behavior_loss': np.float64(0.42424812316894533)}

Episode step 48800, time diff 4.730929851531982, total time dif 14883.127930164337)
step: 48800 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(3.4136844), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07088303826749325), 'actor_loss': np.float64(-0.9972567141056061), 'hyper_actor_loss': np.float64(2.2113329214334955e-05), 'behavior_loss': np.float64(0.4227222383022308)}

Episode step 48810, time diff 4.5386576652526855, total time dif 14887.85886001587)
step: 48810 @ episode report: {'average_total_reward': np.float32(8.1044445), 'reward_variance': np.float32(3.475881), 'max_total_reward': np.float32(10.9), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06102341376245022), 'actor_loss': np.float64(-0.9751391232013702), 'hyper_actor_loss': np.float64(2.384638301009545e-05), 'behavior_loss': np.float64(0.4226599305868149)}

Episode step 48820, time diff 4.5431694984436035, total time dif 14892.397517681122)
step: 48820 @ episode report: {'average_total_reward': np.float32(8.9388895), 'reward_variance': np.float32(2.7084756), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07340067252516747), 'actor_loss': np.float64(-0.9547077357769013), 'hyper_actor_loss': np.float64(2.3643168970011174e-05), 'behavior_loss': np.float64(0.421816810965538)}

Episode step 48830, time diff 4.514188766479492, total time dif 14896.940687179565)
step: 48830 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(2.9647965), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06391158774495125), 'actor_loss': np.float64(-0.9695764303207397), 'hyper_actor_loss': np.float64(2.1763075164926703e-05), 'behavior_loss': np.float64(0.4215507715940475)}

Episode step 48840, time diff 4.487504720687866, total time dif 14901.454875946045)
step: 48840 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(0.7669828), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06427671518176795), 'actor_loss': np.float64(-0.9796972930431366), 'hyper_actor_loss': np.float64(2.0832500194956082e-05), 'behavior_loss': np.float64(0.42571778893470763)}

Episode step 48850, time diff 4.464448928833008, total time dif 14905.942380666733)
step: 48850 @ episode report: {'average_total_reward': np.float32(8.077779), 'reward_variance': np.float32(1.2998031), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07189854085445405), 'actor_loss': np.float64(-0.9777768552303314), 'hyper_actor_loss': np.float64(1.9751349100260995e-05), 'behavior_loss': np.float64(0.4290701478719711)}

Episode step 48860, time diff 4.416829824447632, total time dif 14910.406829595566)
step: 48860 @ episode report: {'average_total_reward': np.float32(8.33889), 'reward_variance': np.float32(4.0411177), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.655556), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07449518591165542), 'actor_loss': np.float64(-0.964602917432785), 'hyper_actor_loss': np.float64(2.175668232666794e-05), 'behavior_loss': np.float64(0.4282565385103226)}

Episode step 48870, time diff 4.51841139793396, total time dif 14914.823659420013)
step: 48870 @ episode report: {'average_total_reward': np.float32(9.575556), 'reward_variance': np.float32(3.6721184), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07550472803413869), 'actor_loss': np.float64(-0.9806836187839508), 'hyper_actor_loss': np.float64(2.3705582680122462e-05), 'behavior_loss': np.float64(0.42633315920829773)}

Episode step 48880, time diff 4.485697984695435, total time dif 14919.342070817947)
step: 48880 @ episode report: {'average_total_reward': np.float32(8.341112), 'reward_variance': np.float32(2.3385444), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.411112), 'average_n_step': np.float32(9.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08357963934540749), 'actor_loss': np.float64(-1.0142036914825439), 'hyper_actor_loss': np.float64(2.3912612959975377e-05), 'behavior_loss': np.float64(0.42497974932193755)}

Episode step 48890, time diff 4.436635494232178, total time dif 14923.827768802643)
step: 48890 @ episode report: {'average_total_reward': np.float32(8.987778), 'reward_variance': np.float32(1.8843565), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08024589419364929), 'actor_loss': np.float64(-0.9927601456642151), 'hyper_actor_loss': np.float64(2.3257544853549916e-05), 'behavior_loss': np.float64(0.4213818430900574)}

Episode step 48900, time diff 4.528240203857422, total time dif 14928.264404296875)
step: 48900 @ episode report: {'average_total_reward': np.float32(8.089999), 'reward_variance': np.float32(3.533271), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07656409405171871), 'actor_loss': np.float64(-0.97427738904953), 'hyper_actor_loss': np.float64(2.4972416758828332e-05), 'behavior_loss': np.float64(0.4208923250436783)}

Episode step 48910, time diff 4.629512310028076, total time dif 14932.792644500732)
step: 48910 @ episode report: {'average_total_reward': np.float32(7.8411117), 'reward_variance': np.float32(2.115482), 'max_total_reward': np.float32(11.022222), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06599541306495667), 'actor_loss': np.float64(-0.986331969499588), 'hyper_actor_loss': np.float64(2.309408664586954e-05), 'behavior_loss': np.float64(0.4222211122512817)}

Episode step 48920, time diff 4.520909070968628, total time dif 14937.42215681076)
step: 48920 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(1.2499214), 'max_total_reward': np.float32(11.022223), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07517194040119649), 'actor_loss': np.float64(-0.9797626733779907), 'hyper_actor_loss': np.float64(2.3628301460121292e-05), 'behavior_loss': np.float64(0.4227057844400406)}

Episode step 48930, time diff 4.387203216552734, total time dif 14941.94306588173)
step: 48930 @ episode report: {'average_total_reward': np.float32(9.475556), 'reward_variance': np.float32(3.938144), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07026578411459923), 'actor_loss': np.float64(-0.9849518358707428), 'hyper_actor_loss': np.float64(2.5299694789282513e-05), 'behavior_loss': np.float64(0.41475388407707214)}

Episode step 48940, time diff 4.4462220668792725, total time dif 14946.330269098282)
step: 48940 @ episode report: {'average_total_reward': np.float32(9.263334), 'reward_variance': np.float32(1.623112), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07597757540643216), 'actor_loss': np.float64(-0.9973577439785004), 'hyper_actor_loss': np.float64(1.972887857846217e-05), 'behavior_loss': np.float64(0.42466844618320465)}

Episode step 48950, time diff 4.378500938415527, total time dif 14950.776491165161)
step: 48950 @ episode report: {'average_total_reward': np.float32(8.802223), 'reward_variance': np.float32(2.8752549), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07796609066426755), 'actor_loss': np.float64(-0.9904451131820678), 'hyper_actor_loss': np.float64(1.7145183664979414e-05), 'behavior_loss': np.float64(0.42551920413970945)}

Episode step 48960, time diff 4.404557466506958, total time dif 14955.154992103577)
step: 48960 @ episode report: {'average_total_reward': np.float32(9.0633335), 'reward_variance': np.float32(0.96385324), 'max_total_reward': np.float32(10.022223), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07187662273645401), 'actor_loss': np.float64(-0.9821338891983032), 'hyper_actor_loss': np.float64(1.8062877279589885e-05), 'behavior_loss': np.float64(0.4087369292974472)}

Episode step 48970, time diff 4.628576993942261, total time dif 14959.559549570084)
step: 48970 @ episode report: {'average_total_reward': np.float32(8.875557), 'reward_variance': np.float32(3.1944897), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07422061376273632), 'actor_loss': np.float64(-0.9794225633144379), 'hyper_actor_loss': np.float64(1.8245645696879366e-05), 'behavior_loss': np.float64(0.42162382900714873)}

Episode step 48980, time diff 4.4578330516815186, total time dif 14964.188126564026)
step: 48980 @ episode report: {'average_total_reward': np.float32(8.290001), 'reward_variance': np.float32(1.895517), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(9.5), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07873798720538616), 'actor_loss': np.float64(-0.9877201676368713), 'hyper_actor_loss': np.float64(2.4033831687120254e-05), 'behavior_loss': np.float64(0.4158211022615433)}

Episode step 48990, time diff 4.471090316772461, total time dif 14968.645959615707)
step: 48990 @ episode report: {'average_total_reward': np.float32(8.863334), 'reward_variance': np.float32(2.245582), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07939909771084785), 'actor_loss': np.float64(-0.9905799090862274), 'hyper_actor_loss': np.float64(3.0698175032739526e-05), 'behavior_loss': np.float64(0.4152521133422852)}

Episode step 49000, time diff 4.5255773067474365, total time dif 14973.11704993248)
step: 49000 @ episode report: {'average_total_reward': np.float32(8.814445), 'reward_variance': np.float32(1.7448164), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.0), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07287688404321671), 'actor_loss': np.float64(-0.9926597237586975), 'hyper_actor_loss': np.float64(3.390156125533394e-05), 'behavior_loss': np.float64(0.4145727425813675)}

Episode step 49010, time diff 4.539446592330933, total time dif 14977.642627239227)
step: 49010 @ episode report: {'average_total_reward': np.float32(10.671112), 'reward_variance': np.float32(2.8288937), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.058695197105407715), 'actor_loss': np.float64(-0.9739393651485443), 'hyper_actor_loss': np.float64(2.549203964008484e-05), 'behavior_loss': np.float64(0.41442790627479553)}

Episode step 49020, time diff 4.638940334320068, total time dif 14982.182073831558)
step: 49020 @ episode report: {'average_total_reward': np.float32(9.836667), 'reward_variance': np.float32(5.4312606), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08300817087292671), 'actor_loss': np.float64(-0.977234947681427), 'hyper_actor_loss': np.float64(2.3046385285852013e-05), 'behavior_loss': np.float64(0.4107487678527832)}

Episode step 49030, time diff 4.9303600788116455, total time dif 14986.821014165878)
step: 49030 @ episode report: {'average_total_reward': np.float32(9.1877775), 'reward_variance': np.float32(2.2611222), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0649287909269333), 'actor_loss': np.float64(-0.9932214796543122), 'hyper_actor_loss': np.float64(1.7231912261195247e-05), 'behavior_loss': np.float64(0.42192032039165495)}

Episode step 49040, time diff 4.767416954040527, total time dif 14991.75137424469)
step: 49040 @ episode report: {'average_total_reward': np.float32(9.712223), 'reward_variance': np.float32(8.675173), 'max_total_reward': np.float32(15.511112), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07550461031496525), 'actor_loss': np.float64(-0.9777352154254914), 'hyper_actor_loss': np.float64(1.619973591004964e-05), 'behavior_loss': np.float64(0.41020887196063993)}

Episode step 49050, time diff 4.790726184844971, total time dif 14996.51879119873)
step: 49050 @ episode report: {'average_total_reward': np.float32(9.087778), 'reward_variance': np.float32(4.5169497), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08195052668452263), 'actor_loss': np.float64(-0.9962745606899261), 'hyper_actor_loss': np.float64(1.9143808094668203e-05), 'behavior_loss': np.float64(0.4102582037448883)}

Episode step 49060, time diff 4.501397132873535, total time dif 15001.309517383575)
step: 49060 @ episode report: {'average_total_reward': np.float32(10.361113), 'reward_variance': np.float32(4.252302), 'max_total_reward': np.float32(15.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07782608047127723), 'actor_loss': np.float64(-1.0070769906044006), 'hyper_actor_loss': np.float64(2.4576831856393254e-05), 'behavior_loss': np.float64(0.42279755175113676)}

Episode step 49070, time diff 4.495891332626343, total time dif 15005.810914516449)
step: 49070 @ episode report: {'average_total_reward': np.float32(10.110001), 'reward_variance': np.float32(3.3437393), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06576895192265511), 'actor_loss': np.float64(-0.9792069256305694), 'hyper_actor_loss': np.float64(3.0036877797101623e-05), 'behavior_loss': np.float64(0.4148460805416107)}

Episode step 49080, time diff 4.5283472537994385, total time dif 15010.306805849075)
step: 49080 @ episode report: {'average_total_reward': np.float32(10.634445), 'reward_variance': np.float32(1.8446776), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07760957758873702), 'actor_loss': np.float64(-0.9685963988304138), 'hyper_actor_loss': np.float64(2.9992538293299732e-05), 'behavior_loss': np.float64(0.4131380796432495)}

Episode step 49090, time diff 4.492388963699341, total time dif 15014.835153102875)
step: 49090 @ episode report: {'average_total_reward': np.float32(10.173333), 'reward_variance': np.float32(3.5722518), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06770753897726536), 'actor_loss': np.float64(-0.9842132985591888), 'hyper_actor_loss': np.float64(2.257475134683773e-05), 'behavior_loss': np.float64(0.41261625587940215)}

Episode step 49100, time diff 4.507209777832031, total time dif 15019.327542066574)
step: 49100 @ episode report: {'average_total_reward': np.float32(10.061111), 'reward_variance': np.float32(3.7205014), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07136331722140313), 'actor_loss': np.float64(-0.9773135781288147), 'hyper_actor_loss': np.float64(1.810342328099068e-05), 'behavior_loss': np.float64(0.411097040772438)}

Episode step 49110, time diff 4.521162033081055, total time dif 15023.834751844406)
step: 49110 @ episode report: {'average_total_reward': np.float32(9.724445), 'reward_variance': np.float32(4.9667854), 'max_total_reward': np.float32(14.388889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07506552748382092), 'actor_loss': np.float64(-0.9848551273345947), 'hyper_actor_loss': np.float64(1.64018225405016e-05), 'behavior_loss': np.float64(0.40990451276302337)}

Episode step 49120, time diff 4.520776748657227, total time dif 15028.355913877487)
step: 49120 @ episode report: {'average_total_reward': np.float32(9.387778), 'reward_variance': np.float32(3.3274677), 'max_total_reward': np.float32(12.022222), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07411850318312645), 'actor_loss': np.float64(-0.990299379825592), 'hyper_actor_loss': np.float64(1.6303711345244664e-05), 'behavior_loss': np.float64(0.4095926582813263)}

Episode step 49130, time diff 4.909636974334717, total time dif 15032.876690626144)
step: 49130 @ episode report: {'average_total_reward': np.float32(9.038889), 'reward_variance': np.float32(1.7572905), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07166597098112107), 'actor_loss': np.float64(-0.9782944798469544), 'hyper_actor_loss': np.float64(1.4976276634115494e-05), 'behavior_loss': np.float64(0.4117576003074646)}

Episode step 49140, time diff 4.618884086608887, total time dif 15037.78632760048)
step: 49140 @ episode report: {'average_total_reward': np.float32(9.251111), 'reward_variance': np.float32(0.9375604), 'max_total_reward': np.float32(11.0222225), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06730452515184879), 'actor_loss': np.float64(-0.9763232111930847), 'hyper_actor_loss': np.float64(1.5549148065474584e-05), 'behavior_loss': np.float64(0.4009944975376129)}

Episode step 49150, time diff 4.727336168289185, total time dif 15042.405211687088)
step: 49150 @ episode report: {'average_total_reward': np.float32(10.5222225), 'reward_variance': np.float32(3.7243962), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07312532886862755), 'actor_loss': np.float64(-0.987491887807846), 'hyper_actor_loss': np.float64(1.46627773574437e-05), 'behavior_loss': np.float64(0.40500212013721465)}

Episode step 49160, time diff 4.506937026977539, total time dif 15047.132547855377)
step: 49160 @ episode report: {'average_total_reward': np.float32(9.263333), 'reward_variance': np.float32(6.2960534), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08169274106621742), 'actor_loss': np.float64(-1.0024835109710692), 'hyper_actor_loss': np.float64(1.728775805531768e-05), 'behavior_loss': np.float64(0.3965644627809525)}

Episode step 49170, time diff 4.59825873374939, total time dif 15051.639484882355)
step: 49170 @ episode report: {'average_total_reward': np.float32(9.612223), 'reward_variance': np.float32(2.5464063), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0799337450414896), 'actor_loss': np.float64(-0.9996863842010498), 'hyper_actor_loss': np.float64(1.7703158664517105e-05), 'behavior_loss': np.float64(0.40477283895015714)}

Episode step 49180, time diff 4.4601664543151855, total time dif 15056.237743616104)
step: 49180 @ episode report: {'average_total_reward': np.float32(9.263333), 'reward_variance': np.float32(1.2066432), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06878759954124689), 'actor_loss': np.float64(-0.9904624938964843), 'hyper_actor_loss': np.float64(1.5783027902216417e-05), 'behavior_loss': np.float64(0.4102684736251831)}

Episode step 49190, time diff 4.355310916900635, total time dif 15060.69791007042)
step: 49190 @ episode report: {'average_total_reward': np.float32(9.536669), 'reward_variance': np.float32(3.4920025), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07108557000756263), 'actor_loss': np.float64(-0.9733152985572815), 'hyper_actor_loss': np.float64(1.6118282292154618e-05), 'behavior_loss': np.float64(0.4046113759279251)}

Episode step 49200, time diff 4.560314178466797, total time dif 15065.05322098732)
step: 49200 @ episode report: {'average_total_reward': np.float32(10.161112), 'reward_variance': np.float32(1.0889444), 'max_total_reward': np.float32(12.0222225), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07801796086132526), 'actor_loss': np.float64(-0.984792023897171), 'hyper_actor_loss': np.float64(1.4774283226870465e-05), 'behavior_loss': np.float64(0.4012116402387619)}

Episode step 49210, time diff 4.563671350479126, total time dif 15069.613535165787)
step: 49210 @ episode report: {'average_total_reward': np.float32(10.073334), 'reward_variance': np.float32(9.016279), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0811217613518238), 'actor_loss': np.float64(-0.9871379315853119), 'hyper_actor_loss': np.float64(1.4944467875466216e-05), 'behavior_loss': np.float64(0.4026972711086273)}

Episode step 49220, time diff 4.396769285202026, total time dif 15074.177206516266)
step: 49220 @ episode report: {'average_total_reward': np.float32(9.151113), 'reward_variance': np.float32(5.680451), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(6.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07885755151510239), 'actor_loss': np.float64(-0.9909938633441925), 'hyper_actor_loss': np.float64(1.5112044729903574e-05), 'behavior_loss': np.float64(0.4083094626665115)}

Episode step 49230, time diff 4.7820446491241455, total time dif 15078.573975801468)
step: 49230 @ episode report: {'average_total_reward': np.float32(8.565557), 'reward_variance': np.float32(3.4123814), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(5.411111), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06979669146239757), 'actor_loss': np.float64(-0.994183361530304), 'hyper_actor_loss': np.float64(2.955115714939893e-05), 'behavior_loss': np.float64(0.40252495408058164)}

Episode step 49240, time diff 4.92577600479126, total time dif 15083.356020450592)
step: 49240 @ episode report: {'average_total_reward': np.float32(7.467778), 'reward_variance': np.float32(1.6485304), 'max_total_reward': np.float32(9.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08120306618511677), 'actor_loss': np.float64(-0.9890293836593628), 'hyper_actor_loss': np.float64(5.9889056501560844e-05), 'behavior_loss': np.float64(0.3966848701238632)}

Episode step 49250, time diff 4.835667133331299, total time dif 15088.281796455383)
step: 49250 @ episode report: {'average_total_reward': np.float32(7.9044447), 'reward_variance': np.float32(0.7909186), 'max_total_reward': np.float32(8.900002), 'min_total_reward': np.float32(6.2888894), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0693933367729187), 'actor_loss': np.float64(-0.9848664462566376), 'hyper_actor_loss': np.float64(3.332710166432662e-05), 'behavior_loss': np.float64(0.40614883303642274)}

Episode step 49260, time diff 4.846837997436523, total time dif 15093.117463588715)
step: 49260 @ episode report: {'average_total_reward': np.float32(8.053334), 'reward_variance': np.float32(2.5842178), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(9.3), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07168183550238609), 'actor_loss': np.float64(-0.9710553646087646), 'hyper_actor_loss': np.float64(2.3715843599347864e-05), 'behavior_loss': np.float64(0.3990083009004593)}

Episode step 49270, time diff 4.823060512542725, total time dif 15097.964301586151)
step: 49270 @ episode report: {'average_total_reward': np.float32(7.4800005), 'reward_variance': np.float32(4.334834), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(8.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06738085485994816), 'actor_loss': np.float64(-0.9912090182304383), 'hyper_actor_loss': np.float64(2.2378422636393223e-05), 'behavior_loss': np.float64(0.3968460500240326)}

Episode step 49280, time diff 4.725569248199463, total time dif 15102.787362098694)
step: 49280 @ episode report: {'average_total_reward': np.float32(7.5066667), 'reward_variance': np.float32(1.9322274), 'max_total_reward': np.float32(9.9), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(8.9), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08806344978511334), 'actor_loss': np.float64(-0.9936570763587952), 'hyper_actor_loss': np.float64(1.6128254083014325e-05), 'behavior_loss': np.float64(0.40799006521701814)}

Episode step 49290, time diff 4.677387237548828, total time dif 15107.512931346893)
step: 49290 @ episode report: {'average_total_reward': np.float32(7.367778), 'reward_variance': np.float32(2.6366782), 'max_total_reward': np.float32(8.900001), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(8.7), 'max_n_step': np.float32(10.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07954104915261269), 'actor_loss': np.float64(-1.0067327260971068), 'hyper_actor_loss': np.float64(1.6095995579235024e-05), 'behavior_loss': np.float64(0.4045501261949539)}

Episode step 49300, time diff 4.945871591567993, total time dif 15112.190318584442)
step: 49300 @ episode report: {'average_total_reward': np.float32(7.7288895), 'reward_variance': np.float32(2.5941532), 'max_total_reward': np.float32(9.777778), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(9.0), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07172597348690032), 'actor_loss': np.float64(-0.9749332726001739), 'hyper_actor_loss': np.float64(1.8701452245295513e-05), 'behavior_loss': np.float64(0.40164986848831175)}

Episode step 49310, time diff 4.605421781539917, total time dif 15117.13619017601)
step: 49310 @ episode report: {'average_total_reward': np.float32(8.890001), 'reward_variance': np.float32(2.0824554), 'max_total_reward': np.float32(10.900001), 'min_total_reward': np.float32(6.4111114), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07616246677935123), 'actor_loss': np.float64(-0.9714710593223572), 'hyper_actor_loss': np.float64(1.9547850024537182e-05), 'behavior_loss': np.float64(0.40405126214027404)}

Episode step 49320, time diff 4.664435386657715, total time dif 15121.74161195755)
step: 49320 @ episode report: {'average_total_reward': np.float32(7.9411116), 'reward_variance': np.float32(3.348446), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(4.4111114), 'average_n_step': np.float32(9.2), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0768175721168518), 'actor_loss': np.float64(-0.9922571420669556), 'hyper_actor_loss': np.float64(1.889596569526475e-05), 'behavior_loss': np.float64(0.4019794285297394)}

Episode step 49330, time diff 4.761929988861084, total time dif 15126.406047344208)
step: 49330 @ episode report: {'average_total_reward': np.float32(8.553334), 'reward_variance': np.float32(1.0867851), 'max_total_reward': np.float32(9.900002), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(9.8), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08054700940847397), 'actor_loss': np.float64(-1.0034328699111938), 'hyper_actor_loss': np.float64(2.0076472901564556e-05), 'behavior_loss': np.float64(0.397988897562027)}

Episode step 49340, time diff 4.78564453125, total time dif 15131.167977333069)
step: 49340 @ episode report: {'average_total_reward': np.float32(9.187778), 'reward_variance': np.float32(3.8028023), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(10.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07335190251469612), 'actor_loss': np.float64(-1.0007754921913148), 'hyper_actor_loss': np.float64(2.5298901709902567e-05), 'behavior_loss': np.float64(0.4025449603796005)}

Episode step 49350, time diff 4.583720922470093, total time dif 15135.953621864319)
step: 49350 @ episode report: {'average_total_reward': np.float32(9.64889), 'reward_variance': np.float32(3.9662032), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(6.655556), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06188340224325657), 'actor_loss': np.float64(-0.9664324700832367), 'hyper_actor_loss': np.float64(2.3876047089288475e-05), 'behavior_loss': np.float64(0.3965386927127838)}

Episode step 49360, time diff 4.516516447067261, total time dif 15140.537342786789)
step: 49360 @ episode report: {'average_total_reward': np.float32(9.736667), 'reward_variance': np.float32(4.298101), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07421679385006427), 'actor_loss': np.float64(-0.9617727637290955), 'hyper_actor_loss': np.float64(2.4498004859196953e-05), 'behavior_loss': np.float64(0.4040887176990509)}

Episode step 49370, time diff 4.487873554229736, total time dif 15145.053859233856)
step: 49370 @ episode report: {'average_total_reward': np.float32(10.522223), 'reward_variance': np.float32(3.2450867), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(7.6555567), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07668274082243443), 'actor_loss': np.float64(-0.9907391726970672), 'hyper_actor_loss': np.float64(2.0936063810950146e-05), 'behavior_loss': np.float64(0.4026733756065369)}

Episode step 49380, time diff 4.597052574157715, total time dif 15149.541732788086)
step: 49380 @ episode report: {'average_total_reward': np.float32(9.475557), 'reward_variance': np.float32(4.4448843), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.6), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07588186040520668), 'actor_loss': np.float64(-0.9941550850868225), 'hyper_actor_loss': np.float64(1.8008412371273153e-05), 'behavior_loss': np.float64(0.4162342339754105)}

Episode step 49390, time diff 4.719157934188843, total time dif 15154.138785362244)
step: 49390 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(5.0959034), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06546991318464279), 'actor_loss': np.float64(-0.9635785460472107), 'hyper_actor_loss': np.float64(1.732287728373194e-05), 'behavior_loss': np.float64(0.39188561737537386)}

Episode step 49400, time diff 4.69473671913147, total time dif 15158.857943296432)
step: 49400 @ episode report: {'average_total_reward': np.float32(9.4), 'reward_variance': np.float32(4.507457), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07620399110019208), 'actor_loss': np.float64(-0.9848371803760528), 'hyper_actor_loss': np.float64(2.5288079450547228e-05), 'behavior_loss': np.float64(0.3984864383935928)}

Episode step 49410, time diff 4.736688137054443, total time dif 15163.552680015564)
step: 49410 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(2.9621735), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07108336128294468), 'actor_loss': np.float64(-0.9952411532402039), 'hyper_actor_loss': np.float64(2.4407672572124284e-05), 'behavior_loss': np.float64(0.3979227364063263)}

Episode step 49420, time diff 4.67237401008606, total time dif 15168.289368152618)
step: 49420 @ episode report: {'average_total_reward': np.float32(9.412223), 'reward_variance': np.float32(3.757419), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(5.4111114), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07625839821994304), 'actor_loss': np.float64(-0.9815520584583283), 'hyper_actor_loss': np.float64(1.9578777573769913e-05), 'behavior_loss': np.float64(0.39937092661857604)}

Episode step 49430, time diff 4.6382951736450195, total time dif 15172.961742162704)
step: 49430 @ episode report: {'average_total_reward': np.float32(9.0), 'reward_variance': np.float32(4.0681486), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07439367622137069), 'actor_loss': np.float64(-0.993748688697815), 'hyper_actor_loss': np.float64(2.2118864217191002e-05), 'behavior_loss': np.float64(0.3946710765361786)}

Episode step 49440, time diff 4.57265567779541, total time dif 15177.60003733635)
step: 49440 @ episode report: {'average_total_reward': np.float32(10.097778), 'reward_variance': np.float32(4.8909087), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06656377539038658), 'actor_loss': np.float64(-0.9952835500240326), 'hyper_actor_loss': np.float64(2.0790543385373893e-05), 'behavior_loss': np.float64(0.39954950511455534)}

Episode step 49450, time diff 4.6175549030303955, total time dif 15182.172693014145)
step: 49450 @ episode report: {'average_total_reward': np.float32(9.975555), 'reward_variance': np.float32(0.8129834), 'max_total_reward': np.float32(12.144445), 'min_total_reward': np.float32(8.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06814364120364189), 'actor_loss': np.float64(-0.9729418754577637), 'hyper_actor_loss': np.float64(1.588097420608392e-05), 'behavior_loss': np.float64(0.39322068095207213)}

Episode step 49460, time diff 4.682480812072754, total time dif 15186.790247917175)
step: 49460 @ episode report: {'average_total_reward': np.float32(10.324446), 'reward_variance': np.float32(4.485255), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.28889), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08056654334068299), 'actor_loss': np.float64(-0.9801652371883393), 'hyper_actor_loss': np.float64(1.4683277458971134e-05), 'behavior_loss': np.float64(0.39457723796367644)}

Episode step 49470, time diff 4.574355602264404, total time dif 15191.472728729248)
step: 49470 @ episode report: {'average_total_reward': np.float32(10.746668), 'reward_variance': np.float32(2.3822184), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.054215078614652155), 'actor_loss': np.float64(-0.9918631017208099), 'hyper_actor_loss': np.float64(1.678867292866926e-05), 'behavior_loss': np.float64(0.39837849140167236)}

Episode step 49480, time diff 4.628671407699585, total time dif 15196.047084331512)
step: 49480 @ episode report: {'average_total_reward': np.float32(9.948889), 'reward_variance': np.float32(4.312918), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06843976005911827), 'actor_loss': np.float64(-0.9621668934822083), 'hyper_actor_loss': np.float64(2.7601997499004937e-05), 'behavior_loss': np.float64(0.40365110635757445)}

Episode step 49490, time diff 4.800040245056152, total time dif 15200.675755739212)
step: 49490 @ episode report: {'average_total_reward': np.float32(8.453334), 'reward_variance': np.float32(1.0416247), 'max_total_reward': np.float32(10.0222225), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(9.7), 'max_n_step': np.float32(11.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06782581061124801), 'actor_loss': np.float64(-0.9578712344169616), 'hyper_actor_loss': np.float64(2.2096774773672224e-05), 'behavior_loss': np.float64(0.3943061262369156)}

Episode step 49500, time diff 4.8185200691223145, total time dif 15205.475795984268)
step: 49500 @ episode report: {'average_total_reward': np.float32(9.600001), 'reward_variance': np.float32(3.5512109), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.533333), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06639564707875252), 'actor_loss': np.float64(-0.9786556541919709), 'hyper_actor_loss': np.float64(1.3820879848935874e-05), 'behavior_loss': np.float64(0.39748410880565643)}

Episode step 49510, time diff 4.684410095214844, total time dif 15210.29431605339)
step: 49510 @ episode report: {'average_total_reward': np.float32(9.836668), 'reward_variance': np.float32(1.6970135), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08120686300098896), 'actor_loss': np.float64(-0.9932727515697479), 'hyper_actor_loss': np.float64(1.3592625236924505e-05), 'behavior_loss': np.float64(0.3879482805728912)}

Episode step 49520, time diff 4.590733766555786, total time dif 15214.978726148605)
step: 49520 @ episode report: {'average_total_reward': np.float32(10.610001), 'reward_variance': np.float32(3.563813), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08608101978898049), 'actor_loss': np.float64(-1.0079578161239624), 'hyper_actor_loss': np.float64(1.3072585170448292e-05), 'behavior_loss': np.float64(0.39779682755470275)}

Episode step 49530, time diff 4.459004878997803, total time dif 15219.569459915161)
step: 49530 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(5.107073), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(5.533334), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07038079723715782), 'actor_loss': np.float64(-0.9943229258060455), 'hyper_actor_loss': np.float64(1.3597649194707629e-05), 'behavior_loss': np.float64(0.3975353628396988)}

Episode step 49540, time diff 4.345712184906006, total time dif 15224.028464794159)
step: 49540 @ episode report: {'average_total_reward': np.float32(10.3122225), 'reward_variance': np.float32(1.872481), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07651140652596951), 'actor_loss': np.float64(-0.9761870682239533), 'hyper_actor_loss': np.float64(1.4680508593301056e-05), 'behavior_loss': np.float64(0.3891818761825562)}

Episode step 49550, time diff 4.580846309661865, total time dif 15228.374176979065)
step: 49550 @ episode report: {'average_total_reward': np.float32(11.107779), 'reward_variance': np.float32(5.013236), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.0), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06882146373391151), 'actor_loss': np.float64(-0.9883212268352508), 'hyper_actor_loss': np.float64(1.4762946466362337e-05), 'behavior_loss': np.float64(0.3982847332954407)}

Episode step 49560, time diff 4.631072521209717, total time dif 15232.955023288727)
step: 49560 @ episode report: {'average_total_reward': np.float32(9.424444), 'reward_variance': np.float32(4.8654766), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(5.5333333), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07511570230126381), 'actor_loss': np.float64(-1.0016436934471131), 'hyper_actor_loss': np.float64(1.4642521091445814e-05), 'behavior_loss': np.float64(0.3910536915063858)}

Episode step 49570, time diff 4.564003229141235, total time dif 15237.586095809937)
step: 49570 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(6.231286), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(4.533334), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(6.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07191534526646137), 'actor_loss': np.float64(-0.9831478893756866), 'hyper_actor_loss': np.float64(1.4272915632318472e-05), 'behavior_loss': np.float64(0.3926450401544571)}

Episode step 49580, time diff 4.483070135116577, total time dif 15242.150099039078)
step: 49580 @ episode report: {'average_total_reward': np.float32(10.012223), 'reward_variance': np.float32(3.0046782), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07067899722605944), 'actor_loss': np.float64(-0.9734435379505157), 'hyper_actor_loss': np.float64(1.3224840131442761e-05), 'behavior_loss': np.float64(0.40810081362724304)}

Episode step 49590, time diff 4.704509258270264, total time dif 15246.633169174194)
step: 49590 @ episode report: {'average_total_reward': np.float32(9.673334), 'reward_variance': np.float32(2.3000305), 'max_total_reward': np.float32(12.266668), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06188980266451836), 'actor_loss': np.float64(-0.9692270576953887), 'hyper_actor_loss': np.float64(1.1436039221734972e-05), 'behavior_loss': np.float64(0.39592623710632324)}

Episode step 49600, time diff 4.562637805938721, total time dif 15251.337678432465)
step: 49600 @ episode report: {'average_total_reward': np.float32(10.373335), 'reward_variance': np.float32(3.263289), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.4111114), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0757990762591362), 'actor_loss': np.float64(-0.9874674201011657), 'hyper_actor_loss': np.float64(1.226444492203882e-05), 'behavior_loss': np.float64(0.3885303735733032)}

Episode step 49610, time diff 4.753270864486694, total time dif 15255.900316238403)
step: 49610 @ episode report: {'average_total_reward': np.float32(9.3), 'reward_variance': np.float32(2.1128645), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(10.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07148819714784622), 'actor_loss': np.float64(-1.0043254017829895), 'hyper_actor_loss': np.float64(1.2637695635930869e-05), 'behavior_loss': np.float64(0.38744436800479887)}

Episode step 49620, time diff 4.685805082321167, total time dif 15260.65358710289)
step: 49620 @ episode report: {'average_total_reward': np.float32(9.785557), 'reward_variance': np.float32(1.4567422), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.8), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07623153440654278), 'actor_loss': np.float64(-0.992743968963623), 'hyper_actor_loss': np.float64(1.3501030935003655e-05), 'behavior_loss': np.float64(0.3871996283531189)}

Episode step 49630, time diff 4.878453493118286, total time dif 15265.339392185211)
step: 49630 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(3.5369651), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06979506090283394), 'actor_loss': np.float64(-0.9839963495731354), 'hyper_actor_loss': np.float64(1.2353083729976787e-05), 'behavior_loss': np.float64(0.3843525439500809)}

Episode step 49640, time diff 4.701200723648071, total time dif 15270.21784567833)
step: 49640 @ episode report: {'average_total_reward': np.float32(10.497778), 'reward_variance': np.float32(3.143723), 'max_total_reward': np.float32(13.266666), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08178802505135536), 'actor_loss': np.float64(-0.9962792456150055), 'hyper_actor_loss': np.float64(1.1611874469963368e-05), 'behavior_loss': np.float64(0.3977989673614502)}

Episode step 49650, time diff 4.810064792633057, total time dif 15274.919046401978)
step: 49650 @ episode report: {'average_total_reward': np.float32(8.987778), 'reward_variance': np.float32(3.940752), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07452642135322093), 'actor_loss': np.float64(-0.9986409664154052), 'hyper_actor_loss': np.float64(1.2202387188153807e-05), 'behavior_loss': np.float64(0.38965125679969786)}

Episode step 49660, time diff 4.671271085739136, total time dif 15279.72911119461)
step: 49660 @ episode report: {'average_total_reward': np.float32(9.375556), 'reward_variance': np.float32(3.27039), 'max_total_reward': np.float32(11.900001), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0703020565211773), 'actor_loss': np.float64(-0.9887320220470428), 'hyper_actor_loss': np.float64(1.1222778903174913e-05), 'behavior_loss': np.float64(0.3897290825843811)}

Episode step 49670, time diff 4.645976305007935, total time dif 15284.40038228035)
step: 49670 @ episode report: {'average_total_reward': np.float32(9.824445), 'reward_variance': np.float32(1.3137479), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(7.655556), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0765304196625948), 'actor_loss': np.float64(-0.9806268393993378), 'hyper_actor_loss': np.float64(1.3812332326779142e-05), 'behavior_loss': np.float64(0.3875858157873154)}

Episode step 49680, time diff 4.461536884307861, total time dif 15289.046358585358)
step: 49680 @ episode report: {'average_total_reward': np.float32(9.900001), 'reward_variance': np.float32(3.7048137), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07755707986652852), 'actor_loss': np.float64(-0.9993369400501251), 'hyper_actor_loss': np.float64(1.5637107480870327e-05), 'behavior_loss': np.float64(0.39073911011219026)}

Episode step 49690, time diff 4.494648218154907, total time dif 15293.507895469666)
step: 49690 @ episode report: {'average_total_reward': np.float32(10.2733345), 'reward_variance': np.float32(2.1766229), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.533334), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06290670335292817), 'actor_loss': np.float64(-1.0006867051124573), 'hyper_actor_loss': np.float64(1.496008990216069e-05), 'behavior_loss': np.float64(0.38583905398845675)}

Episode step 49700, time diff 4.528536081314087, total time dif 15298.00254368782)
step: 49700 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(1.9982727), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07493760120123624), 'actor_loss': np.float64(-0.9823421537876129), 'hyper_actor_loss': np.float64(1.62019421622972e-05), 'behavior_loss': np.float64(0.3821704089641571)}

Episode step 49710, time diff 4.443498373031616, total time dif 15302.531079769135)
step: 49710 @ episode report: {'average_total_reward': np.float32(10.422223), 'reward_variance': np.float32(2.0609877), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060706145316362384), 'actor_loss': np.float64(-0.9745137929916382), 'hyper_actor_loss': np.float64(1.770587459759554e-05), 'behavior_loss': np.float64(0.3894791632890701)}

Episode step 49720, time diff 4.503103017807007, total time dif 15306.974578142166)
step: 49720 @ episode report: {'average_total_reward': np.float32(11.307778), 'reward_variance': np.float32(2.9758782), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07967545539140701), 'actor_loss': np.float64(-0.9793583989143372), 'hyper_actor_loss': np.float64(2.0980795488867444e-05), 'behavior_loss': np.float64(0.3896978974342346)}

Episode step 49730, time diff 4.504371643066406, total time dif 15311.477681159973)
step: 49730 @ episode report: {'average_total_reward': np.float32(11.644445), 'reward_variance': np.float32(2.4650123), 'max_total_reward': np.float32(14.266667), 'min_total_reward': np.float32(9.900001), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(11.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07614197172224521), 'actor_loss': np.float64(-1.0030518889427185), 'hyper_actor_loss': np.float64(2.236699056084035e-05), 'behavior_loss': np.float64(0.3979016959667206)}

Episode step 49740, time diff 4.605420112609863, total time dif 15315.98205280304)
step: 49740 @ episode report: {'average_total_reward': np.float32(10.597778), 'reward_variance': np.float32(2.8513284), 'max_total_reward': np.float32(13.388889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.6), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0607329111546278), 'actor_loss': np.float64(-0.9716444730758667), 'hyper_actor_loss': np.float64(2.0080425747437404e-05), 'behavior_loss': np.float64(0.39722153544425964)}

Episode step 49750, time diff 4.567103385925293, total time dif 15320.58747291565)
step: 49750 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(0.83844596), 'max_total_reward': np.float32(11.144445), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08820226117968559), 'actor_loss': np.float64(-0.9613940358161926), 'hyper_actor_loss': np.float64(1.868242970886058e-05), 'behavior_loss': np.float64(0.4009770303964615)}

Episode step 49760, time diff 4.511458396911621, total time dif 15325.154576301575)
step: 49760 @ episode report: {'average_total_reward': np.float32(10.734446), 'reward_variance': np.float32(2.5434186), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06693692728877068), 'actor_loss': np.float64(-0.9948933184146881), 'hyper_actor_loss': np.float64(1.198059226226178e-05), 'behavior_loss': np.float64(0.3968046933412552)}

Episode step 49770, time diff 4.451939821243286, total time dif 15329.666034698486)
step: 49770 @ episode report: {'average_total_reward': np.float32(10.31), 'reward_variance': np.float32(4.6103086), 'max_total_reward': np.float32(14.511112), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.05455256290733814), 'actor_loss': np.float64(-0.9770478904247284), 'hyper_actor_loss': np.float64(1.0092873435496585e-05), 'behavior_loss': np.float64(0.38854099214077)}

Episode step 49780, time diff 4.518671274185181, total time dif 15334.11797451973)
step: 49780 @ episode report: {'average_total_reward': np.float32(9.885556), 'reward_variance': np.float32(5.938817), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(5.6555557), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(7.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07277811355888844), 'actor_loss': np.float64(-0.9543009877204895), 'hyper_actor_loss': np.float64(1.1398565129638883e-05), 'behavior_loss': np.float64(0.3943524330854416)}

Episode step 49790, time diff 4.430012226104736, total time dif 15338.636645793915)
step: 49790 @ episode report: {'average_total_reward': np.float32(10.273334), 'reward_variance': np.float32(1.4015357), 'max_total_reward': np.float32(12.144444), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.08129938617348671), 'actor_loss': np.float64(-0.9873865842819214), 'hyper_actor_loss': np.float64(1.2720474023808492e-05), 'behavior_loss': np.float64(0.3865823447704315)}

Episode step 49800, time diff 4.652844429016113, total time dif 15343.06665802002)
step: 49800 @ episode report: {'average_total_reward': np.float32(10.710001), 'reward_variance': np.float32(2.701321), 'max_total_reward': np.float32(13.266668), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07327472344040871), 'actor_loss': np.float64(-1.008176040649414), 'hyper_actor_loss': np.float64(9.467507970839506e-06), 'behavior_loss': np.float64(0.3901060700416565)}

Episode step 49810, time diff 4.466144561767578, total time dif 15347.719502449036)
step: 49810 @ episode report: {'average_total_reward': np.float32(9.848889), 'reward_variance': np.float32(0.8315605), 'max_total_reward': np.float32(11.144444), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(10.9), 'max_n_step': np.float32(12.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06491545103490352), 'actor_loss': np.float64(-0.9834581971168518), 'hyper_actor_loss': np.float64(9.138846417044988e-06), 'behavior_loss': np.float64(0.3806283354759216)}

Episode step 49820, time diff 4.468334913253784, total time dif 15352.185647010803)
step: 49820 @ episode report: {'average_total_reward': np.float32(10.51), 'reward_variance': np.float32(2.8187275), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(6.5333333), 'average_n_step': np.float32(11.5), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0617296164855361), 'actor_loss': np.float64(-0.9628040671348572), 'hyper_actor_loss': np.float64(8.783734028838807e-06), 'behavior_loss': np.float64(0.38369450271129607)}

Episode step 49830, time diff 4.486863613128662, total time dif 15356.653981924057)
step: 49830 @ episode report: {'average_total_reward': np.float32(9.961112), 'reward_variance': np.float32(2.0491672), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.0), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07385924160480499), 'actor_loss': np.float64(-0.9807147920131684), 'hyper_actor_loss': np.float64(1.0610444678604836e-05), 'behavior_loss': np.float64(0.3857161670923233)}

Episode step 49840, time diff 4.53027868270874, total time dif 15361.140845537186)
step: 49840 @ episode report: {'average_total_reward': np.float32(10.871111), 'reward_variance': np.float32(6.1164503), 'max_total_reward': np.float32(15.633334), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(11.8), 'max_n_step': np.float32(16.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06475719548761845), 'actor_loss': np.float64(-0.9860899031162262), 'hyper_actor_loss': np.float64(1.296961636398919e-05), 'behavior_loss': np.float64(0.3876890778541565)}

Episode step 49850, time diff 4.529260635375977, total time dif 15365.671124219894)
step: 49850 @ episode report: {'average_total_reward': np.float32(11.356667), 'reward_variance': np.float32(3.7346032), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07303785607218742), 'actor_loss': np.float64(-0.9806252360343933), 'hyper_actor_loss': np.float64(1.0172996917390265e-05), 'behavior_loss': np.float64(0.3784092903137207)}

Episode step 49860, time diff 4.48540735244751, total time dif 15370.20038485527)
step: 49860 @ episode report: {'average_total_reward': np.float32(11.620001), 'reward_variance': np.float32(3.809082), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(8.655556), 'average_n_step': np.float32(12.5), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06412308067083358), 'actor_loss': np.float64(-0.9971019923686981), 'hyper_actor_loss': np.float64(9.123392192122993e-06), 'behavior_loss': np.float64(0.3806643605232239)}

Episode step 49870, time diff 4.504780530929565, total time dif 15374.685792207718)
step: 49870 @ episode report: {'average_total_reward': np.float32(11.432223), 'reward_variance': np.float32(4.196828), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777777), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06696550268679857), 'actor_loss': np.float64(-0.9797708988189697), 'hyper_actor_loss': np.float64(9.328117175755325e-06), 'behavior_loss': np.float64(0.38198950588703157)}

Episode step 49880, time diff 4.713604211807251, total time dif 15379.190572738647)
step: 49880 @ episode report: {'average_total_reward': np.float32(11.407779), 'reward_variance': np.float32(4.2631617), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(12.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.060167265310883525), 'actor_loss': np.float64(-0.9606337964534759), 'hyper_actor_loss': np.float64(8.904673723009183e-06), 'behavior_loss': np.float64(0.3788509458303452)}

Episode step 49890, time diff 4.7939770221710205, total time dif 15383.904176950455)
step: 49890 @ episode report: {'average_total_reward': np.float32(10.14889), 'reward_variance': np.float32(2.0717578), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.5333343), 'average_n_step': np.float32(11.2), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0644176684319973), 'actor_loss': np.float64(-0.9679578006267547), 'hyper_actor_loss': np.float64(9.081843654712429e-06), 'behavior_loss': np.float64(0.3836438864469528)}

Episode step 49900, time diff 4.911301136016846, total time dif 15388.698153972626)
step: 49900 @ episode report: {'average_total_reward': np.float32(11.195557), 'reward_variance': np.float32(2.9368446), 'max_total_reward': np.float32(14.266666), 'min_total_reward': np.float32(8.9), 'average_n_step': np.float32(12.1), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.0652067244052887), 'actor_loss': np.float64(-0.9814360976219177), 'hyper_actor_loss': np.float64(0.00010369150659244042), 'behavior_loss': np.float64(0.39075927436351776)}

Episode step 49910, time diff 4.8731207847595215, total time dif 15393.609455108643)
step: 49910 @ episode report: {'average_total_reward': np.float32(10.134445), 'reward_variance': np.float32(2.8839865), 'max_total_reward': np.float32(12.266667), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.1), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07256323806941509), 'actor_loss': np.float64(-0.9807088494300842), 'hyper_actor_loss': np.float64(0.0005112773680593819), 'behavior_loss': np.float64(0.4037125647068024)}

Episode step 49920, time diff 4.886852502822876, total time dif 15398.482575893402)
step: 49920 @ episode report: {'average_total_reward': np.float32(9.636667), 'reward_variance': np.float32(2.6805696), 'max_total_reward': np.float32(13.266667), 'min_total_reward': np.float32(6.6555557), 'average_n_step': np.float32(10.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(8.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07511784099042415), 'actor_loss': np.float64(-1.0054222822189331), 'hyper_actor_loss': np.float64(0.0005003707978175953), 'behavior_loss': np.float64(0.40503230392932893)}

Episode step 49930, time diff 4.713987112045288, total time dif 15403.369428396225)
step: 49930 @ episode report: {'average_total_reward': np.float32(10.946668), 'reward_variance': np.float32(2.3152795), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07461409196257592), 'actor_loss': np.float64(-1.0028847217559815), 'hyper_actor_loss': np.float64(0.00030759403307456523), 'behavior_loss': np.float64(0.4027988642454147)}

Episode step 49940, time diff 4.557989835739136, total time dif 15408.08341550827)
step: 49940 @ episode report: {'average_total_reward': np.float32(11.332224), 'reward_variance': np.float32(2.2390976), 'max_total_reward': np.float32(14.511111), 'min_total_reward': np.float32(8.777779), 'average_n_step': np.float32(12.2), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07162542454898357), 'actor_loss': np.float64(-0.9733789682388305), 'hyper_actor_loss': np.float64(0.00013531590011552907), 'behavior_loss': np.float64(0.3955565869808197)}

Episode step 49950, time diff 4.403245687484741, total time dif 15412.64140534401)
step: 49950 @ episode report: {'average_total_reward': np.float32(10.261111), 'reward_variance': np.float32(2.8349943), 'max_total_reward': np.float32(13.144444), 'min_total_reward': np.float32(7.6555557), 'average_n_step': np.float32(11.3), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06162180192768574), 'actor_loss': np.float64(-0.9756483554840087), 'hyper_actor_loss': np.float64(5.424442242656369e-05), 'behavior_loss': np.float64(0.3846096873283386)}

Episode step 49960, time diff 4.571241855621338, total time dif 15417.044651031494)
step: 49960 @ episode report: {'average_total_reward': np.float32(10.75889), 'reward_variance': np.float32(4.23773), 'max_total_reward': np.float32(13.38889), 'min_total_reward': np.float32(7.777778), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06463973224163055), 'actor_loss': np.float64(-0.9803315997123718), 'hyper_actor_loss': np.float64(2.402450372755993e-05), 'behavior_loss': np.float64(0.3839000791311264)}

Episode step 49970, time diff 4.616128921508789, total time dif 15421.615892887115)
step: 49970 @ episode report: {'average_total_reward': np.float32(10.95889), 'reward_variance': np.float32(3.6925702), 'max_total_reward': np.float32(14.38889), 'min_total_reward': np.float32(7.7777786), 'average_n_step': np.float32(11.9), 'max_n_step': np.float32(15.0), 'min_n_step': np.float32(9.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07493101581931114), 'actor_loss': np.float64(-0.9847112357616424), 'hyper_actor_loss': np.float64(1.5283098218787927e-05), 'behavior_loss': np.float64(0.38397293984889985)}

Episode step 49980, time diff 4.588797092437744, total time dif 15426.232021808624)
step: 49980 @ episode report: {'average_total_reward': np.float32(10.722223), 'reward_variance': np.float32(1.6449379), 'max_total_reward': np.float32(12.266666), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.7), 'max_n_step': np.float32(13.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.07226026132702827), 'actor_loss': np.float64(-0.9957780599594116), 'hyper_actor_loss': np.float64(1.3469772602547891e-05), 'behavior_loss': np.float64(0.38616168200969697)}

Episode step 49990, time diff 4.499911308288574, total time dif 15430.820818901062)
step: 49990 @ episode report: {'average_total_reward': np.float32(10.446668), 'reward_variance': np.float32(2.089723), 'max_total_reward': np.float32(13.388888), 'min_total_reward': np.float32(8.900001), 'average_n_step': np.float32(11.4), 'max_n_step': np.float32(14.0), 'min_n_step': np.float32(10.0), 'buffer_size': 100000} @ step loss: {'critic_loss': np.float64(0.06951032355427741), 'actor_loss': np.float64(-0.9836357712745667), 'hyper_actor_loss': np.float64(2.5716617528814822e-05), 'behavior_loss': np.float64(0.3810143947601318)}

