Namespace(model='ML1MUserResponse', reader='ML1MDataReader')
Namespace(seed=19, batch_size=128, lr=0.001, epoch=2, model_path='output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003_eval.model', loss='bce', l2_coef=0.0003, feature_dim=16, attn_n_head=2, hidden_dims=[256], dropout_rate=0.2, train_file='dataset/ml1m/ml1m_b_test.csv', val_file='dataset/ml1m/ml1m_b_test.csv', test_file='', n_worker=4, data_separator='@', user_meta_file='dataset/ml1m/user_info.npy', item_meta_file='dataset/ml1m/item_info.npy', max_seq_len=50, meta_data_separator=' ')
init ml1m reader
Loading data filesLoad item meta data
{'length': 713, 'n_item': 1682, 'item_vec_size': 19, 'user_portrait_len': 24, 'max_seq_len': 50}
{'length': 713, 'n_item': 1682, 'item_vec_size': 19, 'user_portrait_len': 24, 'max_seq_len': 50}
epoch 1 training
self.sigmoid(preds):  tensor([0.5079, 0.5044, 0.5047,  ..., 0.5243, 0.5243, 0.5075],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 1.,  ..., 1., 1., 1.])
loss:  tensor(0.6995, grad_fn=<MeanBackward0>) l2:  tensor(0.3466, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4843, 0.5080, 0.4738,  ..., 0.5064, 0.4694, 0.4845],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 1.,  ..., 1., 0., 0.])
loss:  tensor(0.7243, grad_fn=<MeanBackward0>) l2:  tensor(0.3460, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4918, 0.4870, 0.5250,  ..., 0.5023, 0.5106, 0.4908],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 0., 0.,  ..., 0., 0., 1.])
loss:  tensor(0.7078, grad_fn=<MeanBackward0>) l2:  tensor(0.3455, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4816, 0.4893, 0.4816,  ..., 0.5009, 0.4934, 0.4950],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 1.,  ..., 1., 1., 0.])
loss:  tensor(0.7089, grad_fn=<MeanBackward0>) l2:  tensor(0.3450, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4817, 0.4744, 0.4908,  ..., 0.4716, 0.4711, 0.4633],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 1., 1.,  ..., 0., 1., 1.])
loss:  tensor(0.7202, grad_fn=<MeanBackward0>) l2:  tensor(0.3445, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4760, 0.4692, 0.4914, 0.4571, 0.4370, 0.4692, 0.4775, 0.4729, 0.4729,
        0.4901, 0.4649, 0.4660, 0.4633, 0.4660, 0.4633, 0.4498, 0.4649, 0.4649,
        0.4807, 0.4649, 0.4479, 0.4681, 0.4710, 0.4555, 0.4858, 0.4761, 0.4951,
        0.4579, 0.4822, 0.4678, 0.4842, 0.4842, 0.4939, 0.4855, 0.4855, 0.4939,
        0.4592, 0.4855, 0.4463, 0.4770, 0.4635, 0.5144, 0.4672, 0.4635, 0.4952,
        0.4672, 0.4672, 0.4628, 0.4502, 0.4791, 0.4731, 0.4658, 0.4870, 0.4731,
        0.4779, 0.4779, 0.4763, 0.4440, 0.4440, 0.4807, 0.4815, 0.4990, 0.4783,
        0.4530, 0.4745, 0.4512, 0.4654, 0.4717, 0.4793, 0.4941, 0.4658, 0.4658,
        0.4713, 0.4887, 0.4690, 0.4760, 0.4713, 0.4508, 0.4980, 0.4859, 0.4529,
        0.4595, 0.4770, 0.4819, 0.4605, 0.4880, 0.4805, 0.4880, 0.4345, 0.4970,
        0.4878, 0.4739, 0.4878, 0.4882, 0.4878, 0.4785, 0.4882, 0.5025, 0.4922,
        0.4841, 0.4510, 0.4712, 0.4905, 0.4712, 0.4579, 0.4975, 0.4712, 0.4734,
        0.4708, 0.4520, 0.4815, 0.4784, 0.4534, 0.4846, 0.4763, 0.4600, 0.4752,
        0.4752, 0.4560, 0.4658, 0.4676, 0.4458, 0.4576, 0.4629, 0.4666, 0.4460,
        0.4604, 0.4617, 0.4601, 0.4802, 0.4943, 0.4518, 0.4951, 0.4750, 0.4844,
        0.4528, 0.4717, 0.4683, 0.4697, 0.4844, 0.4677, 0.4830, 0.4587, 0.4699,
        0.4733, 0.4930, 0.4699, 0.4733, 0.4712, 0.4699, 0.4653, 0.4691, 0.4698,
        0.4739, 0.4915, 0.4653, 0.4653, 0.4653, 0.4446, 0.4587, 0.4644, 0.5001,
        0.4816, 0.4915, 0.4750, 0.4750, 0.4843, 0.4756, 0.4750, 0.4750, 0.4726,
        0.4668, 0.4726, 0.5056, 0.4654, 0.4630, 0.4767, 0.4810, 0.4726, 0.4726,
        0.4696, 0.4782, 0.5007, 0.4923, 0.4611, 0.4730, 0.4909, 0.4730, 0.4730,
        0.4710, 0.4779, 0.4814, 0.4739, 0.4721, 0.4509, 0.4909, 0.4906, 0.4721,
        0.4767, 0.4739, 0.4723, 0.4723, 0.4718, 0.4723, 0.4768, 0.4723, 0.4735,
        0.5011, 0.4723, 0.5176, 0.4988, 0.4912, 0.4782, 0.4719, 0.4593, 0.4782,
        0.4782, 0.4968, 0.5091, 0.4537, 0.4682, 0.4515, 0.5002, 0.4628, 0.4924,
        0.4660, 0.4660, 0.4423, 0.4628, 0.4660, 0.4910, 0.4701, 0.4695, 0.4775,
        0.4731, 0.4768, 0.4640, 0.4768, 0.4752, 0.4695, 0.4575, 0.4661, 0.4622,
        0.4556, 0.4486, 0.4568, 0.4622, 0.4719, 0.4887, 0.4778, 0.4714, 0.4896,
        0.4882, 0.4538, 0.4999, 0.4752, 0.4618, 0.5047, 0.4796, 0.4833, 0.4664,
        0.4822, 0.4642, 0.4344, 0.4887, 0.4344, 0.4628, 0.4642, 0.4642, 0.4477,
        0.4789, 0.5057, 0.4759, 0.4753, 0.4759, 0.4753, 0.4759, 0.4753, 0.4753,
        0.4747, 0.4718, 0.4573, 0.4718, 0.5105, 0.4574, 0.4803, 0.4706, 0.4718,
        0.4841, 0.4906, 0.4731, 0.4721, 0.4602, 0.4375, 0.4379, 0.4627, 0.4812,
        0.4697, 0.4701, 0.4512, 0.4754, 0.4710, 0.4756, 0.4564, 0.4677, 0.4756,
        0.4742, 0.4564, 0.4802, 0.4661, 0.4881, 0.4484, 0.4649, 0.4563, 0.4621,
        0.4484, 0.4794, 0.4835, 0.5085, 0.4783, 0.4754, 0.4702, 0.4923, 0.4686,
        0.4734, 0.4681, 0.4923, 0.4867, 0.4595, 0.4708, 0.4686, 0.4686, 0.4730,
        0.4543, 0.4730, 0.4810, 0.4673, 0.4793, 0.4885, 0.4601, 0.4712, 0.4504,
        0.4700, 0.4844, 0.4689, 0.4656, 0.4729, 0.4689, 0.4712, 0.4702, 0.4709,
        0.4709, 0.4942, 0.4503, 0.4632, 0.4859, 0.4316, 0.4709, 0.4709, 0.4715,
        0.4683, 0.4689, 0.4683, 0.4736, 0.4936, 0.4921, 0.4547, 0.4600, 0.4471,
        0.4815, 0.4811, 0.4882, 0.4639, 0.4700, 0.4821, 0.4700, 0.4747, 0.4513,
        0.4700, 0.4524, 0.4986, 0.4411, 0.4782, 0.4885, 0.4602, 0.4757, 0.4812,
        0.4462, 0.4723, 0.4955, 0.4641, 0.5038, 0.4527, 0.4726, 0.4641, 0.4357,
        0.5130, 0.4639, 0.4721, 0.4843, 0.4809, 0.4689, 0.4544, 0.4616, 0.4968,
        0.4531, 0.4671, 0.4766, 0.4671, 0.4764, 0.4595, 0.4652, 0.4686, 0.4653,
        0.4727, 0.4766, 0.4766, 0.4615, 0.4767, 0.4610, 0.4579, 0.4522, 0.4773,
        0.4886, 0.4892, 0.4521, 0.4779, 0.4393, 0.4524, 0.4867, 0.4980, 0.4759,
        0.4809, 0.4811, 0.4759, 0.4748, 0.4916, 0.4748, 0.4759, 0.4759, 0.4423,
        0.4818, 0.4637, 0.4780, 0.4780, 0.4764, 0.4942, 0.4948, 0.4850, 0.4742,
        0.4529, 0.4698, 0.4960, 0.4731, 0.4959, 0.4698, 0.4710, 0.4620, 0.4748,
        0.4866, 0.4588, 0.4761, 0.4706, 0.4603, 0.4717, 0.4527, 0.4550, 0.4711,
        0.4809, 0.5076, 0.4883, 0.4916, 0.4916, 0.4883, 0.4883, 0.4883, 0.4883,
        0.4730, 0.4801, 0.4748, 0.4751, 0.4843, 0.4802, 0.4757, 0.4757, 0.4748,
        0.4751, 0.4757, 0.4849, 0.4798, 0.4668, 0.4789, 0.5050, 0.4726, 0.4726,
        0.4726, 0.4774, 0.4821, 0.4756, 0.4668, 0.4872, 0.4872, 0.4589, 0.4933,
        0.4931, 0.4872, 0.4872, 0.4872, 0.5207, 0.4959, 0.4889, 0.4715, 0.4715,
        0.4665, 0.4675, 0.4889, 0.4742, 0.4795, 0.4532, 0.4755, 0.4667, 0.4508,
        0.4753, 0.4656, 0.4727, 0.4773, 0.4635, 0.4731, 0.4800, 0.4729, 0.4631,
        0.4748, 0.4755, 0.4518, 0.4767, 0.4755, 0.4710, 0.5082, 0.4751, 0.4832,
        0.4628, 0.4628, 0.4717, 0.4694, 0.4628, 0.4694, 0.4808, 0.4516, 0.4503,
        0.4743, 0.4433, 0.4699, 0.4713, 0.4664, 0.4540, 0.4834, 0.4738, 0.4859,
        0.4751, 0.4751, 0.4561, 0.4787, 0.4795, 0.4787, 0.4573, 0.4861, 0.4503,
        0.4639, 0.4744, 0.4677, 0.4764, 0.4789, 0.4514, 0.4681, 0.4951, 0.4668,
        0.4668, 0.4642, 0.4512, 0.4889, 0.4614, 0.4763, 0.4679, 0.4727, 0.4582,
        0.4614, 0.4727, 0.4537, 0.4713, 0.4679, 0.4994, 0.4799, 0.4486, 0.4721,
        0.4841, 0.4573, 0.4520, 0.4666, 0.4633, 0.4573, 0.4827, 0.4880, 0.4802,
        0.5038, 0.5034, 0.4802, 0.4776, 0.4802, 0.4690, 0.4833, 0.4825, 0.4542,
        0.4635, 0.4663, 0.4635, 0.4342, 0.4663, 0.4663, 0.4663, 0.4743, 0.5144,
        0.4932, 0.4789, 0.4949, 0.4845, 0.4932, 0.4774, 0.4651, 0.4620, 0.4673,
        0.4687, 0.4736, 0.4736, 0.4964, 0.4687, 0.4736, 0.4736, 0.4736, 0.4687,
        0.4512, 0.4742, 0.4742, 0.4783, 0.4723, 0.4796, 0.4752, 0.4667, 0.4810,
        0.4821, 0.5063, 0.4775, 0.4911, 0.4775, 0.4787, 0.5104, 0.4863, 0.4922,
        0.4787, 0.5027, 0.4787, 0.4651, 0.4681, 0.4826, 0.4433, 0.4643, 0.4793,
        0.4590, 0.5016, 0.4905, 0.4597, 0.4367, 0.4818, 0.4429, 0.4748, 0.4847,
        0.4584, 0.4866, 0.4723, 0.4713, 0.4976, 0.4559, 0.4638, 0.4179, 0.4638,
        0.4764, 0.4458, 0.4673, 0.4638, 0.4638, 0.4892, 0.4587, 0.4772, 0.4662,
        0.4611, 0.4761, 0.4762, 0.4915, 0.4915, 0.4666, 0.4888, 0.4664, 0.4664,
        0.4664, 0.4664, 0.4664, 0.4712, 0.4712, 0.4613, 0.4712, 0.4712, 0.4686,
        0.4548, 0.4575, 0.4680, 0.4569, 0.4934, 0.4845, 0.4534, 0.4534, 0.4686,
        0.4920, 0.4924, 0.4951, 0.4711, 0.4924, 0.4926, 0.4990, 0.4901, 0.4924,
        0.4924], grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0.])
loss:  tensor(0.7190, grad_fn=<MeanBackward0>) l2:  tensor(0.3441, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
Epoch 1; time 0.2323
epoch 1 validating; auc: 0.5351
Model (checkpoint) saved to output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003_eval.model
epoch 2 training
self.sigmoid(preds):  tensor([0.4506, 0.4570, 0.4752,  ..., 0.4753, 0.4409, 0.4405],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 1.,  ..., 0., 0., 0.])
loss:  tensor(0.7079, grad_fn=<MeanBackward0>) l2:  tensor(0.3438, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4494, 0.4603, 0.4630,  ..., 0.4472, 0.4204, 0.4537],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 1.,  ..., 1., 1., 0.])
loss:  tensor(0.7171, grad_fn=<MeanBackward0>) l2:  tensor(0.3435, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4421, 0.4409, 0.4183,  ..., 0.4697, 0.4671, 0.4697],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 1.,  ..., 0., 0., 0.])
loss:  tensor(0.7108, grad_fn=<MeanBackward0>) l2:  tensor(0.3432, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4119, 0.4167, 0.4430,  ..., 0.4430, 0.4451, 0.4589],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 0., 1.,  ..., 1., 0., 1.])
loss:  tensor(0.7037, grad_fn=<MeanBackward0>) l2:  tensor(0.3430, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4373, 0.4456, 0.4679,  ..., 0.4435, 0.4370, 0.4370],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 1., 1.,  ..., 0., 0., 0.])
loss:  tensor(0.7049, grad_fn=<MeanBackward0>) l2:  tensor(0.3428, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4147, 0.4383, 0.4226, 0.4228, 0.4183, 0.4409, 0.4226, 0.4183, 0.4619,
        0.4226, 0.4223, 0.4219, 0.4132, 0.4198, 0.4123, 0.4199, 0.3851, 0.4367,
        0.4665, 0.4173, 0.4134, 0.4267, 0.4087, 0.4154, 0.4273, 0.4152, 0.4254,
        0.4060, 0.4139, 0.4403, 0.4215, 0.4296, 0.3875, 0.4463, 0.4296, 0.4253,
        0.4253, 0.4253, 0.4296, 0.4172, 0.4390, 0.4184, 0.4314, 0.4292, 0.4182,
        0.4028, 0.4439, 0.4292, 0.4209, 0.4290, 0.4232, 0.4232, 0.4232, 0.4232,
        0.4520, 0.4520, 0.4563, 0.4404, 0.4589, 0.4096, 0.4642, 0.4222, 0.4222,
        0.4222, 0.4676, 0.4195, 0.4222, 0.4222, 0.4091, 0.4119, 0.4481, 0.4481,
        0.4481, 0.3780, 0.3781, 0.3781, 0.3936, 0.4166, 0.3978, 0.3978, 0.4039,
        0.4210, 0.4527, 0.4197, 0.4210, 0.3913, 0.4259, 0.4154, 0.4210, 0.4163,
        0.3916, 0.4170, 0.3828, 0.3959, 0.4170, 0.3959, 0.4170, 0.3916, 0.3916,
        0.4214, 0.4637, 0.4315, 0.3911, 0.4020, 0.4216, 0.3803, 0.3615, 0.3940,
        0.4110, 0.4401, 0.4493, 0.4091, 0.4213, 0.4287, 0.4334, 0.4366, 0.4594,
        0.4218, 0.4366, 0.4329, 0.4419, 0.4086, 0.4629, 0.4350, 0.3921, 0.4316,
        0.3993, 0.3744, 0.4343, 0.4005, 0.4353, 0.4094, 0.4097, 0.4255, 0.4482,
        0.4280, 0.4350, 0.4321, 0.4350, 0.4199, 0.4237, 0.4237, 0.4166, 0.4237,
        0.4237, 0.4215, 0.4040, 0.4144, 0.4114, 0.3723, 0.4148, 0.4193, 0.4267,
        0.4070, 0.4099, 0.3939, 0.4406, 0.4126, 0.4126, 0.4756, 0.4059, 0.4721,
        0.4530, 0.4148, 0.4148, 0.4373, 0.4559, 0.4559, 0.4290, 0.4277, 0.4271,
        0.4324, 0.4271, 0.4268, 0.4268, 0.3957, 0.4205, 0.4271, 0.4764, 0.4645,
        0.4209, 0.4026, 0.4614, 0.4135, 0.4135, 0.4188, 0.4493, 0.4026, 0.4209,
        0.4181, 0.4501, 0.4501, 0.4291, 0.4242, 0.4242, 0.4291, 0.4291, 0.4327,
        0.4327, 0.4190, 0.4212, 0.4236, 0.4279, 0.4236, 0.4533, 0.3845, 0.4086,
        0.4737, 0.4475, 0.4850, 0.4390, 0.4376, 0.4664, 0.4332, 0.4217, 0.4237,
        0.4217, 0.4175, 0.4162, 0.4100, 0.4104, 0.4419, 0.4024, 0.4012, 0.4012,
        0.4881, 0.4229, 0.4077, 0.4172, 0.4547, 0.4466, 0.4509, 0.4259, 0.3837,
        0.4028, 0.3995, 0.4211, 0.4284, 0.3949, 0.4093, 0.4293, 0.4293, 0.4445,
        0.4519, 0.4020, 0.4170, 0.4286, 0.4094, 0.4170, 0.3993, 0.3740, 0.3740,
        0.3394, 0.4078, 0.4078, 0.4311, 0.3576, 0.3866, 0.4310, 0.4256, 0.4366,
        0.4078, 0.4017, 0.3923, 0.4018, 0.4202, 0.3759, 0.4242, 0.4063, 0.4134,
        0.4214, 0.4214, 0.4197, 0.4139, 0.4037, 0.4214, 0.4142, 0.4214, 0.4197,
        0.4416, 0.4464, 0.4294, 0.4136, 0.4294, 0.4271, 0.4456, 0.3993, 0.4063,
        0.4549, 0.4229, 0.4276, 0.3983, 0.4764, 0.4276, 0.4177, 0.4226, 0.4226,
        0.4226, 0.4276, 0.4276, 0.4207, 0.4271, 0.4102, 0.4168, 0.4564, 0.4147,
        0.4495, 0.4271, 0.4684, 0.4400, 0.4608, 0.4266, 0.3880, 0.4315, 0.4667,
        0.4236, 0.4274, 0.4274, 0.4274, 0.4315, 0.4169, 0.4419, 0.4189, 0.4233,
        0.4233, 0.4166, 0.4233, 0.4233, 0.4233, 0.3959, 0.4279, 0.4241, 0.4208,
        0.4134, 0.4276, 0.4208, 0.4181, 0.4134, 0.4112, 0.4288, 0.4354, 0.3763,
        0.4175, 0.4094, 0.4561, 0.4094, 0.4177, 0.4122, 0.4446, 0.4177, 0.4142,
        0.4288, 0.4283, 0.4226, 0.4099, 0.4142, 0.4226, 0.4170, 0.4309, 0.4283,
        0.4102, 0.4511, 0.3926, 0.4140, 0.4140, 0.4301, 0.4140, 0.4102, 0.4622,
        0.4161, 0.3920, 0.4086, 0.4116, 0.4182, 0.4116, 0.4116, 0.4124, 0.4430,
        0.3888, 0.3946, 0.3648, 0.4116, 0.4081, 0.3987, 0.4155, 0.4230, 0.4116,
        0.4203, 0.4155, 0.3987, 0.4550, 0.4379, 0.4241, 0.4439, 0.3976, 0.4329,
        0.4555, 0.4531, 0.4127, 0.4328, 0.4551, 0.4159, 0.4120, 0.4371, 0.4962,
        0.4337, 0.4179, 0.4381, 0.4438, 0.4381, 0.4456, 0.4153, 0.4111, 0.4111,
        0.4153, 0.4224, 0.4158, 0.4153, 0.4219, 0.4182, 0.4289, 0.4191, 0.4191,
        0.4063, 0.4107, 0.4191, 0.4622, 0.4506, 0.4107, 0.4288, 0.4563, 0.4421,
        0.4409, 0.4161, 0.4494, 0.3970, 0.4137, 0.4553, 0.4419, 0.4094, 0.4948,
        0.4255, 0.4135, 0.4135, 0.4303, 0.4470, 0.4303, 0.4448, 0.3994, 0.4255,
        0.4230, 0.4479, 0.4205, 0.4279, 0.4247, 0.4198, 0.4314, 0.4259, 0.3987,
        0.4247, 0.4016, 0.4164, 0.4286, 0.4553, 0.4275, 0.4098, 0.4452, 0.4275,
        0.4562, 0.4452, 0.4510, 0.4547, 0.4603, 0.3941, 0.4603, 0.4563, 0.4603,
        0.4603, 0.4292, 0.4603, 0.4535, 0.4291, 0.4710, 0.4626, 0.4362, 0.4254,
        0.4385, 0.4291, 0.4291, 0.4291, 0.4452, 0.4526, 0.4526, 0.4526, 0.4526,
        0.4670, 0.4526, 0.4465, 0.4526, 0.4496, 0.4573, 0.4123, 0.4255, 0.3808,
        0.4479, 0.4210, 0.4121, 0.4167, 0.4153, 0.3961, 0.3925, 0.4247, 0.3961,
        0.4362, 0.4097, 0.3985, 0.4101, 0.4432, 0.4162, 0.4047, 0.4277, 0.4162,
        0.4094, 0.4428, 0.4519, 0.4055, 0.4209, 0.4055, 0.4241, 0.4088, 0.4313,
        0.4313, 0.4313, 0.4260, 0.4280, 0.4006, 0.4467, 0.4148, 0.4313, 0.4135,
        0.4485, 0.4236, 0.4140, 0.3948, 0.4349, 0.4003, 0.4140, 0.4165, 0.4167,
        0.4162, 0.4542, 0.4337, 0.4409, 0.4198, 0.4298, 0.4455, 0.4107, 0.3761,
        0.4059, 0.4198, 0.4738, 0.4446, 0.4139, 0.4139, 0.4139, 0.4139, 0.4738,
        0.4318, 0.4318, 0.4056, 0.4172, 0.4077, 0.4517, 0.4172, 0.4444, 0.4401,
        0.4096, 0.4431, 0.4431, 0.4172, 0.4699, 0.3942, 0.4524, 0.4065, 0.4247,
        0.4173, 0.4350, 0.3821, 0.4106, 0.4106, 0.4548, 0.4234, 0.4097, 0.4650,
        0.4558, 0.4023, 0.4558, 0.4419, 0.4641, 0.4128, 0.4060, 0.4123, 0.4125,
        0.4123, 0.3915, 0.4176, 0.3998, 0.4125, 0.4125, 0.4125, 0.4012, 0.4333,
        0.4289, 0.4155, 0.4217, 0.4241, 0.4308, 0.3689, 0.4230, 0.4292, 0.4197,
        0.4248, 0.4248, 0.4319, 0.4159, 0.4064, 0.4248, 0.4301, 0.4143, 0.4248,
        0.3937, 0.4077, 0.4232, 0.4173, 0.3984, 0.4098, 0.3984, 0.4077, 0.4496,
        0.4005, 0.4598, 0.4220, 0.4471, 0.4284, 0.4201, 0.4528, 0.4256, 0.4618,
        0.4256, 0.4528, 0.4132, 0.4729, 0.4132, 0.4132, 0.3968, 0.4185, 0.4213,
        0.4028, 0.3603, 0.4620, 0.4017, 0.4001, 0.4017, 0.3948, 0.4476, 0.4241,
        0.4071, 0.4191, 0.4404, 0.4071, 0.4183, 0.4131, 0.4204, 0.4421, 0.4455,
        0.4183, 0.4183, 0.4183, 0.3803, 0.3988, 0.4623, 0.4434, 0.4224, 0.4233,
        0.4481, 0.4149, 0.4152, 0.4429, 0.4308, 0.4149, 0.4020, 0.4094, 0.4140,
        0.4563, 0.3890, 0.4140, 0.4094, 0.4129, 0.4617, 0.4370, 0.4409, 0.4187,
        0.4597, 0.4540, 0.4114, 0.4139, 0.4116, 0.4367, 0.3910, 0.4114, 0.4331,
        0.4228, 0.4218, 0.4626, 0.4456, 0.3804, 0.3755, 0.4318, 0.4174, 0.4506,
        0.4168, 0.4224, 0.4168, 0.4088, 0.4438, 0.4061, 0.4308, 0.4273, 0.3850,
        0.4273], grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 0.])
loss:  tensor(0.7105, grad_fn=<MeanBackward0>) l2:  tensor(0.3426, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
Epoch 2; time 0.1498
epoch 2 validating; auc: 0.5335
Model (checkpoint) saved to output/ml1m/env/ml1m_user_env_lr0.001_reg0.0003_eval.model
