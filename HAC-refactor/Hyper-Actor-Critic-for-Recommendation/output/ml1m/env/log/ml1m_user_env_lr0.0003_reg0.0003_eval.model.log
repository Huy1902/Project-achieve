Namespace(model='ML1MUserResponse', reader='ML1MDataReader')
Namespace(seed=19, batch_size=128, lr=0.0003, epoch=2, model_path='output/ml1m/env/ml1m_user_env_lr0.0003_reg0.0003_eval.model', loss='bce', l2_coef=0.0003, feature_dim=16, attn_n_head=2, hidden_dims=[256], dropout_rate=0.2, train_file='dataset/ml1m/ml1m_b_test.csv', val_file='dataset/ml1m/ml1m_b_test.csv', test_file='', n_worker=4, data_separator='@', user_meta_file='dataset/ml1m/user_info.npy', item_meta_file='dataset/ml1m/item_info.npy', max_seq_len=50, meta_data_separator=' ')
init ml1m reader
Loading data filesLoad item meta data
{'length': 713, 'n_item': 1682, 'item_vec_size': 19, 'user_portrait_len': 24, 'max_seq_len': 50}
{'length': 713, 'n_item': 1682, 'item_vec_size': 19, 'user_portrait_len': 24, 'max_seq_len': 50}
epoch 1 training
self.sigmoid(preds):  tensor([0.5079, 0.5044, 0.5047,  ..., 0.5243, 0.5243, 0.5075],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 1.,  ..., 1., 1., 1.])
loss:  tensor(0.6995, grad_fn=<MeanBackward0>) l2:  tensor(0.3466, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4888, 0.5118, 0.4791,  ..., 0.5093, 0.4736, 0.4883],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 1.,  ..., 1., 0., 0.])
loss:  tensor(0.7248, grad_fn=<MeanBackward0>) l2:  tensor(0.3464, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4980, 0.4932, 0.5320,  ..., 0.5103, 0.5155, 0.4980],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 0., 0.,  ..., 0., 0., 1.])
loss:  tensor(0.7085, grad_fn=<MeanBackward0>) l2:  tensor(0.3462, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4937, 0.5032, 0.4937,  ..., 0.5099, 0.5040, 0.5026],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 1.,  ..., 1., 1., 0.])
loss:  tensor(0.7099, grad_fn=<MeanBackward0>) l2:  tensor(0.3460, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4944, 0.4888, 0.5016,  ..., 0.4891, 0.4873, 0.4812],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 1., 1.,  ..., 0., 1., 1.])
loss:  tensor(0.7220, grad_fn=<MeanBackward0>) l2:  tensor(0.3459, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.5011, 0.4908, 0.5099, 0.4762, 0.4572, 0.4908, 0.4991, 0.4959, 0.4959,
        0.5010, 0.4883, 0.4889, 0.4886, 0.4889, 0.4886, 0.4758, 0.4883, 0.4883,
        0.5088, 0.4883, 0.4704, 0.4825, 0.4896, 0.4776, 0.5042, 0.4873, 0.5042,
        0.4829, 0.4990, 0.4828, 0.5031, 0.5031, 0.5111, 0.4990, 0.4990, 0.5111,
        0.4812, 0.4990, 0.4645, 0.4980, 0.4859, 0.5344, 0.4864, 0.4859, 0.5110,
        0.4864, 0.4864, 0.4830, 0.4691, 0.4936, 0.4936, 0.4809, 0.4975, 0.4936,
        0.4970, 0.4970, 0.4968, 0.4636, 0.4636, 0.5044, 0.4956, 0.5116, 0.4982,
        0.4709, 0.4929, 0.4648, 0.4892, 0.4899, 0.4868, 0.5094, 0.4864, 0.4864,
        0.4907, 0.5021, 0.4869, 0.4927, 0.4907, 0.4749, 0.5126, 0.4997, 0.4772,
        0.4831, 0.4939, 0.4970, 0.4837, 0.5118, 0.4920, 0.5118, 0.4534, 0.5161,
        0.5019, 0.4904, 0.5019, 0.5063, 0.5019, 0.4974, 0.5063, 0.5202, 0.5086,
        0.4998, 0.4712, 0.4925, 0.5074, 0.4925, 0.4815, 0.5133, 0.4925, 0.4961,
        0.4909, 0.4664, 0.4957, 0.4972, 0.4682, 0.4996, 0.4902, 0.4837, 0.4951,
        0.4951, 0.4760, 0.4830, 0.4884, 0.4677, 0.4821, 0.4860, 0.4887, 0.4833,
        0.4743, 0.4875, 0.4847, 0.4915, 0.5096, 0.4685, 0.5107, 0.4862, 0.5056,
        0.4682, 0.4963, 0.4796, 0.5006, 0.5056, 0.4870, 0.4956, 0.4892, 0.4887,
        0.4912, 0.5040, 0.4887, 0.4912, 0.4895, 0.4887, 0.4872, 0.4942, 0.4927,
        0.4889, 0.5103, 0.4872, 0.4872, 0.4872, 0.4739, 0.4883, 0.4847, 0.5080,
        0.5054, 0.5082, 0.4947, 0.4947, 0.4988, 0.4948, 0.4947, 0.4947, 0.4911,
        0.4789, 0.4911, 0.5255, 0.4823, 0.4852, 0.4966, 0.5018, 0.4911, 0.4911,
        0.4910, 0.4939, 0.5176, 0.5090, 0.4828, 0.4938, 0.5083, 0.4938, 0.4904,
        0.4878, 0.4938, 0.5056, 0.4949, 0.4945, 0.4710, 0.5094, 0.5111, 0.4945,
        0.4844, 0.4949, 0.4919, 0.4929, 0.4955, 0.4929, 0.4909, 0.4919, 0.5010,
        0.5194, 0.4919, 0.5311, 0.5167, 0.5019, 0.4976, 0.4906, 0.4752, 0.4976,
        0.4976, 0.5097, 0.5134, 0.4675, 0.4781, 0.4889, 0.5175, 0.4803, 0.5087,
        0.4877, 0.4877, 0.4709, 0.4803, 0.4877, 0.5011, 0.4899, 0.4887, 0.4965,
        0.4830, 0.4953, 0.4854, 0.4953, 0.4988, 0.4887, 0.4810, 0.4865, 0.4830,
        0.4715, 0.4745, 0.4749, 0.4830, 0.4851, 0.5063, 0.4978, 0.4926, 0.5037,
        0.5032, 0.4796, 0.5160, 0.5003, 0.4852, 0.5141, 0.4954, 0.4921, 0.4901,
        0.5021, 0.4862, 0.4728, 0.5051, 0.4728, 0.4903, 0.4862, 0.4862, 0.4779,
        0.5022, 0.5239, 0.4953, 0.4958, 0.4953, 0.4940, 0.4953, 0.4940, 0.4940,
        0.4945, 0.4926, 0.4800, 0.4926, 0.5175, 0.4818, 0.4981, 0.4919, 0.4926,
        0.4991, 0.5074, 0.4932, 0.4934, 0.4828, 0.4606, 0.4574, 0.4765, 0.5049,
        0.4911, 0.4906, 0.4707, 0.4951, 0.4945, 0.4962, 0.4778, 0.4879, 0.4962,
        0.4986, 0.4778, 0.5089, 0.4823, 0.5118, 0.4624, 0.4712, 0.4718, 0.4831,
        0.4624, 0.4990, 0.5030, 0.5265, 0.4970, 0.5028, 0.4944, 0.5090, 0.4911,
        0.4942, 0.4923, 0.5090, 0.5070, 0.4802, 0.4931, 0.4883, 0.4883, 0.4927,
        0.4688, 0.4927, 0.5051, 0.4885, 0.4850, 0.5102, 0.4836, 0.4919, 0.4767,
        0.4967, 0.5004, 0.4907, 0.4878, 0.5028, 0.4907, 0.4919, 0.4870, 0.4928,
        0.4928, 0.5116, 0.4887, 0.4852, 0.5007, 0.4523, 0.4928, 0.4928, 0.4889,
        0.4910, 0.4931, 0.4910, 0.5012, 0.5107, 0.5120, 0.4701, 0.4766, 0.4697,
        0.4875, 0.4962, 0.5036, 0.4875, 0.4930, 0.4981, 0.4930, 0.4967, 0.4769,
        0.4930, 0.4786, 0.5184, 0.4665, 0.4978, 0.4993, 0.4793, 0.4928, 0.4914,
        0.4661, 0.4940, 0.5130, 0.4868, 0.5178, 0.4695, 0.4931, 0.4868, 0.4597,
        0.5177, 0.4924, 0.4916, 0.5038, 0.4929, 0.4894, 0.4740, 0.4852, 0.5146,
        0.4689, 0.4886, 0.4952, 0.4886, 0.4966, 0.4827, 0.4822, 0.4895, 0.4827,
        0.4979, 0.4970, 0.4970, 0.4833, 0.4961, 0.4762, 0.4743, 0.4792, 0.4931,
        0.5070, 0.5047, 0.4769, 0.5001, 0.4604, 0.4758, 0.5138, 0.5126, 0.4953,
        0.4984, 0.5056, 0.4953, 0.4954, 0.5045, 0.4954, 0.4953, 0.4953, 0.4608,
        0.5070, 0.4829, 0.5001, 0.5001, 0.5049, 0.5121, 0.5137, 0.4989, 0.4945,
        0.4750, 0.4899, 0.5096, 0.4975, 0.5114, 0.4899, 0.4921, 0.4862, 0.4959,
        0.5038, 0.4775, 0.4940, 0.4908, 0.4796, 0.5031, 0.4771, 0.4742, 0.4903,
        0.5022, 0.5128, 0.5032, 0.5086, 0.5086, 0.5032, 0.5032, 0.5032, 0.5032,
        0.4878, 0.4968, 0.4934, 0.4961, 0.4992, 0.4895, 0.4952, 0.4952, 0.4945,
        0.4961, 0.4952, 0.4983, 0.5041, 0.4897, 0.5036, 0.5249, 0.4938, 0.4938,
        0.4938, 0.4862, 0.5011, 0.5009, 0.4897, 0.5010, 0.5010, 0.4803, 0.5101,
        0.5114, 0.5010, 0.5010, 0.5010, 0.5341, 0.5141, 0.5122, 0.4934, 0.4934,
        0.4830, 0.4883, 0.5122, 0.4974, 0.4959, 0.4674, 0.4965, 0.4836, 0.4711,
        0.4949, 0.4881, 0.4894, 0.4975, 0.4870, 0.4886, 0.4959, 0.4877, 0.4830,
        0.4943, 0.4937, 0.4657, 0.4908, 0.4937, 0.4862, 0.5231, 0.4816, 0.4975,
        0.4850, 0.4850, 0.4863, 0.4884, 0.4850, 0.4884, 0.5002, 0.4715, 0.4769,
        0.4990, 0.4668, 0.4899, 0.4839, 0.4851, 0.4685, 0.4910, 0.4941, 0.5012,
        0.4971, 0.4971, 0.4709, 0.4871, 0.4934, 0.4871, 0.4769, 0.5039, 0.4709,
        0.4957, 0.4935, 0.4813, 0.4918, 0.4909, 0.4724, 0.4933, 0.5057, 0.4877,
        0.4877, 0.4818, 0.4700, 0.5028, 0.4838, 0.4934, 0.4885, 0.4918, 0.4810,
        0.4838, 0.4918, 0.4689, 0.4957, 0.4885, 0.5192, 0.4989, 0.4640, 0.4927,
        0.5014, 0.4794, 0.4757, 0.4836, 0.4772, 0.4794, 0.5052, 0.5065, 0.4946,
        0.5186, 0.5211, 0.4946, 0.4960, 0.4946, 0.4862, 0.4992, 0.4984, 0.4848,
        0.4837, 0.4891, 0.4837, 0.4596, 0.4891, 0.4937, 0.4891, 0.4893, 0.5219,
        0.5080, 0.5007, 0.5126, 0.4924, 0.5088, 0.4955, 0.4861, 0.4878, 0.4872,
        0.4878, 0.4918, 0.4918, 0.5108, 0.4878, 0.4918, 0.4918, 0.4918, 0.4878,
        0.4675, 0.4926, 0.4926, 0.4955, 0.4931, 0.4856, 0.4979, 0.4923, 0.4932,
        0.4998, 0.5194, 0.4982, 0.5162, 0.4982, 0.4978, 0.5264, 0.4989, 0.5110,
        0.4978, 0.5179, 0.4978, 0.4878, 0.4903, 0.5035, 0.4734, 0.4861, 0.4878,
        0.4834, 0.5220, 0.5044, 0.4829, 0.4537, 0.4919, 0.4662, 0.4947, 0.5012,
        0.4753, 0.5001, 0.4912, 0.4911, 0.5124, 0.4772, 0.4874, 0.4409, 0.4874,
        0.4938, 0.4710, 0.4822, 0.4874, 0.4874, 0.5117, 0.4928, 0.4977, 0.4878,
        0.4789, 0.4916, 0.4950, 0.5019, 0.5019, 0.4891, 0.5070, 0.4876, 0.4876,
        0.4876, 0.4876, 0.4876, 0.4918, 0.4918, 0.4784, 0.4918, 0.4918, 0.4831,
        0.4798, 0.4778, 0.4864, 0.4810, 0.5140, 0.4989, 0.4731, 0.4731, 0.4831,
        0.5062, 0.5054, 0.5095, 0.4915, 0.5054, 0.5094, 0.5161, 0.5089, 0.5054,
        0.5054], grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,
        1., 0., 1., 0., 1., 0., 0., 0., 1., 0.])
loss:  tensor(0.7213, grad_fn=<MeanBackward0>) l2:  tensor(0.3457, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
Epoch 1; time 0.2298
epoch 1 validating; auc: 0.5290
Model (checkpoint) saved to output/ml1m/env/ml1m_user_env_lr0.0003_reg0.0003_eval.model
epoch 2 training
self.sigmoid(preds):  tensor([0.4790, 0.4840, 0.5076,  ..., 0.5049, 0.4727, 0.4725],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 1.,  ..., 0., 0., 0.])
loss:  tensor(0.7106, grad_fn=<MeanBackward0>) l2:  tensor(0.3456, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4830, 0.4995, 0.5007,  ..., 0.4779, 0.4565, 0.4842],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 1.,  ..., 1., 1., 0.])
loss:  tensor(0.7206, grad_fn=<MeanBackward0>) l2:  tensor(0.3454, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4791, 0.4772, 0.4633,  ..., 0.4933, 0.4974, 0.4933],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 1., 1.,  ..., 0., 0., 0.])
loss:  tensor(0.7144, grad_fn=<MeanBackward0>) l2:  tensor(0.3453, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4676, 0.4565, 0.4880,  ..., 0.4846, 0.4763, 0.4998],
       grad_fn=<SigmoidBackward0>)
target:  tensor([0., 0., 1.,  ..., 1., 0., 1.])
loss:  tensor(0.7073, grad_fn=<MeanBackward0>) l2:  tensor(0.3451, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4878, 0.4896, 0.5160,  ..., 0.4933, 0.4839, 0.4839],
       grad_fn=<SigmoidBackward0>)
target:  tensor([1., 1., 1.,  ..., 0., 0., 0.])
loss:  tensor(0.7093, grad_fn=<MeanBackward0>) l2:  tensor(0.3450, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
self.sigmoid(preds):  tensor([0.4861, 0.4905, 0.4804, 0.4913, 0.4769, 0.4975, 0.4804, 0.4769, 0.4926,
        0.4804, 0.4792, 0.4787, 0.4588, 0.4893, 0.4878, 0.4658, 0.4622, 0.4913,
        0.4825, 0.4830, 0.4759, 0.4892, 0.4822, 0.4590, 0.4900, 0.4685, 0.4780,
        0.4671, 0.4574, 0.4840, 0.4852, 0.4815, 0.4583, 0.4872, 0.4815, 0.4822,
        0.4822, 0.4822, 0.4815, 0.4859, 0.4939, 0.4935, 0.4883, 0.4837, 0.4828,
        0.4801, 0.5041, 0.4837, 0.4919, 0.4834, 0.4792, 0.4792, 0.4792, 0.4792,
        0.5021, 0.5021, 0.4941, 0.4969, 0.5024, 0.4679, 0.5050, 0.4790, 0.4790,
        0.4790, 0.4982, 0.4700, 0.4790, 0.4790, 0.4723, 0.4746, 0.4917, 0.4917,
        0.4917, 0.4689, 0.4581, 0.4581, 0.4689, 0.4740, 0.4713, 0.4713, 0.4688,
        0.4768, 0.5045, 0.4879, 0.4768, 0.4713, 0.4824, 0.4765, 0.4768, 0.4716,
        0.4522, 0.4749, 0.4588, 0.4489, 0.4749, 0.4489, 0.4749, 0.4522, 0.4522,
        0.4701, 0.4983, 0.4911, 0.4659, 0.4606, 0.4854, 0.4633, 0.4651, 0.4634,
        0.4577, 0.4812, 0.4910, 0.4805, 0.4615, 0.4688, 0.4810, 0.4862, 0.5038,
        0.4784, 0.4862, 0.4941, 0.4883, 0.4700, 0.5149, 0.4828, 0.4591, 0.4856,
        0.4644, 0.4534, 0.4891, 0.4655, 0.4904, 0.4704, 0.4689, 0.4729, 0.4735,
        0.4732, 0.4852, 0.4758, 0.4852, 0.4820, 0.4790, 0.4790, 0.4772, 0.4790,
        0.4790, 0.4782, 0.4714, 0.4764, 0.4688, 0.4508, 0.4603, 0.4894, 0.4694,
        0.4801, 0.4765, 0.4619, 0.4815, 0.4765, 0.4765, 0.5162, 0.4715, 0.5171,
        0.4932, 0.4787, 0.4787, 0.4909, 0.4879, 0.4879, 0.4927, 0.4843, 0.4811,
        0.4850, 0.4811, 0.4842, 0.4842, 0.4723, 0.4860, 0.4811, 0.5037, 0.5000,
        0.4759, 0.4663, 0.5036, 0.4755, 0.4755, 0.4664, 0.4889, 0.4663, 0.4759,
        0.4706, 0.4825, 0.4825, 0.4834, 0.4909, 0.4803, 0.4834, 0.4834, 0.4961,
        0.4961, 0.4791, 0.4794, 0.4727, 0.4803, 0.4727, 0.4888, 0.4740, 0.4708,
        0.5202, 0.4958, 0.5129, 0.4948, 0.4959, 0.5199, 0.4823, 0.4858, 0.4822,
        0.4858, 0.4811, 0.4812, 0.4801, 0.4745, 0.4832, 0.4692, 0.4698, 0.4698,
        0.5126, 0.4761, 0.4738, 0.4876, 0.5109, 0.4875, 0.4914, 0.4804, 0.4661,
        0.4681, 0.4671, 0.4796, 0.4771, 0.4643, 0.4865, 0.4968, 0.4968, 0.4946,
        0.4882, 0.4571, 0.4757, 0.4838, 0.4749, 0.4757, 0.4887, 0.4357, 0.4357,
        0.4390, 0.4770, 0.4770, 0.4874, 0.4429, 0.4647, 0.5039, 0.4841, 0.4946,
        0.4691, 0.4738, 0.4751, 0.4720, 0.4820, 0.4594, 0.4724, 0.4656, 0.4835,
        0.4779, 0.4779, 0.4761, 0.4828, 0.4703, 0.4779, 0.4765, 0.4779, 0.4761,
        0.4914, 0.4764, 0.4944, 0.4567, 0.4944, 0.4797, 0.4882, 0.4698, 0.4754,
        0.5017, 0.4883, 0.4820, 0.4682, 0.5075, 0.4820, 0.4781, 0.4782, 0.4782,
        0.4782, 0.4820, 0.4820, 0.4802, 0.4815, 0.4735, 0.4770, 0.4812, 0.4662,
        0.5017, 0.4815, 0.4986, 0.4999, 0.5005, 0.4791, 0.4616, 0.4832, 0.5150,
        0.4875, 0.4798, 0.4798, 0.4798, 0.4832, 0.4784, 0.4818, 0.4871, 0.4791,
        0.4791, 0.4781, 0.4791, 0.4791, 0.4791, 0.4815, 0.4861, 0.4943, 0.4852,
        0.4796, 0.4867, 0.4852, 0.4901, 0.4796, 0.5012, 0.4835, 0.4892, 0.4536,
        0.4843, 0.4710, 0.5097, 0.4710, 0.4736, 0.4736, 0.4961, 0.4736, 0.4794,
        0.4829, 0.4830, 0.4762, 0.4728, 0.4794, 0.4762, 0.4617, 0.4965, 0.4830,
        0.4747, 0.4877, 0.4477, 0.4686, 0.4686, 0.4838, 0.4686, 0.4747, 0.5035,
        0.4781, 0.4760, 0.4825, 0.4788, 0.4795, 0.4788, 0.4788, 0.4853, 0.4879,
        0.4668, 0.4792, 0.4513, 0.4810, 0.4762, 0.4683, 0.4872, 0.4816, 0.4810,
        0.4827, 0.4872, 0.4683, 0.4954, 0.4739, 0.4827, 0.4956, 0.4668, 0.4891,
        0.4998, 0.4855, 0.4786, 0.5020, 0.4981, 0.4823, 0.4811, 0.4820, 0.5182,
        0.4890, 0.4746, 0.4727, 0.4829, 0.4727, 0.5026, 0.4756, 0.4775, 0.4775,
        0.4756, 0.4821, 0.4619, 0.4756, 0.4712, 0.4840, 0.4887, 0.4829, 0.4829,
        0.4553, 0.4819, 0.4829, 0.5212, 0.4853, 0.4819, 0.5032, 0.4813, 0.4990,
        0.4862, 0.4771, 0.4906, 0.4684, 0.4758, 0.4914, 0.4930, 0.4768, 0.5153,
        0.4829, 0.4684, 0.4684, 0.4871, 0.4982, 0.4871, 0.4940, 0.4777, 0.4829,
        0.4912, 0.4921, 0.4832, 0.4828, 0.4861, 0.4946, 0.5039, 0.4679, 0.4636,
        0.4861, 0.4691, 0.4685, 0.4819, 0.4895, 0.4817, 0.4716, 0.4972, 0.4817,
        0.4926, 0.4972, 0.5022, 0.5057, 0.4976, 0.4871, 0.4976, 0.5107, 0.4976,
        0.4976, 0.4839, 0.4976, 0.5001, 0.4898, 0.4975, 0.5006, 0.4762, 0.4751,
        0.4971, 0.4794, 0.4794, 0.4794, 0.4938, 0.4895, 0.4895, 0.4895, 0.4895,
        0.5036, 0.4895, 0.5018, 0.4895, 0.4979, 0.5104, 0.4738, 0.4796, 0.4703,
        0.4974, 0.4769, 0.4770, 0.4719, 0.4599, 0.4676, 0.4651, 0.4685, 0.4562,
        0.4809, 0.4666, 0.4686, 0.4738, 0.5035, 0.4780, 0.4806, 0.4842, 0.4745,
        0.4733, 0.4833, 0.4836, 0.4636, 0.4793, 0.4636, 0.4806, 0.4791, 0.4758,
        0.4758, 0.4758, 0.4831, 0.4956, 0.4738, 0.4799, 0.4798, 0.4758, 0.4893,
        0.4765, 0.4741, 0.4817, 0.4662, 0.4848, 0.4665, 0.4817, 0.4850, 0.4733,
        0.4728, 0.4989, 0.4797, 0.4893, 0.4867, 0.4638, 0.5068, 0.4791, 0.4444,
        0.4523, 0.4867, 0.5149, 0.4976, 0.4832, 0.4832, 0.4832, 0.4832, 0.5149,
        0.4873, 0.4873, 0.4710, 0.4827, 0.4546, 0.4969, 0.4827, 0.4970, 0.4980,
        0.4762, 0.4780, 0.4780, 0.4827, 0.5010, 0.4579, 0.4894, 0.4632, 0.4848,
        0.4763, 0.4884, 0.4669, 0.4761, 0.4761, 0.4971, 0.4786, 0.4794, 0.4925,
        0.4891, 0.4607, 0.4891, 0.4899, 0.4962, 0.4761, 0.4642, 0.4771, 0.4757,
        0.4771, 0.4574, 0.4722, 0.4771, 0.4757, 0.4757, 0.4757, 0.4667, 0.5022,
        0.4771, 0.4781, 0.4721, 0.4907, 0.4853, 0.4496, 0.4798, 0.4778, 0.4803,
        0.4850, 0.4850, 0.5024, 0.4830, 0.4751, 0.4850, 0.4793, 0.4816, 0.4850,
        0.4625, 0.4557, 0.4815, 0.4784, 0.4680, 0.4761, 0.4680, 0.4557, 0.5071,
        0.4679, 0.5010, 0.4878, 0.5014, 0.4786, 0.4827, 0.4918, 0.4823, 0.5041,
        0.4823, 0.4918, 0.4745, 0.5076, 0.4745, 0.4745, 0.4668, 0.4646, 0.4728,
        0.4785, 0.4560, 0.4949, 0.4709, 0.4695, 0.4709, 0.4700, 0.4898, 0.4822,
        0.4762, 0.4795, 0.4910, 0.4762, 0.4768, 0.4831, 0.4836, 0.4829, 0.5010,
        0.4768, 0.4768, 0.4768, 0.4606, 0.4785, 0.5153, 0.4945, 0.4647, 0.4839,
        0.4962, 0.4759, 0.4779, 0.4890, 0.4727, 0.4759, 0.4729, 0.4799, 0.4777,
        0.4904, 0.4695, 0.4777, 0.4799, 0.4942, 0.5207, 0.4961, 0.4940, 0.4678,
        0.5038, 0.4864, 0.4823, 0.4617, 0.4834, 0.4727, 0.4838, 0.4823, 0.4682,
        0.4793, 0.4910, 0.5032, 0.4936, 0.4391, 0.4696, 0.4881, 0.4801, 0.4895,
        0.4815, 0.4808, 0.4815, 0.4748, 0.4877, 0.4717, 0.4754, 0.4851, 0.4421,
        0.4851], grad_fn=<SigmoidBackward0>)
target:  tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,
        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 0., 1., 0., 0., 1., 1., 1., 0.])
loss:  tensor(0.7160, grad_fn=<MeanBackward0>) l2:  tensor(0.3449, grad_fn=<AddBackward0>) l2*coef tensor(0.0001, grad_fn=<MulBackward0>)
Epoch 2; time 0.1499
epoch 2 validating; auc: 0.5312
Model (checkpoint) saved to output/ml1m/env/ml1m_user_env_lr0.0003_reg0.0003_eval.model
